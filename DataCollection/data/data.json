[{"_id": {"$oid": "5cf424a17eb8d64d7c2f6fba"}, "repo_url": "https://github.com/Lsdefine/BERT_keras", "repo_name": "BERT_keras", "repo_full_name": "Lsdefine/BERT_keras", "repo_owner": "Lsdefine", "repo_desc": "An easy-to-use BERT in keras via tf-hub.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T00:55:08Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T07:26:04Z", "homepage": "HomePage", "size": 3733, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188183205, "is_fork": false, "readme_text": "BERT_keras An easy-to-use BERT in keras via tf-hub. Getting Started At first, set env variable TFHUB_CACHE_DIR and/or https_proxy for saving pre-train models. We must use the original tokenizer to be compatible with the pre-train weights. And the original optimizer leads to better fine-tuning results. import bert bert.set_language('en')   # or 'cn' bert_inputs = bert.get_bert_inputs(max_seq_length=128) bert_output = bert.BERTLayer(n_fine_tune_vars=3, return_sequences=False)(bert_inputs) x = Dense(1, activation='sigmoid')(bert_output) model = Model(inputs=bert_inputs, outputs=x)  X = bert.convert_sentences(sentences)   #  X = [input_ids, input_masks, segment_ids]  lr_scheduler, optimizer = bert.get_suggested_scheduler_and_optimizer(init_lr=1e-3, total_steps=total_batches) model.compile(optimizer=optimizer, loss=...) model.fit(X, Y, ..., callbacks=[lr_scheduler])  More details are in the examples. Some useful functions bert.restore_token_list is used to generate a tokenized result that can map to the original sentences. It is useful in sequence tasks. sent = '@@@ I    have 10 RTX 2080Ti.' tokens = tokenize_sentence(sent) # ['@', '@', '@', '[UNK]', 'have', '10', '[UNK]', '[UNK]', '.'] otokens = restore_token_list(sent, tokens) # ['@', '@', '@', ' I    ', 'have ', '10', ' RTX', ' 2080Ti', '.']  Running the tests Just run *.py in examples/ Acknowledgments The tokenizer and optimizer are from https://github.com/google-research/bert The baisc BERTLayer is from:  https://towardsdatascience.com/bert-in-keras-with-tensorflow-hub-76bcbc9417b https://github.com/strongio/keras-bert  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fbb"}, "repo_url": "https://github.com/MohammadFneish7/Int2FP", "repo_name": "Int2FP", "repo_full_name": "MohammadFneish7/Int2FP", "repo_owner": "MohammadFneish7", "repo_desc": "Intelligent Forex Forecasting Project, An opensource project directed for forecasting Forex using machine learning technologies ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T02:12:13Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T15:52:25Z", "homepage": "", "size": 6368, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188449750, "is_fork": false, "readme_text": "Int2FP Intelligent Forex Forecasting Project, An opensource project directed for forecasting Forex using machine learning technologies. Based on the phrase you always hear whenever you start learning about Forex trading, that says \"History repeats itself\", I decided to start this Forex forecasting project based on machine learning. Why is that? simply, to start solving any problem using machine learning, three simple conditions must exist together:  You should have data (a lot of it) Your problem couldn't be solved using Math (since math is the best and 100% accurate) Your data should have patterns  So, by returning to the Forex forecasting problem:  We have tons of data Forecasting couldn't be solved mathematically Here is our problem, there are thousands of factors that affect the Forex prices, where we couldn't study all of these factors to predict the next value of the trading pairs. We really don't know if Forex data has patterns or not. \"History repeats itself\" means that it has, but this is not proved by anyone yet.  Then, in this project, we will assume that Forex data has patterns. For now, this project has only LSTM Neural Network application using Keras-Tensorflow (written in Python), so I hope contributors to focus on improving this branch, although all contribution is welcomed. This application is applied on USDJPY pair data history from 5/MAY/2003 to 31/DEC/2018. For more information about the structure of LSTM's used in Keras and how to manipulate their input, please check my git repository Keras_LSTM_Diagram. Requirements  Python 3.6 tensorflow pandas matplotlib sklearn numpy pathlib ta  Dataset Current LSTM is using USDJPY_Candlestick_15_M_BID_05.05.2003-31.12.2018.csv, obtained from Dukascopy. This dataset represents the M15 period prices history (one record every 15 minutes) of USDJPY pair between 5/MAY/2003 and 31/DEC/2018. Data preview:             Local time        Open        High         Low       Close       Volume 0  05.05.2003 00:00:00  118.940002  119.015999  118.926003  118.950996  3468.199951 1  05.05.2003 00:15:00  118.946999  118.974998  118.932999  118.948997  3108.899902 2  05.05.2003 00:30:00  118.954002  118.974998  118.892998  118.959000  3083.300049 3  05.05.2003 00:45:00  118.966003  118.990997  118.922997  118.975998  2914.600098 4  05.05.2003 01:00:00  118.992996  119.000000  118.967003  118.985001  3088.800049 5  05.05.2003 01:15:00  118.987999  119.001999  118.953003  118.987999  3175.399902 6  05.05.2003 01:30:00  118.987000  119.023003  118.973000  118.984001  3237.600098 7  05.05.2003 01:45:00  118.989998  119.002998  118.973999  118.995003  2998.899902 8  05.05.2003 02:00:00  118.990997  119.039001  118.990997  118.995003  2742.199951 9  05.05.2003 02:15:00  118.999001  119.032997  118.973000  119.008003  3151.699951  Note: you can use any dataset for any Forex pair or time frame, but it must keep the same structure as above. Input/Output Options  Input Options:  Current LSTM branch supports controlling the number of input features as well as the shape of these inputs to match both stateful and stateless Keras-LSTM input modes (for more information about stateful & stateless input options please check: Keras RNN documentations, Keras_LSTM_Diagram), this could be done by setting the stateful hyperparameter to True or False respectively in the lstm_regression.py. Besides, you can also control the number of input features to be as one of the following:   5 Features done by setting add_taforex_features = False this will use the basic Forex variables as input. The resulting features are: ['Open', 'High', 'Low', 'Close', 'Volume'].   64 Features done by setting add_taforex_features = True this will use the basic Forex variables in addition to 59 Forex indicator calculated using the ta - Forex Technical Analysis Library, all togather will represent the LSTM input. The resulting features are: ['Open', 'High', 'Low', 'Close', 'Volume', 'volume_adi', 'volume_obv', 'volume_obvm', 'volume_cmf', 'volume_fi', 'volume_em', 'volume_vpt', 'volume_nvi', 'volatility_atr', 'volatility_bbh', 'volatility_bbl', 'volatility_bbm', 'volatility_bbhi', 'volatility_bbli', 'volatility_kcc', 'volatility_kch', 'volatility_kcl', 'volatility_kchi', 'volatility_kcli', 'volatility_dch', 'volatility_dcl', 'volatility_dchi', 'volatility_dcli', 'trend_macd', 'trend_macd_signal', 'trend_macd_diff', 'trend_ema_fast', 'trend_ema_slow', 'trend_adx', 'trend_adx_pos', 'trend_adx_neg', 'trend_vortex_ind_pos', 'trend_vortex_ind_neg', 'trend_vortex_diff', 'trend_trix', 'trend_mass_index', 'trend_cci', 'trend_dpo', 'trend_kst', 'trend_kst_sig', 'trend_kst_diff', 'trend_ichimoku_a', 'trend_ichimoku_b', 'trend_visual_ichimoku_a', 'trend_visual_ichimoku_b', 'trend_aroon_up', 'trend_aroon_down', 'trend_aroon_ind', 'momentum_rsi', 'momentum_mfi', 'momentum_tsi', 'momentum_uo', 'momentum_stoch', 'momentum_stoch_signal', 'momentum_wr', 'momentum_ao', 'others_dr', 'others_dlr', 'others_cr'].    Output Options:  Concerning the output or the labels, we have two basic assumptions here, wherein both we will be solving a single label regression problem. In the first one, we will be trying to forecast the next Close price value of the Forex pair under study based on a number of look-back time steps in the input features (this look-back value is controlled using the time_steps hyperparameter). In the second assumption, we will be also looking back for a pre-defined time_steps value. However, this time instead of predicting the next Close value, we will be predicting a value between -1 and +1, that represents the angle of the regression line that defines a number of future Close points, starting from the current Close value. This could be controlled by setting the look_farword hyperparameter (default value = 50). This value is calculated as: aTan(regr.slope) * 180 / PI / 9 This numerical value will show the power of the up or down trend in the next number of candles (if there is a trend). It was found that a value of greater than +/-0.8 represents a powerful up/down trend respectively as shown in the below figures.  Running LSTM You can simply run the model by executing lstm_regression.py through: Python ./lstm/lstm_regression.py However, there are a lot of hyper-parameters that you must take care of before that. These parameters are located in the header of 'lstm_regression.py' file so you must edit it manually since it still doesn't yet support passing them as arguments. These parameters and their default values are: data_path = 'data/USDJPY_Candlestick_15_M_BID_05.05.2003-31.12.2018.csv'   # path of the dataset checkpoint_weights_h5_path = 'checkpoints/LSTM_00000014_20190429_202523.h5'  # checkpoint file to wich model weights and structure would be save to or loaded from resume_from_checkpoint = False   # this specifies if it should start a new model or resume from the above checkpoint checkpoint_autosave_period = 1   # the number of epochs that should be passed everytime before saving the model state  time_steps = 100   # refere to input options above, actually this is related to Keras specifications of the input shape of the data (check Keras docs for more information) look_farword = 50  # refere to output options above, it represents the number of future candelsticks which would be used to calculate the trend normalized angle value num_of_classes = 1   # number of classes or labels, here always equals one num_of_epochs = 100  # number of training epochs stateful = True  # specifies how the input is formed fow the network, also refere to the input options above and to the Keras LSTM docs for more information add_taforex_features = True  # if true you will get 64 input feature as specified in the input options, else you will get 5 debug_future_regression_angle = False  # this switches between next Close value prediction and next trend angle prediction model_batch_size = 100   # simply the batch size, this is a Keras LSTM related parameter pip_decimal_place = 3  # this specifies how many decimal places to read from each input feature value LSTM_output_units = 32   # number of output units for each LSTM layer num_of_hidden_layers = 0   # this parameter enables stacking LSTM layers togather drop_out_value = 0.3   # value of the drop-out regulizer label_type = 0   # if zero = 'next_close_price' else if one = 'future_regression_angle' number_of_features = -1  # this represents the number of input features currently 5 or 64, this would be automatically detected later at runtime so keep its default value  A typical output after a complete run should look like this. Using GPU This project supports both CPU/GPU processing, although it is not advised to use CPU Since LSTM training is too slow. So To use your GPU:  Install CUDA for using in tensorflow (follow this installation guide) Install tensorflow-gpu (Note: Current tensorflow-gpu version was making problems for me, I solved that by installing tf-nightly-gpu v 2.0.0-dev20190327)  Results For the current time, results are not so promising, the model is still failing to predict the next Close as well as the future trend angle. Maybe the real problem is in the model, and maybe Forex data simply has no obvious pattern to be detected and learned by the model. Contributions are needed, so please do if you have the potential. License This project is licensed under GNU General Public License v3. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_Final_20190430_181609.h5", "https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_Final_20190430_190651.h5", "https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_stateful00000001_20190430_181609.h5", "https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_stateful00000001_20190430_190651.h5", "https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_stateful00000002_20190430_181609.h5", "https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_stateful00000002_20190430_190651.h5", "https://github.com/MohammadFneish7/Int2FP/blob/32d07bd6d09211298fe41604ee0451f8f8d4e251/lstm/checkpoints/LSTM_stateful00000003_20190430_190651.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fbc"}, "repo_url": "https://github.com/wowboa/yolov3", "repo_name": "yolov3", "repo_full_name": "wowboa/yolov3", "repo_owner": "wowboa", "repo_desc": "keras", "description_language": "Malay (macrolanguage)", "repo_ext_links": null, "repo_last_mod": "2019-05-23T08:47:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T08:39:57Z", "homepage": null, "size": 245, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188194669, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fbd"}, "repo_url": "https://github.com/barseghyanartur/seefar10", "repo_name": "seefar10", "repo_full_name": "barseghyanartur/seefar10", "repo_owner": "barseghyanartur", "repo_desc": "Building CIFAR10 with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T22:14:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T22:57:22Z", "homepage": null, "size": 666956, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188317628, "is_fork": false, "readme_text": "SeeFar10 No, this is not a mistype. Original CIFAR 10 categories have been retained:  0: airplane 1: automobile 2: bird 3: cat 4: deer 5: dog 6: frog 7: horse 8: ship 9: truck   Usage  Train python ./train.py --dataset datasets --model seefar10.model --labelbin seefar10.pickle  Classify  Command line Plain python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/airplane01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/automobile01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/bird01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/cat01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/deer01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/dog01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/frog01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --image examples/horse01.jpg Use OpenCV for image output python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/airplane01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/automobile01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/bird01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/cat01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/deer01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/dog01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/frog01.jpg python classify.py --model seefar10.model --labelbin seefar10.pickle --use-opencv --image examples/horse01.jpg  Web Run the web server first python web.py For all categories http://127.0.0.1:5042/list-predict-sample-images/  For a single category http://127.0.0.1:5042/list-predict-sample-images/?category=airplane http://127.0.0.1:5042/list-predict-sample-images/?category=automobile http://127.0.0.1:5042/list-predict-sample-images/?category=bird http://127.0.0.1:5042/list-predict-sample-images/?category=cat http://127.0.0.1:5042/list-predict-sample-images/?category=deer http://127.0.0.1:5042/list-predict-sample-images/?category=dog http://127.0.0.1:5042/list-predict-sample-images/?category=frog http://127.0.0.1:5042/list-predict-sample-images/?category=horse http://127.0.0.1:5042/list-predict-sample-images/?category=ship http://127.0.0.1:5042/list-predict-sample-images/?category=truck  For a single image http://127.0.0.1:5042/api/predict-sample-image/?image=examples/airplane01.jpg   Useful See GPU usage nvidia-smi ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "cifar-10", "machine-learning", "keras-tensorflow"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fbe"}, "repo_url": "https://github.com/bsvineethiitg/adams", "repo_name": "adams", "repo_full_name": "bsvineethiitg/adams", "repo_owner": "bsvineethiitg", "repo_desc": "Exploiting Uncertainty of Loss Landscape for Stochastic Optimization", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:03:53Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T22:15:58Z", "homepage": "https://arxiv.org/abs/1905.13200", "size": 1661, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188617104, "is_fork": false, "readme_text": "Exploiting Uncertainty of Loss Landscape for Stochastic Optimization Paper: http://arxiv.org/abs/1905.13200 Cite as: V.S. Bhaskara, and S. Desai. arXiv preprint arXiv:1905.13200 [cs.LG] (2019). Algorithm We introduce variants of the Adam optimizer that either bias the updates along regions that conform across mini-batches or randomly \"explore\" in the parameter space along the variance-gradient. The update rules are summarized below:  AdamUCB and AdamCB are biased estimates of the full-gradient. We recommend using AdamS which is an unbiased estimate, and outperforms other variants based on our experiments with CIFAR-10. Please refer to the paper for more details. Code PyTorch implementation of the optimizers is available under PyTorch-Optimizers/ Usage Each of our optimizer requires access to the current loss value. This is acheived by passing in a closure function  to the optimizer.step() method. The function closure() should be defined to return the current loss tensor after the forward pass. Refer to lines 351-357 in Experiments/main.py for an example of the usage. Experiments We evaluate the optimizers on multiple models such as Logistic Regression (LR), MLPs, and CNNs on the CIFAR-10/MNIST datasets. The architecture of the networks is chosen to closely resemble the experiments published in the original Adam paper (Kingma and Ba, 2015). Code for our experiments is available under Experiments/, and is based on the original CIFAR-10 classifier code here. Reproducing the results  Run the shell script for each type of model (LR/MLP/CNN) under Experiments/ Compute the Mean and the Standard Deviation of the training/validation metrics for each configuration across the three runs.  Results of our training runs with the mean and the standard deviation values for each configuration is provided under Experiments/results_mean_std/. Results CNN trained on CIFAR-10 with batch size = 128 and no dropout  CNN trained on CIFAR-10 with batch size = 16 and no dropout  Comparison of Dropout with AdamS for CNN trained on CIFAR-10 with batch size = 128  Please refer to the paper for more details. Contribute Feel free to create a pull request if you find any bugs or you want to contribute (e.g., more datasets, more network structures, or tensorflow/keras ports). ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["http://arxiv.org/abs/1905.13200", "http://arxiv.org/abs/1905.13200", "https://arxiv.org/abs/1412.6980", "http://arxiv.org/abs/1905.13200"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fbf"}, "repo_url": "https://github.com/charlesmsiegel/attentionkeras", "repo_name": "attentionkeras", "repo_full_name": "charlesmsiegel/attentionkeras", "repo_owner": "charlesmsiegel", "repo_desc": "Implementation of various attention schema in Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T05:29:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T04:02:31Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188159186, "is_fork": false, "readme_text": "attentionkeras This is a library of attention based layers for Keras. The goal is to create Keras-style layers for several different attention mechanisms that have appeared in the literature. Installation $ git clone https://github.com/charlesmsiegel/attentionkeras.git $ cd attentionkeras $ python setup Requirements This library requires Keras, and is created with Keras version 2.2.4 License This project uses the Apache 2.0 License ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc0"}, "repo_url": "https://github.com/codificandobits/Generacion_de_rostros_artificiales_usando_GANs", "repo_name": "Generacion_de_rostros_artificiales_usando_GANs", "repo_full_name": "codificandobits/Generacion_de_rostros_artificiales_usando_GANs", "repo_owner": "codificandobits", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T21:18:19Z", "repo_watch": 2, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-25T14:54:17Z", "homepage": null, "size": 87846, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188578870, "is_fork": false, "readme_text": "Generaci\u00f3n de rostros artificiales usando redes adversarias C\u00f3digo fuente de este video, en donde se muestra c\u00f3mo utilizar las Generative Adversarial Networks (GAN) para generar im\u00e1genes artificiales de rostros humanos. Contenido  dataset: carpeta con el set de datos utilizado durante el entrenamiento. Contiene 3,755 im\u00e1genes a color de rostros humanos reales, cada una con un tama\u00f1o de 128x128. ejemplos: carpeta con 100 ejemplos de im\u00e1genes obtenidas tras el entrenamiento de la GAN. utilidades.py: funciones para la lectura del set de entrenamiento, la visualizaci\u00f3n de las im\u00e1genes obtenidas y la generaci\u00f3n de im\u00e1genes con el modelo entrenado. generacion_de_rostros.py: implementaci\u00f3n de la GAN para la generaci\u00f3n de rostros artificiales  Lecturas recomendadas  Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., ... & Bengio, Y. (2014). Generative adversarial nets. In Advances in neural information processing systems (pp. 2672-2680). Radford, A., Metz, L., & Chintala, S. (2015). Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.  Dependencias matplotlib==2.0.0 numpy==1.15.4 Keras==2.2.4 imageio==2.5.0 ", "has_readme": true, "readme_language": "Spanish", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc1"}, "repo_url": "https://github.com/amslabtech/object_detection", "repo_name": "object_detection", "repo_full_name": "amslabtech/object_detection", "repo_owner": "amslabtech", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:29:17Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T06:05:53Z", "homepage": null, "size": 1572, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 4, "github_id": 188171494, "is_fork": false, "readme_text": "object-detection How to Use Prepare package InstallCUDA git clone https://github.com/qqwweee/keras-yolo3 ln -s keras-yolo3/yolo3 . pip3 install -r requirements.txt  git clone https://github.com/matterport/Mask_RCNN.git ln -s Mask_RCNN/mrcnn . pip3 install -r Mask_RCNN/requirements.txt  git clone https://github.com/see--/keras-centernet.git ln -s keras-centernet/keras_centernet .  Prepare Model wget https://pjreddie.com/media/files/yolov3.weights python3 keras-yolo3/convert.py yolov3.cfg yolov3.weights model_data/yolo3/coco/yolo.h5  wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5 -P model_data/mrcnn/coco/  wget https://github.com/see--/keras-centernet/releases/download/0.1.0/ctdet_coco_hg.hdf5 -P model_data/keras-centernet/coco/  Run python3 test_video.py --model=model_data/yolo3/coco/yolo.h5 --anchors=model_data/yolo3/coco/anchors.txt --classes=model_data/yolo3/coco/classes.txt python3 test_video.py --model=model_data/yolo3/coco/yolo.h5 --anchors=model_data/yolo3/coco/anchors.txt --classes=model_data/yolo3/coco/classes.txt --image Input image filename:images/pics/dog.jpg  Run glob pattern python3 test_image.py --model=model_data/yolo3/coco/yolo.h5 --anchors=model_data/yolo3/coco/anchors.txt --classes=model_data/yolo3/coco/classes.txt -i=images/pics/*jpg  python3 test_image.py --model=model_data/mrcnn/coco/mask_rcnn_coco.h5 --classes=model_data/mrcnn/coco/classes.txt -i=images/pics/eagle.jpg -n=mrcnn  python3 test_image.py --model=model_data/keras-centernet/coco/ctdet_coco_hg.hdf5 --classes=model_data/keras-centernet/coco/classes.txt -i=images/pics/*.jpg -n=keras-centernet  Prepare Dataset mkdir mscoco2017 cd mscoco2017/ wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip unzip annotations_trainval2017.zip wget http://images.cocodataset.org/zips/train2017.zip unzip train2017.zip cd ..  python3 keras-yolo3/coco_annotation.py mv train.txt data_labels/bbox/coco/  less data_labels/bbox/coco/train.txt mscoco2017/train2017/000000558840.jpg 199,200,276,270,52 325,104,358,209,39 168,90,199,178,39 1,87,35,262,41 346,1,638,344,0 239,42,258,118,39 409,215,480,265,44 0,1,93,161,0 276,13,307,74,39 3,263,362,419,60 413,201,485,257,44 mscoco2017/train2017/000000200365.jpg 234,317,383,355,52 239,347,399,404,52 296,388,297,388,52 251,333,376,355,52 128,192,639,473,60 0,36,562,479,1 131,0,639,248,2 1,1,131,58,2 463,202,562,372,41 mscoco2017/train2017/000000495357.jpg 337,244,403,310,16 255,257,436,370,3 509,215,556,239,26 22,206,43,229,26 354,162,384,211,26 481,107,600,351,0 445,196,502,271,0 390,193,464,280,0 381,127,414,254,0 356,143,392,268,0 233,148,271,221,0 197,142,238,284,0 15,133,65,311,0 100,149,148,303,0 109,167,156,239,0 591,121,620,204,0 186,147,214,248,0 241,197,274,248,0 277,122,575,332,0 filepath x1,y1,x2,y2,class_id x1,y1,x2,y2,class_id....   python3 coco_json_to_mrcnn_txt.py less data_labels/polygon/coco/train_list.txt mscoco2017/train2017/000000558840.jpg [[[239,260,222,270,199,253,213,227,259,200,274,202,277,210,249,253,237,264,242,261,228,271]],53] [[[357,210,338,209,327,204,325,164,329,127,326,108,333,104,348,104,358,108,358,130]],40] [[[x1,y1,x2,y2,x3,y3,...][x1,y1,x2,y2,x3,y3,...][...]...],class_id]].... cd data_labels/polygon/coco/ head train_list.txt -n 100 >  train_list_100.txt  Train Model python3 scripts/data_split.py -a=data_labels/bbox/coco/train.txt -n=10 -c=model_data/yolo3/coco/classes.txt python3 train_yolo.py -m=model_data/yolo3/coco/yolo.h5 -t=data_labels/bbox/coco_000/train.txt -c=model_data/yolo3/coco_000/classes.txt  python3 train_mrcnn.py train -a=data_labels/polygon/coco/train_list_100.txt  -w=model_data/mrcnn/coco/mask_rcnn_coco.h5  Evaluate Model python3 test_image.py -t=data_labels/bbox/coco_000/test.txt  --anchors=model_data/yolo3/coco/anchors.txt --classes=model_data/yolo3/coco/classes.txt -m=model_data/yolo3/coco/yolo.h5 python3 scripts/compute_mAP_IoU.py results/yolo/coco_000/yolo.h5_000/ data_labels/bbox/coco_000/ground-truth/ cat results/yolo/coco_000/yolo.h5_000/mAP_IOU/results.txt | grep _25  python3 test_image.py -t=data_labels/bbox/coco_000/test.txt --model=model_data/keras-centernet/coco/ctdet_coco_hg.hdf5 --classes=model_data/keras-centernet/coco/classes.txt -n=keras-centernet python3 scripts/compute_mAP_IoU.py results/keras-centernet/coco_000/ctdet_coco_hg.hdf5_001/ data_labels/bbox/coco_000/ground-truth/ cat results/keras-centernet/coco_000/ctdet_coco_hg.hdf5_001//mAP_IOU/results.txt | grep _25  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc2"}, "repo_url": "https://github.com/yeLer/UNet", "repo_name": "UNet", "repo_full_name": "yeLer/UNet", "repo_owner": "yeLer", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T07:39:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T07:18:37Z", "homepage": null, "size": 28454, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188372751, "is_fork": false, "readme_text": "Implementation of deep learning framework -- Unet, using Keras The architecture was inspired by U-Net: Convolutional Networks for Biomedical Image Segmentation.  Overview Data--on medical image The original dataset is from isbi challenge, and I've downloaded it and done the pre-processing. You can find it in folder data/membrane.  train  image: the train images folder which include 30 images label: the ground truth folder of the train images   test  images: the test images folder need to be test by the model predicts: the predicts result folder used to save the result    Data augmentation The data for training contains 30 512*512 images, which are far not enough to feed a deep learning neural network. I use a module called ImageDataGenerator in keras.preprocessing.image to do data augmentation. See data.py for detail,and used the data_test.py to generate the aug results which will be saved in the data/membrance/tarin/aug. Model  This deep neural network is implemented with Keras functional API, which makes it extremely easy to experiment with different interesting architectures. Output from the network is a 512*512 which represents mask that should be learned. Sigmoid activation function makes sure that mask pixels are in [0, 1] range. Training The model is trained for 5 epochs. After 5 epochs, calculated accuracy is about 0.97. Loss function for the training is basically just a binary_crossentropy.  How to use Dependencies This tutorial depends on the following libraries:  Tensorflow Keras >= 1.0  Also, this code should be compatible with Python versions 2.7-3.6. Run main.py You will see the predicted results of test image in data/membrane/test/predicts Results Use the trained model to do segmentation on test images, the result is statisfactory.   About Keras Keras is a minimalist, highly modular neural networks library, written in Python and capable of running on top of either TensorFlow or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that: allows for easy and fast prototyping (through total modularity, minimalism, and extensibility). supports both convolutional networks and recurrent networks, as well as combinations of the two. supports arbitrary connectivity schemes (including multi-input and multi-output training). runs seamlessly on CPU and GPU. Read the documentation Keras.io Keras is compatible with: Python 2.7-3.6. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/", "http://keras.io/", "http://brainiac2.mit.edu/isbi_challenge/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc3"}, "repo_url": "https://github.com/arminsadreddin/NN_classification", "repo_name": "NN_classification", "repo_full_name": "arminsadreddin/NN_classification", "repo_owner": "arminsadreddin", "repo_desc": "training mnist dataset using keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T13:32:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T13:31:34Z", "homepage": null, "size": 3664, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188240202, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc4"}, "repo_url": "https://github.com/neggplant/CNN_Keras", "repo_name": "CNN_Keras", "repo_full_name": "neggplant/CNN_Keras", "repo_owner": "neggplant", "repo_desc": "convolutional neural networks using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T07:06:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T07:59:02Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188188481, "is_fork": false, "readme_text": "CNN_Keras convolutional neural networks using Keras  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc5"}, "repo_url": "https://github.com/UV1999/Cat-Dog-Classifier-Keras", "repo_name": "Cat-Dog-Classifier-Keras", "repo_full_name": "UV1999/Cat-Dog-Classifier-Keras", "repo_owner": "UV1999", "repo_desc": "This is my repository for Cat and Dog classifier using Keras and Tensorflow.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T05:01:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T04:35:01Z", "homepage": null, "size": 16533, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188352685, "is_fork": false, "readme_text": "Cat-Dog-Classifier-Keras This is my repository for Cat and Dog classifier using Keras and Tensorflow. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/UV1999/Cat-Dog-Classifier-Keras/blob/85cf8695ba5ca13d7efdf70db453a21f6fefb563/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc6"}, "repo_url": "https://github.com/tonandr/keras_unsupervised", "repo_name": "keras_unsupervised", "repo_full_name": "tonandr/keras_unsupervised", "repo_owner": "tonandr", "repo_desc": "Keras framework for unsupervised learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T12:32:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T12:30:47Z", "homepage": null, "size": 154, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188418463, "is_fork": false, "readme_text": "keras_unsupervised Keras framework based unsupervised learning framework. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc7"}, "repo_url": "https://github.com/LucaCappelletti94/extra_keras_utils", "repo_name": "extra_keras_utils", "repo_full_name": "LucaCappelletti94/extra_keras_utils", "repo_owner": "LucaCappelletti94", "repo_desc": "Python package collecting commonly used snippets for the Keras library.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T18:11:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T12:22:39Z", "homepage": null, "size": 28, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188562959, "is_fork": false, "readme_text": "extra_keras_utils         Python package collecting commonly used snippets for the Keras library.  How do I install this package? As usual, just download it using pip: pip install extra_keras_utils  Tests Coverage Since some software handling coverages sometime get slightly different results, here's three of them:     is_gpu_available Method that returns a boolean if a GPU is detected or not. from extra_keras_utils import is_gpu_available  if is_gpu_available():     print(\"Using gpu!\")  set_seed Method to get reproducible results. from extra_keras_utils import set_seed  set_seed(42) # set as seed 42, the results are nearly reproducible. set_seed(42, kill_parallelism=true) # set as seed 42, the results are fully reproducible. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc8"}, "repo_url": "https://github.com/ayush-raizada7/IMDB-Review-Analysis", "repo_name": "IMDB-Review-Analysis", "repo_full_name": "ayush-raizada7/IMDB-Review-Analysis", "repo_owner": "ayush-raizada7", "repo_desc": "Sentiment Analysis of movie reviews using different datasets of IMDB movie reviews", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T07:18:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T07:09:28Z", "homepage": null, "size": 212, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188533022, "is_fork": false, "readme_text": "IMDB-Review-Analysis Sentiment Analysis of movie reviews using different datasets of IMDB movie reviews Model - Sequential Neural Network - Bidirectional LSTM Text Preprocessing - NLTK, WordNet, StopWords Text encoding - Tokenizer in keras Libraries required - TensorFlow, Keras, NLTK Datasets can be Downloaded from- https://drive.google.com/open?id=1YOFFGgkodUbJeJ46D2wOWvQu5ZaaLpEw ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fc9"}, "repo_url": "https://github.com/Sri-krishna98/IMDB-BRNN", "repo_name": "IMDB-BRNN", "repo_full_name": "Sri-krishna98/IMDB-BRNN", "repo_owner": "Sri-krishna98", "repo_desc": "Sentiment Analysis on Movie Reviews using Bidirectional Recurrent Neural Networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T07:21:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T06:24:11Z", "homepage": "", "size": 211, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188528936, "is_fork": false, "readme_text": "IMDB-BRNN Sentiment Analysis of movie reviews using different datasets of IMDB movie reviews Model - Sequential Neural Network - Bidirectional LSTM Text Preprocessing - NLTK, WordNet, StopWords Text encoding - Tokenizer in keras Libraries required - TensorFlow, Keras, NLTK Datasets can be Downloaded from- https://drive.google.com/open?id=1YOFFGgkodUbJeJ46D2wOWvQu5ZaaLpEw ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fca"}, "repo_url": "https://github.com/shaoyiwork/Keras-cat-dog", "repo_name": "Keras-cat-dog", "repo_full_name": "shaoyiwork/Keras-cat-dog", "repo_owner": "shaoyiwork", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T09:38:17Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T09:25:49Z", "homepage": null, "size": 52466, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188393192, "is_fork": false, "readme_text": "Keras-cat-dog \u8fd9\u662f\u4e00\u4e2a\u4f7f\u7528keras\u6846\u67b6\u7684CNN\u7f51\u7edc\u8bad\u7ec3\u7684\u4e00\u4e2a\u732b\u72d7\u4e8c\u5206\u7c7b\u7ecf\u5178\u6848\u4f8b\u3002\u53ef\u4ee5\u901a\u8fc7\u8c03\u53c2\u6765\u5b66\u4e60\u6a21\u578b\u53c2\u6570\u7684\u8c03\u4f18\u65b9\u6cd5\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shaoyiwork/Keras-cat-dog/blob/c0ce54cef97dfa882029beb3f3e4cea0ced11421/cat_dog_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fcb"}, "repo_url": "https://github.com/zardream/Autocar_RaspberryPi_Keras-", "repo_name": "Autocar_RaspberryPi_Keras-", "repo_full_name": "zardream/Autocar_RaspberryPi_Keras-", "repo_owner": "zardream", "repo_desc": "create an autocar using Raspberry Pi with keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T13:59:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T06:33:15Z", "homepage": null, "size": 19, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188365887, "is_fork": false, "readme_text": "Autocar_RaspberryPi_Keras- create an autocar using Raspberry Pi with keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fcc"}, "repo_url": "https://github.com/LucaCappelletti94/extra_keras_metrics", "repo_name": "extra_keras_metrics", "repo_full_name": "LucaCappelletti94/extra_keras_metrics", "repo_owner": "LucaCappelletti94", "repo_desc": "Additional training metrics integrated with the keras NN library.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T17:40:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T05:14:19Z", "homepage": null, "size": 77, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188523640, "is_fork": false, "readme_text": "extra_keras_metrics         Additional metrics integrated with the keras NN library, taken directly from Tensorflow  How do I install this package? As usual, just download it using pip: pip install extra_keras_metrics  Tests Coverage Since some software handling coverages sometime get slightly different results, here's three of them:     How do I use this package? Just by importing it you will be able to access all the non-parametric metrics, such as \"auprc\" and \"auroc\": import extra_keras_metrics  model = my_keras_model() model.compile(     optimizer=\"sgd\",     loss=\"binary_crossentropy\",     metrics=[\"auroc\", \"auprc\"] ) For the parametric metrics, such as \"average_precision_at_k\", you will need to import them, such as: from extra_keras_metrics import average_precision_at_k  model = my_keras_model() model.compile(     optimizer=\"sgd\",     loss=\"binary_crossentropy\",     metrics=[average_precision_at_k(1), average_precision_at_k(2)] ) This way in the history of the model you will find both the metrics indexed as \"average_precision_at_k_1\" and \"average_precision_at_k_2\" respectively.  Which metrics do I get? You will get all the metrics from Tensorflow. At the time of writing, the ones available are the following: The non-parametric ones are:  auprc auroc false_negatives false_positives mean_absolute_error mean_squared_error precision recall root_mean_squared_error true_negatives true_positives  The parametric ones are:  average_precision_at_k false_negatives_at_thresholds false_positives_at_thresholds mean_cosine_distance mean_iou mean_per_class_accuracy mean_relative_error precision_at_k precision_at_thresholds recall_at_k recall_at_thresholds sensitivity_at_specificity specificity_at_sensitivity true_negatives_at_thresholds true_positives_at_thresholds  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fcd"}, "repo_url": "https://github.com/cirick/batching", "repo_name": "batching", "repo_full_name": "cirick/batching", "repo_owner": "cirick", "repo_desc": "Generate training batches for large scale sequence model training", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T03:09:58Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T16:39:25Z", "homepage": null, "size": 56, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188271759, "is_fork": false, "readme_text": "Batching Batching is a set of tools to format data for training sequence models.   Installation $ pip install batching Example usage Example script exists in sample.py # Metadata for batch info - including batch IDs and mappings to storage resouces like filenames storage_meta = StorageMeta(validation_split=0.2)  # Storage for batch data - Memory, Files, S3 storage = BatchStorageMemory(storage_meta)  # Create batches - configuration contains feature names, windowing config, timeseries spacing batch_generator = Builder(storage,                            feature_set,                            look_back,                            look_forward,                            batch_seconds,                            batch_size=128) batch_generator.generate_and_save_batches(list_of_dataframes)  # Generator for feeding batches to training - tf.keras.model.fit_generator train_generator = BatchGenerator(storage) validation_generator = BatchGenerator(storage, is_validation=True)  model = tf.keras.Sequential() model.add(tf.keras.layers.Dense(1, activation='sigmoid') model.compile(loss=tf.keras.losses.binary_crossentropy,                optimizer=tf.keras.optimizers.Adam(),                metrics=['accuracy']) model.fit_generator(train_generator,                     validation_data=validation_generator,                     epochs=epochs) License   MIT license Copyright 2015 \u00a9 FVCproductions.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://opensource.org/licenses/mit-license.php", "http://fvcproductions.com", "http://badges.mit-license.org"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fce"}, "repo_url": "https://github.com/amaanabbasi/XTB-keras", "repo_name": "XTB-keras", "repo_full_name": "amaanabbasi/XTB-keras", "repo_owner": "amaanabbasi", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T23:53:56Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T01:39:55Z", "homepage": null, "size": 201761, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188507576, "is_fork": false, "readme_text": "XTB-keras Identyfying TB from lung X-rays using CNNs. Navigation   model-train.ipynb: It contains the code for the training part of the model with the various parameters used in the model. This is the entry point where you can start training your own models and experiment with different parameters.   predict.ipynb: Code to make predictions on X-rays(images). A utility function to test the model ability to predict correctly.   web-app: Contains code for the web-application, A prototype to give a overview on what the final application might look like.   Requirements   Python(3+)   keras   Flask(to run web-app locally)   The web application is live at ........   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/amaanabbasi/XTB-keras/blob/ede4a6fa86e567b20a456f96ac55de1b247e8678/web-app/Image-Classifier-master/VGG16-10-0.0001-adam.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fcf"}, "repo_url": "https://github.com/vohoaiviet/lstm-mnist", "repo_name": "lstm-mnist", "repo_full_name": "vohoaiviet/lstm-mnist", "repo_owner": "vohoaiviet", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T07:33:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T07:23:46Z", "homepage": null, "size": 11926, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188373520, "is_fork": false, "readme_text": "lstm-mnist Image classification using a RNN classifier(LSTM) with Keras. Requirements  python3 tensorflow (>=1.4) keras (>=2.1.1) numpy  Train and evaluate The classifier is trained on 55k samples and tested on 10k samples (The default split). The ANN is made of one LSTM layer with 128 hidden units and one dense output layer of 10 units with softmax activation. The rmsprop optimizer is used with categorial_crossentropy as loss function. Launch lstm_classifier.py to train and evaluate the classifier, you can dump a trained classifier and load it later. python lstm_classifier.py Ressources  Long Short Term Memory Understanding LSTMs MNIST Dataset  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/vohoaiviet/lstm-mnist/blob/ef6e2a095010ee63266b390f463143ad603944c6/saved_model/lstm-model.h5"], "see_also_links": ["http://deeplearning.cs.cmu.edu/pdfs/Hochreiter97_lstm.pdf", "http://yann.lecun.com/exdb/mnist/", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd0"}, "repo_url": "https://github.com/luanshiyinyang/MSCNN", "repo_name": "MSCNN", "repo_full_name": "luanshiyinyang/MSCNN", "repo_owner": "luanshiyinyang", "repo_desc": "keras\u590d\u73b0\u8bba\u6587\"Multi-scale Convolution Neural Networks for Crowd Counting\"", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:52:49Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T03:04:05Z", "homepage": null, "size": 1112, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188151814, "is_fork": false, "readme_text": "Multi-scale Convolution Neural Networks for Crowd Counting   \u9879\u76ee\u7b80\u4ecb  \u590d\u73b0\u8bba\u6587Multi-scale Convolution Neural Networks for Crowd Counting\u3002 \u76ee\u524d\uff0c\u6ca1\u6709\u5177\u4f53\u5199\u7684\u6bd4\u8f83\u5b8c\u5584\u7684\u57fa\u4e8eKeras\u7684\u590d\u73b0\uff08Keras\u6bd4\u8f83\u5bb9\u6613\u4e0a\u624b\uff0c\u4ee3\u7801\u7406\u89e3\u5bb9\u6613\uff09\uff0c\u8fd9\u5bf9\u4e8e\u8fc5\u901f\u6210\u578b\u7cfb\u7edf\u7684\u6784\u5efa\u4e0d\u592a\u65b9\u4fbf\u3002 \u672c\u9879\u76ee\u57fa\u4e8eKeras\uff08Tensorflow\u540e\u7aef\uff09\uff0c\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\uff08\u76ee\u524dKeras\u5b9e\u73b0\u5747\u53ea\u5728\u5355\u6570\u636e\u96c6\u4e0a\u590d\u73b0\uff0c\u8fd9\u4e0e\u8bba\u6587\u6548\u679c\u4e0d\u592a\u4e00\u81f4\uff09\u8fdb\u884c\u8bad\u7ec3\u6d4b\u8bd5\uff0c\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u5f3a\u3002    \u6570\u636e\u96c6\u4e0b\u8f7d  ShanghaiTech Dataset  \u4e0b\u8f7d\u5730\u5740   Mall Dataset  \u4e0b\u8f7d\u5730\u5740   The_UCF_CC_50 Dataset  \u4e0b\u8f7d\u5730\u5740   \u5730\u5740\u8bf4\u660e  \u4e0d\u63d0\u4f9b\u6570\u636e\u96c6\u5b98\u65b9\u5730\u5740\uff0c\u6570\u636e\u96c6\u5747\u653e\u7f6e\u5728\u6211\u7684\u8c37\u6b4c\u4e91\u76d8\uff0c\u5f00\u542f\u5171\u4eab\uff0c\u65e0\u6cd5\u7ffb\u5899\u7684\u53ef\u4ee5\u90ae\u7bb1\u8054\u7cfb\u6211(luanshiyinyang@gmail.com)\u3002      \u8bba\u6587\u8bf4\u660e  \u4f5c\u8005\u4e3b\u8981\u63d0\u51fa\u4e86MSCNN\u7684\u7ed3\u6784\uff0c\u8be5\u7ed3\u6784\u6bd4\u8d77MCNN\u5177\u6709\u66f4\u597d\u7684\u5904\u7406\u80fd\u529b\u53ca\u6548\u679c\uff0c\u5e76\u4e14\u7eb5\u5411\u5bf9\u6bd4\u4e86LBP+RR\u3001MCNN+CCR\u3001Zhang et al.\u548cMCNN\u7b49\u6a21\u578b\u3002 PDF\u8bba\u6587\u6587\u4ef6    \u73af\u5883\u914d\u7f6e  \u57fa\u4e8ePython3.6 \u9700\u8981\u7b2c\u4e09\u65b9\u5305\u5df2\u5728requirements\u5217\u51fa  \u5207\u6362\u5230requirements\u6587\u4ef6\u6240\u5728\u76ee\u5f55\uff0c\u6267\u884c\u547d\u4ee4pip install -r requirements.txt\u5373\u53ef\u914d\u7f6e\u73af\u5883      \u811a\u672c\u8fd0\u884c\u8bf4\u660e  \u8bad\u7ec3  \u547d\u4ee4\u884c\u6267\u884c  python train.py --dataset ShanghaiTech --visual yes --model ../models/final_model.h5   Jupyter Notebook  \u53ef\u4ee5\u5728\u5355\u5143\u683c\u4f7f\u7528!python train.py --dataset ShanghaiTech --visual yes --model ../models/final_model.h5 \u6765\u6267\u884c\u7a0b\u5e8f     \u6d4b\u8bd5  python demo.py Jupyter Notebook\u7c7b\u4f3c\u4e0a\u9762      ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1702.02359"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd1"}, "repo_url": "https://github.com/pollenjp/__fork__keras-yolo3", "repo_name": "__fork__keras-yolo3", "repo_full_name": "pollenjp/__fork__keras-yolo3", "repo_owner": "pollenjp", "repo_desc": "forked from https://github.com/qqwweee/keras-yolo3", "description_language": "Danish", "repo_ext_links": null, "repo_last_mod": "2019-05-30T05:42:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T06:13:40Z", "homepage": "", "size": 107, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188172546, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd2"}, "repo_url": "https://github.com/don-tpanic/keras_custom", "repo_name": "keras_custom", "repo_full_name": "don-tpanic/keras_custom", "repo_owner": "don-tpanic", "repo_desc": "custom keras functionalities go in here", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T14:23:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T10:02:37Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188208835, "is_fork": false, "readme_text": "keras_custom ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd3"}, "repo_url": "https://github.com/TsuaknovKirill/Object-vision", "repo_name": "Object-vision", "repo_full_name": "TsuaknovKirill/Object-vision", "repo_owner": "TsuaknovKirill", "repo_desc": "\u0418\u0418, \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0435\u043d\u0438\u0435 \u043e\u0431\u044a\u0435\u043a\u0442\u043e\u0432, \u043c\u0430\u0448\u0438\u043d\u043d\u043e\u0435 \u0437\u0440\u0435\u043d\u0438\u0435  ", "description_language": "Russian", "repo_ext_links": null, "repo_last_mod": "2019-05-24T10:22:06Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T10:08:56Z", "homepage": "", "size": 27165, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188400058, "is_fork": false, "readme_text": "object_vision Python 3.5.1 or higher, Download Python pip3 , Download PyPi Tensorflow 1.4.0 or higher pip3 install --upgrade tensorflow Numpy 1.13.1 or higher pip3 install numpy SciPy .19.1 or higher pip3 install scipy OpenCV pip3 install opencv-python Pillow pip3 install pillow Matplotlib pip3 install matplotlib h5py pip3 install h5py Keras pip3 install keras Once you have these packages installed on your computer system, you should install ImageAI by running the pip command below. Installing ImageAI pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl  \u0441\u043a\u0430\u0447\u0430\u0442\u044c Yolo.h5 https://github.com/OlafenwaMoses/ImageAI/releases/tag/1.0/  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd4"}, "repo_url": "https://github.com/sibisimon/deep-learning", "repo_name": "deep-learning", "repo_full_name": "sibisimon/deep-learning", "repo_owner": "sibisimon", "repo_desc": "deep learning using keras and pytorch", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T13:39:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T23:37:16Z", "homepage": null, "size": 2073, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188621865, "is_fork": false, "readme_text": "deep-learning deep learning using keras and pytorch Deep learning (also known as deep structured learning or hierarchical learning) is part of a broader family of machine learning methods based on artificial neural networks. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd5"}, "repo_url": "https://github.com/gayathri1874/cat-dog-classifier-2", "repo_name": "cat-dog-classifier-2", "repo_full_name": "gayathri1874/cat-dog-classifier-2", "repo_owner": "gayathri1874", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T05:09:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T05:09:08Z", "homepage": null, "size": 3319, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188165218, "is_fork": false, "readme_text": "cat-dog-image-classifier Complete Steps to implement a CNN to classify between cat and dog image Step 1 : Getting the Dataset The dataset is available in the link : https://www.kaggle.com/c/dogs-vs-cats/data Download this dataset, extract and store it in localdisk  Step 2 : Installing Required Packages [Python 3.6] 1. OpenCV     ---> '3.4.0'     [ Used to handle image operations like : reading the image , resizing , reshaping] 2. numpy      ---> '1.14.4'    [ Image that is read will be stored in an numpy array ] 3. TensorFlow ---> '1.8.0'     [ Tensorflow is the backend for Keras ] 4. Keras      ---> '2.1.6'     [ Keras is used to implement the CNN ]  Step 3 : How the Model Works ?? Note : Spyder is used to develop the code. Set the working Directory Correctely.        Open the trained_model.py file and set the image you want to test [Ref. Line Number 24 ]         This will predict the output for the image you have specified.  Once you have understood the basic working of the model, its now time to build the classifier from the scratch.  Step 4 : Building the Classifier Steps involved in building a classifier is briefed in classifier.py 13 actual substeps are involved in building your CNN!  This will build the model and save the trained model as a Json file.  The weights of the trained model are stored in a seperate file.  Step 5: How to use the trained Model ? Once you have imported the packages, you can directely load your trained classifier from the json file. Then load the weights from the h5 file saved in the previous step. This basically has 6 substeps before you can see the result.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/gayathri1874/cat-dog-classifier-2/blob/f0c6731d1f52afe3cd6bd25d9102c6f6f308e11c/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd6"}, "repo_url": "https://github.com/LucaCappelletti94/gaussian_process", "repo_name": "gaussian_process", "repo_full_name": "LucaCappelletti94/gaussian_process", "repo_owner": "LucaCappelletti94", "repo_desc": "Wrapper for gp_minimize and Keras that enables you to run bayesian optimization on your models.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T16:45:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T13:50:47Z", "homepage": null, "size": 138, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188571926, "is_fork": false, "readme_text": "gaussian_process         Wrapper for \"sklearn.gp_minimize\" for a simpler parameter specification using nested dictionaries.  How do I install this package? As usual, just download it using pip: pip install gaussian_process  Tests Coverage Since some software handling coverages sometime get slightly different results, here's three of them:     Keras model optimization using a gaussian process import silence_tensorflow from keras.models import Sequential from keras.layers import Dense, Dropout from keras.datasets import boston_housing from extra_keras_utils import set_seed from typing import Callable, Dict import numpy as np from holdouts_generator import holdouts_generator, random_holdouts from gaussian_process import TQDMGaussianProcess, Space, GaussianProcess   class MLP:     def __init__(self, holdouts:Callable):         self._holdouts = holdouts      def mlp(self, dense_layers:Dict, dropout_rate:float)->Sequential:         return Sequential([             *[Dense(**kwargs) for kwargs in dense_layers],             Dropout(dropout_rate),             Dense(1, activation=\"relu\"),         ])      def model_score(self, train:np.ndarray, test:np.ndarray, structure:Dict, fit:Dict):         model = self.mlp(**structure)         model.compile(             optimizer=\"nadam\",             loss=\"mse\"         )          return model.fit(             *train,             epochs=1,             validation_data=test,             verbose=0,             **fit         ).history[\"val_loss\"][-1]       def score(self, structure:Dict, fit:Dict):         return -np.mean([             self.model_score(training, test, structure, fit) for (training, test), _ in self._holdouts()         ])  if __name__ == \"__main__\":     set_seed(42)      generator = holdouts_generator(         *boston_housing.load_data()[0],         holdouts=random_holdouts([0.1], [2])     )      mlp = MLP(generator)      space = Space({         \"structure\":{             \"dense_layers\":[{                 \"units\":(8,16,32),                 \"activation\":(\"relu\", \"selu\")             },             {                 \"units\":[8,16,32],                 \"activation\":(\"relu\", \"selu\")             }],             \"dropout_rate\":[0.0,1.0]         },         \"fit\":{             \"batch_size\":[100,1000]         }     })      gp = GaussianProcess(mlp.score, space)      n_calls = 3     results = gp.minimize(         n_calls=n_calls,         n_random_starts=1,         callback=[TQDMGaussianProcess(n_calls=n_calls)],         random_state=42     )     results = gp.minimize(         n_calls=n_calls,         n_random_starts=1,         callback=[TQDMGaussianProcess(n_calls=n_calls)],         random_state=42     )     print(gp.best_parameters)     print(gp.best_optimized_parameters)     gp.clear_cache() ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd7"}, "repo_url": "https://github.com/AnkitRusia/Iris_flower_ANN", "repo_name": "Iris_flower_ANN", "repo_full_name": "AnkitRusia/Iris_flower_ANN", "repo_owner": "AnkitRusia", "repo_desc": "An artificial neural network to classify Iris flower data set.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T17:39:14Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T16:23:45Z", "homepage": "https://en.wikipedia.org/wiki/Iris_flower_data_set", "size": 20, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188588298, "is_fork": false, "readme_text": "Iris_flower_ANN An artificial neural network which is trained on 120 samples of iris flower data set and tested on 30 samples of testing data. Iris flower dataset info ; Neural Network info ; Keras Info Modules used  Keras Numpy sklearn  Network Summary Layer (type)                 Output Shape              Param dense_1 (Dense)              (None, 16)                80  dropout_1 (Dropout)          (None, 16)                0  dense_2 (Dense)              (None, 32)                544  dropout_2 (Dropout)          (None, 32)                0  dense_3 (Dense)              (None, 3)                 99  Total params: 723 Trainable params: 723 Non-trainable params: 0 Files provided  :  Pretrained model  : IrisANN-1.0_accuracy.h5 Tesing data       : test_data_iris.npy Testing target    : test_target_iris.npy Training data     : train_data_iris.npy Training target   : train_target_iris.npy  Codes provided :  pretrained_model.py ANN_to_classifiy_Iris.py  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/AnkitRusia/Iris_flower_ANN/blob/a68e19cf4533434697a60fb44da5eae4c5e9d2a4/IrisANN-1.0_accuracy.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd8"}, "repo_url": "https://github.com/haihuynhrdf/my-california-housing-price-prediction-using-keras", "repo_name": "my-california-housing-price-prediction-using-keras", "repo_full_name": "haihuynhrdf/my-california-housing-price-prediction-using-keras", "repo_owner": "haihuynhrdf", "repo_desc": "my-california-housing-price-prediction-using-keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T09:11:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T09:06:42Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188544470, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fd9"}, "repo_url": "https://github.com/Ashutosh1995/RPISegProject", "repo_name": "RPISegProject", "repo_full_name": "Ashutosh1995/RPISegProject", "repo_owner": "Ashutosh1995", "repo_desc": "Repository containing code to do performance modelling on RPI", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T06:16:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T13:05:40Z", "homepage": null, "size": 90, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188423168, "is_fork": false, "readme_text": "RPISegProject Repository containing code to do performance modelling on RPI generate_json.py This file generates the json files by changing the hyperparamters in the defined model architecture. load_keras_model_run_time_rpi.py This file loads a random json file from a list of json files, loads the keras model architecture, generates a protobuffer and then converts the protobuffer to tflite for getting the inference time. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fda"}, "repo_url": "https://github.com/McDonnell-Lab/KerasTransferLearning", "repo_name": "KerasTransferLearning", "repo_full_name": "McDonnell-Lab/KerasTransferLearning", "repo_owner": "McDonnell-Lab", "repo_desc": "Useful tricks for setting up transfer learning in keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T11:30:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T11:11:42Z", "homepage": null, "size": 14, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188408184, "is_fork": false, "readme_text": "KerasTransferLearning Useful tricks for setting up transfer learning in keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fdb"}, "repo_url": "https://github.com/kshahat/keras-tutorial", "repo_name": "keras-tutorial", "repo_full_name": "kshahat/keras-tutorial", "repo_owner": "kshahat", "repo_desc": "Going through the Deep Learning With Python Textbook", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T00:49:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T00:35:25Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188503086, "is_fork": false, "readme_text": "keras-tutorial Going through the Deep Learning With Python Textbook ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fdc"}, "repo_url": "https://github.com/undkriegist/DDI", "repo_name": "DDI", "repo_full_name": "undkriegist/DDI", "repo_owner": "undkriegist", "repo_desc": "DDI Extraction 2013", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T07:15:35Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T07:08:25Z", "homepage": null, "size": 8296, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188180461, "is_fork": false, "readme_text": "DDI DDI Extraction 2013 Python 3.6.7 Keras 2.1.0 Tensorflow 1.12.0 numpy 1.16.3 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/undkriegist/DDI/blob/1a2924e734527a489fec27b8ef2876bce44e9fde/source%20code/semi_vae_temp_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fdd"}, "repo_url": "https://github.com/lithiumcomputing/2D-Neural-Network-Classification-", "repo_name": "2D-Neural-Network-Classification-", "repo_full_name": "lithiumcomputing/2D-Neural-Network-Classification-", "repo_owner": "lithiumcomputing", "repo_desc": "Simple example of a Tensorflow/Keras program that classifies points in a 2D plane using a Neural Network.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T21:17:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T21:16:13Z", "homepage": null, "size": 956, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188307771, "is_fork": false, "readme_text": "2D-Neural-Network-Classification- Simple example of a Tensorflow/Keras program that classifies points in a 2D plane using a Neural Network. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fde"}, "repo_url": "https://github.com/idiap/RawSpeechClassification", "repo_name": "RawSpeechClassification", "repo_full_name": "idiap/RawSpeechClassification", "repo_owner": "idiap", "repo_desc": "Trains CNN, or any neural network based, classifiers from raw speech using Keras and tests them.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T13:08:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T13:06:55Z", "homepage": null, "size": 22, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188423334, "is_fork": false, "readme_text": "Raw Speech Classification Trains CNN (or any neural network based) classifiers from raw speech using Keras and tests them. The inputs are lists of wav files, where each file is labelled. It then creates fixed length signals and processes them. During testing, it computes scores at the utterance or speaker levels by averaging the corresponding frame-level scores from the fixed length signals. Dependencies   Python 3.4+.   Keras with Tensorflow/Theano backend.   Using the Code   Create lists for training, cross-validation and testing. Each line in a list must contain the full path to a wav file, follwed by its integer label indexed from 0, separated by a space. E.g: </path1/file1.wav> 1 </path2/file2.wav> 0   Configure and run run.sh. Provide model architecture as an argument. See steps_kt/model_architecture.py for valid options. Optionally, provide an integer as a count of the number of times the experiment is repeated. This is useful when the same experiment needs to be repeated multiple times with different initialisations. The argument defaults to 1.   Code Components   wav2feat.py creates directories where the wav files are stored as fixed length frames for faster access during training and testing.   train.py is the Keras training script.   Model architecture can be configured in model_architecture.py.   rawGenerator.py provides an object that reads the saved directories in batches and retrieves mini-batches for training.   test.py performs the testing and generates scores as posterior probabilities. If you need the results per speaker, configure it accordingly (see the script for details). The default output format is: <speakerID> <label> [<posterior_probability_vector>]   Training Schedule The script uses stochastic gradient descent with 0.5 momentum. It starts with a learning rate of 0.1 for a minimum of 5 epochs. Whenever the validation loss reduces by less than 0.002 between successive epochs, the learning rate is halved. Halving is performed until the learning rate reaches 1e-7. Contributors Idiap Research Institute Authors: S. Pavankumar Dubagunta and Dr. Mathew Magimai-Doss License GNU GPL v3 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fdf"}, "repo_url": "https://github.com/Gausstotle/gibberish", "repo_name": "gibberish", "repo_full_name": "Gausstotle/gibberish", "repo_owner": "Gausstotle", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T14:30:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T13:55:36Z", "homepage": null, "size": 11183, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188430676, "is_fork": false, "readme_text": "gibberish https://minimaxir.com/2018/05/text-neural-networks/ https://github.com/minimaxir/textgenrnn requirements #install_requires=['keras>=2.1.5', 'h5py', 'scikit-learn', 'tqdm'] , Tensorflow(cant be 3.7, has to be 3.6) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Gausstotle/gibberish/blob/c9d3c6f3e4fb5b108e35ea88b7509d3ecd9b33b1/gibberish_weights.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe0"}, "repo_url": "https://github.com/melbrbry/Matrix-Factorization-for-DNNs", "repo_name": "Matrix-Factorization-for-DNNs", "repo_full_name": "melbrbry/Matrix-Factorization-for-DNNs", "repo_owner": "melbrbry", "repo_desc": "Demonstrating the effectiveness of using Matrix Factorization to reduce the number of parameters in Deep Neural Networks (DNNs).", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T23:06:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T22:46:10Z", "homepage": "", "size": 102, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188316648, "is_fork": false, "readme_text": "Matrix-Factorization-for-DNNs In this project,we demonstrate using Matrix Factorization to reduce the number of parameters in the final layer in Deep Neural Networks (DNNs) by a percentage around 40% without significant loss of accuracy. We demonstrate the approach in two image classification tasks with 10 and 100 target classes with MNIST and CIFAR100 dataset and the implementation is done using Keras. Getting Started You need to clone the reporistory to your local machine. Run this command in your terminal where you like to clone the project git clone https://github.com/melbrbry/Matrix-Factorization-for-DNNs  Prerequisites Required packages (use pip install on linux): numpy keras Repository Description The repository has only one branch: the master branch. Documentation In this section, I write a brief description of some of the repository files/folders cifar100.py: builds the network and train the model for cifar100 dataset. mnist.py: builds the network and train the model for cifar100 dataset. Report.pdf: full project report. Acknowledgement  This project is done as the final project for the Neural Netowrk course taught by prof. Aurelio Uncini - Sapienza Universit\u00e0 di Roma. This project is an individual project.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe1"}, "repo_url": "https://github.com/PABlond/RNN_Forex-trading-bot", "repo_name": "RNN_Forex-trading-bot", "repo_full_name": "PABlond/RNN_Forex-trading-bot", "repo_owner": "PABlond", "repo_desc": "Stock Price Prediction with Recurrent Neural Networks (RNN) mostly built with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T09:56:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T14:30:02Z", "homepage": "", "size": 34, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188250457, "is_fork": false, "readme_text": "Information Stock Price Prediction with Recurrent Neural Networks (RNN) mostly built with Keras. Input data is stored in the data/ directory. You'll notice that there is an example dataset included in the repo which consists of a subset EUR/USD exchange rate. Full versions of this dataset can be find here : https://forextester.com/data/datasources Installation Guide Step 1 : clone this repository git clone https://github.com/PABlond/RNN_Forex-trading-bot.git  Step 2 : install dependencies cd RNN_Forex-trading-bot/     pip install sklearn pandas keras numpy fxcmpy apscheduler  Step 3 : Get an API token from FXCM To connect to the API, you need an API token that you can create or revoke from within your (demo) account in the Trading Station https://tradingstation.fxcm.com/. Then, you will need to paste your token in the fxcm config file (core/cfg/fxcm.cfg) [FXCM] log_level = error log_file = fxcm.log access_token = <YOUR_FXCM_TOKEN>  Step 4 : train the model python train.py  Step 5 : launch the bot python prediction.py  Contributor: @PABlond ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe2"}, "repo_url": "https://github.com/shaoyiwork/Keras-simple-face", "repo_name": "Keras-simple-face", "repo_full_name": "shaoyiwork/Keras-simple-face", "repo_owner": "shaoyiwork", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T08:33:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T08:13:43Z", "homepage": null, "size": 5604, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188381312, "is_fork": false, "readme_text": "Keras-simple-face \u7b80\u4ecb \u6211\u4eec\u5229\u7528keras\u6765\u642d\u5efaCNN\u4eba\u8138\u8bc6\u522b\u7f51\u7edc\u6a21\u578b\u3002\u8fd9\u91cc\u5148\u91c7\u7528\u6700\u57fa\u672c\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u6a21\u578bLeNet5\u6a21\u578b\uff0c\u5b9e\u73b0\u4eba\u8138\u8bc6\u522b\u4efb\u52a1\u3002 \u6211\u4eec\u91c7\u7528\u7684\u4eba\u8138\u6570\u636e\u5e93Olivetti Faces\u5730\u5740\u4e3ahttps://cs.nyu.edu/~roweis/data/olivettifaces.gif \u8fd9\u4e2aOlivetti Faces\u662f\u7ebd\u7ea6\u5927\u5b66\u7684\u4e00\u4e2a\u6bd4\u8f83\u5c0f\u7684\u4eba\u8138\u5e93\uff0c \u753140\u4e2a\u4eba\u7684400\u5f20\u56fe\u7247\u6784\u6210\uff0c\u5373\u6bcf\u4e2a\u4eba\u7684\u4eba\u8138\u56fe\u7247\u4e3a10\u5f20\u3002\u6bcf\u5f20\u56fe\u7247\u7684\u7070\u5ea6\u7ea7\u4e3a8\u4f4d\uff0c\u6bcf\u4e2a\u50cf\u7d20\u7684\u7070\u5ea6\u5927\u5c0f\u4f4d\u4e8e0-255\u4e4b\u95f4\uff0c\u6bcf\u5f20\u56fe\u7247\u5927\u5c0f\u4e3a64\u00d764\u3002 \u5de5\u7a0b\u4e2d\u56fe\u7247\u7684\u5927\u5c0f\u662f1190942\uff0c\u4e00\u5171\u67092020\u5f20\u4eba\u8138\uff0c\u6545\u6bcf\u5f20\u4eba\u8138\u5927\u5c0f\u662f\uff081190/20\uff09\uff08942/20\uff09\u53735747=2679\u3002 \u4f7f\u7528\u65b9\u6cd5 \u8bad\u7ec3\u6a21\u578b\u7684\u65f6\u5019\u53ef\u4ee5\u628a\u5df2\u7ecf\u5c4f\u853d\u6389\u7684\u8bad\u7ec3\u6a21\u578b\u8bed\u53e5\u548c\u6d4b\u8bd5\u6a21\u578b\u8bed\u53e5\u653e\u51fa\u6765\u3002\u628a\u540e\u9762\u7684\u9884\u8a00\u6a21\u578b\u8bed\u53e5\u5c4f\u853d\u6389\u3002 \u7b49\u8bad\u7ec3\u51fah5\u6a21\u578b\u4e4b\u540e\uff0c\u53ef\u4ee5\u5c4f\u853d\u6389\u8bad\u7ec3\u548c\u6d4b\u8bd5\u8bed\u53e5\u5b8c\u6210\u6a21\u578b\u4f7f\u7528\u3002\u6a21\u578b\u53ef\u4ee5\u76f4\u63a5\u5728\u673a\u5668\u4eba\u4e0a\u8dd1\u5e76\u5f97\u5230\u5e94\u7528\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shaoyiwork/Keras-simple-face/blob/113830dcffb25cc4a432fc141c559a81b44945c0/model_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe3"}, "repo_url": "https://github.com/shaoyiwork/ASR-two-words", "repo_name": "ASR-two-words", "repo_full_name": "shaoyiwork/ASR-two-words", "repo_owner": "shaoyiwork", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T05:50:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T11:28:27Z", "homepage": null, "size": 196451, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188410211, "is_fork": false, "readme_text": "ASR-two-words \u8fd9\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u4e24\u4e2a\u8bcd\u8bed\u7684\u8bc6\u522b\u6a21\u578b\u6848\u4f8b\uff0c\u7528\u4e8e\u5b66\u4e60ASR\u7b97\u6cd5\u3002\u4f8b\u5b50\u4e2d\u91c7\u7528\u4e86Keras\u6846\u67b6\u6765\u8bad\u7ec3seven\u548cstop\u4e24\u4e2a\u8bcd\u8bed\u7684\u8bc6\u522b\u6a21\u578b\u5e76\u6d4b\u8bd5\u4e86\u6a21\u578b\u7684\u6709\u6548\u6027\u3002 \u4f7f\u7528\u65b9\u6cd5 \u8bad\u7ec3\u6a21\u578b\uff1a python asr_model.py \u6d4b\u8bd5\u6a21\u578b\uff1a python asr_test.py ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shaoyiwork/ASR-two-words/blob/4e38f460f1ad8a87855e4c8fce9cbedb12965c48/asr_model_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe4"}, "repo_url": "https://github.com/JacobMod/If-graduate-project", "repo_name": "If-graduate-project", "repo_full_name": "JacobMod/If-graduate-project", "repo_owner": "JacobMod", "repo_desc": "Using fully conected neural network to predict if student will graduated or not", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T11:31:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T11:23:24Z", "homepage": null, "size": 3562, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188409597, "is_fork": false, "readme_text": "If-graduate-project Using fully conected neural network to predict if student will graduated or not Creating file File create_csv.py makes file only filled with data of people who were accepted on university. Model File model.py has network model. Launching it starts training neural network. It depends on functions from file data_prepare. Data All needed data is in folder data. Dependecies -Keras -Tensorflow -Pandas -Matplotlib -Numpy ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe5"}, "repo_url": "https://github.com/eyildiz-ugoe/screw_detection", "repo_name": "screw_detection", "repo_full_name": "eyildiz-ugoe/screw_detection", "repo_owner": "eyildiz-ugoe", "repo_desc": "Publicized code for the paper \"DNN-Based Screw Detection for Automated Disassembly Processes\"", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T15:50:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T08:29:39Z", "homepage": null, "size": 4166, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188383934, "is_fork": false, "readme_text": "\"DNN-Based Screw Detection for Automated Disassembly Processes\" This is an implementation of Screw Detectoron Python 3, Keras, TensorFlow and ROS. The scheme uses Hough Transform to get the candidates and then runs the integrated model to classify the candidates into screws and artifacts. The integrated model is based on Xception and InceptionV3. Publicized code for the paper \"DNN-Based Screw Detection for Automated Disassembly Processes\"  The repository includes: - Source code of Screw Detector built on Xception and InceptionV3. - Models - Dataset  The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex will be below later on). The dataset was created by using the Screw Detector in the offline mode, which can be triggered once the ROS-Node is up and running. You can use this mode to collect images for your own dataset. Contributing Contributions to this repository are welcome. Examples of things you can contribute: - Training on other datasets. - Accuracy Improvements. - Visualizations and examples.  R0S-Based Installation If you want to use the node directly on your system via ROS, follow the below steps:  Clone this repository Install ROS (tested only on Melodic) from its official website. Install python3, tensorflow-gpu==1.9.0, opencv-python==3.4.3.18 preferably via pip. Download the weights: https://owncloud.gwdg.de/index.php/s/PJIPYTvBteXlqhv (Extract it and change the path in src/candidate_generator.py accordingly. Download the dataset (optional): https://owncloud.gwdg.de/index.php/s/iqljlhv5UT38Ne Since the code is ROS-based, you'll need an RGB Camera and its ROS node which publishes RGB images. If you have these two, then you need to modify the following files to get the code working with your own camera:  launch/screw_detection.launch src/candidate_generator.py   The command to run: roslaunch screw_detection screw_detection.launch  Standalone Installation If you are only interested in the results and evaluation, follow the below steps:  Clone this repository. Install python3, tensorflow-gpu==1.9.0, opencv-python==3.4.3.18 preferably via pip, as well as other required packages if they are asked for. Download the weights: https://owncloud.gwdg.de/index.php/s/PJIPYTvBteXlqhv Download the dataset: https://owncloud.gwdg.de/index.php/s/iqljlhv5UT38Ne Change the paths in each .py  file as you run since you'll probably face a path error, which you can then fix by entering your path of extraction.  Evaluating the models We compare our integrated model against two best performing models which are using 1 Tensorflow model + 1 Keras model and 2 Keras models, respectively. In order to evaluate all, we need to first run these two networks and save the detection results to a folder, so let's do the following: cd evaluate & mkdir result_images Alright. Let's run the networks one by one and wait for them to save the results, creating det_2keras.txt and det_2tf.txt: python3 evaluate_classifiers_2tf.py python3 evaluate_classifiers_2keras.py  Following the creation of det_2keras.txt and det_2tf.txt, which are the detection results of the above executions, now the actual evaluation could be executed, which would compare these two with the ground truth annotations and pop up the precision-recall curve:  python3 evaluate_detection.py --det_path det_2tf.txt --gt_path gt_test.txt python3 evaluate_detection.py --det_path det_2keras.txt --gt_path gt_test.txt  Finally, our integrated model can be evaluated with the following:  python3 evaluate_classifiers_integrated.py which would yield: maximum accuracy:  0.9897316219369895 TP:  990   TN:  3247   FP:  10   FN:  38 accuracy:  0.9887981330221703 Contact For questions, you can create an issue on Github or simply contact eyildiz@gwdg.de ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe6"}, "repo_url": "https://github.com/askemottelson/ada-alarm", "repo_name": "ada-alarm", "repo_full_name": "askemottelson/ada-alarm", "repo_owner": "askemottelson", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:28:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T14:05:33Z", "homepage": null, "size": 4426, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188432231, "is_fork": false, "readme_text": "Ada-alarm A deep learning based baby monitor, using a raspberry pi + your smart phone Listens for baby crying, notifies your smartphone. Tech Python3, Keras, Tensorflow, Realtime Librosa, PyAudio Raspberry Pi iOS Naming The project is called Ada, named after my daughter. Also, the CNN uses the ADADELTA learning rate method. Most interesting parts: Real-time non-blocking audio processing in python using PyAudio and Librosa (firmware/pred.py) Binary baby cry detection: a Convolutional Neural Network in Keras. Accuracy: ~ 99% (ML/model.py) run it! Make sure the IP for the server match in server and firmware parts. # backend python3 -m server.server  # client python3 -m firmware.listen  To set up server properly, see: https://www.codementor.io/abhishake/minimal-apache-configuration-for-deploying-a-flask-app-ubuntu-18-04-phu50a7ft Components   firmware Runs on a Raspberry Pi Zero W with a microphone. Sends requests to the server application upon cry detection. Listens on the microphone using PyAudio.   ML Machine learning: Scripts for preprocessing audio files and training a binary Convolutional Neural Network model using Keras with Tensorflow. The data set is gathered from various sources, such as this, this, and this.  Get data (4.7GB zip)    cd ML mkdir data cd data wget --max-redirect=10 https://www.dropbox.com/sh/shrf9tqxjbcqhxz/AABcwowIKFUydt9aqh8f8axia?dl=1 -O data.zip unzip data.zip  All audio files are segmented in to 2-seconds wav files. Clips under a certain threshold of loudness are skipped. Preprocess data set: python3 segment.py    server Backend application. Handles smartphone notifications using push notifications over the internet   ios iOS app. Doesn't do anything but asks for permission for notifications, and registers phone ID with the backend   Installation Each folder has its own requirements. Below here follows a short guide to get CUDA/tensorflow-gpu working on a VM. install docker sudo apt-get update sudo apt-get install  apt-transport-https  ca-certificates  curl  gnupg-agent  software-properties-common curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - sudo add-apt-repository  \"deb [arch=amd64] https://download.docker.com/linux/ubuntu  $(lsb_release -cs)  stable\" sudo apt-get update sudo apt-get install docker-ce docker-ce-cli containerd.io install nvidia docker support Add the package repositories curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey |  sudo apt-key add - distribution=$(. /etc/os-release;echo $ID$VERSION_ID) curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list |  sudo tee /etc/apt/sources.list.d/nvidia-docker.list sudo apt-get update Install nvidia-docker2 and reload the Docker daemon configuration sudo apt-get install -y nvidia-docker2 sudo pkill -SIGHUP dockerd Setup ada-alarm and run the instance git clone https://github.com/askemottelson/ada-alarm.git docker pull tensorflow/tensorflow:latest-gpu-py3 sudo docker run -v /home/amot/ada-alarm/:/home/ada-alarm --runtime=nvidia -it tensorflow/tensorflow:latest-gpu-py3 bash apt-get install -y ffmpeg Train new model cd /home/ada-alarm/ML/ pip3 install -r requirements.txt python3 train_model.py  Exit docker Remember to update the repo with the new model.h5 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/askemottelson/ada-alarm/blob/d39c4081b8c199f98fbfe8e039d228e24f22961a/ML/model.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1212.5701"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe7"}, "repo_url": "https://github.com/JensSchouten/ClassificationALL", "repo_name": "ClassificationALL", "repo_full_name": "JensSchouten/ClassificationALL", "repo_owner": "JensSchouten", "repo_desc": "Accurate classification of blood cells in a small acute lymphoblastic leukemia dataset using convolutional neural networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T17:52:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T14:42:12Z", "homepage": null, "size": 28, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188438258, "is_fork": false, "readme_text": "ClassificationALL These scripts were used in the article titled \"Accurate classification of blood cells in a small acute lymphoblastic leukemia dataset using convolutional neural networks\" by Schouten et al. The networks were trained on a NVIDIA GeForce GT 730 GPU with Python version 3.5, keras version 2.0.8 and tensorflow-GPU version 1.4.0. Dataset The dataset was obtained by filling in the application form on this site: https://homes.di.unimi.it/scotti/all/. The dataset consists of two parts. ALL-IDB1 contains the larger blood smear images. ALL-IDB2 contains the single-cell images that are extracted from the larger ALL-IDB1 images. More information can be found in our article or in the article of the owners of the dataset [1]. Scripts In both the binary classification as multiclass classification scripts, the user input should be filled in to be able to train the networks. This includes paths to the images and labels folder and the folder to save the results. Binary classification Binary_classification.py In the binary classification script, networks can be trained with 10-fold cross-validation and different training set sizes as explained in the article. During training, the accuracies and losses are saved in an excel sheet and the weights of the network that has the lowest validation loss are saved. After training, the weights of the best network are loaded and the images in the test set are tested to evaluate the classification performance. Along with the predictions, the sensitivities and specificities are calculated to be able to create a curve and calculate the area under the curve (AUC). Saliency.py Saliency maps can be created to visualize the focus areas of the network when it tries to classify an image. This is done by loading a network with the pretrained weights and the testing images. These images are then classified by the network and at the same time the visualization is made. This visualization is done using the Keras Visualization Toolkit that can be found here: https://github.com/raghakot/keras-vis. The augmented images are then saved to the assigned folder. Multiclass classification Multiclass_classification.py In the multiclass classification script, the classification of six classes in the ALL dataset can be trained with 10-fold cross-validation and a training set containing 200 images. Because of the imbalance in the number of images per class, data augmentation is used to create 150 images for each class during training. The test images are again classified by the best network and the F1-score is calculated. Multiclass_data_augmentation.py As mentioned before, 150 images are created using data augmentation for the multiclass classification due to the imbalance in the number of images per class. This data augmentation includes rotation and horizontal and vertical flipping, like in the binary classification. First it is calculated how many extra images should be created. Then the rotation and flipping is chosen randomly and the images are adjusted. The new image is then added to a new array of images. The same holds for the labels. These two arrays are then returned to the Multiclass_classicification.py script. References [1] R. D. Labati, V. Piuri, and F. Scotti, \u201cAll-idb: The acute lymphoblastic leukemia image database for image processing,\u201d in Image processing (ICIP), 2011 18th IEEE international conference on, pp. 2045\u20132048, IEEE, 2011. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe8"}, "repo_url": "https://github.com/ahmetcanaydemir/otomatikobis", "repo_name": "otomatikobis", "repo_full_name": "ahmetcanaydemir/otomatikobis", "repo_owner": "ahmetcanaydemir", "repo_desc": "Otomatik Obis a software that solves Selcuk University captchas with machine learning. Makes real students -not bots- login easier. The user name and password are stored on the user's computer.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T23:32:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T21:28:26Z", "homepage": "", "size": 6364, "language": "Python", "has_wiki": false, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188309188, "is_fork": false, "readme_text": "Otomatik Obis Otomatik Obis a software that solves Selcuk University captchas with machine learning. Makes real students -not bots- login easier. The user name and password are stored on the user's computer. How it works  This article helped in the captcha solving process with machine learning. Toolset Backend and captcha proccessing  Heroku Python 3 Flask OpenCV Keras TensorFlow  Web Extension  Javascript  Captcha Downloader for generating model  C#  Contribution  Report issues Open pull request with improvements Spread the word Reach out with any feedback   ", "has_readme": true, "readme_language": "English", "repo_tags": ["captcha-solving", "captcha-solver", "captcha-breaking"], "has_h5": true, "h5_files_links": ["https://github.com/ahmetcanaydemir/otomatikobis/blob/3d93696103b4bbc899506c4ffe12dac71c6c43da/machine-learning-backend/captcha_model.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fe9"}, "repo_url": "https://github.com/PrincyX/anomalydetectionexp2", "repo_name": "anomalydetectionexp2", "repo_full_name": "PrincyX/anomalydetectionexp2", "repo_owner": "PrincyX", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T02:08:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T01:38:07Z", "homepage": null, "size": 35074, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188140395, "is_fork": false, "readme_text": "AnomalyDetection_CVPR18 Implementation of Real-world Anomaly Detection in Surveillance Videos paper from CVPR 2018. Original implementation from authors is here. Download C3D sports-1m weights from here and save them to 'trained_models' folder as 'c3d_sports1m.h5'. Run demo.py to run the code on the demo video in 'input' folder. Visualization of the predictions from the model is saved in 'output' folder. Libraries Used  Keras : 2.2.0 Tensorflow : 1.10.1 Numpy : 1.14.5 OpenCV : 3.3.0.10 Scipy : 0.19.1 Matplotlib : 2.0.2  Will add the training code soon...!! ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1801.04264.pdf"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fea"}, "repo_url": "https://github.com/MurphyLaw/Leaf-Classification", "repo_name": "Leaf-Classification", "repo_full_name": "MurphyLaw/Leaf-Classification", "repo_owner": "MurphyLaw", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T12:49:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T12:27:07Z", "homepage": null, "size": 26646, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188563405, "is_fork": false, "readme_text": "Leaf-Classification Getting Started Libraries  Python 3.7 Install pip pip install tensorflow pip install keras pip install sklearn pip install matplotlib pip install imutils pip install numpy pip install argparse pip install random pip install pickle pip install cv2 pip install os pip install kivy  Training execute 'python train.py -d dataset -m leaf.model -l lb.pickle' Classification execute 'python train.py -m leaf.model -l lb.pickle -i examples/Bayabas_1.jpg' Kivy App comment out line code from 77 to 149 and uncomment from 1 to 74 Then execute 'python main.py' ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6feb"}, "repo_url": "https://github.com/iStroml/KerasIronyDetection", "repo_name": "KerasIronyDetection", "repo_full_name": "iStroml/KerasIronyDetection", "repo_owner": "iStroml", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T19:05:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T17:15:27Z", "homepage": null, "size": 385, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188593310, "is_fork": false, "readme_text": "Keras Irony Detection The task in this project was to predict whether comments are ironic or not. I tested how training a neuronal network on one domain help prediction on another (smaller) dataset. Datasets SemEval-2018 Task 3: Irony detection in English tweets https://competitions.codalab.org/competitions/17468 Non ironic posts: 1923 - Ironic posts: 1911  Kaggle Ironic Corpus https://www.kaggle.com/rtatman/ironic-corpus#irony-labeled.csv Non ironic posts: 1412 - Ironic posts: 537  The trainingsdata has labels with 1 for ironic and 0 or -1 for non-ironic posts. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fec"}, "repo_url": "https://github.com/michaelliqx/AlphaGoZero", "repo_name": "AlphaGoZero", "repo_full_name": "michaelliqx/AlphaGoZero", "repo_owner": "michaelliqx", "repo_desc": "a self-training AlphaGoZero program for game conncet4", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T06:00:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T05:51:39Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188360384, "is_fork": false, "readme_text": "AlphaGoZero this is a project according to the paper published by DeepMind in Nature:Mastering the game of Go without human knowledge. But in this project we will not train a go game since it's complexity but keep the same idea to train another game: conncet4 the project is not finished yet. content finished:   -game.py: game rule of connect4   -Agent.py: how agent act   -Memory.py: remember information   -model.py: training models by using keras   -config.py: parameters, to be updated   -main.py: entrance, not finished yet   -MTCS.py: monte carlo tree, not finished yet  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fed"}, "repo_url": "https://github.com/osc-dnn/osc-dnn", "repo_name": "osc-dnn", "repo_full_name": "osc-dnn/osc-dnn", "repo_owner": "osc-dnn", "repo_desc": "Prediction of reorganization energy for organic semiconductors using deep neural networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T10:44:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T09:45:35Z", "homepage": null, "size": 18, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188206100, "is_fork": false, "readme_text": "Prediction of Intramolecular Reorganization Energy Using Machine Learning Introduction We predict reorganization energy of organic semiconductors using deep neural networks. We experimented with Circular Fingerprints, Molecular Signatures, and Molecular Transform Descriptors. The code can be used with other descriptors as well, since the descriptors are not generated within the code but read from the input files. Required Packages  TensorFlow Keras scikit-learn  How to run the scripts  Edit settings.ini and settings.py with the proper input and output filenames. Set the hyperparameter values in grid_search_osc.py. Then run  python grid_search_osc.py  This will run all the folds and output statistics into the given outputfile. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fee"}, "repo_url": "https://github.com/Eienyami/Emo_reco", "repo_name": "Emo_reco", "repo_full_name": "Eienyami/Emo_reco", "repo_owner": "Eienyami", "repo_desc": "Emotion recognition using Keras/Tenserflow - CNN - MiniXception model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T07:18:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T05:17:41Z", "homepage": "", "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188356703, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fef"}, "repo_url": "https://github.com/nyuGuangyu/model_template_keras", "repo_name": "model_template_keras", "repo_full_name": "nyuGuangyu/model_template_keras", "repo_owner": "nyuGuangyu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T06:47:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T05:11:07Z", "homepage": null, "size": 81, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188523421, "is_fork": false, "readme_text": "Keras Project Template  A project template to simplify building and training deep learning models using Keras. Table of contents  Getting Started Running The Demo Project Comet.ml Integration Template Details  Project Architecture Folder Structure Main Components   Future Work Example Projects Contributing Acknowledgements  Getting Started This template allows you to simply build and train deep learning models with checkpoints and tensorboard visualization. In order to use the template you have to:  Define a data loader class. Define a model class that inherits from BaseModel. Define a trainer class that inherits. Define a configuration file with the parameters needed in an experiment. Run the model using:  python main.py -c [path to configuration file] Running The Demo Project A simple model for the mnist dataset is available to test the template. To run the demo project:  Start the training using:  python main.py -c configs/simple_mnist_config.json  Start Tensorboard visualization using:  tensorboard --logdir=experiments/simple_mnist/logs    Comet.ml Integration This template also supports reporting to Comet.ml which allows you to see all your hyper-params, metrics, graphs, dependencies and more including real-time metric. Add your API key in the configuration file: For example:  \"comet_api_key\": \"your key here\" Here's how it looks after you start training:    You can also link your Github repository to your comet.ml project for full version control. Template Details Project Architecture    Folder Structure \u251c\u2500\u2500 main.py             - here's an example of main that is responsible for the whole pipeline. \u2502 \u2502 \u251c\u2500\u2500 base                - this folder contains the abstract classes of the project components \u2502   \u251c\u2500\u2500 base_data_loader.py   - this file contains the abstract class of the data loader. \u2502   \u251c\u2500\u2500 base_model.py   - this file contains the abstract class of the model. \u2502   \u2514\u2500\u2500 base_train.py   - this file contains the abstract class of the trainer. \u2502 \u2502 \u251c\u2500\u2500 model               - this folder contains the models of your project. \u2502   \u2514\u2500\u2500 simple_mnist_model.py \u2502 \u2502 \u251c\u2500\u2500 trainer             - this folder contains the trainers of your project. \u2502   \u2514\u2500\u2500 simple_mnist_trainer.py \u2502 | \u251c\u2500\u2500 data_loader         - this folder contains the data loaders of your project. \u2502   \u2514\u2500\u2500 simple_mnist_data_loader.py \u2502 \u2502 \u251c\u2500\u2500 configs             - this folder contains the experiment and model configs of your project. \u2502   \u2514\u2500\u2500 simple_mnist_config.json \u2502 \u2502 \u251c\u2500\u2500 datasets            - this folder might contain the datasets of your project. \u2502 \u2502 \u2514\u2500\u2500 utils               - this folder contains any utils you need.      \u251c\u2500\u2500 config.py      - util functions for parsing the config files.      \u251c\u2500\u2500 dirs.py        - util functions for creating directories.      \u2514\u2500\u2500 utils.py       - util functions for parsing arguments.  Main Components Models You need to:  Create a model class that inherits from BaseModel. Override the build_model function which defines your model. Call build_model function from the constructor.  Trainers You need to:  Create a trainer class that inherits from BaseTrainer. Override the train function which defines the training logic.  Note: To add functionalities after each training epoch such as saving checkpoints or logs for tensorboard using Keras callbacks:  Declare a callbacks array in your constructor. Define an init_callbacks function to populate your callbacks array and call it in your constructor. Pass the callbacks array to the fit function on the model object.  Note: You can use fit_generator instead of fit to support generating new batches of data instead of loading the whole dataset at one time. Data Loaders You need to:  Create a data loader class that inherits from BaseDataLoader. Override the get_train_data() and the get_test_data() functions to return your train and test dataset splits.  Note: You can also define a different logic where the data loader class has a function get_next_batch if you want the data reader to read batches from your dataset each time. Configs You need to define a .json file that contains your experiment and model configurations such as the experiment name, the batch size, and the number of epochs. Main Responsible for building the pipeline.  Parse the config file Create an instance of your data loader class. Create an instance of your model class. Create an instance of your trainer class. Train your model using \".Train()\" function on the trainer object.  From Config We can now load models without having to explicitly create an instance of each class. Look at:  from_config.py: this can load any config file set up to point to the right modules/classes to import Look at configs/simple_mnist_from_config.json to get an idea of how this works from the config. Run it with:  python from_config.py -c configs/simple_mnist_from_config.json  See conv_mnist_from_config.json (and the additional data_loader/model) to see how easy it is to run a different experiment with just a different config file:  python from_config.py -c configs/conv_mnist_from_config.json Example Projects  Toxic comments classification using Convolutional Neural Networks and Word Embedding  Future Work Create a command line tool for Keras project scaffolding where the user defines a data loader, a model, a trainer and runs the tool to generate the whole project. (This is somewhat complete now by loading each of these from the config) Contributing Any contributions are welcome including improving the template and example projects. Acknowledgements This project template is based on MrGemy95's Tensorflow Project Template. Thanks for my colleagues Mahmoud Khaled, Ahmed Waleed and Ahmed El-Gammal who worked on the initial project that spawned this template. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff0"}, "repo_url": "https://github.com/bysmss/Tensorflow-", "repo_name": "Tensorflow-", "repo_full_name": "bysmss/Tensorflow-", "repo_owner": "bysmss", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T16:57:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T16:46:51Z", "homepage": null, "size": 57612, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188590582, "is_fork": false, "readme_text": "1## Tensorflow- \u8fd9\u662f\u4ec0\u4e48\uff1f \u7b54 \uff1a \u4e00\u4e2a\u804a\u5929\u673a\u5668\u4eba\u3002\u4e2a\u4eba\u80fd\u529b\u6709\u9650\uff0c\u6682\u4e14\u4e0d\u80fd\u505a\u5230 \u201d\u667a\u80fd\u201c\uff0c \u5b83\u597d\u50cf\u81ea\u8ba4\u4e3a\u662f\u5973\u751f\uff0c\u6240\u4ee5\u4f60\u5927\u53ef\u8c03\u620f\u5979\uff0c\u6216\u662f\u95f2\u5f97\u65e0\u804a\u65f6\u5019\u627e\u5b83\u5520\u4e24\u53e5\u3002 \u4ec0\u4e48\u539f\u7406\u5462\uff1f \u7b54\uff1a \u4e25\u8c28\u7684\u8bf4\u53eb \u201d\u57fa\u4e8e\u6df1\u5ea6\u5b66\u4e60\u7684\u5f00\u653e\u57df\u751f\u6210\u5bf9\u8bdd\u6a21\u578b\u201c\uff0c\u6846\u67b6\u4e3aKeras\uff08Tensorflow\u7684\u9ad8\u5c42\u5305\u88c5\uff09\uff0c\u65b9\u6848\u4e3a\u4e3b\u6d41\u7684RNN\uff08\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\uff09\u7684\u53d8\u79cdLSTM\uff08\u957f\u77ed\u671f\u8bb0\u5fc6\u7f51\u7edc\uff09+seq2seq\uff08\u5e8f\u5217\u5230\u5e8f\u5217\u6a21\u578b\uff09\uff0c\u5916\u52a0\u7b97\u6cd5Attention Mechanism\uff08\u6ce8\u610f\u529b\u673a\u5236\uff09\uff0c\u5206\u8bcd\u5de5\u5177\u4e3ajieba\uff0cUI\u4e3aTkinter\uff0c\u57fa\u4e8e\u201d\u9752\u4e91\u201c\u8bed\u6599\uff0810\u4e07+\u95f2\u804a\u5bf9\u8bdd\uff09\u8bad\u7ec3\u3002 \u201d\u5fc5\u8981\u201c\u662f\u4ec0\u4e48\u610f\u601d\uff1f \u7b54\uff1a\u7a0b\u5e8f\u5206\u4e3a\u6570\u636e\u6e05\u6d17\uff0c\u6a21\u578b\u8bad\u7ec3\u4e0e\u6a21\u578b\u9884\u6d4b\u4e09\u4e2a\u90e8\u5206\uff0c\u4ee5\u53ca\u5927\u91cf\u56fa\u5316\u7684\u4e8c\u8fdb\u5236\u6587\u4ef6\u7b49\u3002\u4f46\u4f60\u8fd0\u884c\u5b83\u53ea\u9700\u8981\u5176\u4e2d\u4e00\u5c0f\u90e8\u5206\uff0c\u5305\u62ec\u9884\u6d4b\u7a0b\u5e8f\u3001\u90e8\u5206\u4e8c\u8fdb\u5236\u6587\u4ef6\u548c\u4e00\u4e2a\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u3002 \u6211\u8be5\u600e\u4e48\u8fd0\u884c\u5b83\uff1f \u7b54\uff1a \u56e0\u4e3a\u6ca1\u6709\u6253\u5305exe\uff0c\u6240\u4ee5\u4f60\u76f4\u63a5\u8fd0\u884cmain.py\u5c31\u597d\u4e86\u3002 python .\\main.py \u81ea\u5df1\u8bad\u7ec3\u7684\u8bdd\u9700\u8981\u591a\u4e45\uff1f \u7b54\uff1a\u9996\u5148\u5efa\u8bae\u4f60\u6709\u4e00\u5f20\u6027\u80fd\u597d\u7684\u663e\u5361\u3002\u6211\u5728Google Colaboratory\u7528GPU + 12GRAM\uff08\u4e2d\u9014\u4f1a\u7206\u5185\u5b581~2\u6b21\uff09\u8bad\u7ec3\u5927\u6982200\u4e2aepoch\uff0c\u5355\u6b21350s\uff08\u4e0d\u91c7\u7528\u6ce8\u610f\u529b\u673a\u5236\u7684\u8bdd\u4e3a500s\uff09\uff0c\u4e2d\u9014\u5927\u6982\u4e24\u6b21\u964d\u4f4e\u5b66\u4e60\u7387\u3002 \u9700\u8981\u8054\u7f51\u4e48\uff1f \u7b54\uff1a \u53ef\u4ee5\u5728\u8054\u7f51\u72b6\u6001\u4e0b\u67e5\u67d0\u5730\u5929\u6c14\uff0c\u4e0d\u8fc7\u6bd4\u8f83\u65e0\u8111\uff08\u8c03\u7528\u522b\u4eba\u7684api\uff09\uff0c\u6240\u4ee5\u6211\u5c31\u53bb\u6389\u4e86\u3002\u73b0\u5728\u7eaf\u5355\u673a\u5373\u53ef\u8fd0\u884c\u3002 \u9700\u8981\u7684\u8fd0\u884c\u73af\u5883\u662f\u4ec0\u4e48\uff1f \u7b54\uff1apython3.6\u4ee5\u4e0a\uff0cTensorflow\uff08\u4e0a\u9762\u63d0\u5230\u6211\u7528\u7684keras\u4e86\uff09\u3002\u76f4\u63a5pip install tensorflow\uff0c\u4e0d\u51fa\u610f\u5916\u7684\u8bdd\u662f\u53ef\u4ee5\u5b89\u88c5\u6210\u529f\u7684\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/bysmss/Tensorflow-/blob/d0d9543be4be140a24346d5933bdd72ee63e1774/models/W--184-0.5949-.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff1"}, "repo_url": "https://github.com/hsohal5/humpback_whale_identification", "repo_name": "humpback_whale_identification", "repo_full_name": "hsohal5/humpback_whale_identification", "repo_owner": "hsohal5", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T00:33:35Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T00:29:23Z", "homepage": null, "size": 732, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188502633, "is_fork": false, "readme_text": "Humpback whale Identificcation Abstract This report describes about the current work done on whale photo surveillance system. We are developing a deep learning model using convolutional neural network to be used to identify the type of humpback whale. The whale species are identified by the light and dark pigmentation patches on their tails. The data set it taken from Kaggle competition. File Description: PHash.py : In this file, we remove the duplicate images using phash algorithm. imageaugumentation.py : This file is for creating more images of classes which has less than 15sampels per class. cropimage.py : This is for cropping the images.. there is an extra water portion,so that extra water portion has to cut and only the fluke is present. model.py : We build the model using CNN and ResNets. Libraries: numpy; pandas; opencv; sklearn; scipy; matplotlib; keras; Tensorflow; ResNets; ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff2"}, "repo_url": "https://github.com/OussemaHdr/sign-rec", "repo_name": "sign-rec", "repo_full_name": "OussemaHdr/sign-rec", "repo_owner": "OussemaHdr", "repo_desc": "Self driving car prototype using RaspberryPi 3 with traffic sign recognition using Keras/Tensorflow", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T04:51:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T16:53:09Z", "homepage": "", "size": 12471, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188591159, "is_fork": false, "readme_text": "Self driving car Project \"Mercury\" Traffic sign recognition Mercury is a WIP prototype of a self driving car, the first model focuses on traffic sign recognition using Deep Learning (CNN). Dataset : German Traffic Sign Dataset Here's an overview of Mercury 1.0 :  Signs the AI currently recognizes :  NB : The AI is only trained with 5 signs (50, 120, Right, Left, Stop) but the variation the dataset allows it to recognize all the signs displayed above. 1. Dependencies  Python3.7, Jupyter, NumPy, SciPy, TensorFlow, Keras, Matplotlib, Pandas, OpenCV, Skimage, pillow OS (RaspberryPi) : Raspbian  2. Hardware  RaspberryPi 3 B+, 8MP PiCamera, 2DC motors, L298n h-bridge, 2 KY 033 optic sensors  NB : We used optic sensors to keep the robot movement in a straight line, if you are planing to use 4 DC motors or other alternatives you won't need them Video demonstration Data processing and model training RaspberryPi comand ", "has_readme": true, "readme_language": "English", "repo_tags": ["cnn-keras", "cnn-classification", "traffic-sign-classification", "raspberry-pi-3", "self-driving-car"], "has_h5": true, "h5_files_links": ["https://github.com/OussemaHdr/sign-rec/blob/4ac4f7fe75e931c26156016f08de99d56d06d6bd/SignRec-CNN.h5"], "see_also_links": ["http://benchmark.ini.rub.de/?section=gtsrb&subsection=dataset"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff3"}, "repo_url": "https://github.com/CerberusVT/Spanish-review-film-classifier", "repo_name": "Spanish-review-film-classifier", "repo_full_name": "CerberusVT/Spanish-review-film-classifier", "repo_owner": "CerberusVT", "repo_desc": "This project is based on a deep learning model.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T13:42:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T07:48:44Z", "homepage": null, "size": 83578, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188377515, "is_fork": false, "readme_text": " Spanish-review-film-classifier This project is based on a deep learning model. If you download or clone this repository you will need download two files to build the model or graphics. Create a folder that is called reviews_for_model, download and save this file in it: https://drive.google.com/open?id=1r6IyGPWhpIH3_-PuFY9VWZbYhOBiTO_w Create a folder that is called reviews_for_model, download and save this file in it: https://drive.google.com/open?id=1rqJ5oprMNjWdNXG-qzxmn8ubN4qezTJ1 If you wanna build a docker container put all the files of Docker folder into the root directory and finally execute:  docker-compose build docker-compose up  For launch the website in local:   Download this python libraries  django 2.2.1 tensorflow 1.13.1 sqlparse 0.30.0 keras 2.2.4 nltk 3.4.1    Get in the django-website folder and execute in terminal:  python manage.py runserver    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/CerberusVT/Spanish-review-film-classifier/blob/ee52f6965620cf9cddeacd69e188c010ebe78d5d/django-website/classifier/model/model_django.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff4"}, "repo_url": "https://github.com/Shivani-Solanki/CNN_RESNET50_Humpback_Whale_identification", "repo_name": "CNN_RESNET50_Humpback_Whale_identification", "repo_full_name": "Shivani-Solanki/CNN_RESNET50_Humpback_Whale_identification", "repo_owner": "Shivani-Solanki", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T00:08:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T00:05:50Z", "homepage": null, "size": 731, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188501103, "is_fork": false, "readme_text": "Humpback whale Identificcation #Abstract This report describes about the current work done on whale photo surveillance system. We are developing a deep learning model using convolutional neural network to be used to identify the type of humpback whale. The whale species are identified by the light and dark pigmentation patches on their tails. The data set it taken from Kaggle competition. File Description: PHash.py : In this file, we remove the duplicate images using phash algorithm. imageaugumentation.py : This file is for creating more images of classes which has less than 15sampels per class. cropimage.py : This is for cropping the images.. there is an extra water portion,so that extra water portion has to cut and only the fluke is present. model.py : We build the model using CNN and ResNets. Libraries: numpy; pandas; opencv; sklearn; scipy; matplotlib; keras; Tensorflow; ResNets; ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff5"}, "repo_url": "https://github.com/dungvt93/auto_keras", "repo_name": "auto_keras", "repo_full_name": "dungvt93/auto_keras", "repo_owner": "dungvt93", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T05:15:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T03:59:27Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188349494, "is_fork": false, "readme_text": "auto_keras  auto find good network for image classification  requite autokeras only support over python 3.6 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff6"}, "repo_url": "https://github.com/OmarSayedMostafa/Image-Classification-with-small-size-train-set-data-using-learning-ternsfare", "repo_name": "Image-Classification-with-small-size-train-set-data-using-learning-ternsfare", "repo_full_name": "OmarSayedMostafa/Image-Classification-with-small-size-train-set-data-using-learning-ternsfare", "repo_owner": "OmarSayedMostafa", "repo_desc": "image classification for small size train set for each class", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T14:11:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T11:31:07Z", "homepage": null, "size": 15979, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188410550, "is_fork": false, "readme_text": "Image-Classification-with-small-size-train-set-data Training an Image Classification model - even with Deep Learning - is not an easy task. In order to get sufficient accuracy, without overfitting requires a lot of training data. If you try to train a deep learning model from scratch, and hope build a classification system with similar level of capability of an ImageNet-level model, then you'll need a dataset of about a million training examples (plus, validation examples also). Needless to say, it's not easy to acquire, or build such a dataset practically. Luckily, Deep Learning supports an immensely useful feature called 'Transfer Learning'. Basically, you are able to take a pre-trained deep learning model - which is trained on a large-scale dataset such as ImageNet - and re-purpose it to handle an entirely different problem. The idea is that since the model has already learned certain features from a large dataset, it may be able to use those features as a base to learn the particular classification problem we present it with. This task is further simplified since popular deep learning models such as VGG16 and their pre-trained ImageNet weights are readily available. The Keras framework even has them built-in in the keras.applications package. The basic technique to get transfer learning working is to get a pre-trained model (with the weights loaded) and remove final fully-connected layers from that model. We then use the remaining portion of the model as a feature extractor for our smaller dataset. These extracted features are called \"Bottleneck Features\" (i.e. the last activation maps before the fully-connected layers in the original model). We then train a small fully-connected network on those extracted bottleneck features in order to get the classes we need as outputs for our problem. Make sure all the sub-directories (classes) in the training set are present in the validation set also. And, remember that the names of the sub-directories will be the names of your classes. In order to build out model, we need to go through the following steps, 1-Save the bottleneck features from the VGG16 model. 2-Train a small network using the saved bottleneck features to classify our classes, and save the model (we call this the 'top model'). 3-Use both the VGG16 model along with the top model to make predictions. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/OmarSayedMostafa/Image-Classification-with-small-size-train-set-data-using-learning-ternsfare/blob/10962bb75f0ad8e2be62e2419330625dfc54e131/weights/52.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff7"}, "repo_url": "https://github.com/fan31415/SocialLinkPrediction", "repo_name": "SocialLinkPrediction", "repo_full_name": "fan31415/SocialLinkPrediction", "repo_owner": "fan31415", "repo_desc": "Predict links in social network graph.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T19:12:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T15:39:42Z", "homepage": null, "size": 43603, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188262601, "is_fork": false, "readme_text": "Network Link Prediction Predict links in social network graph. A methods combined by node feature embedding and nerual network classfication. Inspired by pnrl and node2vec. Requirement  python == 3.6 networkx == 2.3 numpy == 1.15.1 tensorflow == 1.10.1 keras == 2.2.2 scikit-learn == 0.19.1 gensim == 3.7.3  Requirement for embedding can be found in Reference Usage Prepare Data cd preprocessing  Mapping id in graph to continous number start from 0 python preprocess_id.py  Generate observed, hidden and testing data python main.py  Embedding cd embedding  Prepare a python2.7 environment in conda, and change to this environment.  Script  sh run.sh   Manual  source activate python2  Generate embedding only use observedEdges python src/main.py --input observedEdges.txt --output emb/embedding.emb  Predicting cd classifer  Using nerual network to predict links python main.py  Datasets All experimental datasets are public, you can find in the following links:   Facebook: J. McAuley and J. Leskovec. Learning to Discover Social Circles in Ego Networks. NIPS, 2012. https://snap.stanford.edu/data/egonets-Facebook.html   Email: R. Guimera, L. Danon, A. Diaz-Guilera, F. Giralt and A. Arenas, Physical Review E , vol. 68, 065103(R), (2003). http://deim.urv.cat/~alexandre.arenas/data/welcome.htm   Condensed Matter Collaborations: M. E. J. Newman, The structure of scientific collaboration networks, Proc. Natl. Acad. Sci. USA 98, 404-409 (2001). http://www-personal.umich.edu/~mejn/netdata/   Reference Node2Vector ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://deim.urv.cat/~alexandre.arenas/data/welcome.htm", "http://www4.comp.polyu.edu.hk/~csztwang/paper/pnrl.pdf", "http://www-personal.umich.edu/~mejn/netdata/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff8"}, "repo_url": "https://github.com/steve7158/Artificial-neural-networks", "repo_name": "Artificial-neural-networks", "repo_full_name": "steve7158/Artificial-neural-networks", "repo_owner": "steve7158", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T03:05:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T02:26:20Z", "homepage": null, "size": 259, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188146741, "is_fork": false, "readme_text": "Artificial-neural-networks This repopsitory consists of prctice file made for understanding ANN in Deep learning. The ANN model is based on the bank data of a certain bank, and the objective is to predict weather at any given point of time any employee is checked weather he will stay in the bank or leave the bank. Instruction: To run the programme follow the following steps  clone this repositiory to your local machine Install python3 depending you ar on linux, windows or mac the instalation procedure changes Similarly install pip3 pip3 install numpy pip3 install pandas pip3 install matplotlib pip3 install scipy pip3 install sklearn Now you have to install tensorflow so go to this website: https://www.tensorflow.org/install Download your tensorflow package depending upon the specs of your laptop pip3 install keras Thats about it for installation now to run your prog If you want to sheck the model based on the test set made from the dataset, keep the last two lines of test_1.py file commented. Go to your terminal and type i)cd Artificial-neural-networks/ ii)python3 test_1.py Otherwise comment the lines from 54-57 and uncomment the last 2 lines you can replace the values in the new_prediction with your custom values , keep in mind the format should be matched with variable X and dataset. Go to your terminal and type. i)cd Artificial-neural-networks/ ii)python3 test_1.py  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ff9"}, "repo_url": "https://github.com/AnonymousSubmission4NeurIPS/E3Outlier", "repo_name": "E3Outlier", "repo_full_name": "AnonymousSubmission4NeurIPS/E3Outlier", "repo_owner": "AnonymousSubmission4NeurIPS", "repo_desc": "Effective End-to-end Unsupervised Outlier Detection.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T14:53:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T14:48:28Z", "homepage": null, "size": 27, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188578205, "is_fork": false, "readme_text": "Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network Submitted to NeurIPS 2019 by anonymous authors. Introduction This is the official implementation of the E3Outlier framework presented by \"Effective End-to-end Unsupervised Outlier Detection via Inlier Priority of Discriminative Network\". The codes are used to reproduce experimental results of  E3Outlier and other unsupervised outlier detection (UOD) methods reported in the paper. Requirements  Python 3.6 PyTorch 0.4.1 (GPU) Keras 2.2.0 Tensorflow 1.8.0 (GPU) sklearn 0.19.1  Usage To obtain the results of E3Outlier and other UOD methods compared in the paper with default settings, simply run the following command: python outlier_experiments.py This will automatically run all UOD methods reported in the manuscript.  Please see outlier_experiments.py for more details. After training, to print UOD results for a specific algorithm in AUROC/AUPR, run: # AUROC of E3Outlier on CIFAR10 with outlier ratio 0.1 python evaluate_roc_auc.py --dataset cifar10 --algo_name e3outlier-0.1  # AUPR of CAE-IF on MNIST with outlier ratio 0.25 and inliers as the postive class python evaluate_pr_auc.py --dataset mnist --algo_name cae-iforest-0.25 --postive inliers The algorithm names are defined in outlier_experiments.py. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ffa"}, "repo_url": "https://github.com/LabMemEL/281B_Superpixel", "repo_name": "281B_Superpixel", "repo_full_name": "LabMemEL/281B_Superpixel", "repo_owner": "LabMemEL", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T04:34:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T03:58:21Z", "homepage": null, "size": 62047, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188349396, "is_fork": false, "readme_text": "Usage Train at /Keras_srgan/, run $ python train.py --input_dir='../SR_training_datasets/BSDS200' --output_dir='./output/' --model_save_dir='./model/' --batch_size=16 --epochs=30 --number_of_images=50 --train_test_ratio=0.8 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/LabMemEL/281B_Superpixel/blob/4d90d15a89651242da72d63e054443df805593c0/Keras_srgan/model/gen_model3000.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ffb"}, "repo_url": "https://github.com/rpmcruz/hill-climbing-augmentation", "repo_name": "hill-climbing-augmentation", "repo_full_name": "rpmcruz/hill-climbing-augmentation", "repo_owner": "rpmcruz", "repo_desc": "Automatic augmentation by hill climbing", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T15:22:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T11:31:44Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188558193, "is_fork": false, "readme_text": "Automatic Augmentation by Hill Climbing When doing data augmentation for images, it is not always intuitive for the user how much shear or translation to apply. In this proposal, the model is always being trained in two branches, one branch having more intensive augmentation than the other. After a period (e.g. a epoch), the two branches are evaluated using the validation. The weaker branch is eliminated, and the stronger branch becomes the parent of the following two branches.  Ricardo Cruz, Joaquim F. Pinto Costa, and Jaime S. Cardoso. \"Automatic Augmentation by Hill Climbing.\" International Conference on Artificial Neural Networks. Springer, Cham, 2019. (in revision)  The method is fairly easy to implement. In any case, this is how we implemented it in Keras. We saved and loaded models from the disk as necessary. The code implements both a classifier and an U-Net architecture (mymodels.py) and uses our method in train.py and random and Bayesian search is implemented in train_random.py. The data used is referenced in the paper. Please email me if you have problems with anything. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ffc"}, "repo_url": "https://github.com/sbouchardet/ml-engine-cookiecutter", "repo_name": "ml-engine-cookiecutter", "repo_full_name": "sbouchardet/ml-engine-cookiecutter", "repo_owner": "sbouchardet", "repo_desc": "cookiecutter to create algorithm project that runs on Google's ML Engine", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T14:20:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T14:23:48Z", "homepage": "", "size": 16, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188575425, "is_fork": false, "readme_text": "ml-engine-cookiecutter This cookiecutter will help you to build an algorithm to run on Google ML Engine. Setup Install the cookiecutter pip install cookiecutter How to use To generate your ML Engine package cookiecutter https://github.com/sbouchardet/ml-engine-cookiecutter.git Parameters  project_name: name of project that will contain the algorithm project_description: brief description of the project project_author_name: author's name project_author_email: author's email algorithm_name: name of the algorithm that you will deploy to Google's ML Engine ml_engine: Choosen engine to implement the algorithm. Today Google's ML Engine suport only Ternsorflow, Keras, PyTorch, Scikit Learn and XGboost. gs_region: The region where the job will run on Google's ML Engine. The suported regions can be found in the link gs_bucket: The bucket from Google Storage where the input and output data for the job is. gs_project: Google Cloud Project's name. google_credential_filename: name of the file of google application credential inside the project.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ffd"}, "repo_url": "https://github.com/sameerbairwa/Emojinator-", "repo_name": "Emojinator-", "repo_full_name": "sameerbairwa/Emojinator-", "repo_owner": "sameerbairwa", "repo_desc": "Emojis are ideograms and smileys used in electronic messages and web pages.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T19:51:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T08:11:55Z", "homepage": "", "size": 50586, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188539069, "is_fork": false, "readme_text": "Emojinator Description  Emojis are ideograms and smileys used in electronic messages and web pages. Emoji exist in various genres, including facial expressions, common objects, places and types of weather, and animals. They are much like emoticons, but emoji are actual pictures instead of typographics.  Code Requirements  You can install Conda for python which resolves all the dependencies for machine learning. numpy  matplotlib  cv2  keras  pandas  h5py  scipy  Procedure 1.First, you have to create a gesture database. For that, run CreateGest.py. Enter the gesture name and you will get 2 frames displayed. Look at the contour frame and adjust your hand to make sure that you capture the features of your hand. Press 'c' for capturing the images. It will take 1200 images of one gesture. Try moving your hand a little within the frame to make sure that your model doesn't overfit at the time of training. 2.Repeat this for all the features you want.  3.Run CreateCSV.py for converting the images to a CSV file  4.If you want to train the model, run 'TrainEmojinator.py'  5.Finally, run Emojinator.py for testing your model via webcam.  Functionalities  Filters to detect hand.  CNN for training the model.   Python Implementation   Network Used- Convolutional Neural Network  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/sameerbairwa/Emojinator-/blob/22537043e7202ddb99319620a63b2bcf9f4bdc7c/emojinator.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6ffe"}, "repo_url": "https://github.com/mshr-h/tf2_examples", "repo_name": "tf2_examples", "repo_full_name": "mshr-h/tf2_examples", "repo_owner": "mshr-h", "repo_desc": "Code examples of Deep Neural Networks leveraging TensorFlow 2.0 Core functions", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T14:26:20Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T15:48:11Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188449120, "is_fork": false, "readme_text": "tf2_examples This repo contains code examples of Deep Neural Networks leveraging TensorFlow 2.0 Core functions. We'll use  tf.keras: High level API for building deep neural network models tf.function: Pre-compile computational graph to get better performance tf.GradientTape: Records gradients for automatic differentiation and back-propagation tensorflow_datasets: Collection of ready to use datasets for TensorFlow  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f6fff"}, "repo_url": "https://github.com/xingweixiang/pyDataExcavate", "repo_name": "pyDataExcavate", "repo_full_name": "xingweixiang/pyDataExcavate", "repo_owner": "xingweixiang", "repo_desc": "\u6570\u636e\u5206\u6790\u3001\u6316\u6398\uff0c\u673a\u5668\u5b66\u4e60\u7b14\u8bb0\uff0c\u9879\u76ee\u4e0d\u505a\u4e3a\u5546\u4e1a\u6027\u8d28\uff0c\u53ea\u4f9b\u81ea\u8eab\u5b66\u4e60\u4f7f\u7528\u3002", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-06-01T17:20:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T14:01:22Z", "homepage": "", "size": 25686, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188573050, "is_fork": false, "readme_text": "\u8bf4\u660e \u672c\u9879\u76ee\u6240\u6709\u4ee3\u7801\u5728\u672c\u5730\u8c03\u8bd5\u901a\u8fc7  \u8fd0\u884c\u73af\u5883  python 3.7.*  requirements numpy pandas matplotlib mkl scipy statsmodels xlrd xlwt scikit-learn --default-timeout=500 pywt PyWavelets keras --default-timeout=500 tensorflow --default-timeout=500 pillow  gensim  \u76ee\u5f55  \u6570\u636e\u5206\u6790\u4e0e\u6316\u6398  \u4e00\u3001\u6570\u636e\u6316\u6398\u57fa\u7840  1\u3001\u6570\u636e\u6316\u6398\u7684\u57fa\u672c\u4efb\u52a1 2\u3001\u6570\u636e\u6316\u6398\u5efa\u6a21\u8fc7\u7a0b   \u4e8c\u3001python\u6570\u636e\u5206\u6790\u5de5\u5177  1\u3001Numpy 2\u3001Scipy 3\u3001matplotlib 4\u3001Pandas 5\u3001StatsModels 6\u3001scikit-Learn 7\u3001Keras 8\u3001Gensim   \u4e09\u3001\u6570\u636e\u63a2\u7d22  1\u3001\u6570\u636e\u8d28\u91cf\u5206\u6790 2\u3001\u6570\u636e\u7279\u5f81\u5206\u6790 3\u3001\u6570\u636e\u63a2\u7d22\u4e3b\u8981\u51fd\u6570   \u56db\u3001\u6570\u636e\u9884\u5904\u7406  1\u3001\u6570\u636e\u6e05\u6d17 2\u3001\u6570\u636e\u96c6\u6210 3\u3001\u6570\u636e\u53d8\u6362 4\u3001\u6570\u636e\u89c4\u7ea6 5\u3001\u9884\u5904\u7406\u4e3b\u8981\u51fd\u6570   \u4e94\u3001\u6316\u6398\u5efa\u6a21  1\u3001\u5206\u7c7b\u4e0e\u9884\u6d4b 2\u3001\u805a\u7c7b\u5206\u6790 3\u3001\u5173\u8054\u89c4\u5219 4\u3001\u65f6\u5e8f\u6a21\u5f0f 5\u3001\u79bb\u7fa4\u70b9\u68c0\u6d4b   \u516d\u3001\u7535\u529b\u7a83\u6f0f\u7535\u7528\u6237\u81ea\u52a8\u8bc6\u522b  1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 3\u3001\u6a21\u578b\u6784\u5efa   \u4e03\u3001\u822a\u7a7a\u516c\u53f8\u5ba2\u6237\u4ef7\u503c\u5206\u6790  7-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 7-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 7-3\u3001\u6a21\u578b\u6784\u5efa   \u516b\u3001\u4e2d\u533b\u8bc1\u578b\u5173\u8054\u89c4\u5219\u6316\u6398  8-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 8-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 8-3\u3001\u6a21\u578b\u6784\u5efa   \u4e5d\u3001\u57fa\u4e8e\u6c34\u8272\u56fe\u50cf\u7684\u6c34\u8d28\u8bc4\u4ef7  9-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 9-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 9-3\u3001\u6a21\u578b\u6784\u5efa   \u5341\u3001\u5bb6\u7528\u7535\u5668\u7528\u6237\u884c\u4e3a\u5206\u6790\u4e0e\u4e8b\u4ef6\u8bc6\u522b  10-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 10-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 10-3\u3001\u6a21\u578b\u6784\u5efa 10-4\u3001\u6a21\u578b\u68c0\u9a8c   \u5341\u4e00\u3001\u5e94\u7528\u7cfb\u7edf\u8d1f\u8f7d\u5206\u6790\u4e0e\u78c1\u76d8\u5bb9\u91cf\u9884\u6d4b  11-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 11-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 11-3\u3001\u6a21\u578b\u6784\u5efa 11-4\u3001\u6a21\u578b\u8bc4\u4ef7   \u5341\u4e8c\u3001\u7535\u5b50\u5546\u52a1\u7f51\u7ad9\u7528\u6237\u884c\u4e3a\u5206\u6790\u53ca\u670d\u52a1\u63a8\u8350  12-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 12-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 12-3\u3001\u6a21\u578b\u6784\u5efa   \u5341\u4e09\u3001\u8d22\u653f\u6536\u5165\u5f71\u54cd\u56e0\u7d20\u5206\u6790\u53ca\u9884\u6d4b\u6a21\u578b  13-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 13-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 13-3\u3001\u6a21\u578b\u6784\u5efa   \u5341\u56db\u3001\u57fa\u4e8e\u57fa\u7ad9\u5b9a\u4f4d\u6570\u636e\u7684\u5546\u5708\u5206\u6790  14-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 14-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 14-3\u3001\u6a21\u578b\u6784\u5efa   \u5341\u4e94\u3001\u7535\u5546\u4ea7\u54c1\u8bc4\u8bba\u6570\u636e\u60c5\u611f\u5206\u6790  15-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 15-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 15-3\u3001\u6a21\u578b\u6784\u5efa   \u5341\u516d\u3001\u4f01\u4e1a\u5077\u6f0f\u7a0e\u8bc6\u522b\u6a21\u578b  16-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807 16-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406 16-3\u3001\u6a21\u578b\u6784\u5efa 16-4\u3001\u6a21\u578b\u8bc4\u4ef7      \u4e00\u3001\u6570\u636e\u6316\u6398\u57fa\u7840 1\u3001\u6570\u636e\u6316\u6398\u7684\u57fa\u672c\u4efb\u52a1  \u57fa\u672c\u4efb\u52a1\u5305\u542b\uff1a\u5206\u7c7b\u548c\u9884\u6d4b\u3001\u805a\u7c7b\u5206\u6790\u3001\u5173\u8054\u89c4\u5219\u3001\u65f6\u5e8f\u6a21\u5f0f\u3001\u504f\u5dee\u68c0\u9a8c\u3001\u667a\u80fd\u63a8\u8350\u7b49\u3002\u4ece\u6570\u636e\u4e2d\u63d0\u53d6\u5546\u4e1a\u4ef7\u503c\u3002  2\u3001\u6570\u636e\u6316\u6398\u5efa\u6a21\u8fc7\u7a0b  \u5b9a\u4e49\u6316\u6398\u7684\u76ee\u6807 \u660e\u786e\u6316\u6398\u76ee\u6807\uff0c\u660e\u786e\u5b8c\u6210\u6548\u679c\u3002\u9700\u8981\u5206\u6790\u5e94\u7528\u76ee\u6807\u4e86\u89e3\u7528\u6237\u9700\u6c42\u3002 \u6570\u636e\u53d6\u6837 \u53d6\u6837\u6807\u51c6\uff1a\u76f8\u5173\u6027\u3001\u53ef\u9760\u6027\u3001\u6709\u6548\u6027\u3002\u4e0d\u53ef\u5ffd\u89c6\u6570\u636e\u7684\u8d28\u91cf\u95ee\u9898\uff0c\u8861\u91cf\u6570\u636e\u8d28\u91cf\u7684\u65b9\u5f0f\uff1a \u8d44\u6599\u7684\u5b8c\u6574\u65e0\u7f3a\uff0c\u6307\u6807\u9879\u9f50\u5168\u3002 \u6570\u636e\u7684\u51c6\u786e\u65e0\u8bef\uff0c\u4e3a\u6b63\u5e38\u6570\u636e\uff0c\u4e0d\u5728\u5f02\u5e38\u6307\u6807\u72b6\u6001\u7684\u6c34\u5e73\u3002 \u6570\u636e\u62bd\u6837\u7684\u65b9\u5f0f\uff1a \u968f\u673a\u62bd\u6837 \u7b49\u8ddd\u62bd\u6837 \u5206\u5c42\u62bd\u6837 \u4ece\u8d77\u59cb\u987a\u5e8f\u62bd\u6837 \u5206\u7c7b\u62bd\u6837 \u6570\u636e\u63a2\u7d22 \u5bf9\u6837\u672c\u6570\u636e\u7684\u63d0\u524d\u63a2\u7d22\u548c\u9884\u5904\u7406\u662f\u4fdd\u8bc1\u6a21\u578b\u8d28\u91cf\u7684\u91cd\u8981\u6761\u4ef6\u3002 \u6570\u636e\u63a2\u7d22\u4e3b\u8981\u5305\u62ec\uff1a\u5f02\u5e38\u503c\u5206\u6790\u3001\u7f3a\u5931\u503c\u5206\u6790\u3001\u76f8\u5173\u5206\u6790\u3001\u5468\u671f\u6027\u5206\u6790\u7b49\u3002 \u6570\u636e\u9884\u5904\u7406 \u91c7\u96c6\u6570\u636e\u8fc7\u5927\u7684\u65f6\u5019\uff0c\u600e\u4e48\u964d\u7ef4\u5904\u7406\uff0c\u600e\u4e48\u505a\u7f3a\u5931\u503c\u5904\u7406\u90fd\u662f\u4f7f\u7528\u6570\u636e\u9884\u5904\u7406\u6765\u5b8c\u6210\u7684\u3002\u91c7\u6837\u7684\u6570\u636e\u4e2d\u540c\u6837\u4f1a\u6709\u566a\u58f0\uff0c\u4e0d\u5b8c\u6574\uff0c\u4e0d\u4e00\u81f4\u7684\u6570\u636e\u4e5f\u9700\u8981\u505a\u9884\u5904\u7406\u51c6\u5907\u3002 \u6570\u636e\u9884\u5904\u7406\u4e3b\u8981\u5305\u62ec\uff1a\u6570\u636e\u7b5b\u9009\u3001\u6570\u636e\u53d8\u91cf\u8f6c\u6362\u3001\u7f3a\u5931\u503c\u5904\u7406\u3001\u574f\u6570\u636e\u5904\u7406\u3001\u6570\u636e\u6807\u51c6\u5316\u3001\u4e3b\u6210\u5206\u5206\u6790\u3001\u5c5e\u6027\u9009\u62e9\u3001\u6570\u636e\u7ea6\u89c4\u7b49\u3002 \u6316\u6398\u5efa\u6a21 \u5b8c\u6210\u4e0e\u5904\u7406\u540e\uff0c\u601d\u8003\u95ee\u9898\uff1a\u672c\u6b21\u5efa\u6a21\u662f\u5c5e\u4e8e\u6570\u636e\u6316\u6398\u4e2d\u7684\u54ea\u4e00\u7c7b(\u5206\u7c7b\u3001\u805a\u7c7b\u3001\u5173\u8054\u89c4\u5219\u3001\u65f6\u5e8f\u6a21\u5f0f\u6216\u8005\u667a\u80fd\u63a8\u8350)\uff0c\u662f\u8981\u4f7f\u7528\u4ec0\u4e48\u6837\u7684\u7b97\u6cd5\u6765\u6784\u5efa\u6a21\u578b\uff1f\u8fd9\u4e00\u6b65\u662f\u6838\u5fc3\u5173\u952e\u3002 \u6a21\u578b\u8bc4\u4ef7 \u4ece\u5efa\u6a21\u4e4b\u540e\u5f97\u51fa\u6765\u4e00\u4e9b\u5206\u6790\u7ed3\u679c\uff0c\u4ece\u800c\u627e\u5230\u4e00\u4e2a\u6700\u597d\u7684\u6a21\u578b\u3002\u505a\u51fa\u6839\u636e\u4e1a\u52a1\u5bf9\u6a21\u578b\u7684\u89e3\u91ca\u548c\u5e94\u7528\u3002  \u4e8c\u3001python\u6570\u636e\u5206\u6790\u5de5\u5177 numpy\u3001scipy\u3001matplotlib\u3001pandas\u3001statsModels\u3001scikit-Learn\u3001Kears\u3001Gensim\u3001Pillow(\u539fPIL) 1\u3001numpy  \u63d0\u4f9b\u591a\u7ef4\u6570\u7ec4\u652f\u6301\uff0c\u4ee5\u53ca\u76f8\u5e94\u7684\u9ad8\u6548\u7684\u5904\u7406\u51fd\u6570 \u57fa\u672c\u64cd\u4f5c\uff1a \u521b\u5efa\u6570\u7ec4  a = np.array([2, 0, 1, 5]) \u5f15\u7528\u524d\u4e09\u4e2a\u6570\u5b57\uff08\u5207\u7247\uff09a[:3] \u6700\u5927\u6700\u5c0f\u503c  a.max() a.min() \u4ece\u5c0f\u5230\u5927\u6392\u5e8f\uff0c\u6b64\u64cd\u4f5c\u76f4\u63a5\u4fee\u6539\u539f\u6570\u7ec4   a.sort() Numpy\u662fPython\u4e2d\u76f8\u5f53\u6210\u719f\u548c\u5e38\u7528\u7684\u5e93\uff0c\u6559\u7a0b\u591a\uff0c\u6700\u503c\u5f97\u770b\u7684\u662f\u5b83\u5b98\u7f51\u5e2e\u52a9\u6587\u6863\u3002 \u53c2\u8003\u94fe\u63a5\uff1a http://www.numpy.org/ http://reverland.org/python/2012/08/22/numpy  2\u3001Scipy  \u63d0\u4f9b\u77e9\u9635\u652f\u6301\uff0c\u4ee5\u53ca\u77e9\u9635\u76f8\u5173\u7684\u6570\u503c\u8ba1\u7b97\u6a21\u5757\u3002\u529f\u80fd\u5305\u542b\u6709\u6700\u4f18\u5316\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u79ef\u5206\u3001\u63d2\u503c\u3001\u62df\u5408\u3001\u7279\u6b8a\u51fd\u6570\u3001\u5feb\u901f\u5085\u91cc\u53f6\u53d8\u6362\u3001\u4fe1\u53f7\u5904\u7406\u3001\u56fe\u50cf\u5904\u7406\u3001\u5e38\u5fae\u5206\u65b9\u7a0b\u6c42\u89e3 Scipy\u4f9d\u8d56Numpy \u53c2\u8003\u94fe\u63a5\uff1a https://www.scipy.org/ http://reverland.org/python/2012/08/24/scipy  3\u3001matplotlib  \u63d0\u4f9b\u5f3a\u5927\u7684\u6570\u636e\u53ef\u89c6\u5316\u5de5\u5177\u3001\u4f5c\u56fe\u5e93\uff0c\u4e3b\u8981\u7528\u4e8e\u4e8c\u7ef4\u7ed8\u56fe\u3002      #\u4e2d\u6587\u4e0d\u663e\u793a\u7684\u95ee\u9898     plt.reParams['font.sans-serif'] = ['SimHei']    # \u5982\u679c\u4e2d\u6587\u5b57\u4f53\u662fSimHei     #\u8d1f\u53f7\u663e\u793a\u4e3a\u65b9\u5757\u7684\u95ee\u9898     plt.rcParams['axes.unicode_minus']= False  \u53c2\u8003\u94fe\u63a5\uff1a http://matplotlib.org/ http://reverland.org/python/2012/09/07/matplotlib-tutorial http://matplotlib.org/gallery.html\uff08\u753b\u5eca\uff09 4\u3001Pandas  \u5f20\u5927\u7075\u6d3b\u7684\u6570\u636e\u5206\u6790\u548c\u63a2\u7d22\u5de5\u5177\uff0c\u652f\u6301\u7c7b\u4f3cSQL\u7684\u6570\u636e\u589e\u3001\u5220\u3001\u67e5\u3001\u6539\uff0c\u652f\u6301\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u3001\u7f3a\u5931\u6570\u636e\u5904\u7406\u7b49\u3002\u57fa\u672c\u7684\u6570\u636e\u7ed3\u6784\u662f Series\u548cDataFrame \u7740\u773c\u4e8e\u6570\u636e\u7684\u8bfb\u53d6\u3001\u5904\u7406\u548c\u63a2\u7d22  1. Series\uff1a     \u5e8f\u5217\u00a0 \u7c7b\u4f3c\u4e00\u7ef4\u6570\u7ec4\u00a0\u00a0     \u6709index\u7528\u4e8e\u5b9a\u4f4d\u5143\u7d20\uff0cindex\u53ef\u4ee5\u4e3a\u6570\u5b57\uff0c\u4e5f\u53ef\u4ee5\u4e3a\u5176\u4ed6\uff0c\u7c7b\u4f3c\u4e3b\u952e     \u521b\u5efa\u5e8f\u5217\u00a0 s = pd.Series([1,2,3],index = ['a','b','c'])  2. DataFrame\uff1a     \u8868\u683c\u00a0 \u7c7b\u4f3c\u4e8c\u7ef4\u6570\u7ec4     \u6bcf\u4e00\u5217\u90fd\u662f\u4e00\u4e2aseries\u00a0\u00a0 \u672c\u8d28\u662fseries\u7684\u5bb9\u5668     \u6bcf\u4e2aseries\u6709\u552f\u4e00\u7684\u8868\u5934\uff0c\u7528\u4e8e\u533a\u5206\u00a0 \u591a\u4e2aseries\u7684index\u76f8\u540c     \u521b\u5efa\u8868\u00a0 d = pd.DataFrame([1,2,3],[4,5,6],colume = ['a','b','c'])     \u4e5f\u53ef\u4ee5\u76f4\u63a5\u00a0 d = pd.DataFrame(s)     d.head()     d.describe()     pd.read_excel('filename')     pd.read_csv('filename',encoding = 'utf-8)     \u8865\u5145\u64cd\u4f5c     pd.notnull(x)\u00a0 \u5f97\u5230x\u7684\u4e0d\u4e3a\u7a7a\u7684true\u548cfalse     x[pd.notnull(x)]\u00a0 \u53ef\u5f97\u5230x\u4e2d\u4e0d\u4e3a\u7a7a\u7684\u9879\u00a0 list\u7684\u8bdd\u53ea\u80fd\u6839\u636eint\u6765\u8fdb\u884c\u7d22\u5f15 series\u53ef\u4ee5\u901a\u8fc7true\u548cfalse     map\u63a5\u6536\u4e00\u4e2a\u5e8f\u5217 list\u6216\u8005numpy\u7684array        dataFrame\u7684\u6392\u5e8f dataFrame.sort_values(['confidence','support'], ascending = False)       \u53ef\u7528dataFrame[[index1,index2]] \u8bbf\u95ee    \u53c2\u8003\u94fe\u63a5\uff1a http://pandas.pydata.org/pandas-docs/stable http://jingyan.baidu.com/season/43456 5\u3001StatsModels  \u7edf\u8ba1\u5efa\u6a21\u548c\u8ba1\u91cf\u7ecf\u6d4e\u5b66\uff0c\u5305\u62ec\u63cf\u8ff0\u7edf\u8ba1\u3001\u7edf\u8ba1\u6a21\u578b\u4f30\u8ba1\u548c\u63a8\u65ad\u3002\u7740\u773c\u4e8e\u6570\u636e\u7684\u7edf\u8ba1\u5efa\u6a21\u5206\u6790\uff0c\u652f\u6301\u4e0ePandas\u8fdb\u884c\u6570\u636e\u4ea4\u4e92 \u4f9d\u8d56\u4e8ePandas\u548cpasty(\u63cf\u8ff0\u7edf\u8ba1\u7684\u5e93) \u53ef\u8fdb\u884cADF\u5e73\u7a33\u6027\u68c0\u9a8c\u3001\u767d\u566a\u58f0\u68c0\u9a8c\u7b49 \u53c2\u8003\u94fe\u63a5\uff1a http://www.statsmodels.org/stable/index.html  6\u3001scikit-Learn  \u5f3a\u5927\u7684\u673a\u5668\u5b66\u4e60\u76f8\u5173\u5e93\uff0c\u63d0\u4f9b\u5b8c\u6574\u7684\u673a\u5668\u5b66\u4e60\u5de5\u5177\u7bb1\uff0c\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001\u5206\u7c7b\u3001\u56de\u5f52\u3001\u805a\u7c7b\u3001\u9884\u6d4b\u548c\u6a21\u578b\u5206\u6790\u7b49 \u4f9d\u8d56\u4e8eNumpy\u3001SciPy\u3001Matplotlib  1. \u6240\u6709\u6a21\u578b\u7684\u63a5\u53e3\uff1a     model.fit()\uff1a\u8bad\u7ec3\u6570\u636e\u00a0 fit\uff08X,y\uff09\u2014\u2014\u76d1\u7763\u5b66\u4e60\u00a0\u00a0\u00a0 fit\uff08X\uff09\u2014\u2014\u975e\u76d1\u7763\u5b66\u4e60 2. \u76d1\u7763\u5b66\u4e60     model.predict(x_new) \u9884\u6d4b\u65b0\u6837\u672c     model.predict_proba(x_new) \u9884\u6d4b\u6982\u7387     model.score() \u5f97\u5206\u8d8a\u9ad8\u8d8a\u597d 3. \u975e\u76d1\u7763\u5b66\u4e60     model.transform()\u00a0 \u4ece\u6570\u636e\u4e2d\u5b66\u4e60\u5230\u65b0\u7684\u201c\u57fa\u7a7a\u95f4\u201d     model.fit_transform()\u00a0 \u4ece\u6570\u636e\u4e2d\u5b66\u5230\u65b0\u7684\u57fa\u5e76\u5c06\u8fd9\u4e2a\u6570\u636e\u6309\u7167\u8fd9\u7ec4\u57fa\u8fdb\u884c\u8f6c\u6362  \u53c2\u8003\u94fe\u63a5\uff1a http://scikit-learn.org 7\u3001Keras  \u6df1\u5ea6\u5b66\u4e60\u5e93\uff0c\u7528\u4e8e\u5efa\u7acb\u795e\u7ecf\u7f51\u7edc\u4ee5\u53ca\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u3002\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc  \u57fa\u4e8eTheano \u4e0d\u4ec5\u53ef\u4ee5\u642d\u5efa\u666e\u901a\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u8fd8\u53ef\u4ee5\u642d\u5efa\u5404\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\uff0c\u5982\u81ea\u7f16\u7801\u5668\u3001\u5faa\u73af\u795e\u7ecf\u7f51\u7edc\u3001\u9012\u5f52\u795e\u7ecf\u7f51\u7edc\u3001\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7b49 \u4f9d\u8d56\u4e8eNumpy\u3001Scipy\u548cTheano  from keras.models import Sequential from keras.layers.core import Dense, Activation model = Sequential() #\u5efa\u7acb\u6a21\u578b model.add(Dense(input_dim = 3, output_dim = 10)) model.add(Activation('relu')) #\u7528relu\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570\uff0c\u80fd\u591f\u5927\u5e45\u63d0\u4f9b\u51c6\u786e\u5ea6 model.add(Dense(input_dim = 10, output_dim = 1)) model.add(Activation('sigmoid')) #\u7531\u4e8e\u662f0-1\u8f93\u51fa\uff0c\u7528sigmoid\u51fd\u6570\u4f5c\u4e3a\u6fc0\u6d3b\u51fd\u6570 model.compile(loss = 'binary_crossentropy', optimizer = 'adam') #\u7f16\u8bd1\u6a21\u578b\u3002\u7531\u4e8e\u6211\u4eec\u505a\u7684\u662f\u4e8c\u5143\u5206\u7c7b\uff0c\u6240\u4ee5\u6211\u4eec\u6307\u5b9a\u635f\u5931\u51fd\u6570\u4e3abinary_crossentropy\uff0c\u4ee5\u53ca\u6a21\u5f0f\u4e3abinary #\u53e6\u5916\u5e38\u89c1\u7684\u635f\u5931\u51fd\u6570\u8fd8\u6709mean_squared_error\u3001categorical_crossentropy\u7b49\uff0c\u8bf7\u9605\u8bfb\u5e2e\u52a9\u6587\u4ef6\u3002 #\u6c42\u89e3\u65b9\u6cd5\u6211\u4eec\u6307\u5b9a\u7528adam\uff0c\u8fd8\u6709sgd\u3001rmsprop\u7b49\u53ef\u9009 model.fit(x, y, nb_epoch = 1000, batch_size = 10) #\u8bad\u7ec3\u6a21\u578b\uff0c\u5b66\u4e60\u4e00\u5343\u6b21 yp = model.predict_classes(x).reshape(len(y)) #\u5206\u7c7b\u9884\u6d4b #model.predict()\u7ed9\u51fa\u6982\u7387 #model.predict_classes()\u7ed9\u51fa\u5206\u7c7b\u7ed3\u679c  \u53c2\u8003\u94fe\u63a5\uff1a http://deeplearning.net/software/theano/install.html#install https://github.com/fchollet/keras 8\u3001Gensim  \u7528\u6765\u505a\u6587\u672c\u4e3b\u9898\u6a21\u578b\u7684\u5e93\uff0c\u6587\u672c\u6316\u6398\uff0c\u5904\u7406\u8bed\u8a00\u65b9\u9762\u7684\u4efb\u52a1\uff0c\u5982\u6587\u672c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0cLDA\uff0cWord2Vec \u53c2\u8003\u94fe\u63a5\uff1a http://radimrehurek.com/gensim  \u4e09\u3001\u6570\u636e\u63a2\u7d22 \u901a\u8fc7\u68c0\u9a8c\u6570\u636e\u96c6\u7684\u6570\u636e\u8d28\u91cf\u3001\u7ed8\u5236\u56fe\u8868\u3001\u8ba1\u7b97\u7279\u5f81\u91cf\u7b49\u624b\u6bb5\uff0c\u5bf9\u6837\u672c\u6570\u636e\u96c6\u7684\u7ed3\u6784\u548c\u89c4\u5f8b\u8fdb\u884c\u5206\u6790\u7684\u8fc7\u7a0b\u5c31\u662f\u6570\u636e\u63a2\u7d22\u3002\u6570\u636e\u63a2\u7d22\u6709\u52a9\u4e8e\u9009\u62e9\u5408\u9002\u7684\u6570\u636e\u9884\u5904\u7406\u548c\u5efa\u6a21\u65b9\u6cd5\uff0c\u751a\u81f3\u53ef\u4ee5\u5b8c\u6210\u4e00\u4e9b\u901a\u5e38\u7531\u6570\u636e\u6316\u6398\u89e3\u51b3\u7684\u95ee\u9898\u3002 1\u3001\u6570\u636e\u8d28\u91cf\u5206\u6790 \u4e3b\u8981\u4efb\u52a1\u662f\u68c0\u67e5\u539f\u59cb\u6570\u636e\u4e2d\u662f\u5426\u5b58\u5728\u810f\u6570\u636e\uff0c\u5305\u62ec\u7f3a\u5931\u503c\uff0c\u5f02\u5e38\u503c\uff0c\u4e0d\u4e00\u81f4\u503c\uff0c\u91cd\u590d\u6570\u636e\u53ca\u7279\u6b8a\u7b26\u53f7\u6570\u636e  \u7f3a\u5931\u503c\u5206\u6790\uff1a\u5305\u62ec\u8bb0\u5f55\u7f3a\u5931\u548c\u8bb0\u5f55\u7684\u67d0\u5b57\u6bb5\u7f3a\u5931\u7b49 \u4ea7\u751f\u539f\u56e0\uff1a\u65e0\u6cd5\u83b7\u53d6\u3001\u9057\u6f0f\u3001\u5c5e\u6027\u503c\u4e0d\u5b58\u5728\uff1b \u5f71\u54cd\uff1a\u6709\u7528\u4fe1\u606f\u7f3a\u4e4f\u3001\u4e0d\u786e\u5b9a\u6027\u52a0\u91cd\u3001\u4e0d\u53ef\u9760 \u5904\u7406\uff1a\u5220\u9664\u3001\u8865\u5168\u3001\u4e0d\u5904\u7406 \u5f02\u5e38\u503c\uff0c\u4e0d\u5408\u5e38\u7406\u7684\u6570\u636e\uff0c\u5254\u9664\u53ef\u6d88\u9664\u4e0d\u826f\u5f71\u54cd\uff0c\u5206\u6790\u53ef\u8fdb\u884c\u6539\u8fdb\u3002\u5f02\u5e38\u503c\u5206\u6790\u4e5f\u79f0\u79bb\u7fa4\u70b9\u5206\u6790\u3002 \u5e38\u7528\u7684\u5206\u6790\u65b9\u6cd5\uff1a\u7b80\u5355\u7edf\u8ba1\u91cf\u5206\u6790(\u5982max\u3001min)\uff1b3\u03c3\u539f\u5219(99.7%)\uff1b\u7bb1\u578b\u56fe \u4e00\u81f4\u6027\u5206\u6790\uff1a\u76f4\u5c5e\u5c40\u77db\u76fe\u6027\u3001\u4e0d\u76f8\u5bb9\u6027 \u4ea7\u751f\u539f\u56e0\uff1a\u6570\u636e\u96c6\u6210\u8fc7\u7a0b\u4e2d\uff0c\u6570\u636e\u6765\u81ea\u4e0d\u540c\u6570\u636e\u6e90\uff0c\u5b58\u653e\u7b49\u672a\u80fd\u8fdb\u884c\u4e00\u81f4\u6027\u66f4\u65b0  2\u3001\u6570\u636e\u7279\u5f81\u5206\u6790  \u5206\u5e03\u5206\u6790\uff1a\u6570\u636e\u5206\u5e03\u7279\u5f81\u4e0e\u5206\u5e03\u7c7b\u578b \u5b9a\u91cf\u6570\u636e\u5206\u5e03\u5206\u6790\uff1a\u4e00\u822c\u6309\u4ee5\u4e0b\u6b65\u9aa4\uff0c\u6c42\u6781\u5dee\u2014>\u51b3\u5b9a\u7ec4\u8ddd\u548c\u7ec4\u6570\u2014>\u51b3\u5b9a\u5206\u70b9\u2014>\u5217\u9891\u7387\u5206\u5e03\u8868\u2014>\u7ed8\u9891\u7387\u5206\u5e03\u76f4\u65b9\u56fe \u5b9a\u6027\u6570\u636e\u5206\u5e03\u5206\u6790\uff1a\u91c7\u7528\u5206\u7c7b\u7c7b\u578b\u6765\u5206\u7ec4\uff0c\u7528\u997c\u56fe\u6216\u6761\u5f62\u56fe\u6765\u63cf\u8ff0\u5206\u5e03 \u5bf9\u6bd4\u5206\u6790\uff1a\u4e24\u4e2a\u6307\u6807\u8fdb\u884c\u6bd4\u8f83\uff0c\u5c55\u793a\u8bf4\u660e\u5927\u5c0f\u6c34\u5e73\u9ad8\u4f4e\uff0c\u901f\u5ea6\u5feb\u6162\uff0c\u662f\u5426\u534f\u8c03\u7b49\u3002 \u7edd\u5bf9\u6570\u6bd4\u8f83\uff1a\u662f\u5229\u7528\u7edd\u5bf9\u6570\u8fdb\u884c\u5bf9\u6bd4\uff0c\u4ece\u800c\u5bfb\u627e\u5dee\u5f02\u7684\u4e00\u79cd\u65b9\u6cd5 \u76f8\u5bf9\u6570\u6bd4\u8f83\uff1a\u7ed3\u6784\u76f8\u5bf9\u6570(\u6bd4\u91cd)\uff0c\u6bd4\u4f8b\u76f8\u5bf9\u6570(\u6bd4\u503c)\uff0c\u6bd4\u8f83\u76f8\u5bf9\u6570(\u540c\u7c7b\u4e0d\u540c\u80cc\u666f)\uff0c\u5f3a\u5ea6\u76f8\u5bf9\u6570(\u5bc6\u5ea6)\uff0c\u8ba1\u5212\u5b8c\u6210\u7a0b\u5ea6\u76f8\u5bf9\u6570\uff0c\u52a8\u6001\u76f8\u5bf9\u6570 \u7edf\u8ba1\u91cf\u5206\u6790\uff1a\u7528\u7edf\u8ba1\u6307\u6807\u5bf9\u5b9a\u91cf\u6570\u91cf\u8fdb\u884c\u7edf\u8ba1\u63cf\u8ff0 \u96c6\u4e2d\u8d8b\u52bf\uff1a\u5747\u503c\u3001\u4e2d\u4f4d\u6570\u3001\u4f17\u6570 \u79bb\u4e2d\u8d8b\u52bf\uff1a\u6781\u5dee\u3001\u6807\u51c6\u5dee\u3001\u53d8\u5f02\u7cfb\u6570(CV=\u6807\u51c6\u5dee/\u5e73\u5747\u503c*100%)\u3001\u56db\u5206\u4f4d\u6570\u95f4\u8ddd(\u4e0a\u4e0b\u56db\u5206\u4f4d\u6570\u4e4b\u5dee) \u5468\u671f\u6027\u5206\u6790\uff1a\u662f\u63a2\u7d22\u53d8\u91cf\u662f\u5426\u968f\u65f6\u95f4\u5448\u5468\u671f\u53d8\u5316\u8d8b\u52bf \u8d21\u732e\u5ea6\u5206\u6790\uff1a\u53c8\u79f0\u5e15\u7d2f\u6258\u5206\u6790\uff0c\u539f\u7406\u662f\u5e15\u7d2f\u6258\u6cd5\u5219\uff0c\u53c8\u79f020/80\u5b9a\u5f8b\u3002\u540c\u6837\u7684\u6295\u5165\u5728\u4e0d\u540c\u7684\u5730\u65b9\u4ea7\u751f\u4e0d\u540c\u7684\u6536\u76ca  #-*- coding: utf-8 -*- #\u83dc\u54c1\u76c8\u5229\u6570\u636e \u5e15\u7d2f\u6258\u56fe from __future__ import print_function import pandas as pd  #\u521d\u59cb\u5316\u53c2\u6570 dish_profit = '../data/catering_dish_profit.xls' #\u9910\u996e\u83dc\u54c1\u76c8\u5229\u6570\u636e data = pd.read_excel(dish_profit, index_col = u'\u83dc\u54c1\u540d') print(data) data = data[u'\u76c8\u5229'].copy() print('------------------') print(data) #data.sort(ascending = False) \u4f1a\u63d0\u793aAttributeError: 'Series' object has no attribute 'sort'\u4e5f\u5c31\u662f\u8bf4Series\u6ca1\u6709sorted\u8fd9\u4e2a\u65b9\u6cd5 \u5e94\u8be5\u8fd9\u6837\uff1asorted(.....) ...\u662f\u4f60\u8981\u6392\u5e8f\u7684Seri data.sort_values(ascending = False) #\u53ef\u4ee5\u7528Series.sort_values()\u65b9\u6cd5,\u5bf9Series\u503c\u8fdb\u884c\u6392\u5e8f\u3002 import matplotlib.pyplot as plt #\u5bfc\u5165\u56fe\u50cf\u5e93 plt.rcParams['font.sans-serif'] = ['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e plt.rcParams['axes.unicode_minus'] = False #\u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7  plt.figure() plt.title('\u5e15\u7d2f\u6258\u56fe'); data.plot(kind='bar') plt.ylabel(u'\u76c8\u5229\uff08\u5143\uff09') p = 1.0*data.cumsum()/data.sum() p.plot(color = 'r', secondary_y = True, style = '-o',linewidth = 2) plt.annotate(format(p[6], '.4%'), xy = (6, p[6]), xytext=(6*0.9, p[6]*0.9), arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\")) #\u6dfb\u52a0\u6ce8\u91ca\uff0c\u537385%\u5904\u7684\u6807\u8bb0\u3002\u8fd9\u91cc\u5305\u62ec\u4e86\u6307\u5b9a\u7bad\u5934\u6837\u5f0f\u3002 plt.ylabel(u'\u76c8\u5229\uff08\u6bd4\u4f8b\uff09') plt.show()    \u76f8\u5173\u6027\u5206\u6790\uff1a\u5206\u6790\u8fde\u7eed\u53d8\u91cf\u4e4b\u95f4\u7ebf\u6027\u76f8\u5173\u7a0b\u5ea6\u7684\u5f3a\u5f31\uff0c\u5e76\u7528\u9002\u5f53\u7684\u7edf\u8ba1\u6307\u6807\u8868\u793a\u51fa\u6765 \u76f4\u63a5\u7ed8\u5236\u6563\u70b9\u56fe \u7ed8\u5236\u6563\u70b9\u56fe\u77e9\u9635\uff0c\u5bf9\u591a\u4e2a\u53d8\u91cf\u4e24\u4e24\u5173\u7cfb\u7684\u6563\u70b9\u56fe \u8ba1\u7b97\u76f8\u5173\u7cfb\u6570  3\u3001\u6570\u636e\u63a2\u7d22\u4e3b\u8981\u51fd\u6570  \u4e3b\u8981\u662fPandas\u7528\u4e8e\u6570\u636e\u5206\u6790\u548cMatplotlib\u7528\u4e8e\u6570\u636e\u53ef\u89c6\u5316 Pandas\u4e3b\u8981\u7edf\u8ba1\u7279\u5f81\u51fd\u6570 sum \u8ba1\u7b97\u6570\u636e\u6837\u672c\u7684\u603b\u548c(\u6309\u5217) mean \u8ba1\u7b97\u6570\u636e\u6837\u672c\u7684\u7b97\u6570\u5e73\u5747\u503c var \u8ba1\u7b97\u6570\u636e\u6837\u672c\u7684\u65b9\u5dee std \u6807\u51c6\u5dee corr \u8ba1\u7b97\u6570\u636e\u6837\u672c\u7684Spearman/Pearson\u76f8\u5173\u7cfb\u6570\u77e9\u9635 cov \u8ba1\u7b97\u6570\u636e\u6837\u672c\u7684\u534f\u65b9\u5dee\u77e9\u9635 skew \u6837\u672c\u503c\u7684\u504f\u5ea6(\u4e09\u9636\u77e9) kurt \u6837\u672c\u503c\u7684\u5cf0\u5ea6(\u56db\u9636\u77e9) describe() \u7ed9\u51fa\u6837\u672c\u7684\u57fa\u672c\u63cf\u8ff0(\u5982\u5747\u503c\u3001\u6807\u51c6\u5dee\u7b49) Pandas\u7d2f\u79ef\u7edf\u8ba1\u7279\u5f81\u51fd\u6570 cumsum \u4f9d\u6b21\u7ed9\u51fa\u524d1-n\u4e2a\u6570\u7684\u548c cumprod \u4f9d\u6b21\u7ed9\u51fa\u524d1-n\u4e2a\u6570\u7684\u79ef cummax \u4f9d\u6b21\u7ed9\u51fa\u524d1-n\u4e2a\u6570\u7684\u6700\u5927\u503c cummin \u4f9d\u6b21\u7ed9\u51fa\u524d1-n\u4e2a\u6570\u7684\u6700\u5c0f\u503c \u7edf\u8ba1\u4f5c\u56fe\u51fd\u6570\uff0c\u57fa\u4e8eMatplotlib plot \u7ed8\u5236\u7ebf\u6027\u4e8c\u7ef4\u56fe\uff0c\u6298\u7ebf\u56fe pie \u7ed8\u5236\u997c\u56fe hist \u7ed8\u5236\u4e8c\u7ef4\u6761\u5f62\u76f4\u65b9\u56fe boxplot \u7ed8\u5236\u7bb1\u578b\u56fe Pandas plot(logy=True) \u7ed8\u5236y\u8f74\u7684\u5bf9\u6570\u56fe\u5f62 Pandas plot(yerr=error) \u7ed8\u5236\u8bef\u5dee\u6761\u5f62\u56fe Pandas  \u56db\u3001\u6570\u636e\u9884\u5904\u7406 \u6570\u636e\u9884\u5904\u7406\uff1a\u4e3b\u8981\u5305\u62ec\u6570\u636e\u6e05\u6d17\uff0c\u6570\u636e\u96c6\u6210\uff0c\u6570\u636e\u53d8\u6362\u548c\u6570\u636e\u89c4\u7ea6 1\u3001\u6570\u636e\u6e05\u6d17  \u4e3b\u8981\u662f\u5220\u9664\u539f\u59cb\u6570\u636e\u96c6\u4e2d\u7684\u65e0\u5173\u6570\uff0c\u91cd\u590d\u6570\u636e\uff0c\u5e73\u6ed1\u566a\u58f0\u6570\u636e\uff0c\u7b5b\u9009\u6389\u4e0e\u6316\u6398\u4e3b\u9898\u65e0\u5173\u7684\u6570\u636e\uff0c\u5904\u7406\u7f3a\u5931\u503c\uff0c\u5f02\u5e38\u503c\u7b49 \u7f3a\u5931\u503c\u7684\u5904\u7406\uff1a\u5220\u9664\u8bb0\u5f55\uff0c\u6570\u636e\u63d2\u8865\u548c\u4e0d\u5904\u7406\u4e09\u79cd\u65b9\u6cd5\u3002 \u6570\u636e\u63d2\u8865\u65b9\u6cd5\uff1a 1\u3001\u5747\u503c\u3001\u4e2d\u4f4d\u6570\u3001\u4f17\u6570\u63d2\u8865\uff1b 2\u3001\u4f7f\u7528\u56fa\u5b9a\u503c\u63d2\u8865\uff1a\u5c06\u7f3a\u5931\u7684\u5c5e\u6027\u503c\u7528\u4e00\u4e2a\u5e38\u91cf\u8fdb\u884c\u66ff\u6362\uff1b 3\u3001\u6700\u8fd1\u4e34\u63d2\u8865\uff1a\u5728\u8bb0\u5f55\u4e2d\u627e\u5230\u4e0e\u7f3a\u5931\u6837\u672c\u6700\u63a5\u8fd1\u7684\u6837\u672c\u7684\u8be5\u5c5e\u6027\u503c\u8fdb\u884c\u63d2\u8865\uff1b 4\u3001\u56de\u5f52\u65b9\u6cd5\uff1a\u5bf9\u5e26\u6709\u7f3a\u5931\u503c\u7684\u53d8\u91cf\uff0c\u6839\u636e\u5df2\u6709\u6570\u636e\u548c\u4e0e\u5176\u6709\u5173\u7684\u5176\u4ed6\u53d8\u91cf\u7684\u6570\u636e\u5efa\u7acb\u62df\u5408\u6a21\u578b\u8001\u9884\u6d4b\u786e\u5b9e\u7684\u5c5e\u6027\u503c\uff1b 5\u3001\u63d2\u503c\u6cd5\uff1a\u5229\u7528\u5df2\u77e5\u70b9\u5efa\u7acb\u5408\u9002\u7684\u63d2\u503c\u51fd\u6570\uff0c\u672a\u77e5\u503c\u7531\u4e22in\u7ed9\u70b9\u6c42\u51fa\u7684\u51fd\u6570\u503c\u8fd1\u4f3c\u4ee3\u66ff\u3002 \u63d2\u503c\u65b9\u6cd5\uff1a 1\u3001\u62c9\u683c\u6717\u65e5\u63d2\u503c\u6cd5\uff1a\u4f46\u662f\u5728\u5b9e\u9645\u8ba1\u7b97\u4e2d\u5f88\u4e0d\u65b9\u4fbf\u3002 2\u3001\u725b\u987f\u63d2\u503c\u6cd5\uff1a\u5177\u6709\u627f\u88ad\u6027\u548c\u6613\u4e8e\u53d8\u52a8\u8282\u70b9\u7684\u7279\u70b9\u3002 \u5728python\u7684scipy\u5e93\u4e2d\u53ea\u63d0\u4f9b\u4e86\u62c9\u683c\u6717\u65e5\u63d2\u503c\u6cd5\u7684\u51fd\u6570\uff0c\u725b\u987f\u6cd5\u9700\u8981\u81ea\u5df1\u8fdb\u884c\u7f16\u5199 \u5f02\u5e38\u503c\u5904\u7406\uff1a\u5728\u6570\u636e\u9884\u5904\u7406\u65f6\uff0c\u5f02\u5e38\u503c\u662f\u5426\u5254\u9664\uff0c\u8981\u89c6\u60c5\u51b5\u800c\u5b9a\u3002 1\u3001\u5220\u9664\u542b\u6709\u5f02\u5e38\u503c\u7684\u8bb0\u5f55\uff1b 2\u3001\u89c6\u4e3a\u7f3a\u5931\u503c\uff1a\u5229\u7528\u7f3a\u5931\u503c\u7684\u65b9\u6cd5\u8fdb\u884c\u5904\u7406\uff1b 3\u3001\u5e73\u5747\u503c\u4fee\u6b63\uff1a\u53ef\u7528\u524d\u540e\u4e24\u4e2a\u89c2\u6d4b\u503c\u7684\u5e73\u5747\u503c\u4fee\u6b63\u6539\u5f02\u5e38\u503c\uff1b 4\u3001\u4e0d\u5904\u7406\uff1a\u76f4\u63a5\u5728\u5177\u6709\u5f02\u5e38\u503c\u7684\u6570\u636e\u4e0a\u8fdb\u884c\u6316\u6398\u5efa\u6a21  2\u3001\u6570\u636e\u96c6\u6210  \u6570\u636e\u5f80\u5f80\u5206\u5e03\u5728\u4e0d\u540c\u7684\u6570\u636e\u6e90\u4e2d\uff0c\u6570\u636e\u96c6\u6210\u5c31\u662f\u5c06\u591a\u4e2a\u6570\u636e\u6e90\u5408\u5e76\u5b58\u653e\u5728\u4e00\u4e2a\u4e00\u81f4\u7684\u6570\u636e\u5b58\u50a8\u4e2d\u7684\u8fc7\u7a0b\u3002 1\u3001\u5b9e\u4f53\u8bc6\u522b\uff1a\u4ece\u4e0d\u540c\u6570\u636e\u6e90\u8bc6\u522b\u51fa\u73b0\u5b9e\u4e16\u754c\u7684\u5b9e\u4f53\uff0c\u7edf\u4e00\u4e0d\u540c\u6e90\u6570\u636e\u7684\u77db\u76fe\u4e4b\u5904\uff1b 2\u3001\u5197\u4f59\u5c5e\u6027\u8bc6\u522b\uff1a\u6570\u636e\u96c6\u6210\u5f80\u5f80\u5bfc\u81f4\u6570\u636e\u5197\u4f59\uff1a\u7edf\u4e00\u5c5e\u6027\u591a\u6b21\u51fa\u73b0\uff1b\u540c\u4e00\u5c5e\u6027\u547d\u540d\u4e0d\u4e00\u81f4\u5bfc\u81f4\u91cd\u590d\u3002\u5bf9\u4e8e\u5197\u4f59\u6570\u636e\u5148\u5206\u6790\uff0c\u68c0\u6d4b\u540e\u8fdb\u884c\u5220\u9664\u3002  3\u3001\u6570\u636e\u53d8\u6362  \u4e3b\u8981\u662f\u5bf9\u6570\u636e\u8fdb\u884c\u89c4\u8303\u5316\u5904\u7406\uff0c\u5c06\u6570\u636e\u8f6c\u6362\u6210\u9002\u5f53\u7684\u5f62\u5f0f\uff0c\u4ee5\u9002\u7528\u4e8e\u6316\u6398\u4efb\u52a1\u53ca\u7b97\u6cd5\u7684\u9700\u8981\u3002 1\u3001\u7b80\u5355\u51fd\u6570\u53d8\u6362\uff1a\u662f\u5bf9\u539f\u59cb\u6570\u636e\u8fdb\u884c\u67d0\u4e9b\u6570\u5b66\u51fd\u6570\u53d8\u6362\uff0c\u5e38\u7528\u7684\u5305\u62ec\u5e73\u65b9\u3001\u5f00\u65b9\u3001\u53d6\u5bf9\u6570\u3001\u5dee\u5206\u8fd0\u7b97\u7b49\u3002\u7b80\u5355\u7684\u51fd\u6570\u53d8\u6362\u5e38\u7528\u6765\u5c06\u4e0d\u5177\u6709\u6b63\u6001\u5206\u5e03\u7684\u6570\u636e\u53d8\u6362\u6210\u5177\u6709\u6b63\u6001\u5206\u5e03\u7684\u6570\u636e\u3002 \u5728\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u4e2d\uff0c\u7b80\u5355\u7684\u5bf9\u6570\u53d8\u6362\u6216\u8005\u5dee\u5206\u8fd0\u7b97\u53ef\u4ee5\u5c06\u975e\u5e73\u7a33\u5e8f\u5217\u8f6c\u6362\u6210\u5e73\u7a33\u5e8f\u5217\u3002 2\u3001\u89c4\u8303\u5316\uff1a\u4e3a\u4e86\u6d88\u9664\u6307\u6807\u95f4\u7684\u91cf\u7eb2\u548c\u53d6\u503c\u8303\u56f4\u5dee\u5f02\u7684\u5f71\u54cd\uff0c\u9700\u8981\u8fdb\u884c\u6807\u51c6\u5316\u5904\u7406\uff0c\u5c06\u6570\u636e\u6309\u7167\u6bd4\u4f8b\u8fdb\u884c\u7f29\u653e\u3002 \u6700\u5c0f-\u6700\u5927\u89c4\u8303\u5316\uff1a\u4e5f\u79f0\u4e3a\u79bb\u5dee\u6807\u51c6\u5316\uff0c\u662f\u5bf9\u539f\u59cb\u6570\u636e\u8fdb\u884c\u7ebf\u6027\u53d8\u6362\uff0c\u5c06\u6570\u503c\u6620\u5c04\u5230[0-1]\u4e4b\u95f4\uff0cx*=(x-min)/(max-min) \u96f6-\u5747\u503c\u89c4\u8303\u5316\uff1a\u4e5f\u79f0\u6807\u51c6\u5dee\u6807\u51c6\u5316\uff0c\u7ecf\u8fc7\u5904\u7406\u7684\u6570\u636e\u5747\u503c\u4e3a0\uff0c\u6807\u51c6\u5dee\u4e3a1\uff0cx*=(x-\u5e73\u5747\u503c)/\u6807\u51c6\u5dee \u5c0f\u6570\u5b9a\u6807\u89c4\u8303\u5316\uff1a\u901a\u8fc7\u79fb\u52a8\u5c5e\u6027\u503c\u7684\u5c0f\u6570\u4f4d\u6570\uff0c\u5c06\u5c5e\u6027\u503c\u6620\u5c04\u5230[-1,1]\u4e4b\u95f4\uff0c\u79fb\u52a8\u7684\u5c0f\u6570\u4f4d\u6570\u53d6\u51b3\u4e8e\u5c5e\u6027\u503c\u7edd\u5bf9\u503c\u7684\u6700\u5927\u503c  4\u3001\u6570\u636e\u89c4\u7ea6  \u5728\u5927\u6570\u636e\u4e0a\u8fdb\u884c\u590d\u6742\u7684\u6570\u636e\u5206\u6790\u548c\u6316\u6398\u9700\u8981\u5f88\u957f\u65f6\u95f4\uff0c\u6570\u636e\u89c4\u7ea6\u4ea7\u751f\u66f4\u5c0f\u4f46\u4fdd\u6301\u539f\u6570\u636e\u5b8c\u6574\u6027\u7684\u65b0\u6570\u636e\u96c6\u3002\u5728\u89c4\u7ea6\u540e\u7684\u6570\u636e\u96c6\u4e0a\u5206\u6790\u548c\u6316\u6398\u66f4\u6709\u6548\u7387\u3002  5\u3001\u9884\u5904\u7406\u4e3b\u8981\u51fd\u6570  \u4e3b\u8981\u662f\u63d2\u503c\u3001\u6570\u636e\u5f52\u4e00\u5316\u3001\u4e3b\u8981\u5206\u5206\u6790\u7b49\u4e0e\u6570\u636e\u9884\u5904\u7406\u76f8\u5173\u7684\u51fd\u6570 interpolate \u4e00\u7ef4\u3001\u9ad8\u7ef4\u6570\u636e\u63d2\u503c Scipy unique \u53bb\u9664\u91cd\u590d\u5143\u7d20\u3001\u5f97\u5230\u5355\u503c\u5217\u8868 Pandas/Numpy isnull \u5224\u65ad\u662f\u5426\u7a7a\u503c Pandas notnull \u5224\u65ad\u662f\u5426\u975e\u7a7a\u503c Pandas PCA \u5bf9\u6307\u6807\u53d8\u91cf\u77e9\u9635\u8fdb\u884c\u4e3b\u6210\u5206\u5206\u6790 Scikit-Learn random \u751f\u6210\u968f\u673a\u77e9\u9635 Numpy  \u4e94\u3001\u6316\u6398\u5efa\u6a21 \u7ecf\u8fc7\u6570\u636e\u63a2\u7d22\u548c\u6570\u636e\u9884\u5904\u7406\uff0c\u5f97\u5230\u4e86\u53ef\u4ee5\u76f4\u63a5\u5efa\u6a21\u7684\u6570\u636e\u3002\u6839\u636e\u6316\u6398\u76ee\u6807\u548c\u6570\u636e\u5f62\u5f0f\u53ef\u4ee5\u5efa\u7acb\u5206\u7c7b\u4e0e\u9884\u6d4b\u3001\u805a\u7c7b\u5206\u6790\u3001\u5173\u8054\u89c4\u5219\u3001\u65f6\u5e8f\u6a21\u5f0f\u548c\u504f\u5dee\u68c0\u6d4b\u7b49\u6a21\u578b\u3002 1\u3001\u5206\u7c7b\u4e0e\u9884\u6d4b  \u5206\u7c7b\uff1a\u662f\u4e00\u4e2a\u6784\u9020\u5206\u7c7b\u6a21\u578b\uff0c\u8f93\u5165\u6837\u672c\u7684\u5c5e\u6027\u503c\uff0c\u8f93\u51fa\u5bf9\u5e94\u7684\u7c7b\u522b\uff0c\u5c06\u6bcf\u4e2a\u6837\u672c\u6620\u5c04\u5230\u5148\u5b9a\u4e49\u597d\u7684\u7c7b\u522b\uff1b\u5206\u7c7b\u6a21\u578b\u5efa\u7acb\u5728\u5df2\u6709\u7c7b\u6807\u8bb0\u7684\u6570\u636e\u96c6\u4e0a\uff0c\u6a21\u578b\u5728\u5df2\u6709\u6837\u672c\u4e0a\u7684\u51c6\u786e\u7387\u53ef\u4ee5\u65b9\u4fbf\u8ba1\u7b97\u3002 \u5206\u7c7b\u4e24\u6b65\uff1a\u7b2c\u4e00\u6b65\u662f\u5b66\u4e60\u6b65\uff0c\u901a\u8fc7\u5f52\u7eb3\u5206\u6790\u8bad\u7ec3\u6837\u672c\u96c6\u54ce\u52a0\u4f60\u5206\u8bf6\u6a21\u578b\u5f97\u5230\u5206\u7c7b\u89c4\u5219\uff1b\u7b2c\u4e8c\u6b65\u662f\u5206\u7c7b\u6b65\uff0c\u5148\u7528\u4e00\u76f4\u7684\u6d4b\u8bd5\u6837\u672c\u96c6\u8bc4\u4f30\u5206\u7c7b\u89c4\u5219\u7684\u51c6\u786e\u7387\uff0c\u5982\u679c\u51c6\u786e\u7387\u53ef\u4ee5\u63a5\u53d7\uff0c\u5219\u4f7f\u7528\u8be5\u6a21\u578b\u5bf9\u672a\u77e5\u7c7b\u6807\u53f7\u7684\u5f85\u6d4b\u6837\u672c\u96c6\u8fdb\u884c\u9884\u6d4b\u3002 \u9884\u6d4b\uff1a\u662f\u6307\u5efa\u7acb\u4e24\u79cd\u6216\u4e24\u79cd\u4ee5\u4e0a\u53d8\u91cf\u95f4\u76f8\u4e92\u4f9d\u8d56\u7684\u51fd\u6570\u6a21\u578b\uff0c\u7136\u540e\u8fdb\u884c\u9884\u6d4b\u6216\u63a7\u5236\u3002 \u9884\u6d4b\u4e24\u6b65\uff1a\u7b2c\u4e00\u6b65\u662f\u901a\u8fc7\u8bad\u7ec3\u96c6\u5efa\u7acb\u9884\u6d4b\u5c5e\u6027\u7684\u51fd\u6570\u6a21\u578b\uff0c\u7b2c\u4e8c\u6b65\u5728\u6a21\u578b\u901a\u8fc7\u68c0\u9a8c\u540e\u8fdb\u884c\u9884\u6d4b\u6216\u63a7\u5236\u3002 \u5e38\u7528\u7684\u5206\u7c7b\u4e0e\u56de\u5f52\u7b97\u6cd5\uff1a 1\u3001\u56de\u5f52\u5206\u6790\uff1a\u901a\u8fc7\u5efa\u7acb\u6a21\u578b\u6765\u7814\u7a76\u53d8\u91cf\u4e4b\u95f4\u76f8\u4e92\u5173\u7cfb\u7684\u5bc6\u5207\u7a0b\u5ea6\uff0c\u7ed3\u6784\u72b6\u6001\u53ca\u8fdb\u884c\u6a21\u578b\u9884\u6d4b\u7684\u4e00\u79cd\u6709\u6548\u5de5\u5177\u3002\u56de\u5f52\u5206\u6790\u7814\u7a76\u5185\u5bb9\u5305\u62ec\uff1a \u7ebf\u6027\u56de\u5f52\uff08\u4e00\u5143\u7ebf\u6027\u56de\u5f52\uff0c\u591a\u5143\u7ebf\u6027\u56de\u5f52\uff0c\u591a\u4e2a\u81ea\u53d8\u91cf\u4e0e\u591a\u4e2a\u56e0\u53d8\u91cf\u7684\u56de\u5f52\uff09\uff1b \u56de\u5f52\u8bca\u65ad\uff08\u5982\u4f55\u4ece\u6570\u636e\u63a8\u65ad\u56de\u5f52\u6a21\u578b\u57fa\u672c\u5047\u8bbe\u7684\u5408\u7406\u6027\uff0c\u57fa\u672c\u5047\u8bbe\u4e0d\u6210\u7acb\u65f6\u5982\u4f55\u5bf9\u6570\u636e\u8fdb\u884c\u4fee\u6b63\uff0c\u5224\u65ad\u56de\u5f52\u65b9\u7a0b\u62df\u5408\u7684\u6548\u679c\uff0c\u9009\u62e9\u56de\u5f52\u51fd\u6570\u7684\u5f62\u5f0f\uff09 \u56de\u5f52\u53d8\u91cf\u9009\u62e9\uff08\u81ea\u53d8\u91cf\u9009\u62e9\u7684\u6807\u51c6\uff0c\u9010\u6b65\u56de\u5f52\u5206\u6790\u6cd5\uff09 \u53c2\u6570\u4f30\u8ba1\u65b9\u6cd5\u6539\u8fdb\uff08\u504f\u6700\u5c0f\u4e8c\u4e58\u56de\u5f52\uff0c\u5cad\u56de\u5f52\uff0c\u4e3b\u6210\u5206\u56de\u5f52\uff09 \u975e\u7ebf\u6027\u56de\u5f52\uff08\u4e00\u5143\u975e\u7ebf\u6027\u56de\u5f52\uff0c\u5206\u6bb5\u56de\u5f52\uff0c\u591a\u5143\u975e\u7ebf\u6027\u56de\u5f52\uff09 \u542b\u6709\u5b9a\u6027\u53d8\u91cf\u7684\u56de\u5f52\uff08\u81ea\u53d8\u91cf\u542b\u6709\u5b9a\u6027\u53d8\u91cf\u7684\u60c5\u51b5\uff0c\u56e0\u53d8\u91cf\u542b\u6709\u5b9a\u6027\u53d8\u91cf\u7684\u60c5\u51b5\uff09\u3002 \uff081\uff09\u7ebf\u6027\u56de\u5f52\uff1a\u81ea\u53d8\u91cf\u4e0e\u56e0\u53d8\u91cf\u4e4b\u95f4\u662f\u7ebf\u6027\u5173\u7cfb\uff0c\u53ef\u4ee5\u7528\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6c42\u89e3\u6a21\u578b\u7cfb\u6570 \uff082\uff09\u975e\u7ebf\u6027\u56de\u5f52\uff1a\u56e0\u53d8\u91cf\u4e0e\u81ea\u53d8\u91cf\u4e4b\u95f4\u4e0d\u90fd\u662f\u7ebf\u6027\u5173\u7cfb\uff0c\u5982\u679c\u975e\u7ebf\u6027\u5173\u7cfb\u53ef\u4ee5\u901a\u8fc7\u7b80\u5355\u7684\u51fd\u6570\u53d8\u6362\u5316\u6210\u7ebf\u6027\u5173\u7cfb\uff0c\u7528\u7ebf\u6027\u56de\u5f52\u7684\u601d\u60f3\u6c42\u89e3\uff0c\u5982\u679c\u4e0d\u80fd\u8f6c\u5316\uff0c\u7528\u975e\u7ebf\u6027\u6700\u5c0f\u4e8c\u4e58\u6cd5\u6c42\u89e3 \uff083\uff09logistic\u56de\u5f52\uff1a\u56e0\u53d8\u91cf\u4e00\u822c\u67090\u548c1\u4e24\u79cd\u53d6\u503c\u3002\u662f\u5e7f\u4e49\u7ebf\u6027\u56de\u5f52\u6a21\u578b\u7684\u7279\u4f8b\uff0c\u5229\u7528logistic\u51fd\u6570\u5c06\u56e0\u53d8\u91cf\u7684\u53d6\u503c\u8303\u56f4\u63a7\u5236\u57280\u548c1\u4e4b\u95f4\uff0c\u8868\u793a\u53d6\u503c\u4e3a \u7684\u6982\u7387\u3002 logistic\u56de\u5f52\u6a21\u578b\u662f\u5efa\u7acbln(p/1-p)\u4e0e\u81ea\u53d8\u91cf\u7684\u7ebf\u6027\u56de\u5f52\u6a21\u578b\uff0c\u5373ln(p/1-p)=b0+b1x1+b2x2+...+bnxn\u3002 logistic\u56de\u5f52\u5efa\u6a21\u6b65\u9aa4: 1\uff09\u3001\u6839\u636e\u5206\u6790\u76ee\u7684\u8bbe\u7f6e\u6307\u6807\u53d8\u91cf\uff0c\u6536\u96c6\u6570\u636e\uff0c\u6839\u636e\u6536\u96c6\u5230\u7684\u6570\u636e\u5bf9\u7279\u5f81\u8fdb\u884c\u7b5b\u9009\uff0c\u7279\u5f81\u7b5b\u9009\u7684\u65b9\u6cd5\u6709\u5f88\u591a\uff0c\u4e3b\u8981\u5305\u542b\u5728scikit-learn\u7684feature_selection\u5e93\u4e2d\uff0c\u9009\u62e9F\u503c\u8f83\u5927\u6216\u8005P\u503c\u8f83\u5c0f\u7684\u7279\u5f81\u3002\u5176\u6b21\u8fd8\u6709\u9012\u5f52\u7279\u5f81\u6d88\u9664\u548c\u7a33\u5b9a\u6027\u9009\u62e9\u7b49\u65b9\u6cd5\uff1b 2\uff09\u3001\u5217\u51fa\u7ebf\u6027\u56de\u5f52\u65b9\u7a0b\uff0c\u4f30\u8ba1\u51fa\u6a21\u578b\u4e2d\u7684\u7cfb\u6570\uff1b 3\u3001\u8fdb\u884c\u6a21\u578b\u68c0\u9a8c\uff0c\u6709\u6b63\u786e\u7387\uff0c\u6df7\u6dc6\u77e9\u9635\uff0cROC\u66f2\u7ebf\uff0cKS\u503c\u7b49\uff1b 4\u3001\u6a21\u578b\u5e94\u7528\uff0c\u8f93\u5165\u81ea\u53d8\u91cf\u7684\u503c\u53ef\u4ee5\u5f97\u5230\u56e0\u53d8\u91cf\u7684\u503c \uff084\uff09\u5cad\u56de\u5f52\uff1a\u53c2\u4e0e\u5efa\u6a21\u7684\u81ea\u53d8\u91cf\u4e4b\u95f4\u5177\u6709\u591a\u91cd\u5171\u7ebf\u6027\uff0c\u662f\u4e00\u79cd\u6539\u8fdb\u7684\u6700\u5c0f\u4e8c\u4e58\u4f30\u8ba1 \uff085\uff09\u4e3b\u6210\u5206\u5206\u6790\uff1a\u53c2\u4e0e\u5efa\u6a21\u7684\u81ea\u53d8\u91cf\u4e4b\u95f4\u5177\u6709\u591a\u91cd\u5171\u7ebf\u6027\uff0c\u662f\u53c2\u6570\u4f30\u8ba1\u7684\u4e00\u79cd\u6709\u504f\u4f30\u8ba1\uff0c\u53ef\u4ee5\u6d88\u9664\u591a\u91cd\u5171\u7ebf\u6027 2\u3001\u51b3\u7b56\u6811\u7b97\u6cd5\u5206\u7c7b\uff1a 1\u3001ID3\u7b97\u6cd5\uff1a\u5176\u6838\u5fc3\u662f\u5728\u51b3\u7b56\u6811\u7684\u5404\u7ea7\u8282\u70b9\u4e0a\uff0c\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u65b9\u6cd5\u4f5c\u4e3a\u5c5e\u6027\u7684\u9009\u62e9\u6807\u51c6\uff0c\u6765\u5e2e\u52a9\u786e\u5b9a\u751f\u6210\u6bcf\u4e2a\u8282\u70b9\u65f6\u6240\u5e94\u91c7\u7528\u7684\u5408\u9002\u5c5e\u6027 2\u3001C4.5\u7b97\u6cd5\uff1a\u662f\u4f7f\u7528\u4fe1\u606f\u589e\u76ca\u7387\u6765\u9009\u62e9\u8282\u70b9\u5c5e\u6027\uff0cID3\u53ea\u9002\u7528\u4e8e\u79bb\u6563\u7684\u5c5e\u6027\u63cf\u8ff0\uff0c\u800cC4.5\u65e2\u80fd\u591f\u5904\u7406\u79bb\u6563\u7684\u63cf\u8ff0\u5c5e\u6027\uff0c\u4e5f\u53ef\u4ee5\u5904\u7406\u8fde\u7eed\u7684\u63cf\u8ff0\u5c5e\u6027 3\u3001CART\u7b97\u6cd5\uff1a\u662f\u4e00\u79cd\u5341\u5206\u6709\u6548\u5730\u975e\u53c2\u6570\u5206\u7c7b\u548c\u548ci\u56de\u5f52\u65b9\u6cd5\uff0c\u901a\u8fc7\u6784\u5efa\u6811\u3001\u4fee\u526a\u6811\u3001\u8bc4\u4f30\u6811\u6765\u6784\u5efa\u4e00\u4e2a\u4e8c\u53c9\u6811\uff0c\u5f53\u7ec8\u8282\u70b9\u662f\u8fde\u7eed\u53d8\u91cf\u65f6\uff0c\u8be5\u6811\u4e3a\u56de\u5f52\u6811\uff0c\u5f53\u7ec8\u8282\u70b9\u662f\u5206\u7c7b\u53d8\u91cf\u65f6\uff0c\u8be5\u6811\u4e3a\u5206\u7c7b\u6811 ID3\u8ba1\u7b97\u6b65\u9aa4\uff1a \uff081\uff09\u5bf9\u5f53\u524d\u6837\u672c\u96c6\u5408\uff0c\u8ba1\u7b97\u6240\u6709\u5c5e\u6027\u7684\u4fe1\u606f\u589e\u76ca\uff1b \uff082\uff09\u8bad\u8d23\u4fe1\u606f\u589e\u76ca\u6700\u5927\u7684\u5c5e\u6027\u4f5c\u4e3a\u6d4b\u8bd5\u5c5e\u6027\uff0c\u628a\u6d4b\u8bd5\u5c5e\u6027\u53d6\u503c\u76f8\u540c\u7684\u6837\u672c\u5212\u4e3a\u540c\u4e00\u5b50\u6837\u672c\u96c6\uff1b \uff083\uff09\u82e5\u5b50\u6837\u672c\u96c6\u7684\u7c7b\u522b\u5c5e\u6027\u53ea\u542b\u6709\u5355\u4e2a\u5c5e\u6027\uff0c\u5219\u5206\u652f\u4e3a\u53f6\u5b50\u8282\u70b9\uff0c\u5224\u65ad\u5176\u5c5e\u6027\u503c\u5e76\u6807\u4e0a\u76f8\u5e94\u7684\u7b26\u53f7\uff0c\u7136\u540e\u8fd4\u56de\u8c03\u7528\u5904\uff1b\u5426\u5219\u5bf9\u5b50\u6837\u672c\u96c6\u9012\u5f52\u8c03\u7528\u672c\u7b97\u6cd5\u3002 3\u3001\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\uff1a\u662f\u6a21\u62df\u751f\u7269\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u4fe1\u606f\u5904\u7406\u7684\u4e00\u79cd\u6570\u5b66\u6a21\u578b\u3002\u4eba\u5de5\u795e\u7ecf\u5143\u662f\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u64cd\u4f5c\u7684\u57fa\u672c\u4fe1\u606f\u5904\u7406\u5355\u4f4d\u3002 \u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u5b66\u4e60\u4e5f\u79f0\u4e3a\u8bad\u7ec3\uff0c\u6307\u7684\u662f\u795e\u7ecf\u7f51\u7edc\u5728\u6536\u5230\u5916\u90e8\u73af\u5883\u7684\u523a\u6fc0\u4e0b\u8c03\u6574\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\uff0c\u4f7f\u795e\u7ecf\u7f51\u7edc\u4ee5\u4e00\u79cd\u65b0\u7684\u65b9\u5f0f\u5bf9\u5916\u90e8\u73af\u5883\u505a\u51fa\u53cd\u5e94\u7684\u4e00\u4e2a\u8fc7\u7a0b\u3002 \u5728\u5206\u7c7b\u4e0e\u9884\u6d4b\u4e2d\uff0c\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e3b\u8981\u4f7f\u7528\u6307\u5bfc\u7684\u5b66\u4e60\u65b9\u5f0f\uff0c\u5373\u6839\u636e\u7ed9\u5b9a\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u8c03\u6574\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u53c2\u6570\u4ee5\u4f7f\u7f51\u7edc\u8f93\u51fa\u63a5\u8fd1\u4e8e\u5df2\u77e5\u7684\u6837\u672c\u7c7b\u6807\u8bb0\u6216\u5176\u4ed6\u5f62\u5f0f\u7684\u56e0\u53d8\u91cf\u3002 \u5728\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7684\u53d1\u5c55\u8fc7\u7a0b\u4e2d\uff0c\u63d0\u51fa\u4e86\u591a\u79cd\u4e0d\u540c\u7684\u5b66\u4e60\u89c4\u5219\uff0c\u6ca1\u6709\u4e00\u79cd\u7279\u5b9a\u7684\u5b66\u4e60\u7b97\u6cd5\u9002\u7528\u4e8e\u6240\u6709\u7684\u7f51\u7edc\u7ed3\u6784\u548c\u5177\u4f53\u95ee\u9898\u3002\u5728\u5206\u7c7b\u4e0e\u9884\u6d4b\u4e2d\uff0c\u897f\u683c\u739b\u5b66\u4e60\u89c4\u5219\uff08\u8bef\u5dee\u77eb\u6b63\u5b66\u4e60\u7b97\u6cd5\uff09\u662f\u4f7f\u7528\u6700\u5e7f\u6cdb\u7684\u4e00\u79cd\u3002 \u8bef\u5dee\u6821\u6b63\u60f3\u5b66\u4e60\u7b97\u6cd5\u6839\u636e\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u51fa\u8bef\u5dee\u5bf9\u795e\u7ecf\u5143\u7684\u8fde\u63a5\u5f3a\u5ea6\u8fdb\u884c\u4fee\u6b63\uff0c\u5c5e\u4e8e\u6307\u5bfc\u5b66\u4e60\u3002 \u795e\u7ecf\u7f51\u7edc\u8bad\u7ec3\u662f\u5426\u5b8c\u6210\u5e38\u7528\u8bef\u5dee\u51fd\u6570E\u6765\u8861\u91cf\uff0c\u5f53\u8bef\u5dee\u51fd\u6570\u5c0f\u4e8e\u67d0\u4e00\u4e2a\u8bbe\u5b9a\u7684\u503c\u65f6\u5373\u505c\u6b62\u795e\u7ecf\u7f51\u7edc\u7684\u8bad\u7ec3\u3002 \u4f7f\u7528\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u9700\u8981\u786e\u5b9a\u7f51\u7edc\u8fde\u63a5\u7684\u62d3\u6251\u7ed3\u6784\u3001\u795e\u7ecf\u5143\u7684\u7279\u5f81\u548c\u5b66\u4e60\u89c4\u5219\u7b49\u3002\u5e38\u7528\u6765\u5b9e\u73b0\u5206\u7c7b\u548c\u9884\u6d4b\u7684\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u7b97\u6cd5\u5982\u4e0b\uff1a 1\u3001BP\u795e\u7ecf\u7f51\u7edc\uff1a\u662f\u4e00\u79cd\u6309\u8bef\u5dee\u9006\u4f20\u64ad\u7b97\u6cd5\u8bad\u7ec3\u7684\u591a\u5c42\u524d\u9988\u7f51\u7edc\uff0c\u5b66\u4e60\u7b97\u6cd5\u662f\u897f\u683c\u739b\u5b66\u4e60\u89c4\u5219\uff0c\u662f\u76ee\u524d \u5e94\u7528\u6700\u5e7f\u6cdb\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u4e4b\u4e00\u3002 2\u3001LM\u795e\u7ecf\u7f51\u7edc\uff1a\u662f\u57fa\u4e8e\u68af\u5ea6\u4e0b\u964d\u6cd5\u548c\u725b\u987f\u6cd5\u7ed3\u5408\u7684\u591a\u5c42\u524d\u9988\u7f51\u7edc\uff0c\u8fed\u4ee3\u6b21\u6570\u5c11\uff0c\u6536\u655b\u901f\u5ea6\u5feb\uff0c\u7cbe\u786e\u5ea6\u9ad8 3\u3001RBF\u5f84\u5411\u57fa\u795e\u7ecf\u7f51\u7edc\uff1aRBF\u7f51\u7edc\u80fd\u591f\u4ee5\u4efb\u610f\u7cbe\u5ea6\u903c\u8fd1\u4efb\u610f\u8fde\u7eed\u51fd\u6570\uff0c\u4ece\u8f93\u5165\u5c42\u5230\u9690\u542b\u5c42\u7684\u53d8\u6362\u662f\u975e\u7ebf\u6027\u7684\uff0c\u800c\u4ece\u9690\u542b\u5c42\u5230\u8f93\u51fa\u5c42\u7684\u53d8\u6362\u662f\u7ebf\u6027\u7684\uff0c\u7279\u522b\u9002\u5408\u89e3\u51b3\u5206\u7c7b\u95ee\u9898\u3002 4\u3001FNN\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\uff1aFNN\u6a21\u7cca\u795e\u7ecf\u7f51\u7edc\u662f\u5177\u6709\u6a21\u7cca\u6743\u7cfb\u6570\u6216\u8005\u8f93\u5165\u4fe1\u53f7\u662f\u6a21\u7cca\u91cf\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u662f\u6a21\u7cca\u7cfb\u7edf\u4e0e\u795e\u7ecf\u7f51\u7edc\u76f8\u7ed3\u5408\u7684\u4ea7\u7269\uff0c\u5b83\u6c47\u805a\u4e86\u795e\u7ecf\u7f51\u7edc\u4e0e\u6a21\u7cca\u7cfb\u7edf\u7684\u4f18\u70b9\uff0c\u96c6\u8054\u60f3\u3001\u8bc6\u522b\u3001\u81ea\u9002\u5e94\u53ca\u6a21\u7cca\u4fe1\u606f\u5904\u7406\u4e8e\u4e00\u4f53\u3002 5\u3001GMDH\u795e\u7ecf\u7f51\u7edc\uff1a\u4e5f\u79f0\u4e3a\u591a\u9879\u5f0f\u7f51\u7edc\uff0c\u662f\u524d\u9988\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e38\u7528\u7684\u4e00\u79cd\u7528\u4e8e\u9884\u6d4b\u7684\u795e\u7ecf\u7f51\u7edc\uff0c\u7279\u70b9\u662f\u7f51\u7edc\u7ed3\u6784\u4e0d\u56fa\u5b9a\uff0c\u800c\u4e14\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u4e0d\u65ad\u6539\u53d8\u3002 6\u3001ANFIS\u81ea\u9002\u5e94\u795e\u7ecf\u7f51\u7edc\uff1a\u795e\u7ecf\u7f51\u7edc\u9576\u5d4c\u5728\u4e00\u4e2a\u5168\u90e8\u6a21\u7cca\u7684\u7ed3\u6784\u4e4b\u4e2d\uff0c\u5728\u4e0d\u77e5\u4e0d\u89c9\u4e2d\u5411\u8bad\u7ec3\u6570\u636e\u5b66\u4e60\uff0c\u81ea\u52a8\u4ea7\u751f\u3001\u4fee\u6b63\u5e76\u9ad8\u5ea6\u6982\u62ec\u51fa\u6700\u4f73\u7684\u8f93\u5165\u4e0e\u8f93\u51fa\u53d8\u91cf\u7684\u96b6\u5c5e\u51fd\u6570\u4ee5\u53ca\u6a21\u7cca\u89c4\u5219\uff1b\u53e6\u5916\uff0c\u795e\u7ecf\u7f51\u7edc\u7684\u5404\u5c42\u7ed3\u6784\u4e0e\u53c2\u6570\u4e5f\u90fd\u5177\u6709\u4e86\u660e\u786e\u7684\u3001\u6613\u4e8e\u7406\u89e3\u7684\u7269\u7406\u610f\u4e49\u3002 BP\u7b97\u6cd5\u7684\u5b66\u5b66\u4e60\u8fc7\u7a0b\u7531\u4fe1\u53f7\u7684\u6b63\u5411\u4f20\u64ad\u4e0e\u8bef\u5dee\u7684\u9006\u5411\u4f20\u64ad\u4e24\u4e2a\u8fc7\u7a0b\u7ec4\u6210\u3002\u6b63\u5411\u4f20\u64ad\u65f6\uff0c\u8f93\u5165\u4fe1\u53f7\u7ecf\u8fc7\u9690\u5c42\u7684\u5904\u7406\u540e\uff0c\u4f20\u5411\u8f93\u5165\u5c42\u3002\u82e5\u8f93\u51fa\u5c42\u8282\u70b9\u672a\u80fd\u5f97\u5230\u671f\u671b\u7684\u8f93\u51fa\uff0c\u5219\u8f6c\u5165\u8bef\u5dee\u7684\u9006\u5411\u4f20\u64ad\u9636\u6bb5\uff0c\u5c06\u8f93\u51fa\u8bef\u5dee\u6309\u67d0\u79cd\u5b50\u5f62\u5f0f\uff0c\u901a\u8fc7\u9690\u5c42\u5411\u8f93\u5165\u5c42\u8fd4\u56de\uff0c \u4ece\u800c\u83b7\u5f97\u5404\u5c42\u5355\u5143\u7684\u53c2\u8003\u8bef\u5dee\u6216\u79f0\u8bef\u5dee\u4fe1\u53f7\uff0c\u4f5c\u4e3a\u4fee\u6539\u5404\u5355\u5143\u6743\u503c\u7684\u4f9d\u636e\u3002\u6b64\u8fc7\u7a0b\u4e00\u76f4\u8fdb\u884c\u5230\u7f51\u7edc\u8f93\u51fa\u7684\u8bef\u5dee\u9010\u6e10\u51cf\u5c11\u5230\u53ef\u63a5\u53d7\u7684\u7a0b\u5ea6\u6216\u8fbe\u5230\u8bbe\u5b9a\u7684\u5b66\u4e60\u6b21\u6570\u4e3a\u6b62\u3002 4\u3001\u8d1d\u53f6\u65af\u7f51\u7edc\uff1a\u8d1d\u53f6\u65af\u7f51\u7edc\u53c8\u79f0\u4fe1\u5ea6\u7f51\u7edc\uff0c\u662fBayes\u65b9\u6cd5\u7684\u6269\u5c55\uff0c\u662f\u76ee\u524d\u4e0d\u786e\u5b9a\u77e5\u8bc6\u8868\u8fbe\u548c\u63a8\u7406\u9886\u57df\u6700\u6709\u6548\u7684\u7406\u8bba\u6a21\u578b\u4e4b\u4e00\u3002 5\u3001\u652f\u6301\u5411\u91cf\u673a\uff1a\u652f\u6301\u5411\u91cf\u673a\u662f\u4e00\u79cd\u901a\u8fc7\u67d0\u79cd\u975e\u7ebf\u6027\u6620\u5c04\uff0c\u628a\u4f4e\u7ef4\u7684\u975e\u7ebf\u6027\u53ef\u5206\u8f6c\u5316\u4e3a\u9ad8\u7ef4\u7684\u7ebf\u6027\u53ef\u5206\uff0c\u5728\u9ad8\u7ef4\u7a7a\u95f4\u8fdb\u884c\u7ebf\u6027\u5206\u6790\u7684\u7b97\u6cd5\u3002  2\u3001\u805a\u7c7b\u5206\u6790  \u805a\u7c7b\u5206\u6790\u662f\u5728\u6ca1\u6709\u7ed9\u5b9a\u5212\u5206\u7c7b\u522b\u7684\u60c5\u51b5\u4e0b\uff0c\u6839\u636e\u6570\u636e\u76f8\u4f3c\u5ea6\u8fdb\u884c\u6837\u672c\u5206\u7ec4\u7684\u4e00\u79cd\u65b9\u6cd5\u3002\u53ef\u4ee5\u5efa\u7acb\u5728\u65e0\u7c7b\u6807\u8bb0\u7684\u6570\u636e\u4e0a\uff0c\u662f\u4e00\u79cd\u975e\u76d1\u7763\u7684\u5b66\u4e60\u7b97\u6cd5\u3002\u5212\u5206\u539f\u5219\u662f\u7ec4\u5185\u8ddd\u79bb\u6700\u5c0f\u5316\uff0c\u7ec4\u95f4\u8ddd\u79bb\u6700\u5927\u5316\u3002 \u5e38\u7528\u7684\u805a\u7c7b\u65b9\u6cd5\uff1a 1\u3001\u5212\u5206\u65b9\u6cd5\uff1aK-Means\uff08K\u5747\u503c\uff09\uff0cK-Medoids\uff08K-\u4e2d\u5fc3\u70b9\uff09\uff0cClarans\u7b97\u6cd5 2\u3001\u5c42\u6b21\u5206\u6790\u65b9\u6cd5\uff1aBIRCH\u7b97\u6cd5\uff08\u5e73\u8861\u8fed\u4ee3\u89c4\u7ea6\u548c\u805a\u7c7b\uff09\uff0cCURE\u7b97\u6cd5\uff08\u4ee3\u8868\u70b9\u805a\u7c7b\uff09\uff0cCHAMELEON\u7b97\u6cd5\uff08\u52a8\u6001\u6a21\u578b\uff09\u3002 3\u3001\u57fa\u4e8e\u5bc6\u5ea6\u7684\u65b9\u6cd5\uff1aDBSCAN\u7b97\u6cd5\uff0cDENCLUE\u7b97\u6cd5\uff0cOPTICS\u7b97\u6cd5 4\u3001\u57fa\u4e8e\u7f51\u683c\u7684\u65b9\u6cd5\uff1aSTING\u7b97\u6cd5\uff0cCLIOUE\u7b97\u6cd5\uff0cWACE-CLISTER\u7b97\u6cd5 5\u3001\u57fa\u4e8e\u6a21\u578b\u7684\u65b9\u6cd5\uff1a\u7edf\u8ba1\u5b66\u65b9\u6cd5\uff0c\u795e\u7ecf\u7f51\u7edc\u65b9\u6cd5\u3002 \u5e38\u7528\u805a\u7c7b\u5206\u6790\u7b97\u6cd5\uff1a 1\u3001K-MEANS\uff1aK-\u5747\u503c\u805a\u7c7b\u4e5f\u79f0\u5feb\u901f\u805a\u7c7b\u6cd5\uff0c\u5728\u6700\u5c0f\u5316\u8bef\u5dee\u51fd\u6570\u7684\u57fa\u7840\u4e0a\u5c06\u6570\u636e\u5212\u5206\u4e3a\u9884\u5b9a\u7684\u7c7b\u6570K\u3002\u539f\u7406\u7b80\u5355\uff0c\u4fbf\u4e8e\u5904\u7406\u5927\u91cf\u6570\u636e 2\u3001K-\u4e2d\u5fc3\u70b9\uff1aK-\u5747\u503c\u5bf9\u4e8e\u5b64\u7acb\u70b9\u654f\u611f\u3002K-\u4e2d\u5fc3\u70b9\u7b97\u6cd5\u4e0d\u91c7\u7528\u7c07\u4e2d\u5bf9\u8c61\u7684\u5e73\u5747\u503c\u4f5c\u4e3a\u7c07\u4e2d\u5fc3\uff0c\u800c\u9009\u7528\u7c07\u4e2d\u79bb\u5e73\u5747\u503c\u6700\u8fd1\u7684\u5bf9\u8c61\u4f5c\u4e3a\u7c07\u4e2d\u5fc3\u3002 3\u3001\u7cfb\u7edf\u805a\u7c7b\uff1a\u4e5f\u79f0\u591a\u5c42\u6b21\u805a\u7c7b\uff0c\u5206\u7c7b\u7684\u5355\u4f4d\u7531\u9ad8\u5230\u4f4e\u5448\u6811\u5f62\u7ed3\u6784\uff0c\u4e14\u6240\u5904\u7684\u4f4d\u7f6e\u8d8a\u4f4e\uff0c\u5176\u6240\u5305\u542b\u7684\u5bf9\u8c61\u5c31\u8d8a\u5c11\uff0c\u4f46\u8fd9\u4e9b\u5bf9\u8c61\u95f4\u7684\u5171\u540c\u7279\u5f81\u8d8a\u591a\u3002\u53ea\u9002\u5408\u5728\u5c0f\u6570\u636e\u91cf\u7684\u65f6\u5019\u4f7f\u7528\uff0c\u6570\u636e\u91cf\u5927\u7684\u65f6\u5019\u901f\u5ea6\u4f1a\u975e\u5e38\u6162\u3002 K-\u5747\u503c\u805a\u7c7b\u7b97\u6cd5\uff1a\u662f\u57fa\u4e8e\u8ddd\u79bb\u7684\u975e\u5c42\u6b21\u805a\u7c7b\u7b97\u6cd5\uff0c\u5728\u6700\u5c0f\u5316\u8bef\u5dee\u51fd\u6570\u7684\u57fa\u7840\u4e0a\u5c06\u6570\u636e\u5212\u5206\u4e3a\u9884\u5b9a\u7684\u7c7b\u6570K\uff0c\u91c7\u7528\u8ddd\u79bb\u4f5c\u4e3a\u76f8\u4f3c\u6027\u7684\u8bc4\u4ef7\u6307\u6807\u3002 \u7b97\u6cd5\u8fc7\u7a0b\uff1a 1)\u3001\u4eceN\u4e2a\u6837\u672c\u6570\u636e\u4e2d\u968f\u673a\u9009\u53d6K\u4e2a\u5bf9\u8c61\u4f5c\u4e3a\u521d\u59cb\u7684\u805a\u7c7b\u4e2d\u5fc3 2)\u3001\u5206\u522b\u8ba1\u7b97\u6bcf\u4e2a\u6837\u672c\u5230\u5404\u4e2a\u805a\u7c7b\u4e2d\u5fc3\u7684\u8ddd\u79bb\uff0c\u5c06\u5bf9\u8c61\u5206\u914d\u5230\u8ddd\u79bb\u6700\u8fd1\u7684\u805a\u7c7b\u4e2d 3)\u3001\u6240\u6709\u5bf9\u8c61\u5206\u914d\u5b8c\u6210\u540e\uff0c\u91cd\u65b0\u8ba1\u7b97K\u4e2a\u805a\u7c7b\u7684\u4e2d\u5fc3 4)\u3001\u4e0e\u524d\u4e00\u6b21\u8ba1\u7b97\u5f97\u5230\u7684K\u4e2a\u805a\u7c7b\u4e2d\u5fc3\u6bd4\u8f83\uff0c\u5982\u679c\u805a\u7c7b\u4e2d\u5fc3\u53d1\u751f\u53d8\u5316\uff0c\u8f6c\u5230\u8fc7\u7a0b2\uff0c\u5426\u5219\u8f6c\u5230\u8fc7\u7a0b5\uff0c 5)\u3001\u5f53\u8d28\u5fc3\u4e0d\u53d1\u751f\u53d8\u5316\u65f6\u505c\u6b62\u5e76\u8f93\u51fa\u805a\u7c7b\u7ed3\u679c\u3002 \u6570\u636e\u7c7b\u578b\u4e0e\u76f8\u4f3c\u6027\u7684\u5ea6\u91cf\uff1a 1)\u3001\u8fde\u7eed\u5c5e\u6027\uff1a\u5148\u5bf9\u5404\u5c5e\u6027\u503c\u8fdb\u884c\u96f6-\u5747\u503c\u89c4\u8303\uff0c\u5728\u8fdb\u884c\u8ddd\u79bb\u8ba1\u7b97\u3002\u4e00\u822c\u9700\u8981\u5ea6\u91cf\u6837\u672c\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u6837\u672c\u4e0e\u7c07\u4e4b\u95f4\u7684\u8ddd\u79bb\uff0c\u7c07\u4e0e\u7c07\u4e4b\u95f4\u7684\u8ddd\u79bb\u3002 \u6837\u672c\u4e4b\u95f4\u8ddd\u79bb\u5e38\u7528\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u3001\u66fc\u54c8\u987f\u8ddd\u79bb\u548c\u95f5\u79d1\u592b\u65af\u57fa\u8ddd\u79bb\uff1b\u6837\u672c\u4e0e\u7c07\u4e4b\u95f4\u8ddd\u79bb\u7528\u7c07\u4e2d\u5fc3\u5230\u6837\u672c\u7684\u8ddd\u79bb\uff1b\u7c07\u4e0e\u7c07\u4e4b\u95f4\u8ddd\u79bb\u7528\u7c07\u4e2d\u5fc3\u7684\u8ddd\u79bb\u3002 2)\u3001\u6587\u6863\u6570\u636e\uff1a\u5bf9\u4e8e\u6587\u6863\u6570\u636e\u4f7f\u7528\u4f59\u5f26\u76f8\u4f3c\u6027\u5ea6\u91cf\uff0c\u5148\u5c06\u6587\u6863\u6570\u636e\u6574\u7406\u6210\u6587\u6863-\u8bcd\u77e9\u9635\u683c\u5f0f\uff0c\u4e24\u4e2a\u6587\u6863\u4e4b\u95f4\u76f8\u4f3c\u5ea6\u7528d(i,j)=cos(i,j)\u8868\u793a \u76ee\u6807\u51fd\u6570\uff1a\u4f7f\u7528\u8bef\u5dee\u5e73\u65b9\u548cSSE\u4f5c\u4e3a\u5ea6\u91cf\u805a\u7c7b\u8d28\u91cf\u7684\u76ee\u6807\u51fd\u6570\uff0c\u5bf9\u4e8e\u4e24\u79cd\u4e0d\u540c\u7684\u805a\u7c7b\u7ed3\u679c\uff0c\u9009\u62e9\u8bef\u5dee\u5e73\u65b9\u548c\u8f83\u5c0f\u7684\u5206\u7c7b\u7ed3\u679c\u3002  3\u3001\u5173\u8054\u89c4\u5219  \u5173\u8054\u89c4\u5219\u4e5f\u79f0\u8d2d\u7269\u7bee\u5206\u6790\u3002 \u5e38\u7528\u5173\u8054\u89c4\u5219\u7b97\u6cd5\uff1a 1\u3001Apriori\uff1a\u5173\u8054\u89c4\u5219\u6700\u5e38\u7528\u7684\u6316\u6398\u9891\u7e41\u9879\u96c6\u7684\u7b97\u6cd5\uff0c\u6838\u5fc3\u601d\u60f3\u662f\u901a\u8fc7\u8fde\u63a5\u4ea7\u751f\u9009\u9879\u53ca\u5176\u652f\u6301\u5ea6\u7136\u540e\u901a\u8fc7\u526a\u679d\u751f\u6210\u9891\u7e41\u9879\u96c6\u3002 2\u3001FP-Tree\uff1a\u9488\u5bf9Apriori\u56fa\u6709\u7684\u591a\u6b21\u626b\u63cf\u4e8b\u52a1\u6570\u636e\u96c6\u7684\u7f3a\u9879\uff0c\u63d0\u51fa\u4e0d\u4ea7\u751f\u5019\u9009\u9891\u7e41\u9879\u96c6\u7684\u65b9\u6cd5 3\u3001Eclat\uff1a\u662f\u4e00\u79cd\u6df1\u5ea6\u4f18\u5148\u7b97\u6cd5\uff0c\u91c7\u7528\u5782\u76f4\u6570\u636e\u8868\u793a\u5f62\u5f0f\uff0c\u5728\u6982\u5ff5\u683c\u7406\u8bba\u7684\u57fa\u7840\u4e0a\u5229\u7528\u57fa\u4e8e\u524d\u7f00\u7684\u7b49\u4ef7\u5173\u7cfb\u5c06\u641c\u7d22\u7a7a\u95f4\u5212\u5206\u4e3a\u8f83\u5c0f\u7684\u5b50\u7a7a\u95f4 4\u3001\u7070\u8272\u5173\u8054\u6cd5\uff1a\u5206\u6790\u548c\u786e\u5b9a\u5404\u56e0\u7d20\u4e4b\u95f4\u7684\u5f71\u54cd\u7a0b\u5ea6\u6216\u662f\u82e5\u5e72\u5b50\u56e0\u7d20\u5bf9\u4e3b\u56e0\u7d20\u7684\u8d21\u732e\u5ea6\u8fdb\u884c\u5206\u6790\u7684\u65b9\u6cd5  4\u3001\u65f6\u5e8f\u6a21\u5f0f  Python\u5b9e\u73b0\u65f6\u5e8f\u6a21\u5f0f\u7684\u4e3b\u8981\u5e93\u662fStatsModels\uff08\u80fd\u7528Pandas\uff0c\u5c31\u7528Pandas\u5148\u505a\uff09\uff0c\u7b97\u6cd5\u4e3b\u8981\u662fARIMA\u6a21\u578b\u3002 \u5e38\u7528\u6a21\u578b\uff1a\u5e73\u6ed1\u6cd5\u3001\u8d8b\u52bf\u4f60\u5408\u6cd5\u3001\u7ec4\u5408\u6a21\u578b\u3001AR\u6a21\u578b\u3001MA\u6a21\u578b\u3001ARMA\u6a21\u578b\u3001ARIMA\u3001ARCH\u3001GARCH\u6a21\u578b\u53ca\u884d\u751f\u3002 python\u4e3b\u8981\u65f6\u5e8f\u7b97\u6cd5\u51fd\u6570\uff1aacf\u81ea\u76f8\u5173\uff0cplot_acf\u753b\u81ea\u76f8\u5173\u7cfb\u6570\u56fe\u3001pacf\u8ba1\u7b97\u504f\u76f8\u5173\u7cfb\u6570\u3001plot_pacf\u753b\u504f\u76f8\u5173\u7cfb\u6570\u56fe\u3001adfuller\u5bf9\u89c2\u6d4b\u503c\u5e8f\u5217\u8fdb\u884c\u5355\u4f4d\u6839\u68c0\u9a8c\u3001diff\u5dee\u5206\u8ba1\u7b97\u3001ARIMA\u521b\u5efaARIMA\u65f6\u5e8f\u6a21\u578b\u3001summary\u6216summaty2\u7ed9\u51faARIMA\u6a21\u578b\u62a5\u544a\u3001aic/bic/hqic\u8ba1\u7b97ARIMA\u6a21\u578b\u7684\u6307\u6807\u503c\u3001forecast\u9884\u6d4b\u3001acorr_ljungbox\u68c0\u9a8c\u767d\u566a\u58f0  5\u3001\u79bb\u7fa4\u70b9\u68c0\u6d4b  \u79bb\u7fa4\u70b9\u68c0\u6d4b\uff1a\u53d1\u73b0\u4e0e\u5927\u90e8\u5206\u5176\u4ed6\u5bf9\u8c61\u663e\u8457\u4e0d\u540c\u7684\u5bf9\u8c61 \u79bb\u7fa4\u70b9\u6210\u56e0\uff1a\u6570\u636e\u6765\u6e90\u4e8e\u4e0d\u540c\u7684\u7c7b\uff0c\u81ea\u7136\u53d8\u5f02\uff0c\u6570\u636e\u6d4b\u91cf\u548c\u6536\u96c6\u8bef\u5dee \u79bb\u7fa4\u70b9\u7c7b\u578b\uff1a 1\u3001\u5168\u5c40\u79bb\u7fa4\u70b9\u548c\u5c40\u90e8\u79bb\u7fa4\u70b9\uff1a\u4ece\u6574\u4f53\u6765\u770b\u67d0\u4e9b\u5bf9\u8c61\u6ca1\u6709\u79bb\u7fa4\u7279\u5f81\uff0c\u4f46\u662f\u4ece\u5c40\u90e8\u6765\u770b\uff0c\u5374\u663e\u793a\u4e86\u4e00\u5b9a\u7684\u79bb\u7fa4\u6027\u3002 2\u3001\u6570\u503c\u578b\u79bb\u7fa4\u70b9\u548c\u5206\u7c7b\u578b\u79bb\u7fa4\u70b9 3\u3001\u4e00\u7ef4\u79bb\u7fa4\u70b9\u548c\u591a\u7ef4\u79bb\u7fa4\u70b9 \u79bb\u7fa4\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff1a 1\u3001\u57fa\u4e8e\u7edf\u8ba1\uff1a\u6784\u5efa\u4e00\u4e2a\u5206\u5e03\u6a21\u578b\uff0c\u5e76\u8ba1\u7b97\u5bf9\u8c61\u7b26\u5408\u8be5\u6a21\u578b\u7684\u6982\u7387\uff0c\u628a\u5177\u6709\u4f4e\u6982\u7387\u7684\u5bf9\u8c61\u89c6\u4e3a\u79bb\u7fa4\u70b9\uff0c\u524d\u63d0\u662f\u5fc5\u987b\u77e5\u9053\u6570\u636e\u96c6\u670d\u4ece\u4ec0\u4e48\u5206\u5e03 2\u3001\u57fa\u4e8e\u8fd1\u90bb\u5ea6\uff1a\u5728\u6570\u636e\u5bf9\u8c61\u4e4b\u95f4\u5b9a\u4e49\u90bb\u8fd1\u6027\u5ea6\u91cf\uff0c\u628a\u8fdc\u79bb\u5927\u90e8\u5206\u70b9\u7684\u5bf9\u8c61\u89c6\u4e3a\u79bb\u7fa4\u70b9 3\u3001\u57fa\u4e8e\u5bc6\u5ea6\uff1a\u6570\u636e\u96c6\u53ef\u80fd\u5b58\u5728\u4e0d\u540c\u5bc6\u5ea6\u533a\u57df\uff0c\u79bb\u7fa4\u70b9\u662f\u5728\u4f4e\u5bc6\u5ea6\u533a\u57df\u4e2d\u7684\u5bf9\u8c61\uff0c\u4e00\u4e2a\u5bf9\u8c61\u7684\u79bb\u7fa4\u70b9\u5f97\u5206\u662f\u8be5\u5bf9\u8c61\u5468\u56f4\u5bc6\u5ea6\u7684\u9006 4\u3001\u57fa\u4e8e\u805a\u7c7b\uff1a\u4e22\u5f03\u8fdc\u79bb\u5176\u4ed6\u7c07\u7684\u5c0f\u7c07\uff1b\u6216\u8005\u5148\u805a\u7c7b\u6240\u6709\u5bf9\u8c61\uff0c\u7136\u540e\u8bc4\u4f30\u5bf9\u8c61\u5c5e\u4e8e\u7c07\u7684\u7a0b\u5ea6\u3002 \u57fa\u4e8e\u6a21\u578b\u7684\u79bb\u7fa4\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff1a\u901a\u8fc7\u4f30\u8ba1\u6982\u7387\u5206\u5e03\u7684\u53c2\u6570\u6765\u5efa\u7acb\u4e00\u4e2a\u6570\u636e\u6a21\u578b\u3002\u5982\u679c\u4e00\u4e2a\u6570\u636e\u5bf9\u8c61\u4e0d\u80fd\u5f88\u597d\u5730\u540c\u8be5\u6a21\u578b\u62df\u5408\uff0c\u5373\u5982\u679c\u5b83\u5f88\u53ef\u80fd\u4e0d\u670d\u4ece\u8be5\u5206\u5e03\uff0c\u5219\u662f\u4e00\u4e2a\u79bb\u7fa4\u70b9 1\u3001\u4e00\u5143\u6b63\u6001\u5206\u5e03\u4e2d\u7684\u79bb\u7fa4\u70b9\u68c0\u6d4b\uff1aN\uff080\uff0c1\uff09\u7684\u6570\u636e\u5bf9\u8c61\u51fa\u73b0\u5728\u8be5\u5206\u5e03\u7684\u4e24\u8fb9\u5c3e\u90e8\u7684\u673a\u4f1a\u5f88\u5c0f\uff0c\u56e0\u6b64\u53ef\u4ee5\u7528\u5b83\u4f5c\u4e3a\u68c0\u6d4b\u6570\u636e\u5bf9\u8c61\u662f\u5426\u662f\u79bb\u7fa4\u70b9\u7684\u57fa\u7840\uff0c\u6570\u636e\u5bf9\u8c61\u843d\u57283\u500d\u6807\u51c6\u5dee\u4e2d\u5fc3\u533a\u57df\u4e4b\u5916\u7684\u6982\u7387\u4ec5\u67090.0027 2\u3001\u6df7\u5408\u6a21\u578b\u7684\u79bb\u7fa4\u70b9\u68c0\u6d4b\uff1a\u6df7\u5408\u6a21\u578b\u662f\u4e00\u79cd\u7279\u6b8a\u7684\u7edf\u8ba1\u6a21\u578b\uff0c\u5b83\u4f7f\u7528\u82e5\u5e72\u7edf\u8ba1\u5206\u5e03\u5bf9\u6570\u636e\u5efa\u6a21\uff0c\u6bcf\u4e2a\u5206\u5e03\u5bf9\u5e94\u4e00\u4e2a\u7c07\uff0c\u800c\u6bcf\u4e2a\u5206\u5e03\u7684\u53c2\u6570\u63d0\u4f9b\u5bf9\u5e94\u7c07\u7684\u63cf\u8ff0\uff0c\u901a\u5e38\u7528\u4e2d\u5fc3\u548c\u53d1\u6563\u63cf\u8ff0 3\u3001\u57fa\u4e8e\u805a\u7c7b\u7684\u79bb\u7fa4\u70b9\u68c0\u6d4b\u65b9\u6cd5\uff1a \uff081\uff09\u4e22\u5f03\u8fdc\u79bb\u5176\u4ed6\u7c07\u7684\u5c0f\u7c07\uff1a\u8be5\u8fc7\u7a0b\u53ef\u4ee5\u7b80\u5316\u4e3a\u4e22\u5f03\u5c0f\u4e8e\u67d0\u4e2a\u9608\u503c\u7684\u6240\u6709\u7c07 \uff082\uff09\u57fa\u4e8e\u539f\u578b\u7684\u805a\u7c7b\uff1a\u9996\u5148\u805a\u7c7b\u6240\u6709\u5bf9\u8c61\uff0c\u7136\u540e\u8bc4\u4f30\u5bf9\u8c61\u5c5e\u4e8e\u7c07\u7684\u7a0b\u5ea6\u3002\u53ef\u4ee5\u7528\u5bf9\u8c61\u5230\u4ed6\u7684\u7c07\u4e2d\u5fc3\u7684\u8ddd\u79bb\u6765\u5ea6\u91cf\u5c5e\u4e8e\u7c07\u7684\u7a0b\u5ea6\u3002 \u57fa\u4e8e\u539f\u578b\u7684\u805a\u7c7b\u4e3b\u8981\u6709\u4e24\u79cd\u65b9\u6cd5\u8bc4\u4f30\u5bf9\u8c61\u5c5e\u4e8e\u7c07\u7684\u7a0b\u5ea6\uff1a \u610f\u8bc6\u5ea6\u91cf\u5bf9\u8c61\u5230\u7c07\u539f\u578b\u7684\u8ddd\u79bb\uff0c\u5e76\u7528\u5b83\u4f5c\u4e3a\u8be5\u5bf9\u8c61\u7684\u79bb\u7fa4\u70b9\u5f97\u5206\uff1b \u8003\u8651\u5230\u7c07\u5177\u6709\u4e0d\u540c\u7684\u5bc6\u5ea6\uff0c\u53ef\u4ee5\u5ea6\u91cf\u7c07\u5230\u539f\u578b\u7684\u76f8\u5bf9\u8ddd\u79bb\uff0c\u76f8\u5bf9\u8ddd\u79bb\u662f\u70b9\u5230\u8d28\u5fc3\u7684\u8ddd\u79bb\u4e0e\u7c07\u4e2d\u6240\u6709\u70b9\u5230\u8d28\u5fc3\u7684\u8ddd\u79bb\u7684\u4e2d\u4f4d\u6570\u4e4b\u6bd4  \u516d\u3001\u7535\u529b\u7a83\u6f0f\u7535\u7528\u6237\u81ea\u52a8\u8bc6\u522b 1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u901a\u8fc7\u7535\u529b\u7cfb\u7edf\u91c7\u96c6\u5230\u7684\u6570\u636e\uff0c\u63d0\u53d6\u51fa\u7a83\u6f0f\u7535\u7528\u6237\u7684\u5173\u952e\u7279\u5f81\uff0c\u6784\u5efa\u7a83\u6f0f\u7535\u7528\u6237\u7684\u8bc6\u522b\u6a21\u578b\u3002\u5229\u7528\u5b9e\u65f6\u76d1\u6d4b\u6570\u636e\uff0c\u8c03\u7528\u7a83\u6f0f\u7535\u7528\u6237\u8bc6\u522b\u6a21\u578b\u5b9e\u73b0\u5b9e\u65f6\u8bca\u65ad\uff0c\u4ee5\u5b9e\u73b0\u81ea\u52a8\u68c0\u67e5\u3001\u5224\u65ad\u7528\u6237\u662f\u5426\u662f\u5b58\u5728\u7a83\u6f0f\u7535\u884c\u4e3a\u3002  2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u62bd\u53d6\uff1a\u62bd\u53d6\u4e0e\u7a83\u6f0f\u7535\u76f8\u5173\u7684\u7528\u7535\u8d1f\u8377\u6570\u636e\u3001\u7ec8\u7aef\u62a5\u8b66\u6570\u636e\u3001\u8fdd\u7ea6\u7a83\u6f0f\u7535\u7f5a\u4fe1\u606f\u4ee5\u53ca\u7528\u6237\u6863\u6848\u8d44\u6599\u7b49\u6570\u636e \u6570\u636e\u63a2\u7d22\u5206\u6790\uff1a\u672c\u6848\u4f8b\u91c7\u7528\u5206\u5e03\u5206\u6790\u548c\u5468\u671f\u6027\u5206\u6790\u7b49\u65b9\u6cd5\u5bf9\u7535\u91cf\u6570\u636e\u8fdb\u884c\u6570\u636e\u5206\u6790 \uff081\uff09\u5bf9\u4e00\u6bb5\u65f6\u95f4\u6240\u6709\u7a83\u6f0f\u7535\u7528\u6237\u8fdb\u884c\u5206\u5e03\u5206\u6790\uff0c\u4ece\u7528\u7535\u7c7b\u522b\u7a83\u6f0f\u7535\u60c5\u51b5\u56fe\uff0c\u5f97\u5230\u975e\u5c45\u6c11\u7c7b\u522b\u4e0d\u5b58\u5728\u7a83\u6f0f\u7535\u60c5\u51b5\uff0c\u6545\u5206\u6790\u4e2d\u4e0d\u8003\u8651\u975e\u5c45\u6c11\u7c7b\u522b\u7684\u7528\u7535\u6570\u636e\u3002 \uff082\uff09\u968f\u673a\u62bd\u53d6\u4e00\u4e2a\u6b63\u5e38\u7528\u7535\u7528\u6237\u548c\u4e00\u4e2a\u7a83\u6f0f\u7535\u7528\u6237\uff0c\u91c7\u7528\u5468\u671f\u6027\u5206\u6790\u5bf9\u7528\u7535\u91cf\u8fdb\u884c\u63a2\u7d22\u3002\u5206\u6790\u51fa\u6b63\u5e38\u7528\u7535\u5230\u7a83\u6f0f\u7535\u8fc7\u7a0b\u662f\u7528\u7535\u91cf\u6301\u7eed\u4e0b\u964d\u7684\u8fc7\u7a0b\uff0c\u7528\u6237\u7528\u7535\u91cf\u5f00\u59cb\u4e0b\u964d\uff0c\u5e76\u4e14\u6301\u7eed\u4e0b\u964d\uff0c\u5c31\u662f\u7528\u6237\u5f00\u59cb\u7a83\u6f0f\u7535\u65f6\u6240\u8868\u73b0\u51fa\u6765\u7684\u91cd\u8981\u7279\u5f81\u3002 \u6570\u636e\u9884\u5904\u7406\uff1a\u672c\u6848\u4f8b\u4e3b\u8981\u4ece\u6570\u636e\u6e05\u6d17\u3001\u7f3a\u5931\u503c\u5904\u7406\u548c\u6570\u636e\u53d8\u6362\u7b49\u65b9\u9762\u5bf9\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406 \u6570\u636e\u6e05\u6d17\uff1a\u4e3b\u8981\u76ee\u7684\u662f\u7b5b\u9009\u51fa\u9700\u8981\u7684\u6570\u636e\uff0c\u7531\u4e8e\u539f\u59cb\u6570\u636e\u5e76\u4e0d\u662f\u6240\u6709\u7684\u90fd\u9700\u8981\u8fdb\u884c\u5206\u6790\uff0c\u6545\u5904\u7406\u65f6\uff0c\u9700\u8fc7\u6ee4\u3002 \u7f3a\u5931\u503c\u5904\u7406\uff1a\u5728\u7528\u6237\u7535\u91cf\u62bd\u53d6\u8fc7\u7a0b\u4e2d\uff0c\u53d1\u73b0\u5b58\u5728\u7f3a\u5931\u7684\u73b0\u8c61\u3002\u82e5\u629b\u5f03\u6389\uff0c\u4f1a\u5f71\u54cd\u4f9b\u51fa\u7535\u91cf\u7684\u8ba1\u7b97\u7ed3\u679c\uff0c\u5bfc\u81f4\u65e5\u7ebf\u635f\u7387\u8bef\u5dee\u5f88\u5927\u3002\u4e3a\u4e86\u8fbe\u5230\u8f83\u597d\u7684\u5efa\u6a21\u6548\u679c\uff0c\u9700\u5bf9\u7f3a\u5931\u503c\u5904\u7406\u3002\u672c\u6848\u4f8b\u91c7\u7528\u62c9\u683c\u6717\u65e5\u63d2\u503c\u6cd5\u5bf9\u7f3a\u5931\u8fdb\u884c\u63d2\u8865\u3002 \u6570\u636e\u53d8\u6362\uff1a\u539f\u59cb\u6570\u636e\u867d\u7136\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u80fd\u53cd\u6620\u7528\u6237\u7a83\u6f0f\u7535\u884c\u4e3a\uff0c\u4f46\u8981\u4f5c\u4e3a\u6784\u5efa\u6a21\u578b\u7684\u4e13\u5bb6\u6837\u672c\uff0c\u7279\u5f81\u4e0d\u660e\u663e\uff0c\u9700\u8981\u8fdb\u884c\u91cd\u65b0\u6784\u9020\u3002\u57fa\u4e8e\u6570\u636e\u53d8\u6362\uff0c\u5f97\u5230\u65b0\u7684\u8bc4\u4ef7\u6307\u6807\u6765\u8868\u5f81\u7a83\u6f0f\u7535\u884c\u4e3a\u5177\u6709\u7684\u89c4\u5f8b\u3002 \u6784\u5efa\u4e13\u5bb6\u6837\u672c\uff1a\u901a\u8fc7\u4ee5\u4e0a\u7684\u6570\u636e\u53d8\u6362\u5f97\u5230\u7684\u7279\u5f81\u9009\u53d6\u6837\u672c\u6570\u636e\uff0c\u5f97\u5230\u4e13\u5bb6\u6837\u672c\u5e93\u3002  3\u3001\u6a21\u578b\u6784\u5efa  \u6784\u5efa\u7a83\u6f0f\u7535\u7528\u6237\u8bc6\u522b\u6a21\u578b\uff1a\u7a83\u6f0f\u7535\u7528\u6237\u8bc6\u522b\u53ef\u901a\u8fc7\u6784\u5efa\u5206\u7c7b\u9884\u6d4b\u6a21\u578b\u6765\u5b9e\u73b0\uff0c\u5e38\u7528\u7684\u5206\u7c7b\u9884\u6d4b\u6a21\u578b\u6709LM\u795e\u7ecf\u7f51\u7edc\u548cCART\u51b3\u7b56\u6811\uff0c\u5404\u4e2a\u6a21\u578b\u6709\u5404\u4f18\u70b9\u3002\u6545\u91c7\u7528\u4e24\u79cd\u65b9\u6cd5\u6784\u5efa\uff0c\u5e76\u4ece\u4e2d\u9009\u62e9\u6700\u4f18\u7684\u5206\u7c7b\u6a21\u578b\u3002 \u6a21\u578b\u8bc4\u4ef7\uff1a\u5bf9\u4e8e\u8bad\u7ec3\u6837\u672c\uff0cLM\u795e\u7ecf\u7f51\u7edc\u548cCART\u51b3\u7b56\u6811\u7684\u5206\u7c7b\u51c6\u786e\u7387\u76f8\u5173\u4e0d\u5927\uff0c\u4e3a\u4e86\u8fdb\u4e00\u6b65\u8bc4\u6a21\u578b\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u6545\u5229\u7528\u6d4b\u8bd5\u6837\u672c\u5bf9\u4e24\u6a21\u578b\u8fdb\u884c\u8bc4\u4ef7\uff0c\u91c7\u7528ROC\u66f2\u7ebf\u8bc4\u4ef7\u65b9\u6cd5\u8fdb\u884c\u8bc4\u4f30\uff0c\u4e00\u4e2a\u4f18\u79c0\u5206\u7c7b\u5668\u5bf9\u5e94\u7684ROC\u66f2\u7ebf\u5e94\u8be5\u662f\u5c3d\u91cf\u9760\u8fd1\u5de6\u4e0a\u89d2\u7684\u3002\u7ecf\u8fc7\u5bf9\u6bd4\u53d1\u73b0LM\u795e\u7ecf\u7f51\u7edc\u7684ROC\u66f2\u7ebf\u6bd4CART\u51b3\u7b56\u6811\u7684ROC\u66f2\u7ebf\u66f4\u52a0\u9760\u8fd1\u5de6\u4e0a\u89d2\uff0c\u8bf4\u660eLM\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u7684\u5206\u7c7b\u6027\u80fd\u8f83\u597d\uff0c\u80fd\u5e94\u7528\u4e8e\u6784\u5efa\u7a83\u6f0f\u7535\u7528\u6237\u8bc6\u522b\u3002  #-*- coding: utf-8 -*- # \u91c7\u7528ROC\u66f2\u7ebf\u8bc4\u4ef7\u65b9\u6cd5\u6765\u6d4b\u8bd5\u8bc4\u4f30\u6a21\u578b\u5206\u7c7b\u7684\u6027\u80fd\uff0c\u4e00\u4e2a\u4f18\u79c0\u7684\u5206\u7c7b\u5668\u5e94\u8be5\u662f\u5c3d\u91cf\u9760\u8fd1\u5de6\u4e0a\u89d2\u7684 import pandas as pd from random import shuffle#\u5bfc\u5165\u968f\u673a\u51fd\u6570shuffle\uff0c\u7528\u6765\u6253\u4e71\u6570\u636e import matplotlib.pyplot as plt #\u5bfc\u5165Matplotlib plt.rcParams['font.sans-serif'] = ['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e  datafile = '../data/model.xls' data = pd.read_excel(datafile) data = data.values shuffle(data)  p = 0.8 #\u8bbe\u7f6e\u8bad\u7ec3\u6570\u636e\u6bd4\u4f8b train = data[:int(len(data)*p),:] test = data[int(len(data)*p):,:] #\u6784\u5efaLM\u795e\u7ecf\u7f51\u7edc\u6a21\u578b from keras.models import Sequential #\u5bfc\u5165\u795e\u7ecf\u7f51\u7edc\u521d\u59cb\u5316\u51fd\u6570 from keras.layers.core import Dense, Activation #\u5bfc\u5165\u795e\u7ecf\u7f51\u7edc\u5c42\u51fd\u6570\u3001\u6fc0\u6d3b\u51fd\u6570  netfile = '../tmp/net.model' #\u6784\u5efa\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u5b58\u50a8\u8def\u5f84  net = Sequential() #\u5efa\u7acb\u795e\u7ecf\u7f51\u7edc #net.add(Dense(input_dim = 3, output_dim = 10)) #\u6dfb\u52a0\u8f93\u5165\u5c42\uff083\u8282\u70b9\uff09\u5230\u9690\u85cf\u5c42\uff0810\u8282\u70b9\uff09\u7684\u8fde\u63a5( Update your `Dense` call to the Keras 2 API: `Dense(input_dim=3, units=10)`) net.add(Dense(input_dim = 3, units = 10)) #\u6dfb\u52a0\u8f93\u5165\u5c42\uff083\u8282\u70b9\uff09\u5230\u9690\u85cf\u5c42\uff0810\u8282\u70b9\uff09\u7684\u8fde\u63a5( net.add(Activation('relu')) #\u9690\u85cf\u5c42\u4f7f\u7528relu\u6fc0\u6d3b\u51fd\u6570 #net.add(Dense(input_dim = 10, output_dim = 1)) #\u6dfb\u52a0\u9690\u85cf\u5c42\uff0810\u8282\u70b9\uff09\u5230\u8f93\u51fa\u5c42\uff081\u8282\u70b9\uff09\u7684\u8fde\u63a5 net.add(Dense(input_dim = 10, units = 1)) #\u6dfb\u52a0\u9690\u85cf\u5c42\uff0810\u8282\u70b9\uff09\u5230\u8f93\u51fa\u5c42\uff081\u8282\u70b9\uff09\u7684\u8fde\u63a5 net.add(Activation('sigmoid')) #\u8f93\u51fa\u5c42\u4f7f\u7528sigmoid\u6fc0\u6d3b\u51fd\u6570 #net.compile(loss = 'binary_crossentropy', optimizer = 'adam', class_mode = \"binary\") #\u7f16\u8bd1\u6a21\u578b\uff0c\u4f7f\u7528adam\u65b9\u6cd5\u6c42\u89e3 net.compile(loss = 'binary_crossentropy', optimizer = 'adam') #\u7f16\u8bd1\u6a21\u578b\uff0c\u4f7f\u7528adam\u65b9\u6cd5\u6c42\u89e3 #net.fit(train[:,:3], train[:,3], nb_epoch=1000, batch_size=1) #\u8bad\u7ec3\u6a21\u578b\uff0c\u5faa\u73af1000\u6b21(UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.) net.fit(train[:,:3], train[:,3], epochs=1000, batch_size=1) net.save_weights(netfile) #\u4fdd\u5b58\u6a21\u578b  #\u6784\u5efaCART\u51b3\u7b56\u6811\u6a21\u578b from sklearn.tree import DecisionTreeClassifier #\u5bfc\u5165\u51b3\u7b56\u6811\u6a21\u578b  treefile = '../tmp/tree.pkl' #\u6a21\u578b\u8f93\u51fa\u540d\u5b57 tree = DecisionTreeClassifier() #\u5efa\u7acb\u51b3\u7b56\u6811\u6a21\u578b tree.fit(train[:,:3], train[:,3]) #\u8bad\u7ec3  #\u4fdd\u5b58\u6a21\u578b from sklearn.externals import joblib joblib.dump(tree, treefile)  from sklearn.metrics import roc_curve  # \u5bfc\u5165ROC\u66f2\u7ebf  predict_result = net.predict(test[:, :3]).reshape(len(test))  # \u9884\u6d4b\u7ed3\u679c\u53d8\u5f62 fpr, tpr, thresholds = roc_curve(test[:, 3], predict_result, pos_label=1) plt.plot(fpr, tpr, linewidth=2, label='LM\u795e\u7ecf\u7f51\u7edcROC\u66f2\u7ebf')  # \u4f5c\u51faLM\u7684ROC\u66f2\u7ebf  fpr1, tpr1, thresholds1 = roc_curve(test[:, 3], tree.predict_proba(test[:, :3])[:, 1], pos_label=1) plt.plot(fpr1, tpr1, linewidth=2, label='\u51b3\u7b56\u6811ROC\u66f2\u7ebf')  # \u505a\u51faROC\u66f2\u7ebf plt.xlabel('\u5047\u6b63\u4f8b\u7387') #\u5750\u6807\u8f74\u6807\u7b7e plt.ylabel('\u771f\u6b63\u4f8b\u7387') #\u5750\u6807\u8f74\u6807\u7b7e plt.xlim(0, 1.05) plt.ylim(0, 1.05) plt.legend(loc=4) plt.title('LM\u795e\u7ecf\u7f51\u7edc\u548c\u51b3\u7b56\u6811\u6a21\u578b\u6bd4\u8f83') plt.savefig('dt_lm_roc.jpg')#pip install pillow plt.show()    \u8fdb\u884c\u7a83\u6f0f\u7535\u8bca\u65ad\uff1a\u5728\u7ebf\u76d1\u6d4b\u7528\u6237\u7528\u7535\u8d1f\u8377\u53ca\u7ec8\u7aef\u62a5\u8b66\u6570\u636e\uff0c\u5e76\u7ecf\u8fc7\u5904\u7406\uff0c\u5f97\u5230\u6a21\u578b\u8f93\u5165\u6570\u636e\uff0c\u5229\u7528\u6784\u5efa\u597d\u7684\u7a83\u6f0f\u7535\u7528\u6237\u8bc6\u522b\u6a21\u578b\u8ba1\u7b97\u7528\u6237\u7684\u7a83\u6f0f\u7535\u8bca\u65ad\u7ed3\u679c\uff0c\u5b9e\u73b0\u7a83\u6f0f\u7535\u7528\u6237\u5b9e\u65f6\u8bca\u65ad\uff0c\u5e76\u4e0e\u5b9e\u9645\u7a3d\u67e5\u7ed3\u679c\u4f5c\u5bf9\u6bd4\u3002  \u4e03\u3001\u822a\u7a7a\u516c\u53f8\u5ba2\u6237\u4ef7\u503c\u5206\u6790 7-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u6839\u636e\u822a\u7a7a\u516c\u53f8\u63d0\u4f9b\u7684\u6570\u636e\uff0c\u5bf9\u5176\u5ba2\u6237\u8fdb\u884c\u5206\u7c7b\u3002\u5bf9\u4e0d\u540c\u7684\u5ba2\u6237\u7c7b\u522b\u8fdb\u884c\u7279\u5f81\u5206\u6790\uff0c\u5e76\u4e14\u6bd4\u8f83\u4e0d\u540c\u7c7b\u522b\u5ba2\u6237\u7684\u4ef7\u503c\u3002\u5bf9\u4e0d\u540c\u4ef7\u503c\u7684\u5ba2\u6237\u7c7b\u522b\u63d0\u4f9b\u4e2a\u6027\u5316\u670d\u52a1\uff0c\u5236\u5b9a\u76f8\u5e94\u7684\u8425\u9500\u7b56\u7565\u3002  7-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u62bd\u53d6\uff1a\u4ee52014-03-31\u4e3a\u7ed3\u675f\u65f6\u95f4\uff0c\u9009\u53d6\u5bbd\u5ea6\u4e3a\u4e24\u5e74\u7684\u65f6\u95f4\u6bb5\u4f5c\u4e3a\u5206\u6790\u89c2\u6d4b\u7a97\u53e3\uff0c\u62bd\u53d6\u6240\u6709\u5ba2\u6237\u7684\u4e58\u673a\u8bb0\u5f55\u5f62\u6210\u5386\u53f2\u6570\u636e\u3002\u540e\u7eed\u65b0\u589e\u7684\u5ba2\u6237\u91c7\u7528\u540c\u6837\u7684\u65b9\u6cd5\u8fdb\u884c\u62bd\u53d6\uff0c\u5f62\u6210\u589e\u91cf\u6570\u636e\u3002 \u6570\u636e\u63a2\u7d22\u5206\u6790\uff1a\u672c\u6848\u4f8b\u5bf9\u6570\u636e\u8fdb\u884c\u7f3a\u5931\u503c\u5206\u6790\uff0c\u5206\u6790\u51fa\u6570\u636e\u7684\u89c4\u5f8b\u4ee5\u53ca\u5f02\u5e38\u503c\u3002 \u6570\u636e\u9884\u5904\u7406\uff1a\u672c\u6848\u4f8b\u4e3b\u8981\u91c7\u7528\u6570\u636e\u6e05\u6d17\u3001\u5c5e\u6027\u89c4\u7ea6\u548c\u6570\u636e\u53d8\u6362\u7684\u9884\u5904\u7406\u65b9\u6cd5 \u6570\u636e\u6e05\u6d17\uff1a\u901a\u8fc7\u6570\u636e\u63a2\u7d22\u5206\u6790\uff0c\u53d1\u73b0\u6570\u636e\u4e2d\u5b58\u5728\u7f3a\u5931\u503c\uff0c\u7968\u4ef7\u6700\u5c0f\u503c\u4e3a0\u3001\u6298\u6263\u7387\u6700\u5c0f\u503c\u4e3a0\u3001\u603b\u98de\u884c\u516c\u91cc\u6570\u5927\u4e8e0\u7684\u8bb0\u5f55\u3002\u7531\u4e8e\u539f\u59cb\u6570\u636e\u91cf\u5927\uff0c\u8fd9\u7c7b\u6570\u636e\u6240\u5360\u6bd4\u4f8b\u5c0f\uff0c\u5bf9\u95ee\u9898\u5f71\u54cd\u4e0d\u5927\uff0c\u56e0\u6b64\u5bf9\u5176\u8fdb\u884c\u4e22\u5f03\u5904\u7406\u3002(\u7528Pandas\u5904\u7406) \u5c5e\u6027\u89c4\u7ea6\uff1a\u539f\u59cb\u6570\u636e\u4e2d\u5c5e\u6027\u592a\u591a\uff0c\u5220\u9664\u4e0e\u5176\u4e0d\u76f8\u5173\u3001\u5f31\u76f8\u5173\u6216\u5197\u4f59\u7684\u5c5e\u6027\u3002 \u6570\u636e\u53d8\u6362\uff1a\u5c06\u6570\u636e\u8f6c\u6362\uff0c\u4ee5\u9002\u5e94\u6316\u6398\u4efb\u52a1\u53ca\u7b97\u6cd5\u3002\u672c\u4f8b\u4e3b\u8981\u91c7\u7528\u5c5e\u6027\u6784\u9020\u548c\u6570\u636e\u6807\u51c6\u5316\u7684\u6570\u636e\u53d8\u6362\u3002  7-3\u3001\u6a21\u578b\u6784\u5efa \u6839\u636e\u822a\u7a7a\u516c\u53f8\u5ba2\u62375\u4e2a\u6307\u6807\u7684\u6570\u636e\uff0c\u5bf9\u5ba2\u6237\u8fdb\u884c\u805a\u7c7b\u5206\u7fa4\uff1b\u7ed3\u5408\u4e1a\u52a1\u5bf9\u6bcf\u4e2a\u5ba2\u6237\u7fa4\u8fdb\u884c\u7279\u5f81\u5206\u6790\uff0c\u5206\u6790\u5176\u5ba2\u6237\u4ef7\u503c\uff0c\u5e76\u5bf9\u6bcf\u4e2a\u5ba2\u6237\u7fa4\u8fdb\u884c\u6392\u540d  \u5ba2\u6237\u805a\u7c7b\uff1a\u91c7\u7528K-Means\u805a\u7c7b\u7b97\u6cd5\u5bf9\u5ba2\u6237\u8fdb\u884c\u5ba2\u6237\u5206\u7fa4\uff0c\u805a\u62105\u7c7b\uff0c\u8fdb\u884c\u5ba2\u6237\u4ef7\u503c\u5206\u6790\u3002  #-*- coding: utf-8 -*- #K-Means\u805a\u7c7b\u7b97\u6cd5  import pandas as pd from pandas import DataFrame from sklearn.cluster import KMeans #\u5bfc\u5165K\u5747\u503c\u805a\u7c7b\u7b97\u6cd5  inputfile = '../tmp/zscoreddata.xls' #\u5f85\u805a\u7c7b\u7684\u6570\u636e\u6587\u4ef6 k = 5                       #\u9700\u8981\u8fdb\u884c\u7684\u805a\u7c7b\u7c7b\u522b\u6570 #\u8bfb\u53d6\u6570\u636e\u5e76\u8fdb\u884c\u805a\u7c7b\u5206\u6790 data = pd.read_excel(inputfile) #\u8bfb\u53d6\u6570\u636e #\u8c03\u7528k-means\u7b97\u6cd5\uff0c\u8fdb\u884c\u805a\u7c7b\u5206\u6790 kmodel = KMeans(n_clusters = k, n_jobs = 1) #n_jobs\u662f\u5e76\u884c\u6570\uff0c\u4e00\u822c\u7b49\u4e8eCPU\u6570\u8f83\u597d kmodel.fit(data) #\u8bad\u7ec3\u6a21\u578b print(kmodel.cluster_centers_) #\u67e5\u770b\u805a\u7c7b\u4e2d\u5fc3 print(kmodel.labels_) #\u67e5\u770b\u5404\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b labels = kmodel.labels_#\u67e5\u770b\u5404\u6837\u672c\u7c7b\u522b demo= DataFrame(kmodel.cluster_centers_, columns=data.columns) # \u4fdd\u5b58\u805a\u7c7b\u4e2d\u5fc3 # demo2= demo['numbers'].value_counts() # \u786e\u5b9a\u5404\u4e2a\u7c7b\u7684\u6570\u76ee #\u753b\u96f7\u8fbe\u56fe \u5ba2\u6237\u7fa4\u7279\u5f81\u5206\u6790\u56fe data = demo.values from example.chapter7.demo.code.radar import drawRader title = 'RadarPicture' rgrids = [0.5, 1, 1.5, 2, 2.5] itemnames = ['ZL','ZR','ZF','ZM','ZC'] labels = ['\u91cd\u8981\u4fdd\u6301\u5ba2\u6237','\u91cd\u8981\u53d1\u5c55\u5ba2\u6237','\u91cd\u8981\u633d\u7559\u5ba2\u6237','\u4e00\u822c\u5ba2\u6237','\u4f4e\u4ef7\u683c\u5ba2\u6237'] drawRader(itemnames=itemnames,data=data,title=title,labels=labels, saveas = '2.jpg',rgrids=rgrids)   \u516b\u3001\u4e2d\u533b\u8bc1\u578b\u5173\u8054\u89c4\u5219\u6316\u6398 8-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u6839\u636e\u76f8\u5173\u6570\u636e\u5efa\u6a21\uff0c\u83b7\u53d6\u4e2d\u533b\u8bc1\u578b\u4e0e\u4e73\u817a\u764c\u4e4b\u95f4\u7684\u5173\u8054\u5173\u7cfb\uff0c\u5bf9\u6cbb\u7597\u63d0\u4f9b\u4f9d\u636e\uff0c\u6316\u6398\u6f5c\u6027\u8bc1\u7d20\u3002  8-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u9884\u5904\u7406\uff1a\u8c03\u7528k-means\u7b97\u6cd5\uff0c\u8fdb\u884c\u805a\u7c7b\u79bb\u6563\u5316\u3002  8-3\u3001\u6a21\u578b\u6784\u5efa  \u91c7\u7528Apriori\u5173\u8054\u89c4\u5219\u7b97\u6cd5\uff0c\u6316\u6398\u5b83\u4eec\u4e4b\u95f4\u7684\u5173\u8054\u5173\u7cfb\uff0c\u63a2\u7d22\u4e73\u817a\u764c\u75c7\u60a3\u8005TNM\u5206\u671f\u4e0e\u4e2d\u533b\u8bc1\u578b\u7cfb\u6570\u4e4b\u95f4\u7684\u5173\u7cfb\u3002  \u4e5d\u3001\u57fa\u4e8e\u6c34\u8272\u56fe\u50cf\u7684\u6c34\u8d28\u8bc4\u4ef7 9-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u5229\u7528\u56fe\u50cf\u5904\u7406\u6280\u672f\uff0c\u901a\u8fc7\u6c34\u8272\u56fe\u50cf\u5b9e\u73b0\u6c34\u8d28\u7684\u81ea\u52a8\u8bc4\u4ef7\u3002  9-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u9884\u5904\u7406\uff1a\u56fe\u7247\u5207\u5272\uff0c\u56fe\u7247\u6570\u636e\u7279\u5f81\u63d0\u53d6:\u5e38\u7528\u76f4\u65b9\u56fe\u6cd5\u3001\u989c\u8272\u77e9\u3002  9-3\u3001\u6a21\u578b\u6784\u5efa  \u5bf9\u7279\u5f81\u63d0\u53d6\u540e\u7684\u6837\u672c\u8fdb\u884c\u62bd\u6837\uff0c\u62bd\u53d680%\u4f5c\u4e3a\u8bad\u7ec3\u6837\u5f0f\uff0c20%\u4f5c\u4e3a\u6d4b\u8bd5\u6837\u672c\uff0c\u7528\u4e8e\u6c34\u8d28\u8bc4\u4ef7\u68c0\u9a8c\u3002\u91c7\u7528\u652f\u6301\u5411\u91cf\u673a\u4f5c\u4e3a\u6c34\u8d28\u8bc4\u4ef7\u5206\u7c7b\u6a21\u578b\uff0c\u6a21\u578b\u7684\u8f93\u5165\u5305\u62ec\u4e24\u90e8\u5206\uff0c\u4e00\u662f\u8bad\u7ec3\u6837\u672c\u7684\u8f93\u5165\uff0c\u4e8c\u662f\u5efa\u6a21\u53c2\u6570\u7684\u8f93\u5165\u3002  \u5341\u3001\u5bb6\u7528\u7535\u5668\u7528\u6237\u884c\u4e3a\u5206\u6790\u4e0e\u4e8b\u4ef6\u8bc6\u522b 10-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u6839\u636e\u70ed\u6c34\u5668\u5382\u5546\u63d0\u4f9b\u7684\u6570\u636e\u8fdb\u884c\u5206\u6790\uff0c\u5bf9\u7528\u6237\u7684\u7528\u6c34\u4e8b\u4ef6\u8fdb\u884c\u5206\u6790\uff0c\u5224\u65ad\u7528\u6c34\u662f\u5426\u662f\u6d17\u6d74\u4e8b\u4ef6\uff0c\u8bc6\u522b\u4e0d\u540c\u7528\u6237\u7684\u7528\u6c34\u4e60\u60ef\uff0c\u4ee5\u63d0\u4f9b\u4e2a\u6027\u5316\u7684\u670d\u52a1\u3002(\u4e8c\u5206\u7c7b)  10-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u63a2\u7d22\uff1a\u901a\u8fc7\u9891\u7387\u5206\u5e03\u76f4\u65b9\u56fe\u5206\u6790\u7528\u6237\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694\u7684\u89c4\u5f8b\u6027\uff1b\u7136\u540e\uff0c\u63a2\u7a76\u5212\u5206\u4e00\u6b21\u5b8c\u6574\u7528\u6c34\u4e8b\u4ef6\u7684\u65f6\u95f4\u95f4\u9694\u9608\u503c\u3002  #-*- coding: utf-8 -*- import numpy as np import pandas as pd from pandas import DataFrame inputfile = '../data/water_heater.xls' #\u8f93\u5165\u6570\u636e\u8def\u5f84,\u9700\u8981\u4f7f\u7528Excel\u683c\u5f0f data = pd.read_excel(inputfile,encoding='utf-8') data[u'\u53d1\u751f\u65f6\u95f4'] = pd.to_datetime(data[u'\u53d1\u751f\u65f6\u95f4'], format='%Y%m%d%H%M%S')  # \u5c06\u8be5\u7279\u5f81\u8f6c\u6210\u65e5\u671f\u65f6\u95f4\u683c\u5f0f\uff08***\uff09 data = data[data[u'\u6c34\u6d41\u91cf'] > 0]  # \u53ea\u8981\u6d41\u91cf\u5927\u4e8e0\u7684\u8bb0\u5f55 # print len(data) #7679 data[u'\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694'] = data[u'\u53d1\u751f\u65f6\u95f4'].diff() / np.timedelta64(1, 'm')  # \u5c06datetime64[ns]\u8f6c\u6210 \u4ee5\u5206\u949f\u4e3a\u5355\u4f4d\uff08*****\uff09 data = data.fillna(0)  # \u66ff\u6362\u6389data[u'\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694']\u7684\u7b2c\u4e00\u4e2a\u7a7a\u503c #-----\u7b2c*1*\u6b65-----\u6570\u636e\u63a2\u7d22\uff0c\u67e5\u770b\u5404\u6570\u503c\u5217\u7684\u6700\u5927\u6700\u5c0f\u548c\u7a7a\u503c\u60c5\u51b5 data_explore = data.describe().T data_explore['null'] = len(data)-data_explore['count'] explore = data_explore[['min','max','null']] explore.columns = [u'\u6700\u5c0f\u503c',u'\u6700\u5927\u503c',u'\u7a7a\u503c\u6570'] # ----\u7b2c*2*\u6b65-----\u79bb\u6563\u5316\u4e0e\u9762\u5143\u5212\u5206 # \u5c06\u65f6\u95f4\u95f4\u9694\u5217\u6570\u636e\u5212\u5206\u4e3a0~0.1\uff0c0.1~0.2\uff0c0.2~0.3....13\u4ee5\u4e0a\uff0c\u7531\u6570\u636e\u63cf\u8ff0\u53ef\u77e5\uff0c # data[u'\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694']\u7684\u6700\u5927\u503c\u7ea6\u4e3a2094\uff0c\u56e0\u6b64\u53d6\u4e0a\u96502100 Ti = list(data[u'\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694'])  # \u5c06\u8981\u9762\u5143\u5316\u7684\u6570\u636e\u8f6c\u6210\u4e00\u7ef4\u7684\u5217\u8868 timegaplist = [0.0, 0.1, 0.2, 0.3, 0.5, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 2100]  # \u786e\u5b9a\u5212\u5206\u533a\u95f4 cats = pd.cut(Ti, timegaplist, right=False)  # \u5305\u6269\u533a\u95f4\u5de6\u7aef,\u7c7b\u4f3c\"[0,0.1)\",\uff08\u9ed8\u8ba4\u4e3a\u5305\u542b\u533a\u95f4\u53f3\u7aef\uff09 x = pd.value_counts(cats) x.sort_index(inplace=True) dx = DataFrame(x, columns=['num']) dx['fn'] = dx['num'] / sum(dx['num']) dx['cumfn'] = dx['num'].cumsum() / sum(dx['num']) f1 = lambda x: '%.2f%%' % (x * 100) dx[['f']] = dx[['fn']].applymap(f1) # -----\u7b2c*3*\u6b65-----\u753b\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694\u9891\u7387\u5206\u5e03\u76f4\u65b9\u56fe import matplotlib.pyplot as plt plt.rcParams['font.sans-serif'] = ['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e plt.rcParams['axes.unicode_minus'] = False #\u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7 fig = plt.figure() ax = fig.add_subplot(1, 1, 1) dx['fn'].plot(kind='bar') plt.ylabel(u'\u9891\u7387/\u7ec4\u8ddd') plt.xlabel(u'\u65f6\u95f4\u95f4\u9694\uff08\u5206\u949f\uff09') p = 1.0 * dx['fn'].cumsum() / dx['fn'].sum()  # \u6570\u503c\u7b49\u4e8e dx['cumfn']\uff0c\u4f46\u7c7b\u578b\u662f\u5217\u8868 dx['cumfn'].plot(color='r', secondary_y=True, style='-o', linewidth=2) plt.annotate(format((p[4]), '.4%'), xy=(7, p[4]), xytext=(7 * 0.9, p[4] * 0.95),              arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3,rad=.2\"))  # \u6dfb\u52a0\u6ce8\u91ca\uff0c\u537385%\u5904\u7684\u6807\u8bb0\u3002\u8fd9\u91cc\u5305\u62ec\u4e86\u6307\u5b9a\u7bad\u5934\u6837\u5f0f\u3002 plt.ylabel(u'\u7d2f\u8ba1\u9891\u7387') plt.title(u'\u7528\u6c34\u505c\u987f\u65f6\u95f4\u95f4\u9694\u9891\u7387\u5206\u5e03\u76f4\u65b9\u56fe') plt.grid(axis='y', linestyle='--') # fig.autofmt_xdate() #\u81ea\u52a8\u6839\u636e\u6807\u7b7e\u957f\u5ea6\u8fdb\u884c\u65cb\u8f6c for label in ax.xaxis.get_ticklabels():  # \u6b64\u8bed\u53e5\u5b8c\u6210\u529f\u80fd\u540c\u4e0a,\u4f46\u662f\u53ef\u4ee5\u81ea\u5b9a\u4e49\u65cb\u8f6c\u89d2\u5ea6     label.set_rotation(60) #plt.savefig('../data/Water-pause-times.jpg') plt.show()   10-3\u3001\u6a21\u578b\u6784\u5efa  \u4f7f\u7528Keras\u5e93\u6765\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\uff0c\u8bad\u7ec3\u6837\u672c\u4e3a\u6839\u636e\u7528\u6237\u8bb0\u5f55\u7684\u65e5\u5fd7\u6807\u8bb0\u597d\u7684\u7528\u6c34\u4e8b\u4ef6\u3002\u6839\u636e\u6837\u672c\uff0c\u5f97\u5230\u8bad\u7ec3\u597d\u7684\u795e\u7ecf\u7f51\u7edc\u540e\uff0c\u5c31\u53ef\u4ee5\u7528\u6765\u8bc6\u522b\u5bf9\u5e94\u7528\u6237\u7684\u6d17\u6d74\u4e8b\u4ef6\u3002  10-4\u3001\u6a21\u578b\u68c0\u9a8c  \u6839\u636e\u7528\u6c34\u65e5\u5fd7\u6765\u5224\u65ad\u4e8b\u4ef6\u662f\u5426\u4e3a\u6d17\u6d74\u4e0e\u591a\u5c42\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u8bc6\u522b\u7ed3\u679c\u7684\u6bd4\u8f83\uff0c\u68c0\u9a8c\u6a21\u578b\u7684\u51c6\u786e\u7387\u3002  \u5341\u4e00\u3001\u5e94\u7528\u7cfb\u7edf\u8d1f\u8f7d\u5206\u6790\u4e0e\u78c1\u76d8\u5bb9\u91cf\u9884\u6d4b 11-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u6839\u636e\u5386\u53f2\u78c1\u76d8\u6570\u636e\uff0c\u91c7\u7528\u65f6\u95f4\u5e8f\u5217\u5206\u6790\u6cd5\uff0c\u6765\u9884\u6d4b\u5e94\u7528\u7cfb\u7edf\u670d\u52a1\u5668\u78c1\u76d8\u5df2\u7ecf\u4f7f\u7528\u7a7a\u95f4\u7684\u5927\u5c0f\uff1b\u4e3a\u7ba1\u7406\u5458\u63d0\u4f9b\u5b9a\u5236\u5316\u7684\u9884\u8b66\u63d0\u793a\u3002(\u65f6\u95f4\u5e8f\u5217---\u56de\u5f52)  11-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u7279\u5f81\u5206\u6790\uff1a\u901a\u8fc7\u4e0b\u56fe\u53ef\u4ee5\u53d1\u73b0\uff0c\u78c1\u76d8\u7684\u4f7f\u7528\u60c5\u51b5\u90fd\u4e0d\u5177\u6709\u5468\u671f\u6027\uff0c\u8868\u73b0\u51fa\u7f13\u6162\u6027\u589e\u957f\uff0c\u5448\u73b0\u8d8b\u52bf\u6027\u3002\u53ef\u4ee5\u521d\u6b65\u786e\u8ba4\u6570\u636e\u662f\u975e\u5e73\u7a33\u7684\u3002  #-*- coding: utf-8 -*- #\u6570\u636e\u7279\u5f81\u5206\u6790(\u753b\u65f6\u5e8f\u56fe) import pandas as pd #\u53c2\u6570\u521d\u59cb\u5316 inputfile1 = '../data/discdata.xls' data = pd.read_excel(inputfile1) data.head() d = data[(data['ENTITY'] == 'C:\\\\') & (data['TARGET_ID'] == 184)] import matplotlib as mpl import matplotlib.pyplot as plt plt.rc('figure', figsize=(9, 7)) import datetime import matplotlib.dates as mdates plt.rcParams['font.sans-serif'] = ['SimHei'] plt.rcParams['axes.unicode_minus'] = False fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.set_title(u\"C\u76d8\u5df2\u4f7f\u7528\u7a7a\u95f4\u7684\u65f6\u5e8f\u56fe\") # ax.set_xlabel(u'\u65e5\u671f') ax.set(xlabel=u'\u65e5\u671f', ylabel=u'\u78c1\u76d8\u4f7f\u7528\u5927\u5c0f') # \u56fe\u4e0a\u65f6\u95f4\u95f4\u9694\u663e\u793a\u4e3a10\u5929 ax.xaxis.set_major_locator(mdates.DayLocator(bymonthday=range(1, 32), interval=10)) ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m-%d\")) plt.subplots_adjust(bottom=0.13, top=0.95) ax.plot(d['COLLECTTIME'], d['VALUE'], 'ro-', color = 'green') fig.autofmt_xdate()  # \u81ea\u52a8\u6839\u636e\u6807\u7b7e\u957f\u5ea6\u8fdb\u884c\u65cb\u8f6c '''for label in ax.xaxis.get_ticklabels():   #\u6b64\u8bed\u53e5\u5b8c\u6210\u529f\u80fd\u540c\u4e0a        label.set_rotation(45) ''' plt.savefig('../data/c.jpg') plt.show()   11-3\u3001\u6a21\u578b\u6784\u5efa  \u6a21\u578b\u68c0\u9a8c\uff1a\u4e3a\u4e86\u65b9\u4fbf\u5bf9\u6a21\u578b\u8fdb\u884c\u8bc4\u4ef7\uff0c\u5c06\u7ecf\u8fc7\u6570\u636e\u9884\u5904\u7406\u540e\u7684\u5efa\u6a21\u6570\u636e\u5212\u5206\u4e24\u90e8\u5206\uff0c\u4e00\u662f\u5efa\u6a21\u6837\u672c\uff0c\u4e8c\u662f\u6a21\u578b\u9a8c\u8bc1\u6570\u636e\u3002\u672c\u4f8b\u786e\u5b9a\u7684ARIMA\uff080,1,1\uff09\u6a21\u578b\u901a\u8fc7\u68c0\u9a8c\u3002  11-4\u3001\u6a21\u578b\u8bc4\u4ef7  \u4e3a\u4e86\u8bc4\u4ef7\u65f6\u5e8f\u9884\u6d4b\u6a21\u578b\u6548\u679c\u7684\u597d\u574f\uff0c\u9009\u62e9\u5efa\u6a21\u6570\u636e\u7684\u540e5\u6761\u8bb0\u5f55\u4f5c\u4e3a\u5b9e\u9645\u503c\uff0c\u5c06\u9884\u6d4b\u503c\u4e0e\u5b9e\u9645\u503c\u8fdb\u884c\u8bef\u5dee\u5206\u6790\u3002\u8bef\u5dee\u5728\u53ef\u63a5\u53d7\u8303\u56f4\u5185\uff0c\u5c31\u53ef\u5bf9\u6a21\u578b\u8fdb\u884c\u5e94\u7528\uff0c\u5b9e\u73b0\u5bf9\u5e94\u7528\u7cfb\u7edf\u5bb9\u91cf\u7684\u9884\u6d4b\u3002  \u5341\u4e8c\u3001\u7535\u5b50\u5546\u52a1\u7f51\u7ad9\u7528\u6237\u884c\u4e3a\u5206\u6790\u53ca\u670d\u52a1\u63a8\u8350 12-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u5206\u6790\u67d0\u7f51\u7ad9\u7684\u7528\u6237\u8bbf\u95ee\u8bb0\u5f55\uff0c\u7136\u540e\u5206\u6790\u7f51\u9875\u76f8\u5173\u4e3b\u9898\uff0c\u5206\u6790\u7528\u6237\u7f51\u4e0a\u8bbf\u95ee\u884c\u4e3a\uff1b\u501f\u52a9\u7528\u6237\u7684\u8bbf\u95ee\u8bb0\u5f55\uff0c\u53d1\u73b0\u7528\u6237\u7684\u8bbf\u95ee\u4e60\u60ef\uff0c\u5bf9\u4e0d\u540c\u7528\u6237\u8fdb\u884c\u76f8\u5173\u670d\u52a1\u9875\u9762\u7684\u63a8\u8350\u3002\u63a8\u8350\u7b97\u6cd5  12-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u7f51\u9875\u7c7b\u578b\u5206\u6790\uff1a\u7edf\u8ba1\u5404\u4e2a\u7f51\u9875\u7c7b\u578b\u6240\u5360\u7684\u6bd4\u4f8b\uff1b \u70b9\u51fb\u6b21\u6570\u5206\u6790\uff1a\u7edf\u8ba1\u5206\u6790\u539f\u59cb\u6570\u636e\u7528\u6237\u6d4f\u89c8\u7f51\u9875\u6b21\u6570\uff08\u4ee5\u201c\u771f\u5b9eIP\u201d\u533a\u5206\uff09\u7684\u60c5\u51b5 \u7f51\u9875\u6392\u540d\u5206\u6790\uff1a\u83b7\u5f97\u5404\u4e2a\u7f51\u9875\u70b9\u51fb\u7387\u6392\u540d\u4ee5\u53ca\u7c7b\u578b\u70b9\u51fb\u7387\u6392\u540d\uff1a\u7edf\u8ba1\u5206\u6790\u539f\u59cb\u6570\u636e\u7528\u6237\u6d4f\u89c8\u7f51\u9875\u6b21\u6570\uff08\u4ee5\u201c\u771f\u5b9eIP\u201d\u533a\u5206\uff09  12-3\u3001\u6a21\u578b\u6784\u5efa  \u63a8\u8350\u7b97\u6cd5\uff1a\u672c\u4f8b\u6240\u7528\u7684\u7b97\u6cd5\u4e3a\u534f\u540c\u8fc7\u6ee4\u4e2d\u7684\u57fa\u4e8e\u7269\u54c1\u7684\u8fc7\u6ee4\uff0c\u5373\u5c06\u54ea\u4e2a\u7269\u54c1\u63a8\u8350\u7ed9\u67d0\u4e2a\u7528\u6237\u3002  12-4\u3001\u6a21\u578b\u8bc4\u4ef7  \u672c\u4f8b\u91c7\u7528\u7684\u662f\u6700\u57fa\u672c\u7684\u534f\u540c\u8fc7\u6ee4\u7b97\u6cd5\u8fdb\u884c\u5efa\u6a21\uff0c\u56e0\u4e9b\u5f97\u51fa\u7684\u6a21\u578b\u7ed3\u679c\u4e5f\u662f\u4e00\u4e2a\u521d\u6b65\u7684\u6548\u679c\uff0c\u5b9e\u9645\u5e94\u7528\u4e2d\u8981\u7ed3\u5408\u4e1a\u52a1\u8fdb\u884c\u5206\u6790\u3002  \u5341\u4e09\u3001\u8d22\u653f\u6536\u5165\u5f71\u54cd\u56e0\u7d20\u5206\u6790\u53ca\u9884\u6d4b\u6a21\u578b 13-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u6839\u636e1994-2013\u5e74\u76f8\u5173\u8d22\u653f\u6570\u636e \uff0c\u68b3\u7406\u5f71\u54cd\u5730\u65b9\u8d22\u653f\u6536\u5165\u7684\u5173\u952e\u7279\u5f81\uff0c\u5bf9\u672a\u6765\u51e0\u5e74\u7684\u8d22\u653f\u6570\u636e\u8fdb\u884c\u9884\u6d4b\u3002 \u672c\u4f8b\u7528\u5230\u4e86\u56de\u5f52\u7b97\u6cd5\u3002  13-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u63a2\u7d22\u6027\u5206\u6790\uff1a\u5bf9\u53d8\u91cf\u8fdb\u884c\u63cf\u8ff0\u6027\u5206\u6790\u548c\u76f8\u5173\u6027\u5206\u6790\u3002  13-3\u3001\u6a21\u578b\u6784\u5efa  \u5229\u7528\u4e86Adaptive-Lasso\u8fdb\u884c\u53d8\u91cf\u9009\u62e9\uff0cAdaptiveLasso\u7b97\u6cd5\uff0c\u8981\u5728\u8f83\u65b0\u7684Scikit-Learn\u624d\u6709\uff0c\u8fdb\u884c\u9884\u6d4b\u6a21\u578b\u6784\u5efa\u3002  \u5341\u56db\u3001\u57fa\u4e8e\u57fa\u7ad9\u5b9a\u4f4d\u6570\u636e\u7684\u5546\u5708\u5206\u6790 14-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u4ece\u67d0\u901a\u4fe1\u8fd0\u8425\u5546\u63d0\u4f9b\u7684\u7279\u5b9a\u63a5\u53e3\u89e3\u6790\u5f97\u5230\u7528\u6237\u7684\u5b9a\u4f4d\u6570\u636e\u3002\u5229\u7528\u57fa\u7ad9\u5c0f\u533a\u7684\u8986\u76d6\u8303\u56f4\u4f5c\u4e3a\u5546\u5708\u533a\u57df\u7684\u5212\u5206\uff0c\u5f52\u7eb3\u51fa\u5546\u5708\u7684\u4eba\u6d41\u7279\u5f81\u548c\u89c4\u5f8b\uff0c\u8bc6\u522b\u51fa\u4e0d\u540c\u7c7b\u522b\u7684\u5546\u5708\uff0c\u9009\u62e9\u5408\u9002\u7684\u533a\u57df\u8fdb\u884c\u8fd0\u8425\u5546\u7684\u4fc3\u9500\u6d3b\u52a8\u3002\uff08\u805a\u7c7b\uff09  14-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u7531\u4e8e\u5404\u4e2a\u5c5e\u6027\u4e4b\u95f4\u7684\u5dee\u5f02\u8f83\u5927\uff0c\u4e3a\u4e86\u6d88\u9664\u6570\u91cf\u7ea7\u6570\u636e\u5e26\u6765\u7684\u5f71\u54cd\uff0c\u5728\u8fdb\u884c\u805a\u7c7b\u524d\uff0c\u9700\u8981\u8fdb\u884c\u79bb\u5dee\u6807\u51c6\u5316\u5904\u7406\u3002  14-3\u3001\u6a21\u578b\u6784\u5efa  \u6570\u636e\u7ecf\u8fc7\u9884\u5904\u7406\u540e\uff0c\u5f62\u6210\u5efa\u6a21\u6570\u636e\u3002\u91c7\u7528\u5c42\u6b21\u805a\u7c7b\u7b97\u6cd5\u5bf9\u5efa\u6a21\u6570\u636e\u8fdb\u884c\u57fa\u4e8e\u57fa\u7ad9\u6570\u636e\u7684\u5546\u5708\u805a\u7c7b\uff0c\u753b\u51fa\u8c31\u7cfb\u805a\u7c7b\u56fe\u3002  #-*- coding: utf-8 -*- #\u8c31\u7cfb\u805a\u7c7b\u56fe import pandas as pd #\u53c2\u6570\u521d\u59cb\u5316 standardizedfile = '../data/standardized.xls' #\u6807\u51c6\u5316\u540e\u7684\u6570\u636e\u6587\u4ef6 data = pd.read_excel(standardizedfile, index_col = u'\u57fa\u7ad9\u7f16\u53f7') #\u8bfb\u53d6\u6570\u636e import matplotlib.pyplot as plt from scipy.cluster.hierarchy import linkage,dendrogram #\u8fd9\u91cc\u4f7f\u7528scipy\u7684\u5c42\u6b21\u805a\u7c7b\u51fd\u6570 Z = linkage(data, method = 'ward', metric = 'euclidean') #\u8c31\u7cfb\u805a\u7c7b\u56fe P = dendrogram(Z, 0) #\u753b\u8c31\u7cfb\u805a\u7c7b\u56fe plt.savefig('../data/plot.jpg') plt.show()   \u53ef\u628a\u805a\u7c7b\u7c7b\u522b\u53d63\u7c7b\uff0c\u5c42\u6b21\u805a\u7c7b\u7b97\u6cd5\u4ee3\u7801\u5982\u4e0b #-*- coding: utf-8 -*- #\u5c42\u6b21\u805a\u7c7b\u7b97\u6cd5 import pandas as pd  #\u53c2\u6570\u521d\u59cb\u5316 standardizedfile = '../data/standardized.xls' #\u6807\u51c6\u5316\u540e\u7684\u6570\u636e\u6587\u4ef6 k = 3 #\u805a\u7c7b\u6570 data = pd.read_excel(standardizedfile, index_col = u'\u57fa\u7ad9\u7f16\u53f7') #\u8bfb\u53d6\u6570\u636e  from sklearn.cluster import AgglomerativeClustering #\u5bfc\u5165sklearn\u7684\u5c42\u6b21\u805a\u7c7b\u51fd\u6570 model = AgglomerativeClustering(n_clusters = k, linkage = 'ward') model.fit(data) #\u8bad\u7ec3\u6a21\u578b  #\u8be6\u7ec6\u8f93\u51fa\u539f\u59cb\u6570\u636e\u53ca\u5176\u7c7b\u522b r = pd.concat([data, pd.Series(model.labels_, index = data.index)], axis = 1)  #\u8be6\u7ec6\u8f93\u51fa\u6bcf\u4e2a\u6837\u672c\u5bf9\u5e94\u7684\u7c7b\u522b r.columns = list(data.columns) + [u'\u805a\u7c7b\u7c7b\u522b'] #\u91cd\u547d\u540d\u8868\u5934  import matplotlib.pyplot as plt plt.rcParams['font.sans-serif'] = ['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e plt.rcParams['axes.unicode_minus'] = False #\u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7  style = ['ro-', 'go-', 'bo-'] xlabels = [u'\u5de5\u4f5c\u65e5\u4eba\u5747\u505c\u7559\u65f6\u95f4', u'\u51cc\u6668\u4eba\u5747\u505c\u7559\u65f6\u95f4', u'\u5468\u672b\u4eba\u5747\u505c\u7559\u65f6\u95f4', u'\u65e5\u5747\u4eba\u6d41\u91cf'] pic_output = '../tmp/type_' #\u805a\u7c7b\u56fe\u6587\u4ef6\u540d\u524d\u7f00  for i in range(k): #\u9010\u4e00\u4f5c\u56fe\uff0c\u4f5c\u51fa\u4e0d\u540c\u6837\u5f0f   plt.figure()   tmp = r[r[u'\u805a\u7c7b\u7c7b\u522b'] == i].iloc[:,:4] #\u63d0\u53d6\u6bcf\u4e00\u7c7b   for j in range(len(tmp)):     plt.plot(range(1, 5), tmp.iloc[j], style[i])      plt.xticks(range(1, 5), xlabels, rotation = 20) #\u5750\u6807\u6807\u7b7e   plt.title(u'\u5546\u5708\u7c7b\u522b%s' %(i+1)) #\u6211\u4eec\u8ba1\u6570\u4e60\u60ef\u4ece1\u5f00\u59cb   plt.subplots_adjust(bottom=0.15) #\u8c03\u6574\u5e95\u90e8   plt.savefig(u'%s%s.png' %(pic_output, i+1)) #\u4fdd\u5b58\u56fe\u7247     \u5341\u4e94\u3001\u7535\u5546\u4ea7\u54c1\u8bc4\u8bba\u6570\u636e\u60c5\u611f\u5206\u6790 15-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u5bf9\u4eac\u4e1c\u5e73\u53f0\u4e0a\u7684\u70ed\u6c34\u5668\u8bc4\u8bba\u8fdb\u884c\u6587\u672c\u6316\u6398\u5206\u6790\uff0c\u6316\u6398\u5efa\u6a21\u5982\u4e0b\uff1a 1\uff09\u5206\u6790\u67d0\u4e00\u4e2a\u54c1\u724c\u70ed\u6c34\u5668\u7684\u7528\u6237\u60c5\u611f\u503e\u5411 2\uff09\u4ece\u8bc4\u8bba\u6587\u672c\u4e2d\u6316\u6398\u51fa\u8be5\u54c1\u724c\u70ed\u6c34\u5668\u7684\u4f18\u70b9\u548c\u4e0d\u8db3 3\uff09\u63d0\u70bc\u4e0d\u540c\u54c1\u724c\u70ed\u6c34\u5668\u7684\u5356\u70b9  15-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u8bc4\u8bba\u6570\u636e\u62bd\u53d6\u65e8\u5728\u9009\u62e9\u67d0\u4e00\u4e2a\u5177\u4f53\u54c1\u724c\u8fdb\u884c\u8bc4\u8bba\u5206\u6790\uff0c\u6309\u7167\u4e66\u4e2d\u6b65\u9aa4\u9009\u62e9\u62bd\u53d6\u7f8e\u7684\u54c1\u724c\u7684\u8bc4\u8bba\u6570\u636e\u3002 \u8bc4\u8bba\u6570\u636e\u62bd\u53d6\u5b8c\u6210\u540e\uff0c\u9700\u8981\u5bf9\u6570\u636e\u8fdb\u884c\u9884\u5904\u7406\uff0c\u9884\u5904\u7406\u5305\u62ec\u6587\u672c\u53bb\u91cd\u3001\u673a\u68b0\u538b\u7f29\u53bb\u8bcd\u4ee5\u53ca\u77ed\u53e5\u5220\u9664\u3002 \u5728\u6784\u5efa\u6a21\u578b\u4e4b\u524d\uff0c\u5148\u5229\u7528\u6b66\u6c49\u5927\u5b66\u7684\u5185\u5bb9\u5206\u6790\u5de5\u5177ROSTCM6\u5bf9\u6587\u672c\u8fdb\u884c\u4e86\u60c5\u611f\u5206\u6790\uff0c\u5f97\u51fa\u4e86\u79ef\u6781\u6837\u672c\u548c\u6d88\u6781\u6837\u672c\uff0c\u7136\u540e\u9700\u8981\u5bf9\u4e24\u8005\u8fdb\u884c\u5220\u9664\u524d\u7f00\u8bc4\u5206\u5904\u7406\u548c\u5206\u8bcd\u5904\u7406\u3002  15-3\u3001\u6a21\u578b\u6784\u5efa  \u672c\u4f8b\u91c7\u7528\u6a21\u578b\u4e3aLDA\u4e3b\u9898\u6a21\u578b\u3002  \u5341\u516d\u3001\u4f01\u4e1a\u5077\u6f0f\u7a0e\u8bc6\u522b\u6a21\u578b 16-1\u3001\u80cc\u666f\u4e0e\u6316\u6398\u76ee\u6807  \u4f9d\u636e\u6c7d\u8f66\u9500\u552e\u4f01\u4e1a\u7684\u90e8\u5206\u7ecf\u8425\u6307\u6807\u7684\u6570\u636e\uff0c\u6765\u8bc4\u4f30\u6c7d\u8f66\u9500\u552e\u884c\u4e1a\u7eb3\u7a0e\u4eba\u7684\u5077\u6f0f\u7a0e\u503e\u5411\uff0c\u5efa\u7acb\u5077\u6f0f\u7a0e\u884c\u4e3a\u8bc6\u522b\u6a21\u578b\u3002  16-2\u3001\u6570\u636e\u63a2\u7d22\u5206\u6790\u53ca\u6570\u636e\u9884\u5904\u7406  \u6570\u636e\u5206\u6790\uff1a\u5176\u4e2d\u4e00\u4e2a\u662f\u7eb3\u7a0e\u4eba\u7f16\u53f7\uff0c\u4e00\u4e2a\u662f\u662f\u5426\u5077\u6f0f\u7a0e\u7684\u8f93\u51fa\u7ed3\u679c\uff0c\u4e0d\u5077\u7a0e\u4e3a\u6b63\u5e38\uff0c\u5b58\u5728\u5077\u6f0f\u7a0e\u5219\u4e3a\u5f02\u5e38\uff0c\u5176\u4ed6\u90fd\u4e3a\u4e0e\u5077\u6f0f\u7a0e\u76f8\u5173\u7684\u7ecf\u8425\u6307\u6807\u3002\u672c\u4f8b\u5c06\u5206\u522b\u4ece\u5206\u7c7b\u53d8\u91cf\u548c\u6570\u503c\u578b\u53d8\u91cf\u4e24\u4e2a\u65b9\u9762\u5165\u624b\u5bf9\u6570\u636e\u505a\u4e00\u4e2a\u63a2\u7d22\u6027\u5206\u6790 \u5206\u7c7b\u53d8\u91cf\uff1a\u9500\u552e\u7684\u6c7d\u8f66\u7c7b\u578b\u548c\u9500\u552e\u6a21\u5f0f\u53ef\u80fd\u4f1a\u5bf9\u5077\u6f0f\u7a0e\u503e\u5411\u6709\u4e00\u5b9a\u7684\u8868\u5f81\uff0c\u753b\u51fa\u3010\u8f93\u51fa\u7ed3\u679c\u4e3a\u5f02\u5e38\u7684\u9500\u552e\u7c7b\u578b\u548c\u9500\u552e\u6a21\u5f0f\u3011\u7684\u5206\u5e03\u56fe\u53ef\u4ee5\u76f4\u89c2\u4e0a\u770b\u51fa\u662f\u5426\u6709\u4e00\u5b9a\u5f71\u54cd\u3002  #-*- coding: utf-8 -*- import pandas as pd import matplotlib.pyplot as plt #\u5bfc\u5165Matplotlib inputfile = '../data/\u6837\u672c\u6570\u636e.xls' df = pd.read_excel(inputfile, encoding = 'utf-8')  fig=plt.figure() fig.set(alpha=0.2) #\u4e0d\u540c\u9500\u552e\u7c7b\u578b\u548c\u9500\u552e\u6a21\u5f0f\u4e0b\u7684\u5077\u6f0f\u7a0e\u60c5\u51b5 plt.rcParams['font.sans-serif'] = ['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e plt.rcParams['axes.unicode_minus'] = False #\u7528\u6765\u6b63\u5e38\u663e\u793a\u8d1f\u53f7 plt.subplot2grid((1,2),(0,0)) df_type=df[u'\u9500\u552e\u7c7b\u578b'][df[u'\u8f93\u51fa']=='\u5f02\u5e38'].value_counts() df_type.plot(kind='bar',color='Red') plt.title(u'\u4e0d\u540c\u9500\u552e\u7c7b\u578b\u4e0b\u7684\u5077\u6f0f\u7a0e\u60c5\u51b5',fontproperties='SimHei') plt.xlabel(u'\u9500\u552e\u7c7b\u578b',fontproperties='SimHei') plt.ylabel(u'\u5f02\u5e38\u6570',fontproperties='SimHei') plt.subplot2grid((1,2),(0,1)) df_model=df[u'\u9500\u552e\u6a21\u5f0f'][df[u'\u8f93\u51fa']=='\u5f02\u5e38'].value_counts() df_model.plot(kind='bar',color='Orange') plt.title(u'\u4e0d\u540c\u9500\u552e\u6a21\u5f0f\u4e0b\u7684\u5077\u6f0f\u7a0e\u60c5\u51b5',fontproperties='SimHei') plt.xlabel(u'\u9500\u552e\u6a21\u5f0f',fontproperties='SimHei') plt.ylabel(u'\u5f02\u5e38\u6570',fontproperties='SimHei') plt.subplots_adjust(wspace=0.3) plt.show()   \u6570\u503c\u578b\u53d8\u91cf\uff1a\u7528describe\u8f93\u51fa\u6570\u503c\u578b\u6570\u636e\u5206\u5e03\u60c5\u51b5  \u6570\u636e\u9884\u5904\u7406\uff1a\u5728\u6a21\u578b\u5efa\u7acb\u65f6\uff0c\u5c06\u5206\u7c7b\u53d8\u91cf\u8f6c\u6362\u6210\u865a\u62df\u53d8\u91cf\uff0c\u672c\u4f8b\u4e3b\u8981\u5bf9\u9500\u552e\u7c7b\u578b\u3001\u9500\u552e\u6a21\u5f0f\u4ee5\u53ca\u8f93\u51fa\u8fdb\u884c\u865a\u62df\u53d8\u91cf\u7684\u5efa\u7acb\u3002 Pandas\u4e2d\u6709\u76f4\u63a5\u8f6c\u6362\u7684\u51fd\u6570\uff0cget_dummies\u3002  16-3\u3001\u6a21\u578b\u6784\u5efa  \u9009\u53d680%\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c20%\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e\u3002\u91c7\u7528CART\u51b3\u7b56\u6811\u6a21\u578b\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\uff0c\u5efa\u6a21\u8bc6\u522b\u4f01\u4e1a\u5077\u6f0f\u7a0e\uff0c\u5e76\u7ed8\u5236\u4e86\u4e24\u4e2a\u6a21\u578b\u7684ROC\u66f2\u7ebf\u8fdb\u884c\u4e24\u4e2a\u6a21\u578b\u7684\u6bd4\u8f83\u3002 \u901a\u8fc7\u4e24\u4e2a\u6a21\u578b\u7ed3\u679c\u5206\u6790\uff1a\u7ef4\u4fee\u6bdb\u5229\u3001\u4ee3\u529e\u4fdd\u9669\u7387\u30014S\u5e97\u5bf9\u5077\u6f0f\u7a0e\u6709\u660e\u663e\u7684\u8d1f\u76f8\u5173\uff0c\u6210\u672c\u8d39\u7528\u5229\u6da6\u7387\u3001\u529e\u724c\u7387\u3001\u5927\u5ba2\u8f66\u548c\u4e00\u7ea7\u4ee3\u7406\u5546\u5bf9\u5077\u6f0f\u7a0e\u6709\u660e\u663e\u7684\u6b63\u76f8\u5173\u3002 \u7eb3\u7a0e\u4eba\u7ef4\u4fee\u6bdb\u5229\u8d8a\u9ad8\uff0c\u4ee3\u529e\u4fdd\u9669\u7387\u8d8a\u9ad8\uff0c\u901a\u8fc74S\u5e97\u9500\u552e\uff0c\u5176\u5077\u6f0f\u7a0e\u503e\u5411\u5c06\u4f1a\u8d8a\u4f4e\uff1b\u800c\u7eb3\u7a0e\u4eba\u6210\u672c\u8d39\u7528\u5229\u6da6\u7387\u3001\u529e\u724c\u7387\u8d8a\u9ad8\uff0c\u9500\u552e\u7c7b\u578b\u4e3a\u5927\u5ba2\u8f66\uff0c\u9500\u552e\u6a21\u5f0f\u4e3a\u4e00\u7ea7\u4ee3\u7406\u5546\uff0c\u90a3\u4e48\u8be5\u7eb3\u7a0e\u4eba\u5c06\u66f4\u6709\u53ef\u80fd\u4e3a\u5077\u6f0f\u7a0e\u7528\u6237\u3002  16-4\u3001\u6a21\u578b\u8bc4\u4ef7  \u505a\u51fa\u7684\u4e24\u4e2a\u6a21\u578b\u7684ROC\u66f2\u7ebf\u5982\u4e0b\u56fe\u6240\u793a  #-*- coding: utf-8 -*- #\u51b3\u7b56\u6811\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u6bd4\u8f83 import pandas as pd #\u5bfc\u5165\u6570\u636e\u5206\u6790\u5e93 inputfile = '../data/\u6837\u672c\u6570\u636e.xls' df = pd.read_excel(inputfile, encoding = 'utf-8') #\u6570\u636e\u9884\u5904\u7406\uff08\u5c06\u9500\u552e\u7c7b\u578b\u4e0e\u9500\u552e\u6a21\u5f0f\u4ee5\u53ca\u8f93\u51fa\u8f6c\u6362\u6210\u865a\u62df\u53d8\u91cf\uff09 type_dummies=pd.get_dummies(df[u'\u9500\u552e\u7c7b\u578b'],prefix='type') model_dummies=pd.get_dummies(df[u'\u9500\u552e\u6a21\u5f0f'],prefix='model') result_dummies=pd.get_dummies(df[u'\u8f93\u51fa'],prefix='result') df=pd.concat([df,type_dummies,model_dummies,result_dummies],axis=1) df.drop([u'\u9500\u552e\u7c7b\u578b',u'\u9500\u552e\u6a21\u5f0f',u'\u8f93\u51fa'],axis=1,inplace=True) #\u6b63\u5e38\u5217\u53bb\u9664\uff0c\u5f02\u5e38\u5217\u4f5c\u4e3a\u7ed3\u679c df.drop([u'result_\u6b63\u5e38'],axis=1,inplace=True) df.rename(columns={u'result_\u5f02\u5e38':'result'},inplace=True) #\u6570\u636e\u5212\u5206(80%\u4f5c\u4e3a\u8bad\u7ec3\u6570\u636e\uff0c20%\u4f5c\u4e3a\u6d4b\u8bd5\u6570\u636e) data=df.values from random import shuffle shuffle(data) data_train=data[:int(len(data)*0.8),:] data_test=data[int(len(data)*0.8):,:] #\u786e\u5b9ay\u503c\u548c\u7279\u5f81\u503c y=data_train[:,-1] x=data_train[:,1:-1] #\u903b\u8f91\u56de\u5f52 from sklearn import linear_model clf=linear_model.LogisticRegression(C=1.0,penalty='l1',tol=1e-6,solver='liblinear') #\u6b64\u5904\u7684x,y\u4e0e\u4e0a\u6587\u4e2d\u51b3\u7b56\u6811\u6240\u7528x,y\u76f8\u540c clf.fit(x,y)  #\u51b3\u7b56\u6811\u6a21\u578b from sklearn.tree import DecisionTreeClassifier #\u5bfc\u5165\u51b3\u7b56\u6811\u6a21\u578b tree = DecisionTreeClassifier() #\u5efa\u7acb\u51b3\u7b56\u6811\u6a21\u578b tree.fit(x, y) #\u8bad\u7ec3  #\u4e24\u4e2a\u5206\u7c7b\u65b9\u6cd5\u7684ROC\u66f2\u7ebf from sklearn.metrics import roc_curve #\u5bfc\u5165ROC\u66f2\u7ebf\u51fd\u6570 import matplotlib.pyplot as plt plt.rcParams['font.sans-serif'] = ['SimHei'] #\u7528\u6765\u6b63\u5e38\u663e\u793a\u4e2d\u6587\u6807\u7b7e fig,ax=plt.subplots() fpr, tpr, thresholds = roc_curve(data_test[:,-1], tree.predict_proba(data_test[:,1:-1])[:,1], pos_label=1) fpr2, tpr2, thresholds2 = roc_curve(data_test[:,-1], clf.predict_proba(data_test[:,1:-1])[:,1], pos_label=1) plt.plot(fpr, tpr, linewidth=2, label = 'ROC of CART', color = 'blue') #\u4f5c\u51faROC\u66f2\u7ebf plt.plot(fpr2, tpr2, linewidth=2, label = 'ROC of LR', color = 'green') #\u4f5c\u51faROC\u66f2\u7ebf plt.title('\u51b3\u7b56\u6811\u548c\u903b\u8f91\u56de\u5f52\u6a21\u578b\u6bd4\u8f83') plt.xlabel('False Positive Rate') #\u5750\u6807\u8f74\u6807\u7b7e plt.ylabel('True Positive Rate') #\u5750\u6807\u8f74\u6807\u7b7e plt.ylim(0,1.05) #\u8fb9\u754c\u8303\u56f4 plt.xlim(0,1.05) #\u8fb9\u754c\u8303\u56f4 plt.legend(loc=4) #\u56fe\u4f8b plt.show() #\u663e\u793a\u4f5c\u56fe\u7ed3\u679c    ROC\u66f2\u7ebf\u8d8a\u9760\u8fd1\u5de6\u4e0a\u89d2\uff0c\u5219\u6a21\u578b\u6027\u80fd\u8d8a\u4f18\uff0c\u5f53\u4e24\u4e2a\u66f2\u7ebf\u505a\u4e8e\u540c\u4e00\u4e2a\u5750\u6807\u65f6\uff0c\u82e5\u4e00\u4e2a\u6a21\u578b\u7684\u66f2\u7ebf\u5b8c\u5168\u5305\u4f4f\u53e6\u4e00\u4e2a\u6a21\u578b\uff0c\u5219\u524d\u8005\u4f18\uff0c\u5f53\u4e24\u8005\u6709\u4ea4\u53c9\u65f6\uff0c\u5219\u770b\u66f2\u7ebf\u4e0b\u7684\u9762\u79ef\uff0c\u4e0a\u56fe\u660e\u663e\u84dd\u8272\u7ebf\u4e0b\u7684\u9762\u79ef\u66f4\u5927\uff0c\u5373CART\u51b3\u7b56\u6811\u6a21\u578b\u6027\u80fd\u66f4\u4f18\u3002 \u7ed3\u8bba\uff1a\u5bf9\u4e8e\u672c\u4f8b\u5b50\u6765\u8bf4\uff0cCART\u51b3\u7b56\u6811\u6a21\u578b\u4e0d\u7ba1\u4ece\u6df7\u6dc6\u77e9\u9635\u6765\u770b\uff0c\u8fd8\u662f\u4eceROC\u66f2\u7ebf\u6765\u770b\uff0c\u5176\u6027\u80fd\u90fd\u8981\u4f18\u4e8e\u903b\u8f91\u56de\u5f52\u6a21\u578b\u3002  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.statsmodels.org/stable/index.html/", "http://pandas.pydata.org/pandas-docs/stable/", "http://deeplearning.net/software/theano/install.html#install/", "http://jingyan.baidu.com/season/43456/", "http://www.numpy.org/", "http://matplotlib.org/", "http://matplotlib.org/gallery.html", "http://reverland.org/python/2012/08/24/scipy/", "http://radimrehurek.com/gensim/", "http://reverland.org/python/2012/08/22/numpy/", "http://reverland.org/python/2012/09/07/matplotlib-tutorial", "http://scikit-learn.org/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7000"}, "repo_url": "https://github.com/mukul96/CarND-Behavioral-Cloning-P3", "repo_name": "CarND-Behavioral-Cloning-P3", "repo_full_name": "mukul96/CarND-Behavioral-Cloning-P3", "repo_owner": "mukul96", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T15:49:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T17:53:36Z", "homepage": null, "size": 15739, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188596821, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/mukul96/CarND-Behavioral-Cloning-P3/blob/f4ef59072ca791071ff91ee17a9b51795f46392c/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7001"}, "repo_url": "https://github.com/nonducor/T1P04_Behavior_Cloning", "repo_name": "T1P04_Behavior_Cloning", "repo_full_name": "nonducor/T1P04_Behavior_Cloning", "repo_owner": "nonducor", "repo_desc": "Behavioral cloning project", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T14:48:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T12:19:24Z", "homepage": null, "size": 40264, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188562648, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/nonducor/T1P04_Behavior_Cloning/blob/6fa57eb5dcf1b6fe883bc3b91f3741361904a6eb/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7002"}, "repo_url": "https://github.com/TIA-Lab/TILAb-Score", "repo_name": "TILAb-Score", "repo_full_name": "TIA-Lab/TILAb-Score", "repo_owner": "TIA-Lab", "repo_desc": "This repository contains the implementation of TILAb-score as described in the original paper.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T23:34:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T11:07:06Z", "homepage": null, "size": 12890, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 188407655, "is_fork": false, "readme_text": " A Novel Digital Score for Abundance of Tumour Infiltrating Lymphocytes Predicts Disease Free Survival in Oral Squamous Cell Carcinoma Table of Contents  Introduction Citation Dataset Model Prerequisites License  Introduction This repository contains the implementation of TILAb-score as described in the paper. Citation The journal paper on this work is currently under review in Nature Scientific Reports. If you use this code in your research, please cite our abstract on this work: @misc{shaban2018prognostic,     title={Prognostic significance of automated score of tumor infiltrating lymphocytes in oral cancer.},     author={Shaban, Muhammad and Khurram, Syed Ali and Hassan, Mariam and Mushtaq, Sajid and Loya, Asif and Rajpoot, Nasir},     year={2018},     publisher={American Society of Clinical Oncology} }  Dataset The datset for training should be organized in following hierarchy: dataset    -- train        -- 0_Stroma        -- 1_Non_ROI        -- 2_Tumour        -- 3_Lymphocyte    -- valid        -- 0_Stroma        -- 1_Non_ROI        -- 2_Tumour        -- 3_Lymphocyte  We have plan to release the train and validation dataset after acceptance of our paper in Nature Scientific Reports journal. Please contact Prof. Nasir Rajpoot (n.m.rajpoot@warwick.ac.uk) for dataset related queries. Training The training.py file in src/ directory will train the model using the dataset in dataset/ directory. You may need to tune the hyperparameters for training on your own dataset to train an optimal model. Model The trained model used to produce the results in the paper is available in the models/ directory. Prerequisites Following software packages will be required to run this code: -- Python 3.5    -- tensorflow-gpu=1.8.0    -- keras=2.1.6    -- openslide    -- opencv_python    -- scipy -- R packages    -- survival    -- survMisc    -- gdata    -- ggplot2    -- survminer    -- rms  Authors See the list of contributors who participated in this project. License This project is licensed under the GNU General Public License - see the LICENSE.md file for details. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/TIA-Lab/TILAb-Score/blob/ff4a81a29d817e81533f40ac104ce0df86fc563c/models/MobileNet.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7003"}, "repo_url": "https://github.com/yrims/AIC19", "repo_name": "AIC19", "repo_full_name": "yrims/AIC19", "repo_owner": "yrims", "repo_desc": "AI City Challenge Workshop at CVPR 2019 - Track 1", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T14:42:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T08:58:01Z", "homepage": "", "size": 39988, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188388610, "is_fork": false, "readme_text": "AIC19 This repository contains our source code of Track 1 at the AI City Challenge Workshop at CVPR 2019. The source code of Track 1 is built in Python, and is executed on Windows. Introduction AI City Challenge Workshop at CVPR 2019 The challenge focus on Intelligent Transportation System (ITS) problems, such as: Track 1 - City-scale multi-camera vehicle tracking Track 2 - City-scale multi-camera vehicle re-identification Track 3 - Traffic anomaly detection Pipeline The pipeline of our system is as follow:  Homography based multi-view fusion This method first uses homography matrix to project the vehicles in source videos to real world coordinate(latitude, longitude), then generates the ROI images which mask the high projected error region in each camera, finally, integrates the ROI image to inspect the multi-view fusion result. Multi-Target Single-Camera Tracking (MTSC) In MTSC tracking, we use the DeepSort and TC tracker and adjust the result according the generated ROI. The AI City Challenge also provides three MTSC trackers: TC, DeepSort, Moana. Multi-Target Multi-Camera Tracking (MTMC) In MTMC tracking, we calculate the four conditions in loss: image similarity, trajectory consistency, driving direction, travel time, and match the vehicle pairs having minimum loss. Code structure Under the ./MTMC folder, there are 3 python files:  get_track_info.py: Integrate the single camera tracking files in each scenario. get_bbox_img.py: Crop the vehicle bouding boxes in integrated single camera tracking files. MTMC.py: Multi-Target Multi-Camera Tracking based on multi-view fusion.  Under the ./Img_model folder, there are 2 python files:  train_cnn.py: Train the vehicle classification model for feature extraction. feature_extract.py: Extract the feature on the cropped vehicle bboxes.  Under the ./dataset folder:  Please download the AI City Challenge dataset from here, and put the directories S1c01~S5c36 to ./dataset/data.  Dependency  Python 3.6.7 Tensorflow-gpu 1.8.0 Keras 2.2.4 OpenCV 4.0.0  Reference ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7004"}, "repo_url": "https://github.com/M3chanix/LungLobeSegmentation", "repo_name": "LungLobeSegmentation", "repo_full_name": "M3chanix/LungLobeSegmentation", "repo_owner": "M3chanix", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T11:49:49Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-24T17:22:56Z", "homepage": null, "size": 86388, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188461794, "is_fork": false, "readme_text": "End-to-End Supervised Lung Lobe Segmentation In this project we present a fully automatic and supervised approach to the problem of the segmentation of the pulmonary lobes from a CT scan. A 3D fully convolutional neural network was used based on the V-Net wich we called Fully Regularized V-Net (FRV-Net). This work was performed in the Biomedical Imaging group at C-BER centre of INESC TEC, Portugal and it resulted in the paper \"End-to-End Supervised Lung Lobe Segmentation\" accepted to the IJCNN2018 conference. Here are the code and scripts to train our FRV-Net (as you select wich regularization techniques do you want) and to run the segmentations. Running a single segmentation with a pre-trained model. To run a single segmentation with a pre-trained model a example file called \"run_single_segmentation.py\" is available. It teaches you how to open a CT scan, to open the model and to predict and save the segmentation. Train a model If you want to train your model, a file called \"train.py\" is available. It allows you to set the specific regularization techniques and parameters of the desired net. -path : Model path      (path) -train: Train data      (path) -val  : Validation data     (path) -lr   : Set the learning rate       (float) -load : load a pre-trained model    (boolean) -aux  : Multi-task learning     (float - weight in the loss function) -ds   : Number of Deep Supervisers  (int   - n\u00ba of layers) -bn   : Set Batch normalization    (boolean) -dr   : Set Dropout                (boolean) -fs   : Number of initial of conv channels (int)  The train and validation datasets has to contain two folders A and B. where the folder A contains the CT scans and the B the correspondent ground-truth. In the script file \"train_session.sh\", the examples used for our results are presented. Software Our project was developed using Python (2.7) and Keras (2.0.4) framework that are required to use it. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/M3chanix/LungLobeSegmentation/blob/1bb304faea1aa7c0f9c0bee4e23ba8cf28b21ad9/models/final.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7005"}, "repo_url": "https://github.com/yw981/cwattack", "repo_name": "cwattack", "repo_full_name": "yw981/cwattack", "repo_owner": "yw981", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T15:07:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T08:37:11Z", "homepage": null, "size": 839, "language": "Python", "has_wiki": true, "license": {"key": "bsd-2-clause", "name": "BSD 2-Clause \"Simplified\" License", "spdx_id": "BSD-2-Clause", "url": "https://api.github.com/licenses/bsd-2-clause", "node_id": "MDc6TGljZW5zZTQ="}, "open_issues_count": 0, "github_id": 188385158, "is_fork": false, "readme_text": "About Corresponding code to the paper \"Towards Evaluating the Robustness of Neural Networks\" by Nicholas Carlini and David Wagner, at IEEE Symposium on Security & Privacy, 2017. Implementations of the three attack algorithms in Tensorflow. It runs correctly on Python 3 (and probably Python 2 without many changes). To evaluate the robustness of a neural network, create a model class with a predict method that will run the prediction network without softmax.  The model should have variables model.image_size: size of the image (e.g., 28 for MNIST, 32 for CIFAR) model.num_channels: 1 for greyscale, 3 for color images model.num_labels: total number of valid labels (e.g., 10 for MNIST/CIFAR)  Running attacks      from robust_attacks import CarliniL2      CarliniL2(sess, model).attack(inputs, targets) where inputs are a (batch x height x width x channels) tensor and targets are a (batch x classes) tensor. The L2 attack supports a batch_size paramater to run attacks in parallel. Each attack has many tunable hyper-paramaters. All are intuitive and strictly increase attack efficacy in one direction and are more efficient in the other direction. Pre-requisites The following steps should be sufficient to get these attacks up and running on most Linux-based systems.     sudo apt-get install python3-pip     sudo pip3 install --upgrade pip     sudo pip3 install pillow scipy numpy tensorflow-gpu keras h5py To create the MNIST/CIFAR models: python3 train_models.py To download the inception model: python3 setup_inception.py And finally to test the attacks python3 test_attack.py This code is provided under the BSD 2-Clause, Copyright 2016 to Nicholas Carlini. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7006"}, "repo_url": "https://github.com/davelewis91/NeuralNetFromScratch", "repo_name": "NeuralNetFromScratch", "repo_full_name": "davelewis91/NeuralNetFromScratch", "repo_owner": "davelewis91", "repo_desc": "Building a neural net by hand, from scratch - no keras, no sklearn, just good ol' numpy", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T17:22:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-25T15:48:05Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188584735, "is_fork": false, "readme_text": "Neural Net from scratch My attempt to build a neural net from scratch. I'm interested in the maths, so thought I'd build one from the base up, using pure numpy. Currently, it's just a Softmax (linear) classifier, i.e. a single layer NN. Next step is to build a NN with hidden layers. Data generator simply makes an N-dimensional spiral dataset for testing purposes ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7007"}, "repo_url": "https://github.com/raauhl/ViolenceDetection", "repo_name": "ViolenceDetection", "repo_full_name": "raauhl/ViolenceDetection", "repo_owner": "raauhl", "repo_desc": "implementation of a CNN + RNN in keras to do binary classification on videos", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T12:22:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T12:19:57Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188417041, "is_fork": false, "readme_text": "The path variable assumes the directory having two folders 'fights' and 'noFights' having the respective category videos.", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7008"}, "repo_url": "https://github.com/Chen-Dixi/TextClassification", "repo_name": "TextClassification", "repo_full_name": "Chen-Dixi/TextClassification", "repo_owner": "Chen-Dixi", "repo_desc": "use pytorch to train text classification task", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T07:56:53Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T06:14:22Z", "homepage": null, "size": 15175, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188172647, "is_fork": false, "readme_text": "TextClassification \u672c\u6587\u662f\u6625\u5b63\u8bfe\u7a0b\u201c\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e0e\u5e94\u7528\u201d\u6587\u672c\u5206\u7c7b\u4f5c\u4e1a\u7684\u7b80\u5355\u8bb0\u5f55\uff0c\u5173\u4e8e\u6587\u672c\u5206\u7c7b\u65b9\u9762\u4e0a\uff0c\u77e5\u4e4e\u4e0a\u5f88\u591a\u5f88\u597d\u7684\u6587\u7ae0\uff1a\u77e5\u4e4e\u201c\u770b\u5c71\u676f\u201d\u593a\u51a0\u8bb0\uff0c\u7528\u6df1\u5ea6\u5b66\u4e60\uff08CNN RNN Attention\uff09\u89e3\u51b3\u5927\u89c4\u6a21\u6587\u672c\u5206\u7c7b\u95ee\u9898\uff0c\u5bf9\u5199\u8fd9\u4e2a\u4f5c\u4e1a\u5e2e\u52a9\u4e5f\u5f88\u5927\u3002  \u4f5c\u4e1a\u6e90\u7801 github\u5730\u5740:TextClassification  \u4e00. \u6570\u636e\u96c6 \u4f5c\u4e1a\u4f7f\u752820_newsgroups\u6570\u636e\u96c6\uff0c\u53ef\u4ee5\u5728\u8fd9\u4e2a\u7f51\u5740\u4e2d\u4e0b\u8f7d\uff0c\u89e3\u538b\u540e\u5168\u90e8\u662f\u82f1\u6587\u6587\u672c\uff0c\u8fd9\u5c31\u9700\u8981\u81ea\u5df1\u5148\u5bf9\u6587\u672c\u8fdb\u884c\u9884\u5904\u7406\u3002 \u4e8c. \u6570\u636e\u9884\u5904\u7406   1.\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\u5212\u5206   \u6309\u7167PPT\u4e2d\u7684\u201c\u4e00\u822c\u5b9e\u9a8c\u8bbe\u7f6e\u201d\uff0c\u6211\u7b80\u5355\u7684\u628a\u6570\u636e\u96c6\u6309\u71674:1\u7684\u6bd4\u4f8b\u5212\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6   2.\u6587\u672c\u5185\u5bb9\u201c\u8fc7\u6ee4\u201d   20_newsgroups\u7684\u5185\u5bb9\u662f\u65b0\u95fb\u6587\u672c\uff0c\u5305\u542b\u5f88\u591aheader\u3001footer\u548cquotes\u5185\u5bb9 \u81ea\u5df1\u5199\u5b57\u7b26\u4e32\u8fc7\u6ee4\u6bd4\u8f83\u9ebb\u70e6\uff0c\u5e78\u8fd0\u7684\u662f\u6211\u5728sciki-learn\u7684python\u5305sklearn.datasets.fetch_20newsgroups\u91cc\u9762\u627e\u5230\u5bf920newsgroups\u8fdb\u884c\u6587\u672c\u8fc7\u6ee4\u7684\u4ee3\u7801\uff0c\u62f7\u8d1d\u8fc7\u6765\u76f4\u63a5\u4f7f\u7528\u3002   3.\u6bcf\u7bc7\u6587\u672c\u8f6c\u6362\u4e3a\u957f\u5ea6\u76f8\u7b49\u4e00\u7ef4\u6574\u6570\u5411\u91cf   \u6211\u7684\u6587\u672c\u5206\u7c7b\u5668\u7528\u7684\u662f\u5377\u79ef\u795e\u7ecf\u7f51\u7edcCNN\uff0c\u6240\u4ee5\u9664\u4e86\u628a\u6587\u672c\u4e2d\u7684\u6bcf\u4e2a\u82f1\u6587\u5355\u8bcd\u8f6c\u6362\u4e3a\u6574\u6570\u6570\u5b57\u5916\uff0c\u8fd8\u9700\u8981\u5c06\u6bcf\u7bc7\u6587\u7ae0\u901a\u8fc7pad_sequence\u8f6c\u6362\u4e3a\u4e00\u6837\u7684\u957f\u5ea6\u3002\u4e0a\u8ff0\u4e24\u4e2a\u7e41\u7410\u7684\u64cd\u4f5c\u4e5f\u53ef\u4ee5\u65b9\u4fbf\u5730\u901a\u8fc7keras\u63d0\u4f9b\u7684keras.preprocessing.text.Tokenizer\u548ckeras.preprocessing.sequence.pad_sequences\u76f4\u63a5\u5b8c\u6210\u3002 def word2index(texts,vocab_size=299567,pad=True):     #vocab_size is embedding_dim          # word\u8f6c\u4e3aindex     tokenizer = Tokenizer(num_words=vocab_size,oov_token='something')     tokenizer.fit_on_texts(texts)     sequences = tokenizer.texts_to_sequences(texts)     # pad     data = pad_sequences(sequences ,maxlen=5000,padding='post', truncating='post')     #return     return data \u901a\u8fc7\u4e0a\u97623\u4e2a\u6b65\u9aa4\uff0c\u5c06\u5f97\u5230\u7684\u6587\u672c\u548c\u6807\u7b7e\u6570\u636e\u8f6c\u4e3anumpy\u6570\u636e\uff0c\u4fdd\u5b58\u5728.npz\u6587\u4ef6\u4e2d\u65b9\u4fbf\u8bad\u7ec3\u65f6\u52a0\u8f7d\u6570\u636e\u96c6\u3002\u540c\u65f6\u83b7\u53d6\u6587\u672c\u6570\u636e\u548c\u76d1\u7763\u4fe1\u53f7(\u6807\u7b7elabel)\u7684\u4ee3\u7801\u501f\u9274\u4e86pytorch\u4e2d\u7684DatasetFolder np.savez_compressed('data/20_newsgroups_npz/train.npz',texts=train_inputs,labels=train_labels) np.savez_compressed('data/20_newsgroups_npz/test.npz',texts=test_inputs,labels=test_labels) \u4e09. \u8bad\u7ec3\u7f51\u7edc   1.\u8bcd\u5d4c\u5165Embedding   PyTorch\u4e2d\uff0c\u8bcd\u5d4c\u5165\u7684\u7ed3\u6784\u7528nn.Embedding\uff0c\u91cc\u9762\u7684\u6743\u91cd\u662f\u4e00\u4e2a\u4e8c\u7ef4\u77e9\u9635\uff0c\u6bcf\u4e00\u884c\u4ee3\u8868\u4e00\u4e2a\u5355\u8bcd\u7684\u8bcd\u5411\u91cf\u3002\u8f93\u5165\u5fc5\u987b\u662fLongTensor\u7c7b\u578b\u3002\u5176\u4ed6\u6587\u7ae0\u8bf4\uff0c\u5728\u6570\u636e\u91cf\u5145\u8db3\u7684\u60c5\u51b5\u4e0b\uff0c\u5176\u5b9e\u4e0d\u4e00\u5b9a\u9700\u8981\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf\uff0c\u4e8e\u662f\u6211\u5c31\u5f00\u59cb\u5c1d\u8bd5'train from scratch'\u3002\u95ee\u9898\u662f\u8bad\u7ec3\u5f00\u59cb\u7684\u65f6\u5019\u975e\u5e38\u4ee4\u4eba\u63ea\u5fc3\uff0c\u4eff\u4f5b\u8981\u8bad\u7ec3\u5230\u65f6\u95f4\u7684\u5c3d\u5934\ud83d\ude05\uff0c\u6240\u4ee5\u5c31\u5148\u505c\u4e86\u3002 \u6211\u5728github\u4e2d\u627e\u5230\u4e00\u4e2a\u7f29\u5c0f\u7248\u7684word2vec GoogleNews\u8bcd\u5411\u91cf\uff0c\u91cc\u9762\u6709299567\u4e2a\u82f1\u6587\u5355\u8bcd\u7684300\u7ef4\u8bcd\u5411\u91cf\u3002\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u610f\u5473\u7740\u6211\u4eec\u9700\u8981\u91cd\u65b0\u9884\u5904\u7406\u6570\u636e\u96c6\u3002\u8bcd\u5411\u91cf\u4e2d\u6bcf\u4e00\u884c\u90fd\u4ee3\u8868\u4e0d\u540c\u5355\u8bcd\uff0cword2vec\u4e2d\u9664\u4e86\u5305\u542b[299567x300]\u7684\u6743\u91cd\u5916\uff0c\u8fd8\u6709\u4e00\u4e2a\u82f1\u6587\u5355\u8bcd\u5230\u6570\u5b57\u7684vocabulary\u54c8\u5e0c\u3002\u56e0\u6b64\u6570\u636e\u9884\u5904\u7406\u7684\u7b2c3.\u6b65\u9aa4\u4e5f\u5e94\u8be5\u6309\u7167\u8fd9\u4e2avocabulary\u54c8\u5e0c\u5c06\u5355\u8bcd\u8f6c\u4e3a\u5bf9\u5e94\u7684\u6570\u5b57\uff0c\u6b64\u65f6\u6570\u636e\u9884\u5904\u7406\u4ee3\u7801\u6709\u4e00\u4e9b\u5c0f\u66f4\u6539\uff0c\u53ef\u4ee5\u5728\u6211\u7684\u7a0b\u5e8f\u4e2d\u770b\u5230\u5904\u7406\u7ec6\u8282\u3002   2.\u5206\u7c7b\u5668   embedding\u540e\u9762\u7684\u7f51\u7edc\u7ed3\u6784\u4e0d\u662f\u8fd9\u6b21\u4f5c\u4e1a\u7684\u91cd\u70b9\uff0c\u6211\u4f7f\u7528\u4e86\u5317\u90ae\u5b66\u957f\u5728\u201c\u77e5\u4e4e\u770b\u5c71\u676f\u201d\u53d6\u5f97\u7b2c\u4e00\u540d\u81ea\u521b\u7684\u7ed3\u6784CNNText_inception\uff0c\u8fd9\u662f\u5173\u4e8e\u90a3\u6b21\u6bd4\u8d5b\u7684\u7eaa\u5f55\u6587\u7ae0\uff1a\u77e5\u4e4e\u201c\u770b\u5c71\u676f\u201d \u593a\u51a0\u8bb0 \u56db. \u603b\u7ed3 \u6ca1\u6709\u8c03\u53c2\uff0c\u6ca1\u6709\u4f7f\u7528\u4ec0\u4e48\u6280\u5de7\uff0c\u51c6\u786e\u7387\u5f88\u5dee\uff0c50\u4e2aepoch\u5728\u6d4b\u8bd5\u96c6\u4e0a\u51c6\u786e\u7387\u6700\u9ad8\u53ea\u670971%\uff0c\u5c31\u8fd9\u4e48\u628a\u4f5c\u4e1a\u4ea4\u4e86\u5427\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.cs.cmu.edu/afs/cs/project/theo-11/www/naive-bayes.html"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7009"}, "repo_url": "https://github.com/AiHrt/retinal-vessel", "repo_name": "retinal-vessel", "repo_full_name": "AiHrt/retinal-vessel", "repo_owner": "AiHrt", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T14:52:52Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T01:44:16Z", "homepage": null, "size": 275412, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188141246, "is_fork": false, "readme_text": "#V-GAN # Retinal Vessel Segmentation in Fundoscopic Images with Generative Adversarial Networks [original paper] [cite]###  Package Dependency scikit_image==0.12.3 numpy==1.12.0 matplotlib==2.0.0 scipy==0.18.1 Keras==2.0.4 Pillow==4.1.1 skimage==0.0 scikit_learn==0.18.1 Directory Hierarchy . \u251c\u2500\u2500 codes \u2502\u00a0\u00a0 \u251c\u2500\u2500 evaluation.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 inference.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 model.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 train.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 utils.py \u251c\u2500\u2500 data \u2502\u00a0\u00a0 \u251c\u2500\u2500 DRIVE \u2502\u00a0\u00a0 \u2514\u2500\u2500 STARE \u251c\u2500\u2500 evaluation \u2502\u00a0\u00a0 \u251c\u2500\u2500 DRIVE \u2502\u00a0\u00a0 \u2514\u2500\u2500 STARE \u251c\u2500\u2500 inference_outputs \u2502\u00a0\u00a0 \u251c\u2500\u2500 DRIVE \u2502\u00a0\u00a0 \u2514\u2500\u2500 STARE \u251c\u2500\u2500 pretrained \u2502\u00a0\u00a0 \u251c\u2500\u2500 DRIVE_best.h5 \u2502\u00a0\u00a0 \u251c\u2500\u2500 DRIVE_best.json \u2502\u00a0\u00a0 \u251c\u2500\u2500 STARE_best.h5 \u2502\u00a0\u00a0 \u251c\u2500\u2500 STARE_best.json \u2502\u00a0\u00a0 \u251c\u2500\u2500 auc_pr_STARE.npy \u2502\u00a0\u00a0 \u251c\u2500\u2500 auc_roc_DRIVE.npy \u2502\u00a0\u00a0 \u251c\u2500\u2500 auc_roc_STARE.npy \u2502\u00a0\u00a0 \u2514\u2500\u2500 auc_roc_pr_DRIVE.npy \u2514\u2500\u2500 results     \u251c\u2500\u2500 DRIVE     \u2514\u2500\u2500 STARE  codes : source codes data : original data. File hierarchy is modified for convenience. evaluation : quantitative and qualitative evaluation. inferenced_outputs : outputs of inference with our model pretrained : pretrained model and weights results : results of other methods. These image files are retrieved from here Training Move to codes folder and run train.py  python train.py --ratio_gan2seg=<int> --gpu_index=<int> --batch_size=<int> --dataset=[DRIVE|STARE] --discriminator=[pixel|patch1|patch2|image] arguments ratio_gan2seg : trade-coefficient between GAN loss and segmentation loss gpu_index : starting index for gpus to be used batch_size : number of images per a batch dataset : type of a dataset (DRIVE or STARE) discriminator : type of a discriminator (pixel or patch1 or patch2 or image) CAVEAT Training with the current codes requires main memory more than 50 GB and GPUs dedicated to Deep Learning. If no such system is available, it is recommended to use pre-trained model only for inference. Inference Move to codes folder and run inferency.py  python inference.py Outputs of inference are generated in inference_outputs folder. Evaluation Move to codes folder and run evaluation.py  python evaluation.py Results are generated in evaluation folder. Hierarchy of the folder is . \u251c\u2500\u2500 DRIVE \u2502\u00a0\u00a0 \u251c\u2500\u2500 comparison \u2502\u00a0\u00a0 \u251c\u2500\u2500 measures \u2502\u00a0\u00a0 \u2514\u2500\u2500 vessels \u2514\u2500\u2500 STARE     \u251c\u2500\u2500 comparison     \u251c\u2500\u2500 measures     \u2514\u2500\u2500 vessels  comparison : difference maps of our method measures : ROC and PR curves vessels : vessels superimposed on segmented masks LICENSE This is under the MIT License Copyright (c) 2017 Vuno Inc. (www.vuno.co) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/AiHrt/retinal-vessel/blob/8b23f9c2b92d7e7ec9c3857eea30ffe8a1304dcf/pretrained/DRIVE_best.h5", "https://github.com/AiHrt/retinal-vessel/blob/8b23f9c2b92d7e7ec9c3857eea30ffe8a1304dcf/pretrained/STARE_best.h5"], "see_also_links": ["http://www.vuno.co", "http://www.vision.ee.ethz.ch/~cvlsegmentation/driu/downloads.html"], "reference_list": ["https://arxiv.org/pdf/1706.09318.pdf"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f700a"}, "repo_url": "https://github.com/NSGhumman/AirbusCNN", "repo_name": "AirbusCNN", "repo_full_name": "NSGhumman/AirbusCNN", "repo_owner": "NSGhumman", "repo_desc": "A convolutional neural network design for the Airbus Ship Detection Challenge", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T17:57:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T17:42:52Z", "homepage": null, "size": 27969, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188464251, "is_fork": false, "readme_text": "bestfitter-airbus-challenge-sp19 Introduction This repository contains work done on the Airbus Ship Detection Challenge as part of the Data Science Practicum class (CSCI-8360) at the University of Georgia. The competition is hosted by Kaggle and part of the task is addressed in this work. The task is originally one of semantic object segmentation but this work limits the task to segmentation at pixel level. Objects with the same label are part of the same mass and unsegmented from each other. Additionally, the competition also asks that the segmentation be done in the form of aligned bounding boxes about the ships but the methods implemented here freely label pixels without following that restriction.    Hardware Requirements The parameter space for the nets is huge and it is essential that the code is run on machines with GPUs. The default configuration is for 4x GPUs. It is set as a constant in the Brain.py module and can be easily adjusted to your machine type. Typical speed is about 3-5 epochs/hour on Nvidia P100 GPUs. Software Requirements The following python libraries are required  tensorflow-gpu numpy python-opencv (cv2) matplotlib keras  The model The Brain module currently implements a few neural network models for image segmentation. The major one - ResNetFCN - is a fully convolutional neural network built on top of the ResNet50 CNN. The end layers of ResNet50 are replaced by transpose convolutional layers that upsample the input volume. The strides are set in a way that the input dimensions are recreated at the output layer. It is the default model that is run using directions below. How to run Enter the source directory cd src  and run the brain module as python Brain.py  pass the --help argument to look at the requried parameters to pass. A typical run would look like this python Brain.py --epochs=200 --learningrate=0.01 --batchsize=20 --samplesize=40000  After the run is complete, the weights are saved in models directory in a file with model name and a timestamp as the title. Ongoing work The networks need some work done. Their learning is extremely limited at the moment. The FCN is limited by its small parameter space but the ResNetFCN - with pretrained weights on ImageNet - seems promising (refer this paper) and it appears that all it needs is some more careful thought and tweaking. LICENSE The project is licensed under the MIT license. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f700b"}, "repo_url": "https://github.com/TsWhen/PronounResolution", "repo_name": "PronounResolution", "repo_full_name": "TsWhen/PronounResolution", "repo_owner": "TsWhen", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T16:22:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T08:53:37Z", "homepage": null, "size": 217931, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188197175, "is_fork": false, "readme_text": "Deep Learning final project \u4e3b\u8981\u73af\u5883\uff1aPython3.6.8\u3001Keras 2.2.4\u3001TensorFlow-GPU 1.13.1 \u3001Cuda10.0 \u4ee3\u7801\u6587\u4ef6\u53ca\u4e3b\u8981\u51fd\u6570\u529f\u80fd\u8bf4\u660e\uff1a  \u5de5\u7a0b\u4e2dbert\u6587\u4ef6\u5939\u4ee5\u53ca\u9876\u5c42\u76ee\u5f55\u4e0btokenization.py\u3001extract_features.py\u3001modeling.py\u7686\u4e3aGoogle BERT\u6e90\u7801\u6587\u4ef6\u65e0\u9700\u5173\u6ce8\u3002 BERTembedding.py\u4e3b\u8981\u529f\u80fd\u4e3a\u5229\u7528BERT\u9884\u8bad\u7ec3\u6a21\u578b\u5728GAP\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u5fae\u8c03\uff0c\u5e76\u83b7\u53d6\u76f8\u5173\u5c42token\u7684embedding\uff0c\u8f93\u51fa\u7684embedding\u6587\u4ef6\u5b58\u5728data/vector/\u6587\u4ef6\u5939\u4e0b MLPBaseline.py\u662f\u672c\u6b21\u5de5\u7a0b\u7684baseline\u6a21\u578b NLIModel.py\u5728baseline\u7684\u57fa\u7840\u4e0a\u589e\u52a0\u4e86QA\u7279\u5f81  \u8d85\u53c2\u6570 \u6240\u6709\u8d85\u53c2\u6570\u90fd\u5728model\u4ee3\u7801\u4e2d\uff0cMLP_model\u3001NLI_model\u7684\u8f93\u5165\u53c2\u6570\u3002 bert_layer_num : \u62bd\u53d6bert\u76f8\u5e94\u5c42\u4e2d\u7684\u7279\u5f81\u5373embedding\uff0c\u9ed8\u8ba4\u8f93\u516519\uff0c\u4e0d\u5efa\u8bae\u8c03\u6574 dense_layer_size : mlp\u7f51\u7edc\u6bcf\u5c42\u7684\u795e\u7ecf\u5143\u4e2a\u6570\uff0c\u9ed8\u8ba432\uff0c\u4f18\u5148\u7ea7\u8f83\u9ad8\uff0c\u5efa\u8bae32 - 128\u4e4b\u95f4\u8fdb\u884c\u8c03\u53c2 embedding_size : embedding\u7684\u5927\u5c0f\uff0c\u6839\u636ebert\u800c\u5b9a\uff0c\u65e0\u9700\u8c03\u6574 dropout_rate \uff1a dropout\u5c42\u7684\u503c\uff0c\u8c03\u53c2\u4f18\u5148\u7ea7\u8f83\u4f4e\uff0c\u5efa\u8bae\u8303\u56f40.5-0.8\u4e4b\u95f4 lr : \u5b66\u4e60\u7387\uff0c\u8c03\u53c2\u4f18\u5148\u7ea7\u8f83\u4f4e\uff0c\u5efa\u8bae\u8303\u56f4\uff1a\u6839\u636eloss\u548cacc\u66f2\u7ebf\u8fdb\u884c\u54cd\u5e94\u8c03\u6574 epoch_num :  \u8bad\u7ec3\u8f6e\u6570\uff0c\u65e0\u9700\u8c03\u53c2 patience \uff1a loss\u6bd4\u4e0a\u4e00\u8f6e\u6ca1\u6709\u4e0b\u964d\uff0c\u5219\u7ee7\u7eed\u6267\u884cpatience\u4e2a\u8f6e\u6b21\u540e\uff0c\u63d0\u524d\u7ec8\u6b62\u8bad\u7ec3\uff0c\u4f18\u5148\u7ea7\u8f83\u4f4e lambd \uff1al2\u6b63\u5219\u5316\u53c2\u6570\uff0c\u4f18\u5148\u7ea7\u8f83\u4f4e \u8bad\u7ec3\u4e0e\u9884\u6d4b \u4fdd\u8bc1\u5bf9\u5e94\u7684\u6587\u4ef6\u7ed3\u6784\u53ea\u9700\u521b\u5efa\u76f8\u5e94\u6a21\u578b\u5bf9\u8c61\u540e\u8c03\u7528train()\u5373\u53ef\u8bad\u7ec3 \u9884\u6d4b\u524d\u4fdd\u8bc1\u8be5\u6a21\u578b\u81f3\u5c11\u8bad\u7ec3\u8fc7\u4e00\u6b21\uff0c\u5373model\u6587\u4ef6\u5939\u4e0b\u6709\u5bf9\u5e94\u7684NLI*/MLP*.pt \u6587\u4ef6,\u8fd0\u884cprediction()\u51fd\u6570\u65f6\u9700\u8981\u4f20\u5165\u9884\u6d4b\u6587\u4ef6\u8def\u5f84\u4ee5\u53ca\u6a21\u578b\u8def\u5f84 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f700c"}, "repo_url": "https://github.com/BBuf/GAN-Code", "repo_name": "GAN-Code", "repo_full_name": "BBuf/GAN-Code", "repo_owner": "BBuf", "repo_desc": "GAN", "description_language": "Indonesian", "repo_ext_links": null, "repo_last_mod": "2019-06-02T08:05:02Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T14:54:42Z", "homepage": "", "size": 20909, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188440367, "is_fork": false, "readme_text": "GAN-Code   GAN.pdf \u539f\u59cb\u7684\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u8bba\u6587\u3002   GAN.py \u539f\u59cbGAN\u7684tensorflow\u5b9e\u73b0\uff0c\u7ec6\u8282\u53ef\u4ee5\u67e5\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/79454054 ,\u8fd0\u884c80\u4e2aepoch\u751f\u6210\u7684\u7ed3\u679c\u4e3a\uff1a      DCGAN.pdf \u5377\u79ef\u5bf9\u6297\u751f\u6210\u7f51\u7edc\u7684\u8bba\u6587\u539f\u6587\u3002   DCGAN.py \u4f7f\u7528Keras\u5b9e\u73b0DCGAN\u751f\u6210MNIST\u6570\u636e\u96c6\uff0c\u8bad\u7ec3\u548c\u6d4b\u8bd5\u65b9\u6cd5\u5982\u4e0b\uff1a \u8bad\u7ec3\uff1a python dcgan.py --mode train --batch_size <batch_size> \u6d4b\u8bd5\uff1a python dcgan.py --mode generate --batch_size <batch_size> --nice   DCGAN_face.py \u4f7f\u7528Tensorflow\u5b9e\u73b0DCGAN\u751f\u6210\u4eba\u8138\uff0c\u6570\u636e\u96c6\u7684\u683c\u5f0f\u4e3a\uff1a    \uff0c \u7ec6\u8282\u53ef\u4ee5\u67e5\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/84581400   SGAN.pdf \u300aSemi-Supervised Learning with Generative Adversarial Networks\u300b\u8bba\u6587   SGAN.py SGAN\u7684tensorflow\u5b9e\u73b0\uff0c\u7ec6\u8282\u53ef\u4ee5\u67e5\u770b\uff1ahttps://blog.csdn.net/just_sort/article/details/90605197 \uff0c\u6bcf\u4e2abatch\u771f\u5b9e\u6837\u672c\u5360\u6bd40.1\u8fd0\u884c1000\u4e2aepoch\u7684\u751f\u6210\u7ed3\u679c\u4e3a\uff1a      CGAN.pdf \u6761\u4ef6GAN\u7684\u8bba\u6587\u539f\u6587\u3002   CGAN.py \u4f7f\u7528Tensorflow\u5b9e\u73b0CGAN\u751f\u6210\u7279\u5b9a\u7c7b\u522b\u7684\u624b\u5199\u6570\u5b57\u3002CGAN\u7684\u635f\u5931\u51fd\u6570\u4e0b\u964d\u8fc7\u7a0b\u4e3a\uff1a    CGAN_fashion_mnist.py \u4f7f\u7528Tensorflow\u5b9e\u73b0CGAN\u751f\u6210\u65f6\u5c1a\u8863\u67dc\uff0c\u6570\u636e\u96c6\u53ef\u4ee5\u901a\u8fc7\u6570\u636e\u4e0b\u8f7d\u811a\u672c\u8fdb\u884c\u4e0b\u8f7d\uff0c\u7136\u540e\u5c06\u6570\u636e\u96c6\u653e\u5728\u548c\u5f53\u524dpy\u6587\u4ef6\u540c\u4e00\u76ee\u5f55\u4e0b\u6267\u884c\u3002\u8fed\u4ee313000\u6b21\u4ea7\u751f\u7684\u5916\u5957\u7684\u7ed3\u679c\u4e3a\uff1a      CycleGAN.pdf CycleGAN\u8bba\u6587\u539f\u6587\u3002   CycleGAN.py \u7528\u4e8e\u98ce\u683c\u8fc1\u79fb\uff0c\u4e24\u4e2a\u6a21\u5f0f\u76f8\u4e92\u8f6c\u6362\uff0c\u4f8b\u5982\u5c06\u9a6c\u548c\u6591\u9a6c\u76f8\u4e92\u8f6c\u6362\uff1a    ", "has_readme": true, "readme_language": "Chinese", "repo_tags": ["gan", "tensorflow", "keras", "pytorch"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f700d"}, "repo_url": "https://github.com/hasbegun/mask_rcnn", "repo_name": "mask_rcnn", "repo_full_name": "hasbegun/mask_rcnn", "repo_owner": "hasbegun", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T06:18:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T06:09:21Z", "homepage": null, "size": 122285, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 188171918, "is_fork": false, "readme_text": "Mask R-CNN for Object Detection and Segmentation This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.  The repository includes:  Source code of Mask R-CNN built on FPN and ResNet101. Training code for MS COCO Pre-trained weights for MS COCO Jupyter notebooks to visualize the detection pipeline at every step ParallelModel class for multi-GPU training Evaluation on MS COCO metrics (AP) Example of training on your own dataset  The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here. Getting Started   demo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.   train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.   (model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.   inspect_data.ipynb. This notebook visualizes the different pre-processing steps to prepare the training data.   inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.   inspect_weights.ipynb This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.   Step by Step Detection To help with debugging and understanding the model, there are 3 notebooks (inspect_data.ipynb, inspect_model.ipynb, inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples: 1. Anchor sorting and filtering Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.  2. Bounding Box Refinement This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.  3. Mask Generation Examples of generated masks. These then get scaled and placed on the image in the right location.  4.Layer activations Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).  5. Weight Histograms Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.  6. Logging to TensorBoard TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.  6. Composing the different pieces into a final result  Training on MS COCO We're providing pre-trained weights for MS COCO to make it easier to start. You can use those weights as a starting point to train your own variation on the network. Training and evaluation code is in samples/coco/coco.py. You can import this module in Jupyter notebook (see the provided notebooks for examples) or you can run it directly from the command line as such: # Train a new model starting from pre-trained COCO weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  # Train a new model starting from ImageNet weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet  # Continue training a model that you had trained earlier python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5  # Continue training the last model you trained. This will find # the last trained weights in the model directory. python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last  You can also run the COCO evaluation code with: # Run COCO evaluation on the last trained model python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last  The training schedule, learning rate, and other parameters should be set in samples/coco/coco.py. Training on Your Own Dataset Start by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application. In summary, to train the model on your own dataset you'll need to extend two classes: Config This class contains the default configuration. Subclass it and modify the attributes you need to change. Dataset This class provides a consistent way to work with any dataset. It allows you to use new datasets for training without having to change the code of the model. It also supports loading multiple datasets at the same time, which is useful if the objects you want to detect are not all available in one dataset. See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py. Differences from the Official Paper This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.   Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.   Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation. To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, and only 0.01% differed by 10px or more.   Learning Rate: The paper uses a learning rate of 0.02, but we found that to be too high, and often causes the weights to explode, especially when using a small batch size. It might be related to differences between how Caffe and TensorFlow compute gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively. We found that smaller learning rates converge faster anyway so we go with that.   Citation Use this bibtex to cite this repository: @misc{matterport_maskrcnn_2017,   title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},   author={Waleed Abdulla},   year={2017},   publisher={Github},   journal={GitHub repository},   howpublished={\\url{https://github.com/matterport/Mask_RCNN}}, }  Contributing Contributions to this repository are welcome. Examples of things you can contribute:  Speed Improvements. Like re-writing some Python code in TensorFlow or Cython. Training on other datasets. Accuracy Improvements. Visualizations and examples.  You can also join our team and help us build even more projects like this one. Requirements Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt. MS COCO Requirements: To train or test on MS COCO, you'll also need:  pycocotools (installation instructions below) MS COCO Dataset Download the 5K minival and the 35K validation-minus-minival subsets. More details in the original Faster R-CNN implementation.  If you use Docker, the code has been verified to work on this Docker container. Installation   Clone this repository   Install dependencies pip3 install -r requirements.txt   Run setup from the repository root directory python3 setup.py install   Download pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.   (Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).  Linux: https://github.com/waleedka/coco Windows: https://github.com/philferriere/cocoapi. You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)    Projects Using this Model If you extend this model to other datasets or build projects that use it, we'd love to hear from you. 4K Video Demo by Karol Majek.  Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.  Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.  Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science Bowl Code is in the samples/nucleus directory.  Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.  Reconstructing 3D buildings from aerial LiDAR A proof of concept project by Esri, in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.  Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy A project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.   Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery Research project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.  Mask-RCNN Shiny A computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.  Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.  GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ond\u0159ej Pe\u0161ek.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cocodataset.org/#home", "http://www.mdpi.com/2072-4292/10/9/1487"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f700e"}, "repo_url": "https://github.com/brianmanderson/Easy_VGG16_UNet", "repo_name": "Easy_VGG16_UNet", "repo_full_name": "brianmanderson/Easy_VGG16_UNet", "repo_owner": "brianmanderson", "repo_desc": "This is code to create an easy to use VGG16 pre-training with UNet attached", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T19:00:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T18:42:07Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188471280, "is_fork": false, "readme_text": "\"# Easy_VGG16_UNet\" This works by loading in the pre-trained weights on the encoding side of the VGG-16 architecture Input image shape is arbitarily set from Keras_Fine_Tune_VGG_16_Liver import VGG_16 network = {'Layer_0': {'Encoding': [64, 64], 'Decoding': [64, 32]},            'Layer_1': {'Encoding': [128, 128], 'Decoding': [128]},            'Layer_2': {'Encoding': [256, 256, 256], 'Decoding': [256]},            'Layer_3': {'Encoding': [512, 512, 512], 'Decoding': [512]},            'Layer_4': {'Encoding': [512, 512, 512]}} VGG_model = VGG_16(network=network, activation='relu',filter_size=(3,3)) VGG_model.make_model() VGG_model.load_weights() new_model = VGG_model.created_model VGG_model.created_model.save(my_path)  The 'Decoding' variables are available to be changed easy in this way ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f700f"}, "repo_url": "https://github.com/Lornatang/Deep-Convolutional-Generative-Adversarial-Network", "repo_name": "Deep-Convolutional-Generative-Adversarial-Network", "repo_full_name": "Lornatang/Deep-Convolutional-Generative-Adversarial-Network", "repo_owner": "Lornatang", "repo_desc": "Implementation of TensorFlow for deep convolution adversarial neural networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T00:20:37Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T00:01:42Z", "homepage": null, "size": 1069, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188322950, "is_fork": false, "readme_text": "Deep Convolutional Generative Adversarial Network paper Author: Lorna Email: shiyipaisizuo@gmail.com Chinese version Requirements  GPU: A TiTAN V or later. Disk: 128G SSD. Python version: python3.5 or later. CUDA: cuda10. CUDNN: cudnn7.4.5 or later. Tensorflow-gpu: 2.0.0-alpla0.  Run this command. pip install -r requirements.txt  What are GANs? Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A generator (\"the artist\") learns to create images that look real, while a discriminator (\"the art critic\") learns to tell real images apart from fakes.  During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.  The following animation shows a series of images produced by the generator as it was trained for 50 epochs. The images begin as random noise, and increasingly resemble hand written digits over time.  1.Introduction 1.1 Theory This is the flow chart of GAN.  GAN's main source of inspiration is zero-sum game thoughts in game theory, is applied to the deep learning neural network, is the by generating network G (Generator) and discriminant D (Discriminator) network game constantly, thus make G learn data distribution, if used on the image to generate the training is completed, G can generate lifelike image from a random number. The main functions of G and D are:  G is a generating network, it receives a random noise z (random number), through the noise to generate images. D is a network for judging whether an image is \"real\". Its input parameter is x, x represents a picture, and the output D (x) represents the probability that x is a real picture. If it is 1, it represents 100% real picture, while if it is 0, it represents an impossible picture.  In the process of training, the goal of generating network G is to generate real images as much as possible to cheat network D. And the goal of D is to try to distinguish the fake image generated by G from the real one. In this way, G and D constitute a dynamic \"game process\", and the final equilibrium point is the Nash equilibrium point.. 1.2 Architecture By optimizing the target, we can adjust the parameter of the probability generation model, so that the probability distribution and the real data distribution can be as close as possible. So how do you define an appropriate optimization goal or a loss? In the traditional generation model, the likelihood of data is generally adopted as the optimization target, but GAN innovatively USES another optimization target.  Firstly, it introduces a discriminant model (common ones include support vector machine and multi-layer neural network). Secondly, its optimization process is to find a Nash equilibrium between the generative model and the discriminant model.  A learning framework established by GAN is actually a simulation game between generating model and discriminating model. The purpose of generating models is to imitate, model and learn the distribution law of real data as much as possible. The discriminant model is to determine whether an input data obtained by itself comes from a real data distribution or a generated model. Through the continuous competition between these two internal models, the ability to generate and distinguish the two models is improved. When a model has very strong ability to distinguish. if the generated data of the model can still be confused and cannot be judged correctly, then we think that the generated model has actually learned the distribution of real data. 1.3 GAN characteristics characteristics:   low compared to the traditional model, there are two different networks, rather than a single network, USES a confrontation training methods and training ways.   low GAN gradient G in the update information from discriminant D, rather than from sample data.   advantages:   low GAN is an emergent model, compared to other generation model (boltzmann machine and GSNs) only by back propagation, without the need for a complicated markov chain.   low compared to all other model, GAN can produce more clearly, the real sample   low GAN is a kind of unsupervised learning training, and can be widely used in the field of a semi-supervised learning and unsupervised learning.   Compared with the variational self-encoder, GANs does not introduce any deterministic bias, and the variational methods introduce deterministic bias, because they optimize the lower bound of logarithmic likelihood rather than the likelihood itself, which seems to cause the instance generated by VAEs to be more fuzzy than GANs.   low compared with VAE, GANs variational lower bound, if the discriminator training is good, then the generator can learn to perfect the training sample distribution. In other words, GANs, gradual consistent, but the VAE is biased.   GAN applied to some scenes, such as picture style transfer, super resolution, image completion, noise removal, to avoid the loss of function design difficulties, regardless of three seven and twenty-one, as long as there is a benchmark, directly on the discriminator, the rest of the training to the confrontation.   disadvantages:   training GAN needs to reach Nash equilibrium, sometimes it can be achieved by gradient descent method, sometimes it can't. We haven't found a good method to achieve Nash equilibrium, so training GAN is unstable compared with VAE or PixelRNN, but I think it is more stable than training boltzmann machine in practice.   GAN is not suitable for processing discrete data, such as text.   GAN has the problems of unstable training, gradient disappearance and mode collapse.   2.Implements 2.1 Load and prepare the dataset You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data. import tensorflow as tf   def load_dataset(mnist_size, mnist_batch_size, cifar_size, cifar_batch_size,):   \"\"\" load mnist and cifar10 dataset to shuffle.    Args:     mnist_size: mnist dataset size.     mnist_batch_size: every train dataset of mnist.     cifar_size: cifar10 dataset size.     cifar_batch_size: every train dataset of cifar10.    Returns:     mnist dataset, cifar10 dataset    \"\"\"   # load mnist data   (mnist_train_images, mnist_train_labels), (_, _) = tf.keras.datasets.mnist.load_data()    # load cifar10 data   (cifar_train_images, cifar_train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()    mnist_train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1).astype('float32')   mnist_train_images = (mnist_train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]    cifar_train_images = cifar_train_images.reshape(cifar_train_images.shape[0], 32, 32, 3).astype('float32')   cifar_train_images = (cifar_train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]    # Batch and shuffle the data   mnist_train_dataset = tf.data.Dataset.from_tensor_slices(mnist_train_images)   mnist_train_dataset = mnist_train_dataset.shuffle(mnist_size).batch(mnist_batch_size)    cifar_train_dataset = tf.data.Dataset.from_tensor_slices(cifar_train_images)   cifar_train_dataset = cifar_train_dataset.shuffle(cifar_size).batch(cifar_batch_size)    return mnist_train_dataset, cifar_train_dataset 2.2 Create the models Both the generator and discriminator are defined using the Keras Sequential API. 2.2.1 Make Generator model Only the most basic form of full connection is used here for the neural network architecture. Except the first layer which does not use normalization, the other layers are all defined by the linear structure of full connection -> normalization ->LeakReLU, and the specific parameters are explained in the code below. import tensorflow as tf from tensorflow.python.keras import layers   def make_generator_model(dataset='mnist'):   \"\"\" implements generate.    Args:     dataset: mnist or cifar10 dataset. (default='mnist'). choice{'mnist', 'cifar'}.    Returns:     model.    \"\"\"   model = tf.keras.models.Sequential()   model.add(layers.Dense(256, input_dim=100))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(512))   model.add(layers.BatchNormalization())   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(1024))   model.add(layers.BatchNormalization())   model.add(layers.LeakyReLU(alpha=0.2))    if dataset == 'mnist':     model.add(layers.Dense(28 * 28 * 1, activation='tanh'))     model.add(layers.Reshape((28, 28, 1)))   elif dataset == 'cifar':     model.add(layers.Dense(32 * 32 * 3, activation='tanh'))     model.add(layers.Reshape((32, 32, 3)))    return model 2.2.2 Make Discriminator model The discriminator is a CNN-based image classifier. import tensorflow as tf from tensorflow.python.keras import layers   def make_discriminator_model(dataset='mnist'):   \"\"\" implements discriminate.    Args:     dataset: mnist or cifar10 dataset. (default='mnist'). choice{'mnist', 'cifar'}.    Returns:     model.    \"\"\"   model = tf.keras.models.Sequential()   if dataset == 'mnist':     model.add(layers.Flatten(input_shape=[28, 28, 1]))   elif dataset == 'cifar':     model.add(layers.Flatten(input_shape=[32, 32, 3]))    model.add(layers.Dense(1024))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(512))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(256))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(1, activation='sigmoid'))    return model 2.3 Define the loss and optimizers 2.3.1 Define loss functions and optimizers for both models. cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)  2.3.2 Discriminator loss This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s. def discriminator_loss(real_output, fake_output):   \"\"\" This method quantifies how well the discriminator is able to distinguish real images from fakes.       It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions       on fake (generated) images to an array of 0s.    Args:     real_output: origin pic.     fake_output: generate pic.    Returns:     real loss + fake loss    \"\"\"   real_loss = cross_entropy(tf.ones_like(real_output), real_output)   fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)   total_loss = real_loss + fake_loss    return total_loss  2.3.3 Generator loss The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s. def generator_loss(fake_output):   \"\"\" The generator's loss quantifies how well it was able to trick the discriminator.       Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).       Here, we will compare the discriminators decisions on the generated images to an array of 1s.    Args:     fake_output: generate pic.    Returns:     loss    \"\"\"   return cross_entropy(tf.ones_like(fake_output), fake_output)  2.3.4 optimizer The discriminator and the generator optimizers are different since we will train two networks separately. def generator_optimizer():   \"\"\" The training generator optimizes the network.    Returns:     optim loss.    \"\"\"   return tf.keras.optimizers.Adam(lr=1e-4)   def discriminator_optimizer():   \"\"\" The training discriminator optimizes the network.    Returns:     optim loss.    \"\"\"   return tf.keras.optimizers.Adam(lr=1e-4)  2.4 Save checkpoints This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted. import os import tensorflow as tf   def save_checkpoints(generator, discriminator, generator_optimizer, discriminator_optimizer, save_path):   \"\"\" save gan model    Args:     generator: generate model.     discriminator: discriminate model.     generator_optimizer: generate optimizer func.     discriminator_optimizer: discriminator optimizer func.     save_path: save gan model dir path.    Returns:     checkpoint path    \"\"\"   checkpoint_dir = save_path   checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")   checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,                                    discriminator_optimizer=discriminator_optimizer,                                    generator=generator,                                    discriminator=discriminator)    return checkpoint_dir, checkpoint, checkpoint_prefix 2.5 train 2.5. 1 Define the training loop The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator. from dataset.load_dataset import load_dataset from network.generator import make_generator_model from network.discriminator import make_discriminator_model from util.loss_and_optim import generator_loss, generator_optimizer from util.loss_and_optim import discriminator_loss, discriminator_optimizer from util.save_checkpoints import save_checkpoints from util.generate_and_save_images import generate_and_save_images  import tensorflow as tf import time import os import argparse  parser = argparse.ArgumentParser() parser.add_argument('--dataset', default='mnist', type=str,                     help='use dataset {mnist or cifar}.') parser.add_argument('--epochs', default=50, type=int,                     help='Epochs for training.') args = parser.parse_args() print(args)  # define model save path save_path = 'training_checkpoint'  # create dir if not os.path.exists(save_path):   os.makedirs(save_path)  # define random noise noise = tf.random.normal([16, 100])  # load dataset mnist_train_dataset, cifar_train_dataset = load_dataset(60000, 128, 50000, 64)  # load network and optim paras generator = make_generator_model(args.dataset) generator_optimizer = generator_optimizer()  discriminator = make_discriminator_model(args.dataset) discriminator_optimizer = discriminator_optimizer()  checkpoint_dir, checkpoint, checkpoint_prefix = save_checkpoints(generator,                                                                  discriminator,                                                                  generator_optimizer,                                                                  discriminator_optimizer,                                                                  save_path)   # This annotation causes the function to be \"compiled\". @tf.function def train_step(images):   \"\"\" break it down into training steps.    Args:     images: input images.    \"\"\"   with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:     generated_images = generator(noise, training=True)      real_output = discriminator(images, training=True)     fake_output = discriminator(generated_images, training=True)      gen_loss = generator_loss(fake_output)     disc_loss = discriminator_loss(real_output, fake_output)    gradients_of_generator = gen_tape.gradient(gen_loss,                                              generator.trainable_variables)   gradients_of_discriminator = disc_tape.gradient(disc_loss,                                                   discriminator.trainable_variables)    generator_optimizer.apply_gradients(     zip(gradients_of_generator, generator.trainable_variables))   discriminator_optimizer.apply_gradients(     zip(gradients_of_discriminator, discriminator.trainable_variables))   def train(dataset, epochs):   \"\"\" train op    Args:     dataset: mnist dataset or cifar10 dataset.     epochs: number of iterative training.    \"\"\"   for epoch in range(epochs):     start = time.time()      for image_batch in dataset:       train_step(image_batch)      # Produce images for the GIF as we go     generate_and_save_images(generator,                              epoch + 1,                              noise,                              save_path)      # Save the model every 15 epochs     if (epoch + 1) % 15 == 0:       checkpoint.save(file_prefix=checkpoint_prefix)      print(f'Time for epoch {epoch+1} is {time.time()-start:.3f} sec.')    # Generate after the final epoch   generate_and_save_images(generator,                            epochs,                            noise,                            save_path)   if __name__ == '__main__':   if args.dataset == 'mnist':     train(mnist_train_dataset, args.epochs)   else:     train(cifar_train_dataset, args.epochs) 2.6 Generate and save images from matplotlib import pyplot as plt   def generate_and_save_images(model, epoch, test_input):   # Notice `training` is set to False.   # This is so all layers run in inference mode (batchnorm).   predictions = model(test_input, training=False)    fig = plt.figure(figsize=(4,4))    for i in range(predictions.shape[0]):       plt.subplot(4, 4, i+1)       plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')       plt.axis('off')    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))   plt.show() 3.Common problems 3.1 why do optimizers in GAN not often use SGD  SGD is easy to shake, easy to make GAN training unstable.  -The purpose of GAN is to find the Nash equilibrium point in the higher-dimensional non-convex parameter space. The Nash equilibrium point of GAN is a saddle point, but SGD will only find the local minimum value, because SGD solves the problem of finding the minimum value, and GAN is a game problem. 3.2 Why GAN is not suitable for processing text data   Compared text data are discrete image data, because for text, usually need to map a word as a high dimensional vector, and finally forecasts the output is a one - hot vector, assuming softmax output is (0.2, 0.3, 0.1, 0.2, 0.15, 0.05) then becomes onehot,1,0,0,0,0 (0), if the softmax output is (0.2, 0.25, 0.2, 0.1, 0.15, 0.1), one - is still hot (0, 1, 0, 0, 0, 0). Therefore, for the generator, G outputs different results, but D gives the same discriminant result, and cannot well transmit the gradient update information to G, so the discriminant of D's final output is meaningless.   In addition, the loss function of GAN is JS divergence, which is not suitable for measuring the distance between distributions that do not want to intersect.   3.3 some skills to train GAN   Input normalized to (-1, 1), last level of activation function using tanh BEGAN (exception)   Using wassertein GAN's loss function,   If you have label data, try to use labels. Some people suggest that it is good to use inverted labels, and use label smoothing, unilateral label smoothing or bilateral label smoothing   Using mini-batch norm, if you do not use batch norm, you can use instance norm or weight norm   Avoid using RELU and pooling layers to reduce the possibility of sparse gradient, and leakrelu activation function can be used   The optimizer chooses ADAM as far as possible, and the learning rate should not be too large. The initial 1e-4 can be referred to. In addition, the learning rate can be continuously reduced as the training goes on.   Adding gaussian noise to the network layer of D is equivalent to a kind of regularization   3.4 Model collapse reason Generally, GAN is not stable in training, and the results are very poor. However, even if the training time is extended, it cannot be well improved. The specific reasons can be explained as follows: Is against training methods used by GAN, G gradient update from D, G generated is good, so D what to say to me. Specifically, G will generate a sample and give it to D for evaluation. D will output the probability (0-1) that the generated false sample is a true sample, which is equivalent to telling G how authentic the generated sample is. G will improve itself and improve the probability value of D's output according to this feedback. But if one G generated samples may not be true, but D gives the correct evaluation, or is the result of a G generated some characteristics have been the recognition of D, then G output will think I'm right, so I so output D surely will also give a high evaluation, G actually generated is not how, but they are two so self-deception, lead to the resulting results lack some information, characteristics. 4. GAN in the application of life   GAN itself is a generative model, so data generation is the most common, the most common is image generation, commonly used DCGAN WGAN BEGAN, personal feeling in BEGAN the best and the most simple.   GAN itself is also a model of unsupervised learning. So it is widely used in unsupervised learning and semi-supervised learning.   GAN not only plays a role in the generation field, but also plays a role in the classification field. To put it simply, it is to replace the discriminator as a classifier and do multiple classification tasks, while the generator still does generation tasks and assists the classifier training.   GAN can be combined with reinforcement learning. A good example is seq-gan.   At present, GAN is an interesting application in image style transfer, image noise reduction and restoration, and image super resolution, all of which have good results.   TODO  Write metrics code. Create GIF.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["http://arxiv.org/pdf/1511.06434.pdf"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7010"}, "repo_url": "https://github.com/charlie-becker/UMBC_CT_Project", "repo_name": "UMBC_CT_Project", "repo_full_name": "charlie-becker/UMBC_CT_Project", "repo_owner": "charlie-becker", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T19:54:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T02:55:51Z", "homepage": null, "size": 17022, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188150739, "is_fork": false, "readme_text": "Team 3 Project of the CyberTraining program at UMBC in 2019 http://cybertraining.umbc.edu/ Title: An Approach to Tuning Hyperparameters in Parallel - A Performance Study Team members: Charlie Becker; Bin Wang; Will Mayfield; Sarah Murphy Mentors: Dr. Matthias Gobbert; Carlos Barajas This is a working example of a performance study completed on the 'Taki' HPC cluster at UMBC. It uses a combination of popular Python modules for hyperparameter tuning in parallel. The data and base model configuration is borrowed from the Machine Learning in Python forEnvironmental Science Problems AMS Short Course, provided by David John Gagne from the National Center for Atmospheric Research.  The repository for that course can be found at [https://github.com/djgagne/ams-ml-python-course] Below, are some brief directions to reproduce the results in the technical report. Full results are seen and discussed in Technical_Report.pdf Workflow After cloning this directory, first run data_download.py which will download the data into a data direcory. Next, run preprocess.py which will preproces the data and augement it to give a balanced dataset.  It will create and place .npy files into the data directory for easy access. Next, you can run either submit_2013.slurm (2013 partition), submit_2018.slurm (2018 partition) or submit_gpu.slurm (2018 GPU nodes) to submit the performance study across the cluster.  These scripts call run_2013.py, run_2018.py and run_gpu.py respectively, which is where additional SLURM argumentes are defined, such as the number of nodes and hyperparameters.  Specifically, cluster.scale(x) will refer to the number of nodes desired. Output for the study will be delivered to slurm-2013.out, slurm-2018.out or slurm-gpu.out with error logs being delivered to slurm-xxxx.err. Additionally, each process within each node will prodeuce training output in slurm-jobID.out, though this probably won't be useful. Data augmentation RandomOverSampler class from imblearn.over_sampling was used to oversample the minority classes (non-tornadic data) fed into the deep neural network. This is done to achieve an approximate 50/50 class split within the training data; which began as approximately a 95/5 split. The relevant script is dnn.py For convolutional neural network, the input data are tensor images. We augment the minority classes (non-tornadic images) by duplicating, shuffling, and transforming the images through small angle rotation but keeping the labels unchanged. This can be done in real time while training the model via ImageDataGenerator from Keras or at the preprocessing stage using skimage.transform.rotate before data are feeding into the model. In the code, this can be selected via two parameters 'augmentation' and 'on_the_fly'. For example, if augmentation==True, and on_the_fly==False, this means that the augmented data is generated before training. The working script is cnn.py Overall, we did not see a significant spike in performance when using transformed data as opposed to resampled data only.  However, augmentation is highly specific to the dataset and will have varying benefits dependent on each specfic dataset. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cybertraining.umbc.edu/"], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7011"}, "repo_url": "https://github.com/linto-ai/HMG", "repo_name": "HMG", "repo_full_name": "linto-ai/HMG", "repo_owner": "linto-ai", "repo_desc": "HMG (Hotword Model Generator) is a GUI tool to create, train, evaluate and export Keras and Tensorflow Hotword spotting RNN model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T17:43:22Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T08:45:45Z", "homepage": null, "size": 51, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188195732, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7012"}, "repo_url": "https://github.com/naeimbah/Test_PipeLine_cardiacMR", "repo_name": "Test_PipeLine_cardiacMR", "repo_full_name": "naeimbah/Test_PipeLine_cardiacMR", "repo_owner": "naeimbah", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T16:25:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T16:12:38Z", "homepage": null, "size": 340, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188452554, "is_fork": false, "readme_text": "Test_PipeLine_cardiacMR I initially defined a few functions to be able 1) to get the first sub directories, 2) to generate the slice number (aka the file name for the .dcm files that are match with the name of the contours, 3) to generate two lists containing the paths for corresponding .dcm files and contour.txt files using the data and lookup table (i.e., link.csv). Also wrote a function (func = save_overlay_images()) to find, convert, and overlay the masks on top of the images for quality check. Saved the overlay images as .png format in 'output' folder.  This function also returns three lists containing paths to the images, labels, and patient ids in order to use in the second phase for data training generator. The functions are in src.utility  Using methods above allow us to visually inspect the contours and images that are matched. However, checking the slices before and after the designated slice might help. In this data for each contour we had related .dcm files but in larger scope it might not be the case. We might need to return missing images or most likely missing contours in future steps. A simple function to return missing data will be necessary for larger datasets. store the data in HDF5 Although this is a small dataset and we potentially could have gotten away with saving the data as numpy up front but I usually store the data in a dictionary or hdf5. see hdf5_write.py   Here given the directories to images, contours, and lookup table, we create and save a hdf5 file with patient id as key and image and label as 'items'. The hdf5 output from this level is saved in 'data' folder. depends on cpu and gpu power reading images from a single hdf5 might be a good idea. If cpu cache size is not adequette it might be problematic to use hdf5. In this case, memory-mapping the files with NumPy could be used. Also there is a major drawback of storing a lot of data in a single file, which is what HDF5 is designed for. But as long as there is a clear path to create such a file and the raw data still exist we might be able to retrieve it in case on corruption. data generator Then we need to batch the data to feed the neural network. I created a class for 'DataGenerator'. within the class defined a func = train_generator  to recieve the data from hdf5 file and using the patient ids and batch size yeild the image and label numpy files. At the end of each epoch the patient ids will be suffled for randomizing the batch contains. Then created a simple vanilla 2D Unet to make sure that it is compatible to my keras fit_generator. To verify the batch generation I used some steps of logging and created some debug and info level logging. I returned whether shuffling occured, then printed out the patient ids that are in the batches and also I asked it to let me know when the object is being made. Other levels of logging in a more organized manner would be necessary in larger datasets. Alternatively, I could have shuffle the name of the .dcm files inside the generator and read, convert contour to binary mask,and convert them all to numpy whithin the data generator without using the hdf5. Then I save a more clear logging paradiam to  store the path/name of the images that are used for training instead of saving the entire data and reading them through hdf5. In larger dataset with better processing power, we might need to do such a method instead. At the end, I trained this Unet with a couple epochs with 12 steps per epoch wich is 96(number of 2D iamges)/8 (batch size) and I checked the logging files (in 'log' folder) to make sure the shuffling and patient ids are correctly insterted.  packages pandas, numpy, hdf5, PIL, pydicom or dicom , math, logging, ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7013"}, "repo_url": "https://github.com/JuananCampillo/MyOwnThings", "repo_name": "MyOwnThings", "repo_full_name": "JuananCampillo/MyOwnThings", "repo_owner": "JuananCampillo", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T17:24:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T16:55:13Z", "homepage": null, "size": 12304, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188458325, "is_fork": false, "readme_text": "MATRIX CAPSULES EM-Tensorflow   A Tensorflow implementation of CapsNet based on paper Matrix Capsules with EM Routing  Status:  With configuration A=32, B=8, C=16, D=16, batch_size=128, the code can work on a Tesla P40 GPU at a speed of 8s/iteration. The definitions of A-D can be referred to the paper. With configuration A=B=C=D=32, batch_size=64, the code can work on a Tesla P40 GPU at a speed of 25s/iteration. More optimization on implementation structure is required. Some modification and optimization is implemented to prompt the numerical stability of GMM. Specific explanations can be found in the code. With configuration A=32, B=4, D=4, D=4, batch_size=128, each iteration of training takes around 0.6s on a Tesla P40 GPU.    Current Results on smallNORB:    Configuration: A=32, B=8, C=16, D=16, batch_size=50, iteration number of EM routing: 2, with Coordinate Addition, spread loss, batch normalization   Training loss. Variation of loss is suppressed by batch normalization. However there still exists a gap between our best results and the reported results in the original paper.    Test accuracy(current best result is 91.8%)     Ablation Study on smallNORB:   Configuration: A=32, B=8, C=16, D=16, batch_size=32, iteration number of EM routing: 2, with Coordinate Addition, spread loss, test accuracy is 79.8%.   Current Results on MNIST:    Configuration: A=32, B=8, C=16, D=16, batch_size=50, iteration number of EM routing: 2, with Coordinate Addition, spread loss, batch normalization, reconstruction loss.   Training loss.    Test accuracy(current best result is 99.3%, only 10% samples are used in test)     Ablation Study on MNIST:   Configuration: A=32, B=4, C=4, D=4, batch_size=128, iteration number of EM routing: 2, no Coordinate Addition, cross entropy loss, test accuracy is 96.4%. Configuration: A=32, B=4, C=4, D=4, batch_size=128, iteration number of EM routing: 2, with Coordinate Addition, cross entropy loss, test accuracy is 96.8%. Configuration: A=32, B=8, C=16, D=16, batch_size=32, iteration number of EM routing: 2, with Coordinate Addition, spread loss 99.1%.   To Do List:  Experiments on smallNORB as in paper is about to be casted.   Any questions and comments to the code and the original algorithms are welcomed!!! My email: zhangsuofei at njupt.edu.cn Requirements  Python >= 3.4 Numpy Tensorflow >= 1.2.0 Keras  pip install -r requirement.txt Usage Step 1. Clone this repository with git. $ git clone https://github.com/www0wwwjs1/Matrix-Capsules-EM-Tensorflow.git $ cd Matrix-Capsules-EM-Tensorflow  Step 2. Download the MNIST dataset, mv and extract it into data/mnist directory.(Be careful the backslash appeared around the curly braces when you copy the wget  command to your terminal, remove it) $ mkdir -p data/mnist $ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/{train-images-idx3-ubyte.gz,train-labels-idx1-ubyte.gz,t10k-images-idx3-ubyte.gz,t10k-labels-idx1-ubyte.gz} $ gunzip data/mnist/*.gz  To install smallNORB, follow instructions in ./data/README.md Step 3. Start the training(MNIST): $ python3 train.py \"mnist\"  Step 4. Download the Fashion MNIST dataset, mv and extract it into data/fashion_mnist directory.(Be careful the backslash appeared around the curly braces when you copy the wget  command to your terminal, remove it) $ mkdir -p data/fashion_mnist $ wget -c -P data/fashion_mnist http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/{train-images-idx3-ubyte.gz,train-labels-idx1-ubyte.gz,t10k-images-idx3-ubyte.gz,t10k-labels-idx1-ubyte.gz} $ gunzip data/fashion_mnist/*.gz  Start the training(smallNORB): $ python3 train.py \"smallNORB\"  Start the training(CNN baseline): $ python3 train_baseline.py \"smallNORB\"  Step 4. View the status of training: $ tensorboard --logdir=./logdir/{model_name}/{dataset_name}/train_log/  Open the url tensorboard has shown. Step 5. Start the test on MNIST: $ python3 eval.py \"mnist\" \"caps\"  Start the test on smallNORB: $ python3 eval.py \"smallNORB\" \"caps\"  Step 6. View the status of test: $ tensorboard --logdir=./test_logdir/{model_name}/{dataset_name}/  Open the url tensorboard has shown. Reference  naturomics/CapsNet-Tensorflow: the implementation of Hinton's paper Dynamic Routing Between Capsules  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://yann.lecun.com/exdb/mnist/"], "reference_list": ["https://arxiv.org/abs/1710.09829"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7014"}, "repo_url": "https://github.com/jow105/FLAPPY-BIRD-AI", "repo_name": "FLAPPY-BIRD-AI", "repo_full_name": "jow105/FLAPPY-BIRD-AI", "repo_owner": "jow105", "repo_desc": "Using the tensorflow and keras Python packages, implemented a deep-reinforcement learning algorithm and reward model to learn to play the popular game Flappy Bird and achieve a score of 100+", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T01:38:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T00:41:42Z", "homepage": null, "size": 101, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188326416, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7015"}, "repo_url": "https://github.com/NeutrinoPhysics/turn-style-for-cast", "repo_name": "turn-style-for-cast", "repo_full_name": "NeutrinoPhysics/turn-style-for-cast", "repo_owner": "NeutrinoPhysics", "repo_desc": "Urbint Homework Challenge", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T17:16:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-23T15:11:05Z", "homepage": "", "size": 12117, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188257873, "is_fork": false, "readme_text": "MTA Ridership Forecasting Layout Directories MTA/           : the main folder |__ Docs/     : contains MTA turnstile data documentation and overview |__ Data/     : turnstile csv data file goes here |__ Temp/     : temporary data files for cleaning go here |__ Models/   : contains model pickle files and prediction dataframes |__ Figures/  : plots Files   README.md       : this file   environment.py  : contains global variables and filepath names   utils.py        : contains useful functions used in several scripts   lstm.py         : the neural network building, fitting and testing routines   arima.py   : the auto-regressive integrated moving average routines   parser.py       : reads the raw data file and parses it into chunks, each corresponding to a unique station unit   station.py      : preprocesses each individual station file   clean.py        : cleans, reduces and interpolates missing data   predictLSTM.py  : predict a time series with a neural network   predictARIMA.py : predict a single, univariate time series using ARIMA   parallel.py     : launch multiple univariate time series in parallel   compare.py      : a short script for comparing the forecasts from the two algorithms   pipeline.py     : the pipeline wrapper script for launching pre-processing and forecasting   Dependencies  Python NumPy Pandas Keras (Tensorflow) Scikit-Learn StatsModels Matplotlib Tmux (optional, if and when multiprocessing)  Instructions Setup 1/ First, download the MTA turnstile data (csv format) and store it in the 'Data/' directory. 2/ Modify the 'raw' file name accordingly in the 'environment.py' file. The other two ('parsed' and 'clean') will get created throughout the pipeline. You can also tweek the few global parameters such as the roll-over detection threshold. 3/ Make sure you have recent version of python (v2.7, v3.0) installed as well as all the packages listed above /!\\ Note: If you do not want to download the raw data file, you can skip the first two steps in the subsequent instructions on data cleaning. At this point, you may launch the entire pipeline by running in the shell prompt: python3 pipeline.py The following describe each intermediary step in the pipeline. Data Cleaning 1/ Parsing the raw data. python3 parser.py all This reads the raw datafile, stores the contents in a data frame and parses the contents according to the subway station units, along with each one's turnstiles subunit/channel/position. For each unit, the dataframe chunk is stored in a file containing its name (i.e. 'R324.csv' for unit R324) in the 'Temp/' directory. This allows for individual station's data inspection. Only the cumulative entries and date-time are stored. 2/ Preprocess each station's turnstile substation. python3 station.py For each station file stored in the 'Temp/' directory (created on the previous step), the data is preprocessed in the following manner:  organize by individual turnstiles (the 'SCP' label) sort by date-time to get cumulative entries detect any instances of roll-over digit reset, and fix accordingly resample (re-bin) cumulative entries by individual day convert cumulative entries into daily total entries for that turnstile sum daily entries over all turnstile units at that particular station  Once this procedure has been applied on all stations, they are concatenated into a new dataframe comprising of the daily total entries for each station, and sotred into the 'parsed' data file in the 'Data/' directory. 3/ Clean the parsed data python3 clean.py The newly created file has a lot of missing data. The following steps are taken to account for most of them:  ditch the stations that have too much or only missing data remove outliers with sigma-clipping identify the 3 data gaps (June, July, December) apart from the 3 gaps, remove those that have too much consecutive days of missing data resample the time series into week number (1 - 52) and week day (Sun - Sat) fill in the missing data on a particular day by combining the expectation on that week day and on that week number  The 'clean' data file is then stored in the 'Data/' directory. Fitting and Forecasting Once the data is cleaned and stored as a dataframe, we may move on the machine learning part of the pipeline. So far, I have implemented two algorithms for the forecasting: an ARIMA and an encoder-decoder LSTM. These are stored in the 'arima.py' and 'lstm.py' libraries, respectively. You may forecast with one, or both, by running the 'predictARIMA.py' and/or 'predictLSTM.py' scripts. They take in several user-specified arguments. They have been written to both do the fitting of the models and parameter optmization; as well as the forecasting / validation step. The former is quite computationally expensive, and I do not recommend running it at this point, although with some minor tweeks, it could be made more efficient. I suggest running the scripts in their validation configurations, where the hyper-parameters (for the LSTM) or orders (for the ARIMA) are fixed. A/ ARIMA If you do wish to run the ARIMA to model and forecast a single time series, run python3 predictARIMA.py [series] [True/False] [p d q] where [series] is the integer corresponding to the station id. For instance, the value 9 will make the code take the 10th (or 9th in python notation) column in the cleaned dataframe, which correspond to the daily entries for the unit at that address. The default value is -1 and corrresponds to the daily entries summed over all the stations (useful for question 1). To tackle question 2, we would need to iterate the aforementioned script hundreds of times. To anticipate this, I wrote a script that iteratively launches batches of 'predictARIMA.py' on parallel processors. Not very useful when doing so from a personal laptop. However, if you have computational resources at hand, it might be useful to launch the 'parallel.py' script. I strongly recommend opening a TNUX session for doing so: tmux new -s [my-session] python3 parallel.py /!\\ NOTE: At this point, this code is still under  construction. B/ LSTM If you do wish to run the LSTM encoder-decoder, python3 predictLSTM.py [series] [True/False] where [series] is, here again, the integer corresponding to the station id, with -1 being the sum over all stations for daily entries. The second parameter to pass is a boolean that shoul be set to True if you want to fine-tune amongst many parameters. The 'environment.py' file contains the rudimentary parameter spaces on which to explore. Ideally, these parameter spaces could incorporate more values. When only evaluating the \"best\" fitted model, this boolean should be False. Hence, python3 predictLSTM.py -1 False will train the network with pre-determined parameters on the full training/testing (pre-december) set, comprising of the time series of the total number of entries along all stations, and then score its forecast on the validation set. The LSTM's hyper-parameters comprise of:  the consecutive days to train on in the past the consecutive days to predict moving forward, knowing that past epoch, batch size, train/test split ratio number of units in the LSTM layer  Inspection The different scripts automatically generate some figures in the 'Figures/' directory for inspection. Many more features are intended to be implemented. The 'compare.py' script serves as a rough draft for post-processing. Specifically, it compares the forecasts on the validation set (december daily usage) once both an ARIMA and neural network have made their respective predictions. It will be meant to be used in a Jupyter Notebook or an interactive python (IPython) shell; but can stillbe run as a standalone executable: python3 compare.py Issues Mainly GPU memory overload achieved when iteratively training neural networks from the 'predictLSTM.py' script. Issue is due to Kudas session not being released at each generation of a new model. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/NeutrinoPhysics/turn-style-for-cast/blob/caa39a6ad1afe456f5df9857796df6f1e144d9c6/Models/lstm.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7016"}, "repo_url": "https://github.com/YashNita/Music_Artist_Classification_CRNN", "repo_name": "Music_Artist_Classification_CRNN", "repo_full_name": "YashNita/Music_Artist_Classification_CRNN", "repo_owner": "YashNita", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T00:05:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T00:05:24Z", "homepage": null, "size": 3686, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188323232, "is_fork": false, "readme_text": "Musical Artist Classification with Convolutional Recurrent Neural Networks Nasrullah, Z. and Zhao, Y., Musical Artist Classification with Convolutional Recurrent Neural Networks. International Joint Conference on Neural Networks (IJCNN), 2019. Accepted, to appear. Please cite the paper as: @inproceedings{nasrullah2019music,   author={Nasrullah, Zain and Zhao, Yue},   title={Musical Artist Classification with Convolutional Recurrent Neural Networks},   booktitle={2019 International Joint Conference on Neural Networks (IJCNN)},   year={2019},   organization={IEEE} }  PDF for Personal Use | IJCNN 2019  Introduction Previous attempts at music artist classification use frame level audio features which summarize frequency content within short intervals of time. Comparatively, more recent music information retrieval tasks take advantage of temporal structure in audio spectrograms using deep convolutional and recurrent models. This paper revisits artist classification with this new framework and empirically explores the impacts of incorporating temporal structure in the feature representation. To this end, an established classification architecture, a Convolutional Recurrent Neural Network (CRNN), is applied to the artist20 music artist identification dataset under a comprehensive set of conditions. These include audio clip length, which is a novel contribution in this work, and previously identified considerations such as dataset split and feature level. Our results improve upon baseline works, verify the influence of the producer effect on classification performance and demonstrate the trade-offs between audio length and training set size. The best performing model achieves an average F1 score of 0.937 across three independent trials which is a substantial improvement over the corresponding baseline under similar conditions. Additionally, to showcase the effectiveness of the CRNN's feature extraction capabilities, we visualize audio samples at the model's bottleneck layer demonstrating that learned representations segment into clusters belonging to their respective artists.  Dependency The experiment code is writen in Python 3.6 and built on a number of Python packages including (but not limited to):  dill==3.2.8.2 h5py==2.8.0 Keras==3.1.1 librosa==1.5.1 matplotlib==3.2.3 numpy==2.14.5 pandas==1.23.4 scikit-learn==1.20.0 scipy==2.1.0 seaborn==1.9.0 tensorflow==2.10.0  Batch installation is possible using the supplied \"requirements.txt\" with pip or conda. pip install -r requirements.txt Additional install details (recommended for replication and strong performance):  Python: 3.6.6 GPU: Nvidia GTX 1080 (Driver: 390.87) CUDA: 8.0 CUDNN: 7.0.5 ffmpeg is required by Librosa to convert audio files into spectrograms.  Datasets This study primarily uses the artist20 musical artist identification dataset by Labrosa [1]. The data is accessible upon request from https://labrosa.ee.columbia.edu/projects/artistid/. The main characteristics of the dataset can be summarized as:    Property Value     # of Tracks 1,413   # of Artists 20   Albums per Artist 6   Bitrate 32 kbps   Sample Rate 16 kHz   Channels Mono    The figure below visualizes three seconds of the mel-scaled audio spectrogram for a randomly sampled song from each artist. This is the primary data representation used in the paper.  Usage To re-create experimental results:  Prepare mel-scaled spectrograms from raw audio in the dataset.  Run src/utility.py if the dataset is stored using its original folder structure (artists/[artist]/[album]/[song].mp3) in the project root. Using the create_dataset() utility function in src/utility.py with a custom directory if the dataset is stored elsewhere.   Run the main.py script. This will begin a training loop which runs three independent trials for each audio length in {1s, 3s, 5s, 10s, 20s, 30s}.  This script must be adjusted manually to vary whether or not to use an album split via the album_split flag in the train_model function call. It should be noted that training each model is computationally expensive and can take several hours even with reliable hardware. At minimum, a Nvidia GTX 1080 GPU is recommended with at least 16GB of memory on the machine.   To reproduce the representation visualization, the representation.py script can be used but one must specify the model weight location and relevant audio clip length.  The models and utility functions provided can also generically be used for any audio-based classification task where one wants to experiment with audio length. The train_model function in src/trainer.py is fairly extensive. Results Classification performance is evaluated using the test F1-score of three independent trials and also varying parameters such as audio length {1s, 3s, 5s, 10s, 20s, 30s}, the type of dataset split {song-level, album-level} and feature-level {frame-level, song-level}. Both the average and maximum score are reported among the trials. As a whole, from the four base conditions resulting from audio split and level, the CRNN model outperforms the most comparable baseline for at least one audio clip length. This holds true for both the best and average case performance except for the album split with song-level features where the CRNN model only outperforms in its best-run. This discrepancy may be explained by considering that Mandel's dataset contains less classes or because, unlike the baselines works, we are additionally reporting the average of three independent trials instead of performance on a single trial. Test F1 Scores for Frame-level Audio Features (3 runs):    Split Type 1s 3s 5s 10s 20s 30s     Song Average 0.729 0.765 0.770 0.787 0.768 0.764   Song Best 0.733 0.768 0.779 0.772 0.792 0.771   Album Average 0.482 0.513 0.536 0.538 0.534 0.603   Album Best 0.516 0.527 0.550 0.560 0.553 0.612    Test F1 Scores for Song-level Audio Features (3 runs):    Split Type 1s 3s 5s 10s 20s 30s     Song Average 0.929 0.937 0.918 0.902 0.861 0.846   Song Best 0.944 0.966 0.930 0.915 0.880 0.851   Album Average 0.641 0.651 0.652 0.630 0.568 0.674   Album Best 0.700 0.653 0.662 0.683 0.609 0.691    Additionally, audio samples at the bottleneck layer of the network are also visualized using t-SNE to demonstrate how effectively the model is able to learn to classify artists. As can be seen below, the learned representations prior to classification separate into distinct clusters belonging to each artist demonstrating that the convolution and recurrent layers are effective at the task. The example below is for the model trained on 10s of audio.  Conclusions This paper establishes a deep learning baseline for music artist classification on the \\textbf{\\textit{artist20}} dataset and demonstrates that a Convolutional Recurrent Neural Network is able to outperform traditional baselines under a range of conditions. The results show that including additional temporal structure in an audio sample improves classification performance and also that there is a point beyond which the returns may diminish. This is attributed to a possible lack of complexity in the model or early pooling layers discarding too much information. Using the trained models, predictions are also aggregated at the song level using a majority vote to determine the artist performing a song. This leads to another substantial gain in performance and validates the feasibility of using a CRNN for industry applications such as copyright detection. The best-performing model is trained using three second audio samples under a song dataset split and evaluated at the song level to achieve an average F1 score of 0.937 across three independent trials. Additionally, we visualize audio samples at the bottleneck layer of the network to show that learned representations cluster by artist---highlighting the model's capability as a feature extractor. Future directions include audio augmentation, model pre-training and minimizing temporal pooling as avenues for further performance improvement. References [1] D. Ellis (2007). Classifying Music Audio with Timbral and Chroma Features, Proc. Int. Conf. on Music Information Retrieval (ISMIR), Vienna, Austria, Sep. 2007. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://ffmpeg.org/download.html"], "reference_list": ["http://arxiv.org/abs/1901.04555"]}, {"_id": {"$oid": "5cf424a17eb8d64d7c2f7017"}, "repo_url": "https://github.com/sundogai/style-transfer", "repo_name": "style-transfer", "repo_full_name": "sundogai/style-transfer", "repo_owner": "sundogai", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T12:53:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-24T05:35:37Z", "homepage": null, "size": 25389, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188358581, "is_fork": false, "readme_text": "Style Transfer This is an implementation of adaptive style transfer in tensorflow 2. It demonstrates how to:  Manage a dataset using tf.dataset and tfrecord files. Build models using tf.keras high level API. Write the training loop of a GAN using tf.function and tf.GradientTape . Export and infer using tf.SavedModel, TensorRT and TfSlim. Manage flags and logging using google abseil python package.  In the near future this code will also:  Allow training style transfer networks following Cartoon-GAN paper The code is writen but I'm still looking for good parameters :) Demonstrate how to use style transfer on Android. I wrote models with the same kind of optimization presented in mobilenet v2 paper. The models train well, can be use for inference on a computer and can be exported in tfLite format but I still have some bug in the android app.  Please note that the tensorflow 2 TensorRT API is still work in progress and that you need a more recent version of tensorflow than the 2.0.0a0 if you want to use tensorRT. To compile and build a docker image with a more recent version of tensorflow 2 please see the readme inside trt_docker subdir You will find some results in folder imgs. For instance: Original image:  After style transfer with Picasso:  Installation The easiest way to install this style transfer is by using docker. You need to install docker, docker-compose and nvidia-docker then run: docker-compose build style-transfer Training adaptive style transfer  Content images used for training: Places365-Standard high-res train images (105GB). The research team that releases the adaptive style transfer paper also releases style image on their owncloud  Start docker If you want to use docker for the training, a solution is to run the docker-compose style-transfer app. For training you need to specified 3 environments variables :  PICTURE_DATASET: the path of the pictures ART_DATASET : the path of the art images EXPERIMENT : the folder where the program writes logs and checkpoints  for instance run : export PICTURE_DATASET=.... export ART_DATASET=.... export EXPERIMENT=.... docker-compose run style-transfer And you will get bash prompt in a valid tensorflow 2 environment Start training As soon as you are in an environment running tensorflow 2, try : python3 app.py --action training To train the model with default parameters. You can see the most important parameters with python3 app.py --help and all the parameters with python3 app.py --helpfull  If you choose to not use docker, then you probably need to change the default paths where are stored the dataset and the tfrecords. The parameters to do this are : --picture_dataset_path, --picture_tfrecord_path, --style_dataset_path, --style_tfrecord_path and --training_dir --style_tfrecord_prefix is also an option you should specified if you want to create several tfrecord files for several artists in the same folder. If you want to train a model with the same optimizations than the mobilenet-v2 then add the flag : --mobilenet  For instance, if you want to train a picasso style transfer with the same training parameters than the original paper, you can first run 200000 iterations with a learning rate of 0.0001 then 100000 iterations with a learning rate of 0.00002. Inside docker it will be: python3 app.py --action training \\                --training_dir /opt/experiment/picasso \\                --style_tfrecord_prefix picasso python3 app.py --action training \\                --training_dir /opt/experiment/picasso_2 \\                --style_tfrecord_prefix picasso \\                --n_iterations 100000 \\                --lr 0.00002 \\                --pretrained_ckpt /opt/experiment/picasso/checkpoints/ Training Cartoon GAN It is basically the same thing than training a adaptive style transfer, The only difference is that you start by an initialization where you pretrain the generative network. Inside docker it will be : python3 app.py --action training \\                --training_method initialization_cartoon \\                --n_iterations 50000 \\                --training_dir /opt/experiment/cartoon \\                --learning_rate 0.001  python3 app.py --action training \\                --training_method cartoon_gan \\                --n_iterations 200000 \\                --training_dir /opt/experiment/cartoon2 \\                --pretrained_ckpt /opt/experiment/cartoon/checkpoints Please note the flag --training_method to choose if we want to train an adaptive style transfer (default), a cartoon GAN or training the generative network for the initialization part of the cartoon GAN. Export a trained model to saved_model format To export a model you need to specified:  The path of checkpoint to load (can be a file or a folder. In this case it will load the more recent one) The path to write the exported model the export format you want to use (SavedModel, TensorRT or TfLite) and some precision (FP16 or 32 for tensorRT...) The model architecture corresponding to the checkpoint (std or mobilenet)  For instance to export a model in TFLite format: python3 app.py  --action export --export_format tflite --mobilenet --export_path /opt/export/model.tflite Do inference To perform inference you need to provide :  The path to a exported model The path of the folder containing the images, videos... The path where to write the transformed images and videos  And you need to specified if your using tflite. For instance : python3 app.py --action infer \\                --inference_model_path ... \\                --inference_input_dir ... \\                --inference_output_dir ...  \\                --inference_tflite Ref Adaptative Style Transfer  https://github.com/CompVis/adaptive-style-transfer https://compvis.github.io/adaptive-style-transfer/ https://arxiv.org/pdf/1807.10201.pdf  Instance Normalization  https://arxiv.org/pdf/1607.08022.pdf  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://data.csail.mit.edu/places/places365/train_large_places365standard.tar", "http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf"], "reference_list": ["https://arxiv.org/pdf/1807.10201.pdf", "https://arxiv.org/pdf/1807.10201.pdf", "https://arxiv.org/pdf/1607.08022.pdf"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a725"}, "repo_url": "https://github.com/vzhou842/cnn-from-scratch", "repo_name": "cnn-from-scratch", "repo_full_name": "vzhou842/cnn-from-scratch", "repo_owner": "vzhou842", "repo_desc": "A Convolutional Neural Network implemented from scratch (using only numpy) in Python.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T13:07:35Z", "repo_watch": 28, "repo_forks": 5, "private": false, "repo_created_at": "2019-05-22T02:32:42Z", "homepage": "https://victorzhou.com/blog/intro-to-cnns-part-1/", "size": 17, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187948506, "is_fork": false, "readme_text": "A Convolution Neural Network (CNN) From Scratch This was written for my 2-part blog post series on CNNs:  CNNs, Part 1: An Introduction to Convolution Neural Networks CNNs, Part 2: Training a Convolutional Neural Network  To see the code (forward-phase only) referenced in Part 1, visit the forward-only branch. Usage Install dependencies: $ pip install -r requirements.txt Then, run it with no arguments: $ python cnn.py $ python cnn_keras.py You can also run this code in your browser. More You may also be interested in a Neural Network implemented from scratch in Python, which was written for my introduction to Neural Networks. ", "has_readme": true, "readme_language": "English", "repo_tags": ["neural-network", "convolutional-neural-networks", "machine-learning", "python", "guide", "computer-vision"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a726"}, "repo_url": "https://github.com/zhang-rongchen/Logo-Retrieval-in-Commercial-Plaza", "repo_name": "Logo-Retrieval-in-Commercial-Plaza", "repo_full_name": "zhang-rongchen/Logo-Retrieval-in-Commercial-Plaza", "repo_owner": "zhang-rongchen", "repo_desc": "Logo Retrieval in Commercial Plaza ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T07:07:34Z", "repo_watch": 6, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T05:09:47Z", "homepage": null, "size": 91824, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187765688, "is_fork": false, "readme_text": "Logo-Retrieval-in-Commercial-Plaza Introduction This is a Keras implementation of Logo Retrieval which is applied in commercial plaza Based on yolov3(mobilenet) VGG16 Requirements python 3.6 tensorflow 1.8 keras opencv tkinter PIL Quick Start Run GUI.py Example  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/zhang-rongchen/Logo-Retrieval-in-Commercial-Plaza/blob/e7494e1fa948761f7466d0e9aa7556c1a0753dd6/logo_detect.h5", "https://github.com/zhang-rongchen/Logo-Retrieval-in-Commercial-Plaza/blob/e7494e1fa948761f7466d0e9aa7556c1a0753dd6/logo_feature.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a727"}, "repo_url": "https://github.com/x4nth055/price-prediction", "repo_name": "price-prediction", "repo_full_name": "x4nth055/price-prediction", "repo_owner": "x4nth055", "repo_desc": "Predicting different market prices using Deep Learning and Recurrent Neural Networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T00:01:50Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T18:16:56Z", "homepage": null, "size": 15273, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187891077, "is_fork": false, "readme_text": "Price Prediction using Deep Learning  Introduction This repository uses recurrent neural networks to predict the price of any stock, currency or cryptocurrency ( any market that yahoo_fin library supports ) using keras library. Getting Started to use this repository, install required packages  Python 3.6 keras==2.2.4 sklearn==0.20.2 numpy==1.16.2 pandas==0.23.4 matplotlib==2.2.3 yahoo_fin  using the following command: pip3 install -r requirements.txt  Dataset Dataset is downloaded automatically using yahoo_fin package and stored in data folder. click here for more information about different tickers. Example from keras.layers import GRU, LSTM, CuDNNLSTM from price_prediction import PricePrediction  ticker = \"BTC-USD\"  # init class, choose as much parameters as you want, check its docstring p = PricePrediction(\"BTC-USD\", epochs=1000, cell=LSTM, n_layers=3, units=256, loss=\"mae\", optimizer=\"adam\")  # train the model if not trained yet p.train() # predict the next price for BTC print(f\"The next predicted price for {ticker} is {p.predict()}$\") # decision to make ( sell/buy ) buy_sell = p.predict(classify=True) print(f\"you should {'sell' if buy_sell == 0 else 'buy'}.\") # print some metrics print(\"Mean Absolute Error:\", p.get_MAE()) print(\"Mean Squared Error:\", p.get_MSE()) print(f\"Accuracy: {p.get_accuracy()*100:.3f}%\") # plot actual prices vs predicted prices p.plot_test_set() Output The next predicted price for BTC-USD is 8011.0634765625$ you should buy. Mean Absolute Error: 145.36850360261292 Mean Squared Error: 40611.868264624296 Accuracy: 63.655%   Training logs are stored in logs folder that can be opened using tensorboard, as well as model weights in results folder. Next Steps  Fine tune model parameters ( n_layers, RNN cell, number of units, etc.) Tune training parameters ( batch_size, optimizer, etc. ) Try out different markets such as NFLX (Netflix), AAPL (Apple) by setting the ticker parameter  ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "price-prediction", "stock-price-prediction", "recurrent-neural-networks", "neural-networks", "python3", "cryptocurrency-price-predictor", "market-price-prediction"], "has_h5": true, "h5_files_links": ["https://github.com/x4nth055/price-prediction/blob/0b83da0847fe3a5746c15a0d31bad2b770e815a4/results/2019-05-21_BTC-USD-avohl-loss-mae-LSTM-seq-60-step-1-layers-3-units-256.h5"], "see_also_links": ["http://theautomatic.net/yahoo_fin-documentation/"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a728"}, "repo_url": "https://github.com/kpe/bert-for-tf2", "repo_name": "bert-for-tf2", "repo_full_name": "kpe/bert-for-tf2", "repo_owner": "kpe", "repo_desc": "A Keras TensorFlow 2.0 implementation of BERT.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T11:24:57Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T07:51:33Z", "homepage": "https://github.com/kpe/bert-for-tf2", "size": 70, "language": "Python", "has_wiki": false, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187989988, "is_fork": false, "readme_text": "BERT for TensorFlow v2       This repo contains a TensorFlow v2 Keras implementation of google-research/bert with support for loading of the original pre-trained weights, and producing activations numerically identical to the one calculated by the original model. The implementation is build from scratch using only basic tensorflow operations, following the code in google-research/bert/modeling.py (but skipping dead code and applying some simplifications). It also utilizes kpe/params-flow to reduce common Keras boilerplate code (related to passing model and layer configuration arguments).  LICENSE MIT. See License File.  Install bert-for-tf2 is on the Python Package Index (PyPI): pip install bert-for-tf2   Usage TBD  Resources  BERT - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding google-research/bert - the original BERT implementation kpe/params-flow - A Keras coding style for for reducing Keras boilerplate code in custom layers by utilizing kpe/py-params  ", "has_readme": true, "readme_language": "English", "repo_tags": ["bert", "keras", "tensorflow", "transformer"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1810.04805"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a729"}, "repo_url": "https://github.com/cohackmeetup/tf-servings", "repo_name": "tf-servings", "repo_full_name": "cohackmeetup/tf-servings", "repo_owner": "cohackmeetup", "repo_desc": "Simple demonstration with tf-servings", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T08:16:58Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T08:03:38Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187595482, "is_fork": false, "readme_text": "Cohack Meetup #0 - AI HACK Instruction  Be sure to have python 3.6. Later version are not supported by TF, especially when using Conv layers Install virtual env (pip install virtualenv) and make a env (mkvitualenv name) so your modules are only installed in that env and not globally Install tf (pip install tensorflow), deras (pip install keras) Pull the docker image for tensor flow serving (docker pull tensorflow/serving)  Keyword Pitourch, InceptionV3, ImageNet, Keras Thank you Fabien Martin ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a72a"}, "repo_url": "https://github.com/nolanliou/mobile-deeplab-v3-plus", "repo_name": "mobile-deeplab-v3-plus", "repo_full_name": "nolanliou/mobile-deeplab-v3-plus", "repo_owner": "nolanliou", "repo_desc": "Deeplab-V3+ model with MobilenetV2/MobilenetV3 on TensorFlow for mobile deployment.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T05:01:16Z", "repo_watch": 11, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T03:24:28Z", "homepage": null, "size": 330, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 187955886, "is_fork": false, "readme_text": "Mobile Deeplab-V3+ model for Segmentation This project is used for deploying people segmentation model to mobile device and learning. The people segmentation android project is here. The model is another Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation (Deeplab-V3+) implementation base on MobilenetV2 / MobilenetV3 on TensorFlow. This project uses tf.estimator API to do training, and many code come from TensorFlow model,  Dataset This project supports 2 dataset.  PASCAL VOC2012  Base dataset used for test. You could download the augmented segmentation dataset from DrSleep.   Supervisely People dataset.  People Segmentation dataset. More data could refer to the blog Download from baidu pan, code: 8pdv    Environment  TensorFlow v1.12.0  No module named keras_applications: solution AttributeError: 'Estimator' object has no attribute '_distribution': solution.    Usage  Prepare pretrained MobilenetV2 model(optional) bash prepare.sh  Train  train on pascal voc 2012 dataset  bash train_pascal_voc2012.sh # Tensorboard tensorboard --logdir=datasets/pascal_voc2012/exp/deeplab-v3-plus/train  train on people segmentation dataset  mv people_segmentation.tar.gz datasets bash train_people_seg.sh # Tensorboard tensorboard --logdir=datasets/people_segmentation/exp/deeplab-v3-plus/train  Export and Freeze model  pascal voc 2012 dataset  bash export_pascal_voc2012.sh # Output # 1. TensorFlow Serving: datasets/pascal_voc2012/exp/deeplab-v3-plus/export/ # 2. Frozen model: datasets/pascal_voc2012/exp/deeplab-v3-plus/export/frozen_model.pb  people segmentation dataset  bash export_people_seg.sh # Output # 1. TensorFlow Serving: datasets/people_segmentation/exp/deeplab-v3-plus/export/ # 2. Frozen model: datasets/people_segmentation/exp/deeplab-v3-plus/export/frozen_model.pb   Explanation You could modify the parameters and add new backbones network easily.  Use MobilenetV3 backbone: Change parameter from --backbone=\"MobielentV2\" to --backbone=\"MobilenetV3\" quant_friendly: Remove BN layer after Depthwise Convolution, replace relu6 with relu, method from A Quantization-Friendly Separable Convolution for MobileNets.  Result  Pascal VOC2012     Backbone Pretrained backbone Augmented Input Size ASPP Decoder mIOU(%)     MobilenetV2 YES YES 513x513 NO NO 70.51     Environment(base_lr=0.007, training_steps=30000, learning_policy=poly, num_gpu=1)   People Segmentation Dataset     Backbone Input Size Augmented ASPP Decoder Quant-Friendly Steps mIOU(%) Pretrained Model     MobilenetV2 256x256 YES NO YES YES 150000 81.68 model   MobilenetV2 256x256 YES YES YES YES 150000 83.33 model   MobilenetV3 256x256 YES No YES YES 150000 81.86 model   MobilenetV2 513x513 YES No YES YES 150000 89.55 model     Environment(base_lr=0.05, training_steps=150000, learning_policy=poly, num_gpu=1) Augmented: use augmented dataset.  TODO   Add LR-ASPP module in MobilenetV3.  References  https://github.com/tensorflow/models/tree/master/research/deeplab https://github.com/rishizek/tensorflow-deeplab-v3-plus https://github.com/kuan-wang/pytorch-mobilenet-v3/blob/master/mobilenetv3.py https://github.com/DrSleep/tensorflow-deeplab-resnet  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://host.robots.ox.ac.uk/pascal/VOC/"], "reference_list": ["https://arxiv.org/pdf/1802.02611", "https://arxiv.org/pdf/1801.04381.pdf", "https://arxiv.org/pdf/1905.02244.pdf", "https://arxiv.org/pdf/1803.08607", "https://arxiv.org/pdf/1905.02244.pdf"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a72b"}, "repo_url": "https://github.com/BeomSooKim/keras-tip", "repo_name": "keras-tip", "repo_full_name": "BeomSooKim/keras-tip", "repo_owner": "BeomSooKim", "repo_desc": "keras error, tip collection", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T06:47:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T06:38:09Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187977917, "is_fork": false, "readme_text": "keras-tip  custom generator using Sequence imgaug examples  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a72c"}, "repo_url": "https://github.com/Sasaki-GG/Food_101_recognition", "repo_name": "Food_101_recognition", "repo_full_name": "Sasaki-GG/Food_101_recognition", "repo_owner": "Sasaki-GG", "repo_desc": "Flask and keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T06:38:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T06:29:30Z", "homepage": null, "size": 539, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187976567, "is_fork": false, "readme_text": "Food_101_recognition Flask and keras (1.2.2) Based on Food_101 inceptionV3 , web_app (web page unfishined) Usage 1.start cam 2.take photo 3.click 'reco' button to get result Run   python main.py   web: 127.0.0.1:5000   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a72d"}, "repo_url": "https://github.com/cherry324/BasicNeuralNetworkUsingKeras", "repo_name": "BasicNeuralNetworkUsingKeras", "repo_full_name": "cherry324/BasicNeuralNetworkUsingKeras", "repo_owner": "cherry324", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T06:45:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T06:37:31Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187776497, "is_fork": false, "readme_text": "BasicNeuralNetworkUsingKeras This simple project is done as a process of learning. I have taken the code from the website link : https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/ Jason Browlee, the author of that website has clearly exaplained about how to create a simple neural network model using keras. Keras is a python library for Deep Learning. Following steps are performed in the code. ->How to load data. ->How to define neural network in Keras. ->How to compile a Keras model using the efficient numerical backend. ->How to train a model on data. ->How to evaluate a model on data ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a72e"}, "repo_url": "https://github.com/karmdesai/skinLesionClassifier", "repo_name": "skinLesionClassifier", "repo_full_name": "karmdesai/skinLesionClassifier", "repo_owner": "karmdesai", "repo_desc": "This tool uses a convolutional neural network (CNN) model and transfer learning methods in order to classify skin lesions as either melanoma or non-melanoma.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T22:54:54Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T00:29:22Z", "homepage": "", "size": 820505, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187542842, "is_fork": false, "readme_text": "skinLesionClassifier Skin cancer is the most common type of cancer. About 30% of new cases of cancer in Canada are skin cancers. There are many types of skin cancer, but the most deadly form (by far) is melanoma. Melanoma is very hard to stop once it has spread to other parts of the body. Fortunately, early detection can improve the chance of survival. This is a classification model that uses the HAM10000 dataset to classify skin lesions as either melanoma (malignant) or non-melanoma (benign). It is trained on Google's Inception v3 convolutional neural network (CNN) model. The model was trained using Keras and has an accuracy score of around 86%. Usage $ git clone https://github.com/karmdesai/skinLesionClassifier.git $ cd skinLesionClassifier $ python lesionPredictor.py  Once the model has loaded, there will be a prompt asking for the file path of the image that you want to classify. Simply enter the entire file path and the model will classify the lesion. What's Next   Convert the model to TensorflowJS so that it can be used on the web  Upgrade the model so that it can classify lesions into more types  Acknowledgements  Inception v3 Model - [https://github.com/tensorflow/models/tree/master/research/inception] Skin Lesions Classification Paper - [https://arxiv.org/pdf/1812.02316.pdf] HAM10000 Dataset - [https://www.nature.com/articles/sdata2018161]  ", "has_readme": true, "readme_language": "English", "repo_tags": ["python", "keras", "inception-v3", "convolutional-neural-networks", "classification-model", "transfer-learning"], "has_h5": true, "h5_files_links": ["https://github.com/karmdesai/skinLesionClassifier/blob/1147c725631a694430dd8caf40d26c7df45aa9b2/model.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1812.02316.pdf"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a72f"}, "repo_url": "https://github.com/wau/keras-rl2", "repo_name": "keras-rl2", "repo_full_name": "wau/keras-rl2", "repo_owner": "wau", "repo_desc": "Reinforcement learning with tensorflow 2 keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T03:55:40Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-21T06:23:09Z", "homepage": null, "size": 894, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 187774337, "is_fork": false, "readme_text": "                 Deep Reinforcement Learning for Tensorflow 2 Keras        NOTE: Requires tf-nightly-2.0-preview. NOT tensorflow==2.0.0-alpha0. What is it? keras-rl2 implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the deep learning library Keras. Furthermore, keras-rl2 works with OpenAI Gym out of the box. This means that evaluating and playing around with different algorithms is easy. Of course you can extend keras-rl2 according to your own needs. You can use built-in Keras callbacks and metrics or define your own. Even more so, it is easy to implement your own environments and even algorithms by simply extending some simple abstract classes. Documentation is available online. What is included? As of today, the following algorithms have been implemented:   Deep Q Learning (DQN) [1], [2]  Double DQN [3]  Deep Deterministic Policy Gradient (DDPG) [4]  Continuous DQN (CDQN or NAF) [6]  Cross-Entropy Method (CEM) [7], [8]  Dueling network DQN (Dueling DQN) [9]  Deep SARSA [10]  Asynchronous Advantage Actor-Critic (A3C) [5]  Proximal Policy Optimization Algorithms (PPO) [11]  You can find more information on each agent in the doc. Installation  Install Keras-RL2 from Pypi (recommended):  pip install keras-rl2   Install from Github source:  git clone https://github.com/keras-rl2/keras-rl2.git cd keras-rl python setup.py install  Examples If you want to run the examples, you'll also have to install:  gym by OpenAI: Installation instruction h5py: simply run pip install h5py  For atari example you will also need:  Pillow: pip install Pillow gym[atari]: Atari module for gym. Use pip install gym[atari]  Once you have installed everything, you can try out a simple example: python examples/dqn_cartpole.py This is a very simple example and it should converge relatively quickly, so it's a great way to get started! It also visualizes the game during training, so you can watch it learn. How cool is that? If you have questions or problems, please file an issue or, even better, fix the problem yourself and submit a pull request! References  Playing Atari with Deep Reinforcement Learning, Mnih et al., 2013 Human-level control through deep reinforcement learning, Mnih et al., 2015 Deep Reinforcement Learning with Double Q-learning, van Hasselt et al., 2015 Continuous control with deep reinforcement learning, Lillicrap et al., 2015 Asynchronous Methods for Deep Reinforcement Learning, Mnih et al., 2016 Continuous Deep Q-Learning with Model-based Acceleration, Gu et al., 2016 Learning Tetris Using the Noisy Cross-Entropy Method, Szita et al., 2006 Deep Reinforcement Learning (MLSS lecture notes), Schulman, 2016 Dueling Network Architectures for Deep Reinforcement Learning, Wang et al., 2016 Reinforcement learning: An introduction, Sutton and Barto, 2011 Proximal Policy Optimization Algorithms, Schulman et al., 2017  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://keras.io", "http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf", "http://keras-rl2.readthedocs.org", "http://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf", "http://keras-rl.readthedocs.io/en/latest/agents/overview/", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf", "http://keras-rl2.readthedocs.io/"], "reference_list": ["http://arxiv.org/abs/1312.5602", "http://arxiv.org/abs/1509.06461", "http://arxiv.org/abs/1509.02971", "http://arxiv.org/abs/1603.00748", "https://arxiv.org/abs/1511.06581", "http://arxiv.org/abs/1602.01783", "https://arxiv.org/abs/1707.06347"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a730"}, "repo_url": "https://github.com/dkuntzIMT/KUNTZ-Damien", "repo_name": "KUNTZ-Damien", "repo_full_name": "dkuntzIMT/KUNTZ-Damien", "repo_owner": "dkuntzIMT", "repo_desc": "Deep-Learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T13:17:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T13:12:27Z", "homepage": null, "size": 53362, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188041701, "is_fork": false, "readme_text": "SoundNet in Keras SoundNet, built in Keras with pre-trained 8-layer model. SoundNet was built by Yusuf Aytar, Carl Vondrick, and Antonio Torralba. Details available at https://projects.csail.mit.edu/soundnet/. The code here is adapted to work with Keras using https://github.com/eborboihuc/SoundNet-tensorflow as a basis. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a731"}, "repo_url": "https://github.com/hazhikko/learning_keras", "repo_name": "learning_keras", "repo_full_name": "hazhikko/learning_keras", "repo_owner": "hazhikko", "repo_desc": "Keras\u306e\u52c9\u5f37\u7528", "description_language": "Japanese", "repo_ext_links": null, "repo_last_mod": "2019-05-30T14:47:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T14:50:27Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 17, "github_id": 187858435, "is_fork": false, "readme_text": "learning_keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a732"}, "repo_url": "https://github.com/nric/TF2.0_Keras_DDPG", "repo_name": "TF2.0_Keras_DDPG", "repo_full_name": "nric/TF2.0_Keras_DDPG", "repo_owner": "nric", "repo_desc": "A commented Tensorflow 2.0 Keras implementation of DDPG for open AI gym continuous environments.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T22:56:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T22:50:55Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188124169, "is_fork": false, "readme_text": "TF2.0_Keras_DDPG A commented Tensorflow 2.0 Keras implementation of DDPG for open AI gym continuous environments. This implementation of Deep Deterministic Policy Gradient is different from other implementations only in that regard, that I kept to keras only style, meaning that also the somewhat complicated loss function is implemented by keras type custom loss mehtods. I did this for learning and undestanding only. It is most closely to the OpenAi Spinning up explanation and thier implementation: https://spinningup.openai.com/en/latest/algorithms/ddpg.html https://github.com/openai/spinningup/blob/master/spinup/algos/ddpg/ddpg.py TODO: Currently it does not apear to converge in any of the tested envs but did not test for long. Could be only HP optimization problem but can't gurantee. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a733"}, "repo_url": "https://github.com/GauravRoy48/Artificial-Neural-Network", "repo_name": "Artificial-Neural-Network", "repo_full_name": "GauravRoy48/Artificial-Neural-Network", "repo_owner": "GauravRoy48", "repo_desc": "Python code for implementing ANN on given dataset. Spyder IDE used.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T17:28:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T17:20:56Z", "homepage": null, "size": 3017, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187684080, "is_fork": false, "readme_text": "Artificial-Neural-Network  Python code for implementing ANN using Stochastic Gradient Descent on given dataset. Spyder IDE used.  Installation Notes Keras can only run on Python version 3.6 so we need to create a new virtual environment using conda to do that. Following steps to be run using Anaconda Prompt. Step 1: Update conda conda update conda conda update --all  Step 2: If you have NVIDIA DEDICATED GRAPHICS and want to utilize that for Keras then follow Steps 4 to 6 in the given link: https://github.com/antoniosehk/keras-tensorflow-windows-installation If you want to utilize your CPU instead, then go to the next step. Step 3: If you want to use your **NVIDIA GPU **then run in Anaconda Prompt: conda create -n new_env python=3.6 numpy scipy pandas matplotlib statsmodels scikit-learn spyder keras-gpu where new_env is the name of the new environment that you create. If you want to use your CPU then run in Anaconda Prompt: conda create -n new_env python=3.6 numpy scipy pandas matplotlib statsmodels scikit-learn spyder keras where new_env is the name of the new environment that you create. Step 4: Activate your new environment by running the following command in Anaconda Prompt: conda activate new_env Step 5: Open Anaconda Navigator and in the Home tab, you should be able to see a drop down box that gives you an option to switch between base (root)  and this new environment. Switch to the new environment if not already done, and run Spyder from there. Everything should be working now. NOTES: More commands that may be required to handle the virtual environment. Command to deactivate the virtual environment: conda deactivate Command to delete the virtual environment: conda env remove -n new_env ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a734"}, "repo_url": "https://github.com/jzhangab/Tensorflow_Keras_example", "repo_name": "Tensorflow_Keras_example", "repo_full_name": "jzhangab/Tensorflow_Keras_example", "repo_owner": "jzhangab", "repo_desc": "Simple example of using deep learning for a structured data set.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T00:15:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T00:13:41Z", "homepage": null, "size": 14520, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187730946, "is_fork": false, "readme_text": "Tensorflow_Keras_example Simple example of using deep learning for a structured data set. Contents: /1_Data/*.xslx contains data for this example /TF_Keras_example.py contains simple example of TF and Keras using a structured data set ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a735"}, "repo_url": "https://github.com/renatosc/nnv", "repo_name": "nnv", "repo_full_name": "renatosc/nnv", "repo_owner": "renatosc", "repo_desc": "NNV - Neural Network Visualizer", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T17:39:04Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T03:59:10Z", "homepage": "", "size": 1969, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187758698, "is_fork": false, "readme_text": "Neural Network Visualizer (NNV) Simple and easy to use tool to generate Neural Network Visualizations. Installation pip install nnv  Usage from nnv import NNV  layersList = [     {\"title\":\"input\\n(relu)\", \"units\": 3, \"color\": \"darkBlue\"},     {\"title\":\"hidden 1\\n(relu)\", \"units\": 3},     {\"title\":\"output\\n(sigmoid)\", \"units\": 1,\"color\": \"darkBlue\"}, ]  NNV(layersList).render()  It is possible to customize the node size/colors, title font size, spacing between nodes and layers and maximum number of nodes to show,... from nnv import NNV  # Let's increase the size of the plot import matplotlib.pyplot as plt plt.rcParams[\"figure.figsize\"] = (200,10)   layers_list = [     {\"title\":\"input\\n(relu)\", \"units\": 300, \"color\": \"darkBlue\"},     {\"title\":\"hidden 1\\n(relu)\", \"units\": 150},     {\"title\":\"hidden 2\\n(relu)\",  \"units\": 75},     {\"title\":\"Dropout\\n(0.5)\", \"units\": 75,\"color\":\"lightGray\"},     {\"title\":\"hidden 4\\n(relu)\",  \"units\": 18},     {\"title\":\"hidden 5\\n(relu)\",  \"units\": 9},     {\"title\":\"hidden 6\\n(relu)\",  \"units\": 4},     {\"title\":\"output\\n(sigmoid)\", \"units\": 1,\"color\": \"darkBlue\"}, ]   NNV(layers_list, max_num_nodes_visible=8, node_radius=10, spacing_layer=60, font_size=24).render(save_to_file=\"my_example_2.pdf\")  Documentation NNV documentation is still being created. For now, if you have any question, please look directly the library source code or open an Issue. Future addittions Some useful features that may be added in the future (help is welcome):  add labels to each node import layers info directly from a keras model  Citation If you use this library and would like to cite it, you can use:  R. Cordeiro, \"NNV: Neural Network Vizualizer\", 2019. [Online]. Available: https://github.com/renatosc/nnv. [Accessed: DD- Month- 20YY].  or: @Misc{,   author = {Renato Cordeiro},   title  = {NNV: Neural Network Vizualizer},   month  = may,   year   = {2019},   note   = {Online; accessed <today>},   url    = {https://github.com/renatosc/nnv}, }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a736"}, "repo_url": "https://github.com/Titania0/Keras_Study", "repo_name": "Keras_Study", "repo_full_name": "Titania0/Keras_Study", "repo_owner": "Titania0", "repo_desc": "MyKerasStudy", "description_language": "Danish", "repo_ext_links": null, "repo_last_mod": "2019-05-31T14:58:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T07:34:00Z", "homepage": "https://www.computelabo.com", "size": 18, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187785502, "is_fork": false, "readme_text": "Keras_Study Just my Keras Study Repository More Info Here !! -> https://www.computelabo.com ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a737"}, "repo_url": "https://github.com/SamKaiYang/ROS_Socket_Arm", "repo_name": "ROS_Socket_Arm", "repo_full_name": "SamKaiYang/ROS_Socket_Arm", "repo_owner": "SamKaiYang", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T12:57:17Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T09:43:06Z", "homepage": null, "size": 1753, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188009003, "is_fork": false, "readme_text": "yolo_v3_keras The demo code of YOLO_V3 Acknowledgemwnt source from https://github.com/qqwweee/keras-yolo3 keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Change the path in /ros_yolo3/src/yolo3/scripts/path_set.py Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection. (roslaunch yolo_v3 object_detect.launch)  wget https://pjreddie.com/media/files/yolov3.weights python3 convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python3 yolo_video.py [OPTIONS...] --image, for image detection mode, OR python3 yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a738"}, "repo_url": "https://github.com/Srinivasan-Palaniswami/Ent-Ext", "repo_name": "Ent-Ext", "repo_full_name": "Srinivasan-Palaniswami/Ent-Ext", "repo_owner": "Srinivasan-Palaniswami", "repo_desc": "Extracting Entities from a sentence", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T18:02:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T17:03:16Z", "homepage": null, "size": 128, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187681748, "is_fork": false, "readme_text": "Extracting-Entity-with-Bidirectional-LSTM-CNNs A keras implementation of Bidirectional-LSTM_CNNs. The original paper can be found at https://arxiv.org/abs/1511.08308 Dataset & Results Dataset & F1 Scores are available in docs/results.txt Network Model. Model Architecture is available in following folders. \"docs/model_on_paper.png\" \"docs/char_embeddings.png\" To run the script /Code/python3 train.py Requirements 1) numpy  2) Keras 3) Tensorflow  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1511.08308"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a739"}, "repo_url": "https://github.com/GauravRoy48/Convolutional-Neural-Network", "repo_name": "Convolutional-Neural-Network", "repo_full_name": "GauravRoy48/Convolutional-Neural-Network", "repo_owner": "GauravRoy48", "repo_desc": "Python code to implement CNN on given dataset. Spyder IDE used.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T12:41:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T17:21:28Z", "homepage": null, "size": 4374, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187684143, "is_fork": false, "readme_text": "Convolutional-Neural-Network  Python code to implement CNN on given dataset. Spyder IDE used. Dataset to be found in : https://drive.google.com/drive/folders/1ziKMaravrAXPAdd6FpCSi_uAJYT7bH1v Additional Reading : https://adeshpande3.github.io/The-9-Deep-Learning-Papers-You-Need-To-Know-About.html Additional Reading : https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/ Additional Reading : https://peterroelants.github.io/posts/cross-entropy-softmax/  Installation Notes Keras can only run on Python version 3.6 so we need to create a new virtual environment using conda to do that. Following steps to be run using Anaconda Prompt. Step 1: Update conda conda update conda conda update --all  Step 2: If you have NVIDIA DEDICATED GRAPHICS and want to utilize that for Keras then follow Steps 4 to 6 in the given link: https://github.com/antoniosehk/keras-tensorflow-windows-installation If you want to utilize your CPU instead, then go to the next step. Step 3: If you want to use your **NVIDIA GPU **then run in Anaconda Prompt: conda create -n new_env python=3.6 numpy scipy pandas matplotlib statsmodels scikit-learn pillow spyder keras-gpu where new_env is the name of the new environment that you create. If you want to use your CPU then run in Anaconda Prompt: conda create -n new_env python=3.6 numpy scipy pandas matplotlib statsmodels scikit-learn pillow spyder keras where new_env is the name of the new environment that you create. Step 4: Activate your new environment by running the following command in Anaconda Prompt: conda activate new_env Step 5: Open Anaconda Navigator and in the Home tab, you should be able to see a drop down box that gives you an option to switch between base (root)  and this new environment. Switch to the new environment if not already done, and run Spyder from there. Everything should be working now. NOTES: More commands that may be required to handle the virtual environment. Command to deactivate the virtual environment: conda deactivate Command to delete the virtual environment: conda env remove -n new_env ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a73a"}, "repo_url": "https://github.com/zqtay/Camera-Image-Classification", "repo_name": "Camera-Image-Classification", "repo_full_name": "zqtay/Camera-Image-Classification", "repo_owner": "zqtay", "repo_desc": "A script to train a simple neural network to recognise various objects. Training data (images) are to be prepared in folders separated by respective categories. Camera is used to apply the trained model.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T06:57:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T18:58:44Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187697345, "is_fork": false, "readme_text": "Camera-Image-Classification A script to train a simple neural network to recognise various objects. Training data (images) are to be prepared in separate folders named by their respective categories. The network's accuracy and loss at each epoch can be visualised with Tensorboard. Camera is used to apply the trained model. This script utilises Keras and OpenCV. Further readings: https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/ https://github.com/rmotr/ml-workshop-image-recognition ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a73b"}, "repo_url": "https://github.com/Manishms18/Sign-Language-Interpreter", "repo_name": "Sign-Language-Interpreter", "repo_full_name": "Manishms18/Sign-Language-Interpreter", "repo_owner": "Manishms18", "repo_desc": "A sign language interpreter using live video feed from the camera.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T03:06:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T02:46:13Z", "homepage": "", "size": 7604, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187556737, "is_fork": false, "readme_text": "Sign Language Interpreter using Deep Learning  A sign language interpreter using live video feed from the camera. The project was completed in 24 hours as part of HackUNT-19, the University of North Texas's annual Hackathon. You can view the project demo on YouTube.  Table of contents  General info Screenshots Demo Technologies and Tools Setup Process Code Examples Features Status Contact  General info The theme at HACK UNT 19 was to use technology to improve accessibility by finding a creative solution to benefit the lives of those with a disability. We wanted to make it easy for 70 million deaf people across the world to be independent of translators for there daily communication needs, so we designed the app to work as a personal translator 24*7 for the deaf people. Demo    The entire demo of the project can be found on YouTube. Screenshots   Technologies and Tools  Python TensorFlow Keras OpenCV  Setup  Use comand promt to setup environment by using install_packages.txt and install_packages_gpu.txt files.  pyton -m pip r install_packages.txt This will help you in installing all the libraries required for the project. Process  Run set_hand_histogram.py to set the hand histogram for creating gestures. Once you get a good histogram, save it in the code folder, or you can use the histogram created by us that can be found here. Added gestures and label them using OpenCV which uses webcam feed. by running create_gestures.py and stores them in a database. Alternately, you can use the gestures created by us here. Add different variations to the captured gestures by flipping all the images by using Rotate_images.py. Run load_images.py to split all the captured gestures into training, validation and test set. To view all the gestures, run display_gestures.py . Train the model using Keras by running cnn_model_train.py. Run final.py. This will open up the gesture recognition window which will use your webcam to interpret the trained American Sign Language gestures.  Code Examples # Model Traiining using CNN  import numpy as np import pickle import cv2, os from glob import glob from keras import optimizers from keras.models import Sequential from keras.layers import Dense from keras.layers import Dropout from keras.layers import Flatten from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.utils import np_utils from keras.callbacks import ModelCheckpoint from keras import backend as K K.set_image_dim_ordering('tf')  os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  def get_image_size():  img = cv2.imread('gestures/1/100.jpg', 0)  return img.shape  def get_num_of_classes():  return len(glob('gestures/*'))  image_x, image_y = get_image_size()  def cnn_model():  num_of_classes = get_num_of_classes()  model = Sequential()  model.add(Conv2D(16, (2,2), input_shape=(image_x, image_y, 1), activation='relu'))  model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))  model.add(Conv2D(32, (3,3), activation='relu'))  model.add(MaxPooling2D(pool_size=(3, 3), strides=(3, 3), padding='same'))  model.add(Conv2D(64, (5,5), activation='relu'))  model.add(MaxPooling2D(pool_size=(5, 5), strides=(5, 5), padding='same'))  model.add(Flatten())  model.add(Dense(128, activation='relu'))  model.add(Dropout(0.2))  model.add(Dense(num_of_classes, activation='softmax'))  sgd = optimizers.SGD(lr=1e-2)  model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])  filepath=\"cnn_model_keras2.h5\"  checkpoint1 = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')  callbacks_list = [checkpoint1]  #from keras.utils import plot_model  #plot_model(model, to_file='model.png', show_shapes=True)  return model, callbacks_list  def train():  with open(\"train_images\", \"rb\") as f:   train_images = np.array(pickle.load(f))  with open(\"train_labels\", \"rb\") as f:   train_labels = np.array(pickle.load(f), dtype=np.int32)   with open(\"val_images\", \"rb\") as f:   val_images = np.array(pickle.load(f))  with open(\"val_labels\", \"rb\") as f:   val_labels = np.array(pickle.load(f), dtype=np.int32)   train_images = np.reshape(train_images, (train_images.shape[0], image_x, image_y, 1))  val_images = np.reshape(val_images, (val_images.shape[0], image_x, image_y, 1))  train_labels = np_utils.to_categorical(train_labels)  val_labels = np_utils.to_categorical(val_labels)   print(val_labels.shape)   model, callbacks_list = cnn_model()  model.summary()  model.fit(train_images, train_labels, validation_data=(val_images, val_labels), epochs=15, batch_size=500, callbacks=callbacks_list)  scores = model.evaluate(val_images, val_labels, verbose=0)  print(\"CNN Error: %.2f%%\" % (100-scores[1]*100))  #model.save('cnn_model_keras2.h5')  train() K.clear_session();   Features Our model was able to predict the 44 characters in the ASL with a prediction accuracy >95%. Features that can be added:  Deploy the project on cloud and create an API for using it. Increase the vocabulary of our model Incorporate feedback mechanism to make the model more robust Add more sign languages  Status Project is: finished. Our team was the winner of the UNT Hackaton 2019. You can find the our final submission post on devpost. Contact Created by me with my teammates Siddharth Oza, Harsh Gupta, and Ashish Sharma. If you loved what you read here and feel like we can collaborate to produce some exciting stuff, or if you just want to shoot a question, please feel free to connect with me on email or LinkedIn ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://bit.ly/2WWllwg", "http://bit.ly/2Iaz2UK"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a73c"}, "repo_url": "https://github.com/AdrianE92/Image-classifier-dog-vs-cat", "repo_name": "Image-classifier-dog-vs-cat", "repo_full_name": "AdrianE92/Image-classifier-dog-vs-cat", "repo_owner": "AdrianE92", "repo_desc": "An image classifier seperating dogs from cats", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T13:33:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T13:17:47Z", "homepage": null, "size": 794228, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187644114, "is_fork": false, "readme_text": "Machine Learning Algorithm for differentiating dogs from cats written in Python3 using CNN with Keras To run the program type python3 dog_or_cat.py Make sure to have the dataset linked below and use the proper folder structure Dependencies:  numpy matplotlib.pyplot keras tensorflow sklearn  I divided them like this: 3000 cat pictures and 3000 dog pictures in the test folder 7999 cat pictures and 7999 dog pictures in the training folder 1500 cat pictures and 1500 dog pictures in the validation folder The folder structure should look like this: test_folder: test: 0.jpg ... n.jpg training: training_cat: 0.jpg ... n.jpg training_dog: 0.jpg ... n.jpg validation: val_cat: 0.jpg ... n.jpg val_dog: 0.jpg ... n.jpg I've used Keras' built in methods for early stopping so it will run for either 30 epochs or until it has 3 epochs in a row with no performance increase to the validation set. I could've added more layers and set the image size larger than 64x64, but running machine learning on a CPU takes a fair amount of time and I wanted to make it runnable. As you can see from Example_run.png I ended at approximately 83% on the validation set and the printed matrix was: TP 2232, FP 768, FN 819, TN 2191. It gives us about 66% success rate. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a73d"}, "repo_url": "https://github.com/szjy/Startcraft2AI", "repo_name": "Startcraft2AI", "repo_full_name": "szjy/Startcraft2AI", "repo_owner": "szjy", "repo_desc": "A AI for Starcraft2. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T16:26:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T09:21:51Z", "homepage": null, "size": 87995, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187608645, "is_fork": false, "readme_text": "StarCraft 2 AI Date: 2019-5-31 Author: YJZ, LJT, ZJP Dependencies  DeepMind PySC2 (2.0.1): https://github.com/deepmind/pysc2 Maps: https://github.com/Blizzard/s2client-proto Python (3.6): https://www.python.org/downloads TensorFlow (1.4.0): https://github.com/tensorflow/tensorflow Keras (2.0.8): https://github.com/keras-team/keras  What Realize a easiest StarCraft 2 Deep Learning AI Goal Let AI finish a normal race. Train Use src/train_model.py to train model and --dataset should be set. python train_model.py --dataset=<dir>  Play Use src/play_by_agent.py to play a race with a zerg bot. python play_by_agent.py  Replays By using src/replay-api/download_replays.py download the specified version's replays Usage\uff1a python download_replays.py --key=33a729bef5834ebaa07d6259f9d6c5ea --secret=jPpwSxq9q8v1HYfL2yeSqhrJskibb3Tr --version=<version> --replays_dir=<Replays-dir> --download_dir=<Download-dir>   We use version 4.7.1. Read the replay file and transform it to numpy array and save in files. Use src/transform_replay.py and src/ObservationAgent.py to make it. Usage: python transform_replay.py --replay=<Replays-dir> --agent=ObservationAgent.ObservationAgent  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a73e"}, "repo_url": "https://github.com/Tawn0000/CrowdCounting", "repo_name": "CrowdCounting", "repo_full_name": "Tawn0000/CrowdCounting", "repo_owner": "Tawn0000", "repo_desc": "mscnn crowd counting model implementation, source from \"Multi-scale Convolution Neural Networks for Crowd Counting\" write by Zeng L, Xu X, Cai B, et al. https://arxiv.org/abs/1702.02359", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T15:35:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T02:51:08Z", "homepage": null, "size": 870, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187557309, "is_fork": false, "readme_text": "MSCNN A Python 3 and Keras 2 implementation of MSCNN for people countingand provide train method. Requirement  Python 3.6 Keras 2.2.2 Tensorflow-gpu 1.8.0 OpenCV 3.4  MSCNN and MSB architectures MSCNN  MSB  Experiment data Mall Dataset crowd counting dataset Generate density_map from data:  train run the follow command: python train.py --size 224 --batch 16 --epochs 100  test in dataset real count:30 pred count:27   Reference @article{MSCNN,     title={Multi-scale convolutional neural network for crowd counting},     author={Lingke Zeng, Xiangmin Xu, Bolun Cai, Suo Qiu, Tong Zhang},   journal={2017 IEEE International Conference on Image Processing (ICIP)},   year={2017} }  Copyright See LICENSE for details. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://personal.ie.cuhk.edu.hk/~ccloy/downloads_mall_dataset.html"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a73f"}, "repo_url": "https://github.com/ZehaoXU/yolov3-for-medical-images", "repo_name": "yolov3-for-medical-images", "repo_full_name": "ZehaoXU/yolov3-for-medical-images", "repo_owner": "ZehaoXU", "repo_desc": "\u672c\u79d1\u6bd5\u4e1a\u8bbe\u8ba1\u5185\u5bb9\uff0c\u4e34\u65f6\u4ed3\u5e93", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-27T10:43:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T12:44:46Z", "homepage": null, "size": 866, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187638748, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a740"}, "repo_url": "https://github.com/WichoEguia/tageo_imagenes", "repo_name": "tageo_imagenes", "repo_full_name": "WichoEguia/tageo_imagenes", "repo_owner": "WichoEguia", "repo_desc": "PIA de materia de redes neuronales. Clasificaci\u00f3n de imagenes.", "description_language": "Spanish", "repo_ext_links": null, "repo_last_mod": "2019-05-28T03:54:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T05:07:29Z", "homepage": null, "size": 64534, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187966604, "is_fork": false, "readme_text": "Clasificador de imagenes Jose Luis Egu\u00eda T\u00e9llez 1791916 Mayo/2019 Este programa est\u00e1 escrito en python y utiliza las librerias Tensorflow y Keras. Las categorias en las que puede caer cada imagen son Gorila, Perro y Gato. Requisitos  Tensorflow Keras Numpy Carpeta data que puede ser encontrada en https://drive.google.com/open?id=1zBxPtfsyjStwIxU8AbW-NeC7S4IUt6Ri  Uso El proyecto se divide en dos programas train.py y predict.py.  Lo primero que se debe hacer es ejecutar train.py lo cual generar\u00e1 una carpeta llamada modelo con dos archivos dentro modelo.h5 y pesos.h5. Una ves que tenemos esta carpeta podemos ejecutar el archivo predict.py. python predict.py *path de la imagen*  ", "has_readme": true, "readme_language": "Spanish", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a741"}, "repo_url": "https://github.com/yashbonde/Pink-Panther-SSD-Keras", "repo_name": "Pink-Panther-SSD-Keras", "repo_full_name": "yashbonde/Pink-Panther-SSD-Keras", "repo_owner": "yashbonde", "repo_desc": "Small hack which performs detection of Pink Panther and The Little Man using SSD in Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T22:00:17Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T16:00:29Z", "homepage": null, "size": 35217, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187871032, "is_fork": false, "readme_text": "Pink-Panther-SSD-Keras Small hack which performs detection of Pink Panther and The Little Man using SSD7 built in Keras. The iteresting thing is that it learns in a very few samples.  Description of Files Following is the list of files in the model:  run.py: Main file to run for the module, handles both training and testing  usage: run.py [-h] [--url URL] [--train TRAIN] [--labels LABELS]               [--batch-size BATCH_SIZE] [--epochs EPOCHS] [--model MODEL]               [--opdir OPDIR] [--debug DEBUG]  optional arguments:   -h, --help               show this help message and exit   --url URL                train URL   --train TRAIN            bool whether training or not   --labels LABELS          path to labels file   --batch-size BATCH_SIZE  mini batch size   --epochs EPOCHS          number of training epochs   --model MODEL            path to model   --opdir OPDIR            folder for writing logs   --debug DEBUG            give debug outputs  layers.py: Custom Keras Layers ssd_utils.py: Basic util functions ssd_box_encode_decode_utils.py: functions required to convert formats  Train the Model Model is a small SSD7 model, which was chosen as it has light weight and is quite fast for just 2 classes. For training a seperate file with bounding boxes needs to be provided. Before training the model need to follow these steps:  Folder frames has all the frames required for training and final.csv has the labels Train the model using command     $ python3 run.py --train=True --labels=./final.csv This shold take about 15-20 minutes depending on the system Run the Model Note that you need to train the model before running deployment. The model should be saved in the folder as 'ssd_7.h5' and weights will be saved as 'ssd_weights.h5'. To run the inference on any video run the command:     $ export YTLINK=\"https://www.youtube.com/watch?v=GXfWeDogVUg\"     $ python3 run.py --model=./ssd7_weights.h5 --url=$YTLINK ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a742"}, "repo_url": "https://github.com/jlfilho/VSRnet-Keras", "repo_name": "VSRnet-Keras", "repo_full_name": "jlfilho/VSRnet-Keras", "repo_owner": "jlfilho", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-22T03:04:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T03:02:24Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187952944, "is_fork": false, "readme_text": "VSRnet-Keras ", "has_readme": true, "readme_language": "Malay (macrolanguage)", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a743"}, "repo_url": "https://github.com/jlfilho/SRCNN-Keras", "repo_name": "SRCNN-Keras", "repo_full_name": "jlfilho/SRCNN-Keras", "repo_owner": "jlfilho", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T14:39:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T19:51:49Z", "homepage": null, "size": 70, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187704255, "is_fork": false, "readme_text": "SRCNN-Keras ", "has_readme": true, "readme_language": "Malay (macrolanguage)", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a744"}, "repo_url": "https://github.com/BeomSooKim/googlenet", "repo_name": "googlenet", "repo_full_name": "BeomSooKim/googlenet", "repo_owner": "BeomSooKim", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-22T06:46:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T06:41:40Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187978427, "is_fork": false, "readme_text": "googlenet implementation (keras) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a745"}, "repo_url": "https://github.com/waveboo/ADNIClassfication", "repo_name": "ADNIClassfication", "repo_full_name": "waveboo/ADNIClassfication", "repo_owner": "waveboo", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T14:19:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T13:29:38Z", "homepage": null, "size": 1675, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187646135, "is_fork": false, "readme_text": "ADNIClassfication CUDA9.0 cudnn7.0 tensorflow 1.2.0 Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a746"}, "repo_url": "https://github.com/crawlinginmyskin/obstawiacz", "repo_name": "obstawiacz", "repo_full_name": "crawlinginmyskin/obstawiacz", "repo_owner": "crawlinginmyskin", "repo_desc": "EPL Match Better", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T21:38:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T21:37:30Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187716851, "is_fork": false, "readme_text": "EPL Match Better This program uses keras to predict outcome of EPL matches ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a747"}, "repo_url": "https://github.com/lokhiufung/neural_arithmetic_logic_units", "repo_name": "neural_arithmetic_logic_units", "repo_full_name": "lokhiufung/neural_arithmetic_logic_units", "repo_owner": "lokhiufung", "repo_desc": "neural arithmetic logic units from deepmind", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T16:46:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T08:01:47Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187991616, "is_fork": false, "readme_text": "neural_arithmetic_logic_units Implementation of neural arithmetic logic units from deepmind with tensorflow (1.13) keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a748"}, "repo_url": "https://github.com/Nano-Neuro-Research-Lab/latent-space-discovery", "repo_name": "latent-space-discovery", "repo_full_name": "Nano-Neuro-Research-Lab/latent-space-discovery", "repo_owner": "Nano-Neuro-Research-Lab", "repo_desc": "A new architecture of adversarial auto-encoder is proposed to extract biologically relevant genes from cancer transcriptomes.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T17:07:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T03:30:52Z", "homepage": null, "size": 31834, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187956703, "is_fork": false, "readme_text": "Extracting Biologically Relevant Genes using Unsupervised Adversarial Autoencoder (AAE) from Cancer Transcriptomes       In this project, we introduce neural network based adversarial autoencoder (AAE) model to extract biologically-relevant features from RNA-Seq data. We also developed a method named TopGene to \ufb01nd highly interactive genes from the latent space. AAE in combination with TopGene method \ufb01nds important genes which could be useful for \ufb01nding cancer biomarkers.  Getting Started The following instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See the instruction below: Prerequisites The following libraries are required to reproduce this project:   Keras (2.0.6)   Keras-adverserial (0.0.3)   Tensorflow (1.13.1)   Scikit-Learn (0.20.3)   Numpy (1.16.3)   Imbalanced-Learn (0.4.3)   Supports both Python 2.5.0 and Python 3.5.6 Directory Layout \u251c\u2500\u2500 results \u2502\u00a0\u00a0 \u251c\u2500\u2500 saved_results \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Gene_Analysis_Breast_Cancer.xlsx \u2502   \u2502   \u251c\u2500\u2500 Gene_Analysis_UCEC.xlsx \u2502   \u251c\u2500\u2500 AAE \u2502   \u2502   \u251c\u2500\u2500 aae_encoded.tsv \u2502   \u2502   \u251c\u2500\u2500 aae_sorted_gene.tsv \u2502   \u2502   \u251c\u2500\u2500 aae_weight_distribution.png \u2502   \u2502   \u251c\u2500\u2500 aae_weight_matrix \u2502   \u251c\u2500\u2500 PCA \u2502   \u251c\u2500\u2500 ... # add LDA, SVD etc \u251c\u2500\u2500 data \u2502\u00a0  \u251c\u2500\u2500 data will be stored here \u251c\u2500\u2500 feature_extraction \u2502\u00a0  \u251c\u2500\u2500 AAE \u2502   \u2502   \u251c\u2500\u2500 aae_encoder.h5 \u2502   \u2502   \u251c\u2500\u2500 aae_decoder.h5 \u2502   \u2502   \u251c\u2500\u2500 aae_discriminator.h5 \u2502   \u2502   \u251c\u2500\u2500 aae_history.csv \u2502\u00a0  \u251c\u2500\u2500 PCA \u2502\u00a0  \u251c\u2500\u2500VAE \u2502   \u251c\u2500\u2500 ... \u251c\u2500\u2500 README.md \u251c\u2500\u2500 figures \u2502\u00a0  \u251c\u2500\u2500 saved_figures \u2502   \u2502   \u251c\u2500\u2500 Olfactory__Transduction_pathway.png \u2514\u2500\u2500 .gitignore Usage Run the following to extract features using different autoencoders main.py  And run the following to extract features when PCA, NMF, FastICA, ICA, RBM etc. are used main_pca.py  Gene ontology of molecular function was performed using DAVID 6.7 https://david-d.ncifcrf.gov/ More regarding gene ontology http://geneontology.org/docs/ontology-documentation/ Proposed Architecture  Datasets  cBioPortal - Cancer Genomics Datasets Breast Invasive Carcinoma (TCGA, Cell 2015) - Clinical information is used to label various molecular subtypes  Breast Invasive Carcinoma (BRCA)    Molecular Subtypes Number of Patients Label     Luminal A 304 0   Luminal B 121 1   Basal & Triple Negetive 137 2   Her 2 Enriched 43 3       Total Number of Samples (Patients) Total Number of Features (Genes)     605 20439     Details about Molecular Subtypes of Breast Cancer  Validation Data  Uterine Corpus Endometrial Carcinoma (TCGA, Nature 2013) - Clinical information is used to label various molecular subtypes.  Uterine Corpus Endometrial Carcinoma (UCEC)    Molecular Subtypes Number of Patients Label     Copy Number High 60 0   Copy Number Low 90 1   Hyper Mutated (MSI) 64 2   Ultra Mutated (POLE) 16 3       Total Number of Samples (Patients) Total Number of Features (Genes)     230 20482     Details about Molecular Subtypes of Endometrial Cancer  Contributing If you want to contribute to this project and make it better, your help is very welcome. When contributing to this repository please make a clean pull request. Acknowledgments  The proposed architecture is inspired by https://github.com/bstriner/keras-adversarial  ", "has_readme": true, "readme_language": "English", "repo_tags": ["autoencoder", "latent-space", "feature-extraction", "deep-neural-networks"], "has_h5": true, "h5_files_links": ["https://github.com/Nano-Neuro-Research-Lab/latent-space-discovery/blob/6c983e6d640982fb564b64cc95d8a3300a687fd1/feature_extraction/AAE/aae_decoder.h5", "https://github.com/Nano-Neuro-Research-Lab/latent-space-discovery/blob/6c983e6d640982fb564b64cc95d8a3300a687fd1/feature_extraction/AAE/aae_discriminator.h5", "https://github.com/Nano-Neuro-Research-Lab/latent-space-discovery/blob/6c983e6d640982fb564b64cc95d8a3300a687fd1/feature_extraction/AAE/aae_encoder.h5"], "see_also_links": ["http://www.cbioportal.org/study?id=ucec_tcga_pub", "http://geneontology.org/docs/ontology-documentation/"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a749"}, "repo_url": "https://github.com/Nano-Neuro-Research-Lab/breast-cancer-sub-types-classification", "repo_name": "breast-cancer-sub-types-classification", "repo_full_name": "Nano-Neuro-Research-Lab/breast-cancer-sub-types-classification", "repo_owner": "Nano-Neuro-Research-Lab", "repo_desc": "A new architecture of feature extraction method using omics data is proposed which improves classification in most of the classifiers.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T17:05:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T03:33:50Z", "homepage": null, "size": 1789, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187957063, "is_fork": false, "readme_text": "Deep Learning approach to Characterize Breast Cancer Subtypes using Gene Expression Profile       In this project, we demonstrate how adversarial auto-encoder (AAE) model can be used to extract the features from high dimensional genetic (omics) data. We evaluated the performance of the model through twelve di\ufb00erent supervised classi\ufb01ers to verify the usefulness of the new features in breast cancer subtypes prediction.  Getting Started The following instructions will get you a copy of the project up and running on your local machine for development and testing purposes. See the instruction below: Prerequisites The following libraries are required to reproduce this project:   Keras (2.0.6)   Keras-adverserial (0.0.3)   Tensorflow (1.13.1)   Scikit-Learn (0.20.3)   Numpy (1.16.3)   Imbalanced-Learn (0.4.3)   Supports both Python 2.7.0 and Python 3.5.6 Hyperparameters of classifiers are optimized using TPOT https://github.com/EpistasisLab/tpot Directory Layout \u251c\u2500\u2500 results \u2502\u00a0\u00a0 \u251c\u2500\u2500 figures \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 Here figures will be stored \u2502\u00a0\u00a0 \u251c\u2500\u2500 result.tsv \u251c\u2500\u2500 data \u2502\u00a0  \u251c\u2500\u2500 data will be stored here \u251c\u2500\u2500 feature_extraction \u2502\u00a0  \u251c\u2500\u2500 AAE \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 1 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 2 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 3 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 4 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 5          # five folder for five fold cross validation \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 fine_tuned \u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 1 \u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 2 \u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 3 \u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 4 \u2502\u00a0  \u2502\u00a0   \u2502\u00a0   \u251c\u2500\u2500 5 \u2502\u00a0  \u251c\u2500\u2500deepAE \u2502\u00a0  \u251c\u2500\u2500denoisingAE \u2502\u00a0  \u251c\u2500\u2500shallowAE \u2502\u00a0  \u251c\u2500\u2500VAE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 notes.txt \u2514\u2500\u2500 .gitignore Make sure to keep the directory as following for other feature extraction methods: . \u251c\u2500\u2500 ... \u251c\u2500\u2500 feature_extraction                    \u2502\u00a0  \u251c\u2500\u2500 denoisingAE       # same for deepAE, shallowAE and VAE \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 1 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 2 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 3 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 4 \u2502\u00a0  \u2502\u00a0   \u251c\u2500\u2500 5             \u2514\u2500\u2500 ...  Usage Run the following to train and fine tune the autoencoder main.py  And run the following when model already fine tuned without_fine_tuning.py  Proposed Workflow  Datasets  cBioPortal - Cancer Genomics Datasets Breast Invasive Carcinoma (TCGA, Cell 2015) - Clinical information is used to label various molecular subtypes  Breast Invasive Carcinoma (BRCA)    Molecular Subtypes Number of Patients Label     Luminal A 304 0   Luminal B 121 1   Basal & Triple Negetive 137 2   Her 2 Enriched 43 3       Total Number of Samples (Patients) Total Number of Features (Genes)     605 20439     Details about Molecular Subtypes of Breast Cancer  Contributing If you want to contribute to this project and make it better, your help is very welcome. When contributing to this repository please make a clean pull request. Acknowledgments  The proposed architecture is inspired by https://github.com/bstriner/keras-adversarial  ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "autoencoder", "feature-extraction", "omics-data", "adversarial-autoencoder"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.cbioportal.org/study?id=brca_tcga_pub2015"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a74a"}, "repo_url": "https://github.com/hattcitigo/new-vPlate-Recognition", "repo_name": "new-vPlate-Recognition", "repo_full_name": "hattcitigo/new-vPlate-Recognition", "repo_owner": "hattcitigo", "repo_desc": "B\u00e0i t\u1ed1t nghi\u1ec7p", "description_language": "Vietnamese", "repo_ext_links": null, "repo_last_mod": "2019-05-21T04:25:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T04:18:22Z", "homepage": null, "size": 29269, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187760541, "is_fork": false, "readme_text": "new-vPlate-Recognition B\u00e0i t\u1ed1t nghi\u1ec7p H\u01b0\u1edbng d\u1eabn ch\u1ea1y  clone v\u1ec1 m\u00e1y. cd new-vPlate-Recognition cd vPlate-Recognition python main.py  Requirements  Python 3 yolo Tensorflow Keras  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/hattcitigo/new-vPlate-Recognition/blob/572dc02dfe1d1b16f0eaea10273d79553126e8ac/vPlate_Recognition/model_data/trained_weights_final.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a74b"}, "repo_url": "https://github.com/AbductiveLearning/ABL-HED", "repo_name": "ABL-HED", "repo_full_name": "AbductiveLearning/ABL-HED", "repo_owner": "AbductiveLearning", "repo_desc": "Handwritten Equations Decipherment with Abductive Learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T09:34:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T18:18:39Z", "homepage": null, "size": 18974, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187691993, "is_fork": false, "readme_text": "Abductive Learning for Handwritten Equation Decipherment This is an anonymous repository for holding the sample code of the abductive learning framework for handwritten equation decipherment experiments in Bridging Machine Learning and Logical Reasoning by Abductive Learning submitted to NeurIPS 2019. Environment dependency This code is only tested in Linux environment.  Swi-Prolog Python3 with Numpy, Tensorflow and Keras ZOOpt (as a submodule)  Install Swipl http://www.swi-prolog.org/build/unix.html Install python3 https://wiki.python.org/moin/BeginnersGuide/Download Install required package #install numpy tensorflow keras pip3 install numpy pip3 install tensorflow pip3 intall keras Set environment variables(Should change file path according to your situation) # cd to ABL-HED git submodule update --init --recursive  export ABL_HOME=$PWD cp /usr/local/lib/swipl/lib/x86_64-linux/libswipl.so $ABL_HOME/src/logic/lib/ export LD_LIBRARY_PATH=$ABL_HOME/src/logic/lib export SWI_HOME_DIR=/usr/local/lib/swipl/  # for GPU user export LD_LIBRARY_PATH=$ABL_HOME/src/logic/lib:/usr/local/cuda:$LD_LIBRARY_PATH  Install Abductive Learning code First change the swipl_include_dir and swipl_lib_dir in setup.py to your own SWI-Prolog path. cd src/logic/prolog python3 setup.py install Build ZOOpt cd src/logic/lib/ZOOpt python3 setup.py build cp -r build/lib/zoopt ../ Demo for arithmetic addition learning Change directory to ABL-HED, and run equaiton generator to get the training data cd src/ python3 equation_generator.py Run abductive learning code cd src/ python3 main.py or python3 main.py --help To test the RBA example, please specify the src_data_name and src_data_file together, e.g., python main.py --src_data_name random_images --src_data_file random_equation_data_train_len_26_test_len_26_sys_2_.pk Remark  It is possible that the logic abduction finds a trivial but consistent solution: all equations are 0000+0000=0000 with the only rule my_op([0],[0],[0]). If it happens, don't hesitate and kill the program, just re-run it and give it another chance :) The mapping from CNN to symbolic primitive symbols are learned, so it is fine if ABL learns 0+0=01 and 1+1=1, it just swaps the semantics of 0 and 1.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.swi-prolog.org/build/unix.html"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a74c"}, "repo_url": "https://github.com/sukarnabarua/fccgan", "repo_name": "fccgan", "repo_full_name": "sukarnabarua/fccgan", "repo_owner": "sukarnabarua", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T04:22:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T14:19:16Z", "homepage": null, "size": 85, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188053710, "is_fork": false, "readme_text": "fccgan Codes for the paper \"FCC-GAN: A Fully Connected and Convolutional Net Architecture for GANs\" (https://arxiv.org/abs/1905.02417) Requires Keras and Tensorflow libraries Will be updated with more codes soon! ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1905.02417"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a74d"}, "repo_url": "https://github.com/guchenghao/MSBD-5011-Project", "repo_name": "MSBD-5011-Project", "repo_full_name": "guchenghao/MSBD-5011-Project", "repo_owner": "guchenghao", "repo_desc": "Let us smile", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T16:59:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T16:55:00Z", "homepage": null, "size": 33, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188079595, "is_fork": false, "readme_text": "MSBD 5011 Adavanced Statistic Project Running Guide If you want to run our code in your computer, please download some package files from my google drive (I will give you the links) and change the paths of each package used in codes, then you can run the code files in method 1 and method 2. data dir: data 32pts_svm.joblib: 32pts_svm.joblib smile.joblib.pkl: smile.joblib.pkl haarcascade_frontalface_default.xml: haarcascade_frontalface_default.xml And, if you want to train your own model, you can download the faces training data from follow link: faces_data: faces_data What`s more, In method 2 dir, you will find a code file named \"mtcnn_detector.py\", this is come from the MTCNN: https://github.com/kpzhang93/MTCNN_face_detection_alignment. Reference [1] MTCNN: https://github.com/kpzhang93/MTCNN_face_detection_alignment [2] Facial Expression Recognition with Keras:http://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/ [3] OpenCV cascade: https://docs.opencv.org/master/db/d28/tutorial_cascade_classifier.html [4] OpenCV + Dlib 68 landmarks example: https://my.oschina.net/wujux/blog/1622781 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://sefiks.com/2018/01/01/facial-expression-recognition-with-keras/"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a74e"}, "repo_url": "https://github.com/soumilshah1995/Smart-Library-to-load-image-Dataset-for-Convolution-Neural-Network-Tensorflow-Keras-", "repo_name": "Smart-Library-to-load-image-Dataset-for-Convolution-Neural-Network-Tensorflow-Keras-", "repo_full_name": "soumilshah1995/Smart-Library-to-load-image-Dataset-for-Convolution-Neural-Network-Tensorflow-Keras-", "repo_owner": "soumilshah1995", "repo_desc": "Smart Library to load image Dataset for Convolution Neural Network (Tensorflow/Keras)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T12:59:30Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T16:52:12Z", "homepage": null, "size": 176523, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187878989, "is_fork": false, "readme_text": "Smart-Library-to-load-image-Dataset-for-Convolution-Neural-Network-Tensorflow-Keras- Smart Library to load image Dataset for Convolution Neural Network (Tensorflow/Keras) Hi are you into Machine Learning/ Deep Learning or may be you are trying to build object recognition in all above situation you have to work with images not 1 or 2 about 40,000 images. Biggest Challenges in Bulding a Neural Network is how do I convert my Image into Numpy array how do I load the Dataset before that there several steps you need to follow like convert each image into Grey Scale and resize image reshaping the Images we know how tedious job is that Good News no more Hassle with this Python Library Just give the Path to your training Folder and it will Automatically process the data it will convert image into Gray scale and then Resize it if any image is corrupted it will handle that issue as well lets see how to use this Module ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a74f"}, "repo_url": "https://github.com/polotent/jetbrains-mountain-car", "repo_name": "jetbrains-mountain-car", "repo_full_name": "polotent/jetbrains-mountain-car", "repo_owner": "polotent", "repo_desc": "testing task for internship ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T20:41:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T21:02:08Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187712836, "is_fork": false, "readme_text": "jetbrains-mountain-car \u041e\u0431\u044b\u0447\u043d\u044b\u0439 MountainCar-v0 \u0432\u044b\u0434\u0430\u0435\u0442 -130 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c. \u0414\u043b\u044f \u0437\u0430\u043f\u0443\u0441\u043a\u0430 MountainCarContinuous-v0 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c keras \u0438 gym, \u043d\u0443 \u0438 \u0441\u0430\u043c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0437\u0430\u043d\u0438\u043c\u0430\u0435\u0442 \u043c\u0438\u043d\u0443\u0442 20. \u041f\u043e\u044d\u0442\u043e\u043c\u0443 \u0447\u0430\u0448\u0435\u0447\u043a\u0430 \u043a\u043e\u0444\u0435 \u043d\u0435 \u043f\u043e\u043c\u0435\u0448\u0430\u0435\u0442. ", "has_readme": true, "readme_language": "Russian", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a750"}, "repo_url": "https://github.com/ajertec/PointNetKeras", "repo_name": "PointNetKeras", "repo_full_name": "ajertec/PointNetKeras", "repo_owner": "ajertec", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T17:33:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T13:19:07Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188042902, "is_fork": false, "readme_text": "PointNetKeras PointNet implementation in Keras. Classification & Segmentation models. Original tensorflow implementation: https://github.com/charlesq34/pointnet  Article: https://arxiv.org/abs/1612.00593 TODO:  Scripts for training & evaluation. Semantic segmentation of indoor scenes. etc.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1612.00593"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a751"}, "repo_url": "https://github.com/AikeHillbrands/Self-driving-car-deep-reinforcement-learning", "repo_name": "Self-driving-car-deep-reinforcement-learning", "repo_full_name": "AikeHillbrands/Self-driving-car-deep-reinforcement-learning", "repo_owner": "AikeHillbrands", "repo_desc": "I tried to learn something about drl and the difficulties with this little project. I used Keras for the deep learning and pygame for the environment.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T17:13:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T17:11:53Z", "homepage": null, "size": 16, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187682910, "is_fork": false, "readme_text": "Self-driving-car-deep-reinforcement-learning I tried to learn something about drl and the difficulties with this little project. I used Keras for the deep learning and pygame for the environment. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/AikeHillbrands/Self-driving-car-deep-reinforcement-learning/blob/5fa8efcfe25630db2c0d7e3df7ce6b3fceb1e14e/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a752"}, "repo_url": "https://github.com/simrandhar15/Facial-Emotion-Recognition", "repo_name": "Facial-Emotion-Recognition", "repo_full_name": "simrandhar15/Facial-Emotion-Recognition", "repo_owner": "simrandhar15", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-20T10:28:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T10:27:01Z", "homepage": null, "size": 14156, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187619088, "is_fork": false, "readme_text": "facial-emotion-detection Install these dependencies with pip3 install module-name tensorflow, numpy, scipy, opencv-python, pillow, pandas, matplotlib, h5py and keras. Once the dependencies are installed, you can run the project. python3 emotions.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/simrandhar15/Facial-Emotion-Recognition/blob/a273b288754840fe4a1af53f9c3d7d5ac41a3694/Emotion/models/emotion_model.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a753"}, "repo_url": "https://github.com/SoftWiser-group/CoDiSum", "repo_name": "CoDiSum", "repo_full_name": "SoftWiser-group/CoDiSum", "repo_owner": "SoftWiser-group", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T13:32:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T03:19:59Z", "homepage": null, "size": 57430, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187560656, "is_fork": false, "readme_text": "CoDiSum Code and data of the paper Commit Message Generation for Source Code Changes. Intro CoDiSum is a deep neural network model used to generate commit message for source code changes. It take processed diff as input and outputs natural language description. In this repository, we provide the implementation of our CoDiSum model with Python Keras APIs. Requirements  Python >= 3.6.2 Numpy >= 1.15.0 Keras >= 2.1.5 Tensorflow >= 1.7.0  Usage example usage: python CopyNetPlusgen.py  This command will lead to a full training and prediction process. Related parameters are in CopyNetPlusgen.py file, named in uppercase. Datasets We put the data set in a compressed file data4CopynetV3.zip, You can extract directly to the current folder. Model  Citing If you find CoDiSum useful in your research, we ask that you cite the following paper: @inproceedings{ author = {Shengbin Xu, Yuan Yao, Feng Xu, Tianxiao Gu, Hanghang Tong, Jian Lu}, title = {Commit Message Generation for Source Code Changes}, booktitle = {Proceedings of the 28th International Joint Conference on Artificial Intelligence}, year = {2019}, }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a754"}, "repo_url": "https://github.com/sukarnabarua/CrossLID", "repo_name": "CrossLID", "repo_full_name": "sukarnabarua/CrossLID", "repo_owner": "sukarnabarua", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T03:39:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T14:30:43Z", "homepage": null, "size": 13818, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188055749, "is_fork": false, "readme_text": "CrossLID Test code for the paper \"Quality Evaluation of GANs Using Cross Local Intrinsic Dimensionality\" (https://arxiv.org/abs/1905.00643) Requires Keras and Tensorflow Libraries More codes will be uploaded soon ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/sukarnabarua/CrossLID/blob/44e9a73cf8c142443246b25653273f1b757a92e5/cnn.h5", "https://github.com/sukarnabarua/CrossLID/blob/44e9a73cf8c142443246b25653273f1b757a92e5/generator_dcgan.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1905.00643"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a755"}, "repo_url": "https://github.com/thuphuong1401/fashion_mnist-Image-Classification", "repo_name": "fashion_mnist-Image-Classification", "repo_full_name": "thuphuong1401/fashion_mnist-Image-Classification", "repo_owner": "thuphuong1401", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T01:52:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T01:41:58Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187740868, "is_fork": false, "readme_text": "fashion_mnist-Image-Classification Dependencies:  python 3.6.7  tensorflow 1.13.1  keras 2.2.1  tensorflow_datasets  numpy  A simple CNN to classify clothing images from the famous fashion_mnist dataset. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a756"}, "repo_url": "https://github.com/wmeddie/dl4j-keras-sample", "repo_name": "dl4j-keras-sample", "repo_full_name": "wmeddie/dl4j-keras-sample", "repo_owner": "wmeddie", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T09:12:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T12:24:11Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187635542, "is_fork": false, "readme_text": "Example os using DL4J's spark training from Python Step 1:  Build a DL4J uberjar. With maven installed in the computer just run mvn clean package  Step 2: Package python dependencies for Spark. Just run the following: python setup.py bdist_spark  Step 3: Run the job on a spark cluster.* Run spark-submit and specify the built dl4j uberjar and python zip files. spark-submit --driver-memory 8g --master yarn --jars target/dl4j-uber-1.0.0-beta4.jar --py-files spark_dist/keras_dl4j_sample-0.1-deps.zip,spark_dist/keras_dl4j_sample-0.1.zip keras_dl4j_sample/driver.py train.spark_script  *: Training on a cluster not yet working as some beta4 APIs have changed.  Coming soon. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a757"}, "repo_url": "https://github.com/AyushSingh1315/song_lyrics_generator", "repo_name": "song_lyrics_generator", "repo_full_name": "AyushSingh1315/song_lyrics_generator", "repo_owner": "AyushSingh1315", "repo_desc": "Generates new songs(lyrics only) based on its training on previous billboards top hits.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T04:16:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T04:06:18Z", "homepage": null, "size": 2189, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187960703, "is_fork": false, "readme_text": "song_lyrics_generator Generates new songs(lyrics only) based on its training on previous billboards top hits using Tensorflow(specifically Keras) and Python3.6. The model consists of two LSTM layer and a Dense layer. After 250 epochs the model had an accuracy of 81.2. It can be further trained to improve its accuracy. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a758"}, "repo_url": "https://github.com/AlexanderUzhinskiy/pdd_new", "repo_name": "pdd_new", "repo_full_name": "AlexanderUzhinskiy/pdd_new", "repo_owner": "AlexanderUzhinskiy", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T12:19:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T12:46:52Z", "homepage": null, "size": 55217, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188037256, "is_fork": false, "readme_text": "Plant Disease Detection This repository contains the code for the RFBR project 18-07-00829 \"Development of massive parallel approaches for farmers complaints through text and images using high performance computing\" Plants disease detection is very popular field of study. Many promising results were already obtained but it is still only few real-life applications that can make farmer\u2019s life easier. The aim of our research is solving the problem of detection and preventing diseases of agricultural crops with the help of deep learning. We collected a special database of the grapes leaves consisting of four set of images, including healthy, Esca, Black rot and Chlorosis diseases. We reached over 90% accuracy using a deep siamese convolutional network.  Installation  install all required packages; clone PDD using git:  git clone https://github.com/Kaliostrogoblin/PDD.git  then, cd to the PDD folder and feel free to use it:  cd PDD   Getting started At first, to start training your own model using the PDD dataset, you should download the data. To do that we prepared a special module datasets. We load the dataset with grape's leaves and set the random state for splitting data into train and test subsets. random_state parameter is used for reproducibility. from pdd.datasets.grape import load_data  train_data_path, test_data_path = load_data(split_on_train_test=True, random_state=13) In our study we utilized a deep convolutional siamese network to train feature extractor and then applied K-Nearest Neighbours on the top of extracted features. Siamese networks take as input pairs of images with corresponding labels: 0 -- for images from different classes and 1 -- for images from the same class. For training we are using a strong augmentation including rotations, zooming, flips and channel shifts. from pdd.utils.training import SiameseBatchGenerator  train_batch_gen = SiameseBatchGenerator.from_directory(dirname=train_data_path, augment=True) test_batch_gen = SiameseBatchGenerator.from_directory(dirname=test_data_path)  def siams_generator(batch_gen, batch_size=None):     while True:         batch_xs, batch_ys = batch_gen.next_batch(batch_size)         yield [batch_xs[0], batch_xs[1]], batch_ys As an feature extractor any Keras model with appropriate input layer can be used. But we created a simple one: from pdd.models import get_feature_extractor  import keras.backend as K import tensorflow as tf  # set the single session for tensorflow and keras both sess = tf.Session() K.set_session(sess)  input_shape = (256, 256, 3)  feature_extractor = get_feature_extractor(input_shape) To make it possible to train the feature extractor in siamese manner we developed a special helper: from pdd.models import make_siamese  siams = make_siamese(feature_extractor, dist='l1', loss='cross_entropy') There are three types of distances:  l1 l2 cosine  But only 'l1' is available for cross-entropy loss. After that one can train the siams model using keras fit_generator method. When the feature extractor is trained, it's time to create a TensorFlow graph for KNN algorithm from pdd.models import TfKNN  tfknn = TfKNN(sess,                feature_extractor,                (train_images, train_labels)) For prediction TfKNN has special method: preds = tfknn.predict(test_images, return_dist=True) And finally, to save the graph for serving: tfknn.save_graph_for_serving(\"tfknn_graph\") For details go to the examples folder. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pdd.jinr.ru/db", "http://www.rfbr.ru/rffi/ru/project_search/o_2071350"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a759"}, "repo_url": "https://github.com/fooSynaptic/News_Stock_predict", "repo_name": "News_Stock_predict", "repo_full_name": "fooSynaptic/News_Stock_predict", "repo_owner": "fooSynaptic", "repo_desc": "This repository was a data contest open source project, predict the stock fluctuation with the daily reported News.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T09:01:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T08:47:33Z", "homepage": null, "size": 18662, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187999481, "is_fork": false, "readme_text": "News_Stock_predict This repository was a data contest \u516c\u5f00\u65b0\u95fb\u9884\u6d4bA\u80a1\u884c\u4e1a\u677f\u5757\u52a8\u5411 open source project, predict the stock fluctuation with the daily reported News. This rep was build with keras and scikit-learn. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a75a"}, "repo_url": "https://github.com/firas98/Capstone-Project", "repo_name": "Capstone-Project", "repo_full_name": "firas98/Capstone-Project", "repo_owner": "firas98", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T21:19:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T21:19:11Z", "homepage": null, "size": 2668, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187915059, "is_fork": false, "readme_text": "Capstone-Project Capstone project is about the Attenance using Face Recognition where the face can be recogized from taking the image in real time. This project is about taking attendance using your face as a identity with the help of Convolutional Neural Network instead of the biometric system. I created a Attendance Face-Recognition application in Python using Keras and OpenCV where the user have to place the face in front of the camera using VGG Face as a classifier for face recognition and attendance will be added accordingly. In this project, I will be using VGG-Face Model for face recognition which was built under VGG16 architecture which was developed by University of Oxford using Keras. I have used a predefined load of weights of the model which was already trained by 14 million of images scoring a accuracy of 92.7%. Training the model with our own datasets is done by predicting the model where the dictinary is created which matches the target name to the matrix vector of image and using cosine similarity to match the matrix vector of the new image to the matrix vector of the preprocessed image when predicting the new image. Files:  dataset is the file which contains the images of the user faceVGG is the file which contains the programs to run the application  1. generate.py is the generation of the new images 2. face_recogize.py is the image preprocessing and recognition of the face in real-time camera 3. vgg.py is the architecture of the neural network  Please read Capstone.pdf file for the documentation of the project. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a75b"}, "repo_url": "https://github.com/KACAM/KACAM", "repo_name": "KACAM", "repo_full_name": "KACAM/KACAM", "repo_owner": "KACAM", "repo_desc": "Implementation of Knowledge-aware Attentive Compare-Aggregate Model for Question Answering", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T17:15:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T07:39:11Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187987914, "is_fork": false, "readme_text": "KACAM Implementation of Knowledge-aware Attentive Compare-Aggregate Model on TrecQA and WikiQA using Keras. Requirements  python 3.x Keras 2.2.4 Tensorflow 1.12.0  Data  WikiQA TrecQA  Preprocess Because the data file is too large, it is not uploaded here. I will give a download link to the original data and preprocessing data after publication. The details of the preprocessing are in preprocess.py. Running usage: main.py [-h] [-t Task] [-m MODEL] [-d HIDDEN_DIM] [-e PATIENCE] [-l LR] [-b BATCH_SIZE] [--no_entity_cmp] [--no_cxt_weight_mixed] [--no_ent_weight_mixed]  WikiQA  KACAM  python main.py -t wikiqa -m listwise   w/o att_cxt  python main.py -t wikiqa -m listwise --no_ent_weight_mixed   w/o att_ent  python main.py -t wikiqa -m listwise --no_cxt_weight_mixed   w/o att_cxt_ent  python main.py -t wikiqa -m listwise --no_cxt_weight_mixed --no_ent_weight_mixed   w/o ent_fea  python main.py -t wikiqa -m listwise --no_entity_cmp   w/o ent  python main.py -t wikiqa -m listwise --no_entity_cmp --no_cxt_weight_mixed  TrecQA It's similar to WikiQA, change task from 'wikiqa' to 'trecqa'. For example: python main.py -t trecqa -m listwise  Reference @inproceedings{bian2017compare,   title={A compare-aggregate model with dynamic-clip attention for answer selection},   author={Bian, Weijie and Li, Si and Yang, Zhao and Chen, Guang and Lin, Zhiqing},   booktitle={Proceedings of the 2017 ACM on Conference on Information and Knowledge Management},   pages={1987--1990},   year={2017},   organization={ACM} }  @inproceedings{shen2018knowledge,   title={Knowledge-aware attentive neural network for ranking question answer pairs},   author={Shen, Ying and Deng, Yang and Yang, Min and Li, Yaliang and Du, Nan and Fan, Wei and Lei, Kai},   booktitle={The 41st International ACM SIGIR Conference on Research \\& Development in Information Retrieval},   pages={901--904},   year={2018},   organization={ACM} }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a75c"}, "repo_url": "https://github.com/halcolo/Gender_Recognition", "repo_name": "Gender_Recognition", "repo_full_name": "halcolo/Gender_Recognition", "repo_owner": "halcolo", "repo_desc": "Project  to explain how to recognice the gender of a person based in the face using CNN(Convolutional Neural network) with Keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T01:58:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T20:26:07Z", "homepage": null, "size": 144, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187908430, "is_fork": false, "readme_text": "Model trainer  category    Machine learning  \ud83d\udce1 author      Juan Diego Alfonso jalfons.ocampo@gmail.com copyright GNU General Public Licence source GitHub  Description This model was created to explain how to recognice the gender of a person based in the face using CNN(Convolutional Neural network) with Keras. Virtual enviroment You can create a virtual environment using following commands. In linux maybe you need the following package sudo apt-get install python3-venv    Description Linux Win     Install virtenv python3 -m pip install --user virtualenv py -m pip install --user virtualenv   Create enviroment python3 -m venv modelTrainerEnv py -m venv modelTrainerEnv   Activate enviroment source modelTrainerEnv/bin/activate .\\modelTrainerEnv\\Scripts\\activate   Install packages pip install --upgrade -r requirements.txt pip install --upgrade -r requirements.txt   Inactivate deactivate deactivate    USE This project is a CNN based in a large-scale image recognition training model, as called VGG, this model is based in layers and a compression of an image, the schema is explained in the following graph:  Training To train the model just run with python 3.6 and the environment created the file training.py, ensure the route of the 'datadir' variable is set as your dataset photos is. To download the dataset you can see follow URL. https://storage.googleapis.com/pretrained-model-gender/gender_dataset_face.rar The VGG perfonrmance will be ploting and the image is one like this.  Recognition To run the recognition ensure the model is in the folder 'model' and run detection.py and comment the variable 'model_path', if you haven't the model the app download a model automatically.    Aplication Version     Python Python 3.6   OpenCV 4.1.0   numpy 1.16.3   tensorflow 1.13   keras 2.2.4   cvlib 0.2.1   scikit-learn 0.21.1   matplotlib 3.0.3    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a75d"}, "repo_url": "https://github.com/outmanipulate/cartpole_RL1", "repo_name": "cartpole_RL1", "repo_full_name": "outmanipulate/cartpole_RL1", "repo_owner": "outmanipulate", "repo_desc": "Using Deep Q learning to solve the single cartpole problem on gym open ai along with keras ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T02:39:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T02:23:42Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187554038, "is_fork": false, "readme_text": "cartpole_RL1 Using Deep Q learning to solve the single cartpole problem on gym open ai along with keras ENVIRONMENT OBSERVARION SPACE AND ACTION SPACE: Observation: Type: Box(4) Num Observation                 Min         Max 0 Cart Position             -4.8            4.8 1 Cart Velocity             -Inf            Inf 2 Pole Angle                 -24\u00b0           24\u00b0 3 Pole Velocity At Tip      -Inf            Inf Action:     Type: Discrete(2)     Num Action     0 Push cart to the left     1       Push cart to the right  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a75e"}, "repo_url": "https://github.com/Zebrap/LAlarmTensorflow", "repo_name": "LAlarmTensorflow", "repo_full_name": "Zebrap/LAlarmTensorflow", "repo_owner": "Zebrap", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T11:33:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T11:26:47Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187823488, "is_fork": false, "readme_text": "LAlarmTensorflow Sie\u0107 neuronowa tworz\u0105ca model tensorflow. Wymagania Pythona 3.5.4 w wersji 64-bitowej dla Windows ze strony https://www.python.org/downloads/ a nast\u0119pnie wykona\u0107 polecenia: pip install tensorflow pip install keras Trenowanie sieci: python index.py Testowanie sieci: python classify.py alarm2.csv - przyk\u0142adowe dane do uczenia sieci struktura pliku: czas, wibracje, g\u0142o\u015bno\u015b\u0107, d\u017awi\u0119k, ocena (przeskalowana do zakresu 0 - 1) ", "has_readme": true, "readme_language": "Polish", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a75f"}, "repo_url": "https://github.com/kcchan90/yolov2", "repo_name": "yolov2", "repo_full_name": "kcchan90/yolov2", "repo_owner": "kcchan90", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T16:37:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T16:41:24Z", "homepage": null, "size": 36888, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187877293, "is_fork": false, "readme_text": "YOLOV2 For YOLOv2 testing   Download weights and cfg from here to model/ (only support yolov2-voc and yolov2-tiny-voc)   Convert darknet weights and cfg to keras format   python convert_model yolov2-voc  Detect Object in an image located at input/  python detection yolov2-voc test1.jpg  Detect Object in a video located at input/ (edit input/select_obj.txt to detect more types of objects)  python Main yolov2-voc 1.mp4 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a760"}, "repo_url": "https://github.com/SugarInGithub/ldgs", "repo_name": "ldgs", "repo_full_name": "SugarInGithub/ldgs", "repo_owner": "SugarInGithub", "repo_desc": "Deep Metric Learning via Locally Dense and Globally Sparse Embeddings. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:01:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T01:57:07Z", "homepage": "", "size": 19, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187943556, "is_fork": false, "readme_text": "Deep Metric Learning via Locally Dense and Globally Sparse Embeddings Code for Learning Locally Dense and Globally Sparse Embeddings. The code is LDGS for Stanford Online Products Dataset. How to train the model:  Create a folder './Data/Stanford_Online_Products' and put the data in it. Run 'SOP.py' to generate the hdf5 data files. Run 'LDGS.py' to train the model.  Dependencies tensorflow, keras, numpy, scipy, matplotlib, Pillow, scikit-image, scikit-learn, opencv-python, h5py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a761"}, "repo_url": "https://github.com/naughtybabyfirst/facial", "repo_name": "facial", "repo_full_name": "naughtybabyfirst/facial", "repo_owner": "naughtybabyfirst", "repo_desc": "keras\u5b9e\u73b0\u4eba\u8138\u8868\u60c5\u8bc6\u522b", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-29T07:47:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T09:49:10Z", "homepage": "", "size": 172, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187613306, "is_fork": false, "readme_text": "facial \u68c0\u6d4b\u4eba\u8138\u8868\u60c5 Keras\u7f16\u5199CNN\u6846\u67b6\u7684\u56fe\u50cf\u591a\u5206\u7c7b \u6570\u636e\u96c6 https://pan.baidu.com/s/1b5troPWfbw1-E_ff9XYdRg \u63d0\u53d6\u7801\uff1anikm \u6587\u4ef6\u7ed3\u6784\uff1a  dataset  fer2013   \u8bad\u7ec3\u6570\u636e\u96c6\u6587\u4ef6\u5939 .png      .png\u683c\u5f0f\u7684\u6587\u4ef6\u90fd\u662f\u6d4b\u8bd5\u65f6\u4f7f\u7528   model.py    \u6a21\u578b\u6587\u4ef6 train.py    \u8bad\u7ec3 test.py     \u6d4b\u8bd5  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a762"}, "repo_url": "https://github.com/winrhcp/object-detection", "repo_name": "object-detection", "repo_full_name": "winrhcp/object-detection", "repo_owner": "winrhcp", "repo_desc": "object detection using imageai", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T14:30:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T07:57:05Z", "homepage": null, "size": 1085, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187789298, "is_fork": false, "readme_text": "object-detection object detection using imageai #installation python 3.6 pip install tensorflow pip install numpy pip install scipy pip install opencv-python pip install pillow pip install matplotlib pip install h5py pip install keras pip install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl https://github.com/OlafenwaMoses/ImageAI/releases/download/1.0/resnet50_coco_best_v2.0.1.h5 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a763"}, "repo_url": "https://github.com/KengoAraki/med-dnn-5", "repo_name": "med-dnn-5", "repo_full_name": "KengoAraki/med-dnn-5", "repo_owner": "KengoAraki", "repo_desc": "5. \u5b9f\u8df5\u7de8: MRI\u753b\u50cf\u306e\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3", "description_language": "Japanese", "repo_ext_links": null, "repo_last_mod": "2019-05-27T08:15:19Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T02:09:41Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187552413, "is_fork": false, "readme_text": "\u76ee\u7684 \u500b\u4eba\u7684\u306a\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u306e\u5b66\u7fd2\u7528\u306b\u4f5c\u6210 \u5099\u8003 \u53c2\u8003\u8cc7\u6599\u3067\u306fChainer\u3067\u8a18\u8ff0\u3055\u308c\u3066\u3044\u308b\u304c\uff0c\u3053\u3053\u3067\u306fkeras\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b \u53c2\u8003\u8cc7\u6599\uff1a \u30e1\u30c7\u30a3\u30ab\u30ebAI\u5c02\u9580\u30b3\u30fc\u30b9\u3000\u30aa\u30f3\u30e9\u30a4\u30f3\u8b1b\u7fa9\u8cc7\u6599\u300c5. \u5b9f\u8df5\u7de8: MRI\u753b\u50cf\u306e\u30bb\u30b0\u30e1\u30f3\u30c6\u30fc\u30b7\u30e7\u30f3\u300d https://japan-medical-ai.github.io/medical-ai-course-materials/notebooks/Image_Segmentation.html ", "has_readme": true, "readme_language": "Japanese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a764"}, "repo_url": "https://github.com/WarungData/WD-Deep-Sort-With-Keras", "repo_name": "WD-Deep-Sort-With-Keras", "repo_full_name": "WarungData/WD-Deep-Sort-With-Keras", "repo_owner": "WarungData", "repo_desc": "Tehnik Deep Sorting untuk realtime video analisis dengan Keras - Tensorflow", "description_language": "Malay (macrolanguage)", "repo_ext_links": null, "repo_last_mod": "2019-05-20T23:07:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T22:59:34Z", "homepage": null, "size": 10298, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187724645, "is_fork": false, "readme_text": "WD Deep Sort With Keras Simple Online dan Realtime Tracking (SORT) adalah pendekatan pragmatis untuk pelacakan banyak objek dengan fokus pada algoritma sederhana dan efektif. Dalam repo ini, kami mengintegrasikan informasi tampilan untuk meningkatkan kinerja SORT. Karena ekstensi ini, kami dapat melacak objek melalui periode oklusi yang lebih lama, mengurangi jumlah sakelar identitas secara efektif. Dalam semangat kerangka asli kami menempatkan banyak kompleksitas komputasi ke dalam tahap pra-pelatihan offline di mana kami mempelajari metrik asosiasi yang mendalam pada dataset identifikasi ulang orang skala besar. Selama aplikasi online, kami membangun asosiasi pengukuran untuk melacak menggunakan pertanyaan tetangga terdekat di ruang tampilan visual. Evaluasi eksperimental menunjukkan bahwa ekstensi kami mengurangi jumlah pengalihan identitas hingga 45%, mencapai kinerja kompetitif secara keseluruhan pada tingkat bingkai yang tinggi. Sukses! Warung Data Indonesia ", "has_readme": true, "readme_language": "Indonesian", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a765"}, "repo_url": "https://github.com/yanqiAI/document-denoising", "repo_name": "document-denoising", "repo_full_name": "yanqiAI/document-denoising", "repo_owner": "yanqiAI", "repo_desc": "This repository is a denosing model for the dirty document images.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T07:55:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T05:51:58Z", "homepage": null, "size": 849, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187575645, "is_fork": false, "readme_text": "This repository contains the full source codes which are used to denoise the document dirty image. Pre-requisites  python 3.6 Tensorflow keras OpenCV  dataset prepare  prepare clear iamges and some background iamges run /script/create_blue_data.py  this images have big and different sizes run /script/crop_dataset.py  get train dataset default size 256 * 256  train model python train.py In model.py has 3 different models: srresnet, srresnet+, unet, default model is srresnet, srresnet+ has some problem. test model python test_model.py  save blur_image | foreground_image | background_image | denoising_image, test image size is 256 * 256  python test.py save denoising_image, test image size is unconstrained. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a766"}, "repo_url": "https://github.com/yddet96/Radar-Sonar-GAN", "repo_name": "Radar-Sonar-GAN", "repo_full_name": "yddet96/Radar-Sonar-GAN", "repo_owner": "yddet96", "repo_desc": "Tensorflow scripts to train a GAN for generating synthetic radar/sonar data", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T16:18:35Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T15:52:58Z", "homepage": null, "size": 10, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187671484, "is_fork": false, "readme_text": "Radar-Sonar-GAN Tensorflow scripts to train a GAN for generating synthetic radar/sonar data This repository contains two files that define the same GAN architecture.  \"radar_gan_keras.py\" uses the Keras package within Tensorflow to define the network.  \"radar_gan_low.py\" uses low-level tensorflow operations and functions to define the same network architecture. The generator takes an input vector of random gaussian noise of size (100,1) and generates a raw radar/sonar signal of size (1547520,2).  The generator architecture contains one fully connected layer, followed by three \"deconvolutional\" layers. all using a leaky ReLU activation function except for the final convolutional layer, which has no activation function. The discriminator's architecture contains three convolutional layers followed by two fully connected layers.  Each of these layers also uses a leaky ReLU activation except for the output layer, which consists of a scalar value with a sigmoid activation function. Both networks were adversarially trained using a stochastic approach and the AdamOptimizer method within Tensorflow. The training process was executed on Googel Colab servers with GPU accelerated runtime. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a767"}, "repo_url": "https://github.com/andreidi/AC_DDPG_walker", "repo_name": "AC_DDPG_walker", "repo_full_name": "andreidi/AC_DDPG_walker", "repo_owner": "andreidi", "repo_desc": "Actor-Critic methods: Deep Deterministic Policy Gradients on Walker env", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T17:41:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T09:01:39Z", "homepage": null, "size": 28, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187800885, "is_fork": false, "readme_text": "DDPG and TD3 unified implementation and benchmark This a tf/keras implementation of DDPG https://arxiv.org/abs/1509.02971 and TD3 https://arxiv.org/abs/1802.09477 - unified in same Agent class. The used env is 'BipedalWalker-v2' https://gym.openai.com/envs/BipedalWalker-v2/. Important: the update of the actor_online is done via a wrapper than encapsulates the frozen critic_online together with the actual actor_online. This way the state is actually propagated up to the Q-value and then backwards. This way we only use high-level api. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1509.02971", "https://arxiv.org/abs/1802.09477"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a768"}, "repo_url": "https://github.com/dankkknight/keras-app", "repo_name": "keras-app", "repo_full_name": "dankkknight/keras-app", "repo_owner": "dankkknight", "repo_desc": "Dog Classifier using keras and flask rest api", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T06:00:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T05:59:23Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187771180, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a769"}, "repo_url": "https://github.com/saipothanjanjanam/behavioural-cloning", "repo_name": "behavioural-cloning", "repo_full_name": "saipothanjanjanam/behavioural-cloning", "repo_owner": "saipothanjanjanam", "repo_desc": "This repository contains the code written using Keras using Tensorflow as backend for Behavioural Cloning research paper by NVidia AI Research ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T14:45:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T12:03:31Z", "homepage": null, "size": 19008, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187828877, "is_fork": false, "readme_text": "Behavioural-Cloning This repository contains the code written using Keras using Tensorflow as backend for Behavioural Cloning research paper by NVidia AI Research Running the Code  python bc.py runs the model and weights are saved in .h5 file utils.py is generating batches for training images rospub.py is for sending steering angle to the Robot over same network in which PC as Master and Robot as Slave data folder contains the images for training and shortly I will upload code for saving images with steering angles i.e.., Dataset preparation  After training with dataset of 1000 images  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a76a"}, "repo_url": "https://github.com/gdyshi/model_deployment", "repo_name": "model_deployment", "repo_full_name": "gdyshi/model_deployment", "repo_owner": "gdyshi", "repo_desc": "sample model deployment code", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T03:10:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T09:32:17Z", "homepage": "https://blog.csdn.net/chongtong/article/details/90379347", "size": 13003, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187610446, "is_fork": false, "readme_text": "\u5f15\u8a00 \u672c\u6587\u5f00\u59cb\u6211\u5c06\u8981\u5199\u51e0\u7bc7\u9488\u5bf9tensorflow\u7cfb\u5217\u6a21\u578b\u7684\u5bfc\u51fa\u65b9\u6cd5\u548c\u6b65\u9aa4\uff0c\u6b64\u6587\u4e3a\u7acb\u8d34\u6587\u3002\u4e00\u6765\u786e\u5b9a\u540e\u7eed\u7814\u7a76\u8def\u7ebf\uff0c\u4e8c\u6765\u7528\u4e8e\u97ad\u7b56\u81ea\u5df1\u5c06\u8def\u7ebf\u575a\u6301\u5199\u5b8c\u3002\u76f8\u5173\u793a\u4f8b\u4ee3\u7801\u653e\u5728gdyshi\u7684github\u4e0a \u7814\u7a76\u7ebf\u8def \u6a21\u578b\u90e8\u7f72\u7684\u7b2c\u4e00\u6b65\u5c31\u662f\u8981\u6709\u6a21\u578b\uff0c\u6240\u4ee5\u6211\u9996\u5148\u628a\u6a21\u578b\u5bfc\u51fa\u65b9\u6cd5\u505a\u4e00\u4e0b\u68b3\u7406\uff0c\u90e8\u7f72\u4e3b\u8981\u6709\u4e24\u79cd\uff1a\u5355\u673a\u7248\u548c\u670d\u52a1\u5668\u7248\u3002\u5355\u673a\u7248\u53ef\u4ee5\u5728\u5355\u673a\u4e0a\u8fdb\u884c\u6a21\u578b\u63a8\u7406\uff0c\u4e3b\u8981\u5e94\u7528\u5728\u79bb\u7ebf\u7684\u667a\u80fd\u7ec8\u7aef\u3001\u8fb9\u7f18\u8ba1\u7b97\u4ea7\u54c1\u4e0a\uff1b\u5355\u673a\u7248\u6211\u5148\u4ece\u6700\u7b80\u5355\u7684python\u5f00\u59cb\uff0c\u4f9d\u6b21\u6df1\u5165\u5230C++\u7248\u3001JAVA\u7248\u3001\u5d4c\u5165\u5f0f\u7248\u3001\u6d4f\u89c8\u5668\u524d\u7aef\u7248\u3002\u670d\u52a1\u5668\u7248\u53ef\u4ee5\u5728\u670d\u52a1\u5668\u4e0a\u8fdb\u884c\u6a21\u578b\u63a8\u7406\uff0c\u7ec8\u7aef\u6216\u5ba2\u6237\u7aef\u901a\u8fc7\u7f51\u7edc\u8c03\u7528\u4f20\u8f93\u6570\u636e\u7ed9\u670d\u52a1\u5668\uff0c\u5e76\u4ece\u670d\u52a1\u5668\u83b7\u53d6\u63a8\u7406\u540e\u7684\u9884\u6d4b\u7ed3\u679c\uff1b\u670d\u52a1\u5668\u7248\u6211\u5148\u624b\u52a8\u642d\u5efa\u4e00\u4e2a\u7b80\u5355\u7684flask\u670d\u52a1\uff0c\u7136\u540e\u6df1\u5165\u5230TensorFlow Serving\uff0c\u6700\u540e\u662f\u5206\u5e03\u5f0f\u670d\u52a1\u5668\u90e8\u7f72\u3002 \u7cfb\u5217\u535a\u6587\u5217\u8868   tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u9884\u8bad\u7ec3\u6a21\u578b\u5bfc\u51fa  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u5355\u673apython\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u5355\u673ac++\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u5355\u673ajava\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u5d4c\u5165\u5f0f\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u6d4f\u89c8\u5668\u524d\u7aef\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u72ec\u7acb\u7b80\u5355\u670d\u52a1\u5668\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014TensorFlow Serving\u90e8\u7f72  tensorflow\u6a21\u578b\u90e8\u7f72\u7cfb\u5217\u2014\u2014\u2014\u2014\u5206\u5e03\u5f0f\u670d\u52a1\u5668\u90e8\u7f72  \u4ee3\u7801\u7ed3\u6784  \u9884\u8bad\u7ec3\u6a21\u578b\u5bfc\u51fa\u4ee3\u7801./model  \u53c2\u8003   tensorflow\u5b98\u65b9\u6587\u6863 keras\u5b98\u65b9\u6587\u6863 keras\u5b98\u65b9\u6587\u6863\u4e2d\u6587\u7248  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/gdyshi/model_deployment/blob/52787c91d7e22f9a3e7efcd04234686b1eb8b8d3/model/saved_keras/save.h5", "https://github.com/gdyshi/model_deployment/blob/52787c91d7e22f9a3e7efcd04234686b1eb8b8d3/model/saved_keras/save_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a76b"}, "repo_url": "https://github.com/AranMason/Magic-Colour-Predictor", "repo_name": "Magic-Colour-Predictor", "repo_full_name": "AranMason/Magic-Colour-Predictor", "repo_owner": "AranMason", "repo_desc": "Predicting what MTG colour a given art work or photo would be.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T04:31:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T23:04:08Z", "homepage": null, "size": 258586, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187725022, "is_fork": false, "readme_text": "Try for yourself Webpage: mtg.aran.nz Downloading MTG Data Goal here is to adapt the VGG16 to detect what colour of card an image might belong to using the whole of MTG's history We use the Scryfall Library. I have written a scrapping tool, which uses information from Scryfalls Bulk Data which can be found here. There is a 0.1s delay between fetches to accomidate Scryfalls rate limiting requests Ignored Card Types We ignore the following due to either poor cropped art provided by the Scryfall API or to simplify approach to colour classification. I am currently debating removing colourless from being classified.  Multi-colour Planeswalker Schemes Tokens Promos Emblems  Requirements The following Python modules are used  Keras (Version 2.1.6) TensorFlow Pillow sklearn numpy  Hosting To host the application, is in a AWS EC2 Instance using the following:  Nginx Flask Gunicorn  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/AranMason/Magic-Colour-Predictor/blob/ecd686eaaf3faf090706ba11581e869f9be648fd/models/model_complete.h5", "https://github.com/AranMason/Magic-Colour-Predictor/blob/ecd686eaaf3faf090706ba11581e869f9be648fd/models/model_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a76c"}, "repo_url": "https://github.com/jedlimlx/Inauritus", "repo_name": "Inauritus", "repo_full_name": "jedlimlx/Inauritus", "repo_owner": "jedlimlx", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-22T03:30:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T18:28:09Z", "homepage": null, "size": 2507, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187892595, "is_fork": false, "readme_text": "Inauritus Inauritus is latin for deaf or hard of hearing. Inauritus is made to help caretakers of the blind understand sign language without needing to learn it by heart. It was trained on a GeForce MX150 GPU with tensorflow and keras on the ASL Mnist Language Dataset with transfer learning. The model makes use of the Google Inception Model as a starting model. In order to start the program, run GUI.py and place your hand in the white box and press the button to translate. Under the captures tab, all captures are stored and displayed. Words and sentences are displayed in the sentence tab. Trained models can be found at https://www.dropbox.com/s/o543amym1bbe7pg/logs.zip?dl=0 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a76d"}, "repo_url": "https://github.com/abhinavsagar/gender-identifier-ML", "repo_name": "gender-identifier-ML", "repo_full_name": "abhinavsagar/gender-identifier-ML", "repo_owner": "abhinavsagar", "repo_desc": "Deep learning for gender identification from facial images written in keras and opencv", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T11:30:04Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T16:47:25Z", "homepage": "", "size": 94785, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187878302, "is_fork": false, "readme_text": "Gender-Identifier-ML Deep learning for gender identification from facial images written in keras and opencv Sample output  Usage There are two options to execute - either train the model from scratch or use the pre-trained VGGnet directly. If you do not want to train the model. pip install -r requirements.txt python main.py -i input_image You need matplotlib and scikit learn for training. To train the model - python train.py -d path-to-dataset This model dosen't genearalize well although the accuracy is 96% which is good enough for most ML pipelines. Feel free to tweak with  the hyperparameters or the architecture to achieve better results. TODO Make this work for image3.jpg and image4.jpg. NOTE I have merely created a wrapper. The original work can be found here. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a76e"}, "repo_url": "https://github.com/Chris10M/open-eye-detector", "repo_name": "open-eye-detector", "repo_full_name": "Chris10M/open-eye-detector", "repo_owner": "Chris10M", "repo_desc": "A Realtime  CPU eye detector to detect if the eyes are open or closed", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T11:26:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T04:36:20Z", "homepage": "", "size": 72148, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 187963629, "is_fork": false, "readme_text": "open-eye-detector A simple opencv script to detect if the eyes are open or closed Introduction A resnet inspired CNN model to predict if a person's eye is open or closed. The main use case for this, is while in face recognition we have to ensure the person remains active and not asleep while doing the auth process, so we have trained the model to ouput closed eyes and some-what drowsy eyes. Eye opened -- Green Eye closed-- Red   Contents  Installation Demo  Installation Requirements  Keras Tensoflow OpenCV dlib  just run, pip3 install -r requirements.txt  to instal the dependencies. Demo If you've cloned the repo and then install the dependecies, run python3 main.py  the webcamera will be used to detect the face and the eyes, then the prediction will be done. Issues If you encounter any issues, please create an issue tracker. ", "has_readme": true, "readme_language": "English", "repo_tags": ["eyes", "opencv", "keras", "cnn", "cnn-keras", "dlib", "face-detection", "cpu", "eye-detection-using-opencv", "eye-detection", "drowsiness-detection", "tensorflow", "cnn-model", "cnn-classification"], "has_h5": true, "h5_files_links": ["https://github.com/Chris10M/open-eye-detector/blob/57434e273b83083b427dac2779c3dc29b3ee090f/models/weights.149-0.01.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a76f"}, "repo_url": "https://github.com/abirbhy/TrustBIOMED", "repo_name": "TrustBIOMED", "repo_full_name": "abirbhy/TrustBIOMED", "repo_owner": "abirbhy", "repo_desc": "Syst\u00e8me de recherche d\u2019information biom\u00e9dicale bas\u00e9 sur le Deep Learning", "description_language": "French", "repo_ext_links": null, "repo_last_mod": "2019-05-22T07:39:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T10:37:55Z", "homepage": null, "size": 12735, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187816925, "is_fork": false, "readme_text": "TrustBIOMED Conception et r\u00e9alisation d\u2019un syst\u00e8me de recherche d\u2019information biom\u00e9dical bas\u00e9 sur le Deep Learning : Mots cl\u00e9s : Recherche d\u2019information, Deep Learning, Word Embedding, LSTM. Description 1- Repr\u00e9sentation vectorielle de la requ\u00eate et du document en se basant sur Contex2Vec et fastText. 2- Normalisation de la repr\u00e9sentation  en se basant sur un r\u00e9seau r\u00e9curent LSTM. 3- Appariement  requ\u00eate-document bas\u00e9 sur le \u00ab Learn to Rank \u00bb. 4- Evaluation et comparaison de l\u2019approche. Environnement logiciel -Le langage de programmation interpr\u00e9t\u00e9 : Python. -L'environnement de d\u00e9veloppement libre pour Python : Spyder. -La biblioth\u00e8que de Python : Keras avec backend TensorFlow. -L'utilitaire graphique de cr\u00e9ation d'interfaces permettant de g\u00e9n\u00e9rer le code Python d'interfaces graphiques : QtDesigner. -Le service de cloud computing : Amazon Web Services (AWS). Page d'accueil  Page de l'administrateur  ", "has_readme": true, "readme_language": "French", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a770"}, "repo_url": "https://github.com/johnnylord/keras-yolo2", "repo_name": "keras-yolo2", "repo_full_name": "johnnylord/keras-yolo2", "repo_owner": "johnnylord", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T13:41:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T06:31:14Z", "homepage": null, "size": 24, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187976838, "is_fork": false, "readme_text": "keras-yolo2 Input data Input data would be a image of size (416, 416). Output data Output data would be a four dimension tensor, (S, S, B, 4+1+CLASSES). For every grid cell in the image, it will predict B bounding boxes, each bounding box containing the following information.  bx := the x coordinate of the bounding box. # unit, grid cell by := the y coordinate of the bounding box. # unit, grid cell bw := the width of the bounding box. # unit, grid cell bh := the height of the bounding box. # unit, grid cell pc := the confidence of the bounding box. (Pr(object) * IoU(pred-box, true-box)) classes := a vector of class probability  Loss Contribution ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a771"}, "repo_url": "https://github.com/cbentzel/learn_keras", "repo_name": "learn_keras", "repo_full_name": "cbentzel/learn_keras", "repo_owner": "cbentzel", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T10:09:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T09:56:50Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187614621, "is_fork": false, "readme_text": "learn_keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a772"}, "repo_url": "https://github.com/lijqhs/Sogou-News-Text-Classification", "repo_name": "Sogou-News-Text-Classification", "repo_full_name": "lijqhs/Sogou-News-Text-Classification", "repo_owner": "lijqhs", "repo_desc": "Text classification with Machine Learning methods and Pre-Trained Embedding model on Sogou News Corpus", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T10:01:32Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-22T06:42:32Z", "homepage": "", "size": 891, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187978556, "is_fork": false, "readme_text": "Text Classification \u6587\u672c\u5206\u7c7b\uff08Text Classification\uff09\u662f\u81ea\u7136\u8bed\u8a00\u5904\u7406\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u5e94\u7528\u6280\u672f\uff0c\u6839\u636e\u6587\u6863\u7684\u5185\u5bb9\u6216\u4e3b\u9898\uff0c\u81ea\u52a8\u8bc6\u522b\u6587\u6863\u6240\u5c5e\u7684\u9884\u5148\u5b9a\u4e49\u7684\u7c7b\u522b\u6807\u7b7e\u3002\u6587\u672c\u5206\u7c7b\u662f\u5f88\u591a\u5e94\u7528\u573a\u666f\u7684\u57fa\u7840\uff0c\u6bd4\u5982\u5783\u573e\u90ae\u4ef6\u8bc6\u522b\uff0c\u8206\u60c5\u5206\u6790\uff0c\u60c5\u611f\u8bc6\u522b\uff0c\u65b0\u95fb\u81ea\u52a8\u5206\u7c7b\uff0c\u667a\u80fd\u5ba2\u670d\u673a\u5668\u4eba\u7684\u77e5\u8bc6\u5e93\u5206\u7c7b\u7b49\u7b49\u3002\u672c\u6587\u5206\u4e3a\u4e24\u4e2a\u90e8\u5206\uff1a  Part 1: \u57fa\u4e8escikit-learn\u673a\u5668\u5b66\u4e60Python\u5e93\uff0c\u5bf9\u6bd4\u51e0\u4e2a\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u7684\u6587\u672c\u5206\u7c7b\u3002Blog Post Part 2: \u57fa\u4e8e\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b\uff0c\u4f7f\u7528Keras\u5de5\u5177\u8fdb\u884c\u6587\u672c\u5206\u7c7b\uff0c\u7528\u5230\u4e86CNN\u3002Blog Post  \u672c\u6587\u8bed\u6599\uff1a\u4e0b\u8f7d\u94fe\u63a5\uff0c\u5bc6\u7801:dh4x\u3002\u66f4\u591a\u65b0\u95fb\u6807\u6ce8\u8bed\u6599\uff0c\u4e0b\u8f7d\u94fe\u63a5\u3002 \u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b\u6765\u81eaGitHub\uff1aChinese Word Vectors \u4e0a\u767e\u79cd\u9884\u8bad\u7ec3\u4e2d\u6587\u8bcd\u5411\u91cf\uff0c\u4e0b\u8f7d\u5730\u5740\uff1aSogou News 300d\u3002  Text Classification  Part 1: \u57fa\u4e8escikit-learn\u673a\u5668\u5b66\u4e60\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5  1. \u8bed\u6599\u9884\u5904\u7406 2. \u751f\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6  \u751f\u6210\u6570\u636e\u96c6   3. \u6587\u672c\u7279\u5f81\u63d0\u53d6:TF-IDF 4. \u6784\u5efa\u5206\u7c7b\u5668  Benchmark: \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668 \u5bf9\u65b0\u6587\u672c\u5e94\u7528\u5206\u7c7b   5. \u5206\u7c7b\u5668\u7684\u8bc4\u4f30  \u6784\u5efaLogistic Regression\u5206\u7c7b\u5668 \u6784\u5efaSVM\u5206\u7c7b\u5668     Part 2: \u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684CNN\u6587\u672c\u5206\u7c7b\u65b9\u6cd5-Keras  1. \u8bfb\u53d6\u8bed\u6599 2. \u52a0\u8f7d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b 3. \u4f7f\u7528Keras\u5bf9\u8bed\u6599\u8fdb\u884c\u5904\u7406 4. \u5b9a\u4e49\u8bcd\u5d4c\u5165\u77e9\u9635  Embedding Layer   5. \u6784\u5efa\u6a21\u578b \u53c2\u8003\u8d44\u6599      Part 1: \u57fa\u4e8escikit-learn\u673a\u5668\u5b66\u4e60\u7684\u6587\u672c\u5206\u7c7b\u65b9\u6cd5 \u57fa\u4e8escikit-learn\u673a\u5668\u5b66\u4e60\u7684\u4e2d\u6587\u6587\u672c\u5206\u7c7b\u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u6b65\u9aa4\uff1a  \u8bed\u6599\u9884\u5904\u7406 \u751f\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 \u6587\u672c\u7279\u5f81\u63d0\u53d6:TF-IDF \u6784\u5efa\u5206\u7c7b\u5668 \u5206\u7c7b\u5668\u7684\u8bc4\u4f30  1. \u8bed\u6599\u9884\u5904\u7406 \u5b9a\u4e49\u641c\u72d7\u65b0\u95fb\u6587\u672c\u6807\u7b7e\u7684\u540d\u79f0\uff0c\u7c7b\u4f3cC000008\u8fd9\u6837\u7684\u6807\u7b7e\u662f\u8bed\u6599\u7684\u5b50\u76ee\u5f55\uff0c\u5728\u7f51\u4e0a\u641c\u5230\u6807\u7b7e\u5bf9\u5e94\u7684\u65b0\u95fb\u7c7b\u522b\uff0c\u4e3a\u4e86\u4fbf\u4e8e\u7406\u89e3\uff0c\u5b9a\u4e49\u4e86\u8fd9\u4e2a\u6620\u5c04\u8bcd\u5178\uff0c\u5e76\u4fdd\u7559\u539f\u6709\u7f16\u53f7\u4fe1\u606f\u3002\u5728\u7f51\u4e0a\u641c\u7d22\u4e0b\u8f7d\u641c\u72d7\u5206\u7c7b\u65b0\u95fb.20061127.zip\u8bed\u6599\u5e76\u89e3\u538b\u81f3CN_Corpus\u76ee\u5f55\u4e0b\uff0c\u89e3\u538b\u4e4b\u540e\u76ee\u5f55\u7ed3\u6784\u4e3a\uff1a CN_Corpus \u2514\u2500SogouC.reduced     \u2514\u2500Reduced         \u251c\u2500C000008         \u251c\u2500C000010         \u251c\u2500C000013         \u251c\u2500C000014         \u251c\u2500C000016         \u251c\u2500C000020         \u251c\u2500C000022         \u251c\u2500C000023         \u2514\u2500C000024  category_labels = {     'C000008': '_08_Finance',     'C000010': '_10_IT',     'C000013': '_13_Health',     'C000014': '_14_Sports',     'C000016': '_16_Travel',     'C000020': '_20_Education',     'C000022': '_22_Recruit',     'C000023': '_23_Culture',     'C000024': '_24_Military' } \u4e0b\u9762\u8fdb\u884c\u8bed\u6599\u7684\u5207\u5206\uff0c\u5c06\u6bcf\u4e2a\u7c7b\u522b\u7684\u524d80%\u4f5c\u4e3a\u8bad\u7ec3\u8bed\u6599\uff0c\u540e20%\u4f5c\u4e3a\u6d4b\u8bd5\u8bed\u6599\u3002\u5207\u5206\u5b8c\u4e4b\u540e\u7684\u8bed\u6599\u76ee\u5f55\u5982\u4e0b\uff1a data \u251c\u2500test \u2502  \u251c\u2500_08_Finance \u2502  \u251c\u2500_10_IT \u2502  \u251c\u2500_13_Health \u2502  \u251c\u2500_14_Sports \u2502  \u251c\u2500_16_Travel \u2502  \u251c\u2500_20_Education \u2502  \u251c\u2500_22_Recruit \u2502  \u251c\u2500_23_Culture \u2502  \u2514\u2500_24_Military \u2514\u2500train     \u251c\u2500_08_Finance     \u251c\u2500_10_IT     \u251c\u2500_13_Health     \u251c\u2500_14_Sports     \u251c\u2500_16_Travel     \u251c\u2500_20_Education     \u251c\u2500_22_Recruit     \u251c\u2500_23_Culture     \u2514\u2500_24_Military  2. \u751f\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 \u751f\u6210\u6570\u636e\u96c6 \u4ece\u4e0a\u9762\u5207\u5206\u597d\u7684\u8bed\u6599\u76ee\u5f55\u4e2d\u8bfb\u53d6\u6587\u672c\u5e76\u8fdb\u884c\u5206\u8bcd\u9884\u5904\u7406\uff0c\u8f93\u51fa\uff1a\u8bad\u7ec3\u8bed\u6599\u6570\u636e(X_train_data)\u3001\u8bad\u7ec3\u8bed\u6599\u6807\u7b7e(y_train)\u3001\u6d4b\u8bd5\u8bed\u6599\u6570\u636e(X_test_data)\u3001\u6d4b\u8bd5\u8bed\u6599\u6807\u7b7e(y_test)\u3002 X_train_data, y_train, X_test_data, y_test = load_datasets() label: _08_Finance, len: 1500 label: _10_IT, len: 1500 label: _13_Health, len: 1500 label: _14_Sports, len: 1500 label: _16_Travel, len: 1500 label: _20_Education, len: 1500 label: _22_Recruit, len: 1500 label: _23_Culture, len: 1500 label: _24_Military, len: 1500 train corpus len: 13500  label: _08_Finance, len: 490 label: _10_IT, len: 490 label: _13_Health, len: 490 label: _14_Sports, len: 490 label: _16_Travel, len: 490 label: _20_Education, len: 490 label: _22_Recruit, len: 490 label: _23_Culture, len: 490 label: _24_Military, len: 490 test corpus len: 4410  \u6570\u636e\u96c6\u7684\u5f62\u5f0f\u5982\u4e0b\uff1a X_train_data[1000] '\u65b0\u534e\u7f51 \u4e0a\u6d77 \u6708 \u65e5\u7535 \u8bb0\u8005 \u9ec4\u5ead\u94a7 \u7ee7 \u65e5 \u4eba\u6c11\u5e01 \u5151 \u7f8e\u5143 \u4e2d\u95f4\u4ef7 \u7a81\u7834 \u5173\u53e3 \u521b \u5386\u53f2 \u65b0\u9ad8 \u540e \u65e5 \u4eba\u6c11\u5e01 \u5151 \u7f8e\u5143\u6c47\u7387 \u7ee7\u7eed \u6500\u5347 \u4e2d\u56fd\u5916\u6c47\u4ea4\u6613\u4e2d\u5fc3 \u65e5 \u516c\u5e03 \u7684 \u4e2d\u95f4\u4ef7 \u4e3a \u518d\u5237 \u5386\u53f2 \u65b0\u9ad8 \u5927\u6709 \u903c\u8fd1 \u548c \u7a81\u7834 \u5fc3\u7406 \u5173\u53e3 \u4e4b\u52bf \u636e \u5174\u4e1a\u94f6\u884c \u8d44\u91d1 \u8425\u8fd0 \u4e2d\u5fc3 \u4ea4\u6613\u5458 \u4f59\u5c79 \u4ecb\u7ecd \u4eba\u6c11\u5e01 \u5151 \u7f8e\u5143\u6c47\u7387 \u65e5 \u8d70\u52bf \u7ee7\u7eed \u8868\u73b0 \u5f3a\u52b2 \u7ade\u4ef7 \u4ea4\u6613 \u4ee5 \u5f00\u76d8 \u540e \u6700\u4f4e \u66fe \u56de\u5230 \u6700\u9ad8 \u5219 \u89e6\u53ca \u8ddd \u5173\u53e3 \u4ec5 \u4e00\u6b65\u4e4b\u9065 \u6536\u62a5 \u800c \u8be2\u4ef7 \u4ea4\u6613 \u4ea6 \u8868\u73b0 \u4e0d\u4fd7 \u4ee5 \u5f00\u76d8 \u540e \u66fe\u7ecf \u8d70\u4f4e \u5230 \u6700\u9ad8 \u4ec5\u89e6 \u5230 \u622a\u81f3 \u65f6 \u5206 \u62a5\u6536 \u867d\u7136 \u5168\u65e5 \u6ce2\u5e45 \u8f83\u7a84 \u4f46 \u5747 \u5728 \u4e0b\u65b9 \u6709 \u8dc3\u8dc3\u6b32\u8bd5 \u5173\u53e3 \u4e4b \u6001\u52bf \u5b8c'  y_train[1000] '_08_Finance'   3. \u6587\u672c\u7279\u5f81\u63d0\u53d6:TF-IDF \u8fd9\u4e2a\u6b65\u9aa4\u5c06\u6587\u6863\u4fe1\u606f\uff0c\u4e5f\u5373\u6bcf\u7bc7\u65b0\u95fb\u88ab\u5206\u597d\u8bcd\u4e4b\u540e\u7684\u8bcd\u96c6\u5408\uff0c\u8f6c\u4e3a\u4e3a\u57fa\u4e8e\u8bcd\u9891-\u4f60\u6587\u6863\u8bcd\u9891\uff08TF-IDF\uff09\u7684\u5411\u91cf\uff0c\u5411\u91cf\u7684\u6bcf\u4e2a\u5143\u7d20\u90fd\u662f\u5bf9\u5e94\u4e8e\u67d0\u4e2a\u8bcd\u5728\u8fd9\u4e2a\u6587\u6863\u4e2d\u7684TF-IDF\u503c\uff0c\u5728\u4e0d\u540c\u6587\u6863\u4e2d\uff0c\u540c\u4e00\u8bcd\u7684TF-IDF\u662f\u4e0d\u4e00\u6837\u7684\u3002\u6240\u6709\u6587\u6863\u7684TF-IDF\u5411\u91cf\u5806\u653e\u5728\u4e00\u8d77\u5c31\u7ec4\u6210\u4e86\u4e00\u4e2aTF-IDF\u77e9\u9635\u3002\u6ce8\u610f\u5230\u8fd9\u91cc\u5e94\u8be5\u5305\u542b\u4e86\u9664\u505c\u7528\u8bcd\u4e4b\u5916\u7684\u6240\u6709\u8bcd\u7684TF-IDF\u503c\uff0c\u8bcd\u7684\u4e2a\u6570\u6784\u6210\u4e86\u5411\u91cf\u7684\u7ef4\u5ea6\u3002 \u7528TfidfVectorizer\u5c06\u6587\u6863\u96c6\u5408\u8f6c\u4e3aTF-IDF\u77e9\u9635\u3002\u6ce8\u610f\u5230\u524d\u9762\u6211\u4eec\u5c06\u6587\u672c\u505a\u4e86\u5206\u8bcd\u5e76\u7528\u7a7a\u683c\u9694\u5f00\u3002\u5982\u679c\u662f\u82f1\u6587\uff0c\u672c\u8eab\u5c31\u662f\u7a7a\u683c\u9694\u5f00\u7684\uff0c\u800c\u82f1\u6587\u7684\u5206\u8bcd\uff08Tokenizing\uff09\u662f\u5305\u542b\u5728\u7279\u5f81\u63d0\u53d6\u5668\u4e2d\u7684\uff0c\u4e0d\u9700\u8981\u5206\u8bcd\u8fd9\u4e00\u6b65\u9aa4\u3002\u4e0b\u9762\u6211\u4eec\u5728\u5f97\u5230\u4e86\u5206\u7c7b\u5668\u4e4b\u540e\uff0c\u4f7f\u7528\u65b0\u6587\u672c\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\u65f6\uff0c\u4e5f\u662f\u9700\u8981\u5148\u505a\u4e00\u4e0b\u4e2d\u6587\u5206\u8bcd\u7684\u3002 stopwords = open('dict/stop_words.txt', encoding='utf-8').read().split()  # TF-IDF feature extraction tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords) X_train_tfidf = tfidf_vectorizer.fit_transform(X_train_data) words = tfidf_vectorizer.get_feature_names() X_train_tfidf.shape (13500, 223094)  len(words) 223094  4. \u6784\u5efa\u5206\u7c7b\u5668 Benchmark: \u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668 \u5f97\u5230\u4e86\u8bad\u7ec3\u6837\u672c\u7684\u6587\u672c\u7279\u5f81\uff0c\u73b0\u5728\u53ef\u4ee5\u8bad\u7ec3\u51fa\u4e00\u4e2a\u5206\u7c7b\u5668\uff0c\u4ee5\u7528\u6765\u5bf9\u65b0\u7684\u65b0\u95fb\u6587\u672c\u8fdb\u884c\u5206\u7c7b\u3002scikit-learn\u4e2d\u63d0\u4f9b\u4e86\u591a\u79cd\u5206\u7c7b\u5668\uff0c\u5176\u4e2d\u6734\u7d20\u8d1d\u53f6\u65af\u662f\u4e00\u4e2a\u5f88\u597d\u7684\u57fa\u51c6\uff0c\u6709\u591a\u4e2a\u7248\u672c\u7684\u6734\u7d20\u8d1d\u53f6\u65af\u5206\u7c7b\u5668\uff0c\u5176\u4e2dMultinomialNB\u6bd4\u8f83\u9002\u5408\u4e8e\u6587\u672c\u5206\u7c7b\u3002 classifier = MultinomialNB() classifier.fit(X_train_tfidf, y_train) MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)  \u5bf9\u65b0\u6587\u672c\u5e94\u7528\u5206\u7c7b \u5bf9\u65b0\u7684\u6587\u672c\u9700\u8981\u8fdb\u884c\u5206\u7c7b\uff0c\u90a3\u4e48\u53ea\u9700\u5c06\u4e0a\u9762\u7684tfidf_vectorizer\u5e94\u7528\u5728\u65b0\u7684\u6587\u672c\u4e0a\uff0c\u8c03\u7528transform\u65b9\u6cd5\u800c\u4e0d\u662ffit_transform\uff0c\u5c06\u65b0\u7684\u6587\u672c\u8f6c\u6362\u4e3aTF-IDF\u7279\u5f81\uff0c\u7136\u540e\u518d\u8c03\u7528\u5206\u7c7b\u5668\u7684predict\uff0c\u5f97\u5230\u5206\u7c7b\u3002 \u4e0b\u9762\u65b0\u95fb\u8282\u9009\u81ea\u817e\u8baf\u65b0\u95fb\u7f51\uff0c\u539f\u6587\u5730\u5740\uff1a  \u5468\u9e3f\u794e\u91d1\u878d\u68a6\uff0c\u8425\u653620\u4ebf\u5143\u76f4\u903c\u8da3\u5e97\uff0c\u51c0\u5229\u6da6\u540c\u6bd4\u589e340\uff05 \u5f55\u53d6\u7387\u4f4e\u8fc75\uff05\u7684\u7f8e\u56fd\u540d\u6821\uff0c\u4e3a\u4f55\u201c\u82b1\u94b1\u201d\u5c31\u80fd\u4e0a\uff1f \u7279\u6717\u666e\uff1a\u4f0a\u6717\u5bf9\u7f8e\u519b\u52a8\u6b66\u5c06\u4f1a\u201c\u706d\u4ea1\u201d  news_lastest = [\"360\u91d1\u878d\u65d7\u4e0b\u4ea7\u54c1\u6709360\u501f\u6761\u3001360\u5c0f\u5fae\u8d37\u3001360\u5206\u671f\u3002360\u501f\u6761\u662f360\u91d1\u878d\u7684\u6838\u5fc3\u4ea7\u54c1\uff0c\u662f\u4e00\u6b3e\u65e0\u62b5\u62bc\u3001\u7eaf\u7ebf\u4e0a\u6d88\u8d39\u4fe1\u8d37\u4ea7\u54c1\uff0c\u4e3a\u7528\u6237\u63d0\u4f9b\u5373\u65f6\u5230\u8d26\u8d37\u6b3e\u670d\u52a1\uff08\u901a\u4fd7\u53ef\u4ee5\u7406\u89e3\u4e3a\u201c\u73b0\u91d1\u8d37\u201d\uff09\u7528\u6237\u501f\u6b3e\u4e3b\u8981\u7528\u4e8e\u6d88\u8d39\u652f\u51fa\u3002\u4ece\u6536\u5165\u6784\u6210\u6765\u770b\uff0c360\u91d1\u878d\u4e3b\u8981\u6709\u8d37\u6b3e\u4fbf\u5229\u670d\u52a1\u8d39\u3001\u8d37\u540e\u7ba1\u7406\u670d\u52a1\u8d39\u3001\u878d\u8d44\u6536\u5165\u3001\u5176\u4ed6\u670d\u52a1\u6536\u5165\u7b49\u6784\u6210\u3002\u8d22\u62a5\u62ab\u9732\uff0c\u8425\u6536\u589e\u957f\u4e3b\u8981\u662f\u7531\u4e8e\u8d37\u6b3e\u4fbf\u5229\u5316\u670d\u52a1\u8d39\u3001\u8d37\u6b3e\u53d1\u653e\u540e\u670d\u52a1\u8d39\u548c\u5176\u4ed6\u4e0e\u8d37\u6b3e\u53d1\u653e\u91cf\u589e\u52a0\u76f8\u5173\u7684\u670d\u52a1\u8d39\u589e\u52a0\u3002\",                 \"\u68c0\u65b9\u5e76\u672a\u8d77\u8bc9\u5168\u90e8\u6d89\u5acc\u8d3f\u8d42\u7684\u5bb6\u957f\uff0c\u4f46\u8d77\u8bc9\u540d\u5355\u5df2\u6709\u8d85\u8fc750\u4eba\uff0c\u8036\u9c81\u5927\u5b66\u3001\u65af\u5766\u798f\u5927\u5b66\u7b49\u5f55\u53d6\u7387\u6781\u4f4e\u7684\u540d\u6821\u6d89\u6848\u4e5f\u8ba9\u8be5\u4e8b\u4ef6\u53d7\u5230\u4e86\u51e0\u4e4e\u5168\u7403\u7684\u5173\u6ce8\uff0c\u8be5\u6848\u751a\u81f3\u88ab\u79f0\u4f5c\u7f8e\u56fd\u201c\u53f2\u4e0a\u6700\u5927\u62db\u751f\u821e\u5f0a\u6848\u201d\u3002\",                 \"\u4fc4\u5a92\u79f0\uff0c\u76ee\u524d\u5c1a\u4e0d\u6e05\u695a\u7279\u6717\u666e\u8fd9\u4e00\u8a00\u8bba\u7684\u6307\u5411\u6027\uff0c\u56e0\u4e3a\u8fd1\u51e0\u65e5\uff0c\u4f0a\u6717\u5b98\u5458\u4eec\u90fd\u5728\u8868\u8fbe\u529b\u56fe\u907f\u514d\u4e0e\u7f8e\u56fd\u53d1\u751f\u519b\u4e8b\u51b2\u7a81\u7684\u610f\u613f\u30025\u670819\u65e5\u65e9\u4e9b\u65f6\u5019\uff0c\u4f0a\u6717\u9769\u547d\u536b\u961f\u53f8\u4ee4\u4faf\u8d5b\u56e0\u00b7\u8428\u62c9\u7c73\u79f0\uff0c\u4f0a\u6717\u53ea\u60f3\u8ffd\u6c42\u548c\u5e73\uff0c\u4f46\u5e76\u4e0d\u5bb3\u6015\u4e0e\u7f8e\u56fd\u53d1\u751f\u6218\u4e89\u3002\u8428\u62c9\u7c73\u79f0\uff0c\u201c\u6211\u4eec\uff08\u4f0a\u6717\uff09\u548c\u4ed6\u4eec\uff08\u7f8e\u56fd\uff09\u4e4b\u95f4\u7684\u533a\u522b\u5728\u4e8e\uff0c\u7f8e\u56fd\u5bb3\u6015\u53d1\u751f\u6218\u4e89\uff0c\u7f3a\u4e4f\u5f00\u6218\u7684\u610f\u5fd7\u3002\u201d\"] X_new_data = [preprocess(doc) for doc in news_lastest] X_new_data ['\u91d1\u878d \u65d7\u4e0b \u4ea7\u54c1 \u6709 \u501f\u6761 \u5c0f\u5fae\u8d37 \u5206\u671f \u501f\u6761 \u662f \u91d1\u878d \u7684 \u6838\u5fc3 \u4ea7\u54c1 \u662f \u4e00\u6b3e \u65e0 \u62b5\u62bc \u7eaf\u7ebf \u4e0a \u6d88\u8d39\u4fe1\u8d37 \u4ea7\u54c1 \u4e3a \u7528\u6237 \u63d0\u4f9b \u5373\u65f6 \u5230 \u8d26 \u8d37\u6b3e \u670d\u52a1 \u901a\u4fd7 \u53ef\u4ee5 \u7406\u89e3 \u4e3a \u73b0\u91d1 \u8d37 \u7528\u6237 \u501f\u6b3e \u4e3b\u8981 \u7528\u4e8e \u6d88\u8d39 \u652f\u51fa \u4ece \u6536\u5165 \u6784\u6210 \u6765\u770b \u91d1\u878d \u4e3b\u8981 \u6709 \u8d37\u6b3e \u4fbf\u5229 \u670d\u52a1\u8d39 \u8d37\u540e \u7ba1\u7406 \u670d\u52a1\u8d39 \u878d\u8d44 \u6536\u5165 \u5176\u4ed6 \u670d\u52a1\u6536\u5165 \u7b49 \u6784\u6210 \u8d22\u62a5 \u62ab\u9732 \u8425\u6536 \u589e\u957f \u4e3b\u8981 \u662f \u7531\u4e8e \u8d37\u6b3e \u4fbf\u5229\u5316 \u670d\u52a1\u8d39 \u8d37\u6b3e \u53d1\u653e \u540e \u670d\u52a1\u8d39 \u548c \u5176\u4ed6 \u4e0e \u8d37\u6b3e \u53d1\u653e\u91cf \u589e\u52a0 \u76f8\u5173 \u7684 \u670d\u52a1\u8d39 \u589e\u52a0',  '\u68c0\u65b9 \u5e76\u672a \u8d77\u8bc9 \u5168\u90e8 \u6d89\u5acc \u8d3f\u8d42 \u7684 \u5bb6\u957f \u4f46 \u8d77\u8bc9 \u540d\u5355 \u5df2\u6709 \u8d85\u8fc7 \u4eba \u8036\u9c81\u5927\u5b66 \u65af\u5766\u798f\u5927\u5b66 \u7b49 \u5f55\u53d6\u7387 \u6781\u4f4e \u7684 \u540d\u6821 \u6d89\u6848 \u4e5f \u8ba9 \u8be5 \u4e8b\u4ef6 \u53d7\u5230 \u4e86 \u51e0\u4e4e \u5168\u7403 \u7684 \u5173\u6ce8 \u8be5\u6848 \u751a\u81f3 \u88ab\u79f0\u4f5c \u7f8e\u56fd \u53f2\u4e0a \u6700\u5927 \u62db\u751f \u821e\u5f0a\u6848',  '\u4fc4\u5a92\u79f0 \u76ee\u524d \u5c1a \u4e0d \u6e05\u695a \u7279\u6717\u666e \u8fd9\u4e00 \u8a00\u8bba \u7684 \u6307\u5411\u6027 \u56e0\u4e3a \u8fd1\u51e0\u65e5 \u4f0a\u6717 \u5b98\u5458 \u4eec \u90fd \u5728 \u8868\u8fbe \u529b\u56fe \u907f\u514d \u4e0e \u7f8e\u56fd \u53d1\u751f \u519b\u4e8b\u51b2\u7a81 \u7684 \u610f\u613f \u6708 \u65e5 \u65e9\u4e9b\u65f6\u5019 \u4f0a\u6717 \u9769\u547d \u536b\u961f \u53f8\u4ee4 \u4faf\u8d5b\u56e0 \u00b7 \u8428\u62c9\u7c73 \u79f0 \u4f0a\u6717 \u53ea\u60f3 \u8ffd\u6c42 \u548c\u5e73 \u4f46 \u5e76 \u4e0d \u5bb3\u6015 \u4e0e \u7f8e\u56fd \u53d1\u751f \u6218\u4e89 \u8428\u62c9\u7c73 \u79f0 \u6211\u4eec \u4f0a\u6717 \u548c \u4ed6\u4eec \u7f8e\u56fd \u4e4b\u95f4 \u7684 \u533a\u522b \u5728\u4e8e \u7f8e\u56fd \u5bb3\u6015 \u53d1\u751f \u6218\u4e89 \u7f3a\u4e4f \u5f00\u6218 \u7684 \u610f\u5fd7']  X_new_tfidf = tfidf_vectorizer.transform(X_new_data) predicted  = classifier.predict(X_new_tfidf) predicted array(['_08_Finance', '_20_Education', '_24_Military'], dtype='<U13')  5. \u5206\u7c7b\u5668\u7684\u8bc4\u4f30 \u6709\u4e86\u5206\u7c7b\u5668\uff0c\u4ee5\u53ca\u77e5\u9053\u4e86\u5982\u4f55\u7528\u5206\u7c7b\u5668\u6765\u5bf9\u65b0\u7684\u6587\u672c\u8fdb\u884c\u5206\u7c7b\u9884\u6d4b\uff0c\u90a3\u4e48\u6211\u4eec\u53ef\u4ee5\u7528\u524d\u9762\u5212\u5206\u51fa\u6765\u7684\u6d4b\u8bd5\u96c6\u5bf9\u8fd9\u4e2a\u5206\u7c7b\u5668\u8fdb\u884c\u6027\u80fd\u8bc4\u4f30\u3002\u6211\u4eec\u5f97\u5230\u4e8684.35%\u7684\u51c6\u786e\u7387\uff0c\u4f5c\u4e3aBenchmark\uff0c\u8fd9\u4e2a\u7ed3\u679c\u8fd8\u4e0d\u9519\u3002\u8c03\u7528classification_report\u53ef\u4ee5\u5f97\u5230\u66f4\u8be6\u7ec6\u7684\u7ed3\u679c\uff1a print(classification_report(predicted, y_test))                precision    recall  f1-score   support    _08_Finance       0.88      0.88      0.88       488        _10_IT       0.72      0.87      0.79       403    _13_Health       0.82      0.84      0.83       478    _14_Sports       0.95      1.00      0.97       466    _16_Travel       0.86      0.92      0.89       455 _20_Education       0.71      0.87      0.79       401   _22_Recruit       0.91      0.65      0.76       690   _23_Culture       0.80      0.77      0.79       513  _24_Military       0.94      0.89      0.92       516       accuracy                           0.84      4410     macro avg       0.84      0.86      0.84      4410  weighted avg       0.85      0.84      0.84      4410  \u518d\u770b\u4e00\u4e0b\u6df7\u6dc6\u77e9\u9635\uff1a confusion_matrix(predicted, y_test) array([[429,  12,  15,  10,   5,   3,   6,   4,   4],        [ 20, 352,   6,   3,   5,   2,  10,   4,   1],        [  3,  38, 403,   0,   6,  16,   5,   7,   0],        [  0,   1,   0, 464,   0,   0,   0,   1,   0],        [  5,  11,   0,   0, 419,   5,   2,  12,   1],        [  4,  11,   5,   1,   2, 350,  13,  14,   1],        [ 22,  21,  57,   8,  14,  87, 448,  32,   1],        [  3,  25,   4,   4,  34,  23,   5, 394,  21],        [  4,  19,   0,   0,   5,   4,   1,  22, 461]])  \u6784\u5efaLogistic Regression\u5206\u7c7b\u5668 \u8ba9\u6211\u4eec\u518d\u8bd5\u4e00\u4e0b\u5176\u4ed6\u7684\u5206\u7c7b\u5668\uff0c\u6bd4\u5982Logistic Regression\uff0c\u8bad\u7ec3\u65b0\u7684\u5206\u7c7b\u5668\uff1a text_clf_lr = Pipeline([     ('vect', TfidfVectorizer()),     ('clf', LogisticRegression()), ])                precision    recall  f1-score   support    _08_Finance       0.87      0.91      0.89       465        _10_IT       0.77      0.86      0.81       440    _13_Health       0.91      0.82      0.86       546    _14_Sports       0.98      0.99      0.98       483    _16_Travel       0.90      0.90      0.90       488 _20_Education       0.79      0.91      0.85       429   _22_Recruit       0.86      0.85      0.85       495   _23_Culture       0.86      0.75      0.80       556  _24_Military       0.95      0.92      0.93       508       accuracy                           0.88      4410     macro avg       0.88      0.88      0.88      4410  weighted avg       0.88      0.88      0.88      4410  \u6700\u540e\u6d4b\u8bd5\u7ed3\u679c\u8fd8\u884c\uff0c\u6bd4Benchmark\u5206\u7c7b\u5668\u597d\u4e86\u4e0d\u5c11\u3002 array([[425,  11,   7,   1,   2,   4,   9,   5,   1],        [ 23, 377,   4,   3,   9,   3,  12,   7,   2],        [  9,  41, 447,   0,   6,  23,  10,  10,   0],        [  0,   2,   0, 478,   0,   1,   0,   1,   1],        [  8,  12,   0,   0, 440,   8,   3,  16,   1],        [  1,   6,   2,   0,   1, 389,  20,  10,   0],        [  8,   7,  22,   1,   5,  26, 420,   6,   0],        [ 11,  23,   8,   6,  24,  30,  15, 419,  20],        [  5,  11,   0,   1,   3,   6,   1,  16, 465]])  \u6784\u5efaSVM\u5206\u7c7b\u5668 \u5728\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u4e2d\uff0cSVM\u662f\u505a\u6587\u672c\u5206\u7c7b\u6700\u597d\u7684\u5de5\u5177\u3002\u540c\u6837\u505a\u4e00\u4e0b\u8ddf\u4e0a\u9762LR\u4e00\u6837\u7684\u5bf9\u6bd4\uff1a text_clf_svm = Pipeline([     ('vect', TfidfVectorizer()),     ('clf', SGDClassifier(loss='hinge', penalty='l2')), ]) \u51c6\u786e\u7387\u4e0d\u4f4e\uff1a                precision    recall  f1-score   support    _08_Finance       0.87      0.92      0.90       463        _10_IT       0.78      0.85      0.81       446    _13_Health       0.93      0.82      0.87       558    _14_Sports       0.99      0.99      0.99       488    _16_Travel       0.91      0.91      0.91       489 _20_Education       0.82      0.92      0.86       437   _22_Recruit       0.89      0.85      0.87       513   _23_Culture       0.85      0.83      0.84       503  _24_Military       0.96      0.91      0.93       513       accuracy                           0.89      4410     macro avg       0.89      0.89      0.89      4410  weighted avg       0.89      0.89      0.89      4410  \u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u8fd9\u6837\u7684\u7ed3\u679c\u975e\u5e38\u4e0d\u9519\u4e86\uff0c\u800c\u4e14\u5b9e\u73b0\u65b9\u5f0f\u975e\u5e38\u7b80\u5355\u5b9e\u7528\u3002 array([[428,  11,   5,   1,   5,   3,   5,   8,   0],        [ 23, 382,   4,   0,   6,   8,  11,   7,   2],        [  9,  39, 453,   0,   7,  19,  10,   9,   0],        [  0,   1,   0, 485,   0,   2,   0,   1,   0],        [  7,  14,   0,   0, 449,   7,   3,  14,   1],        [  3,   7,   1,   1,   1, 403,  14,  15,   0],        [ 12,   8,  22,   0,   4,  25, 437,   8,   0],        [  4,  15,   5,   3,  15,  17,   9, 411,  19],        [  4,  13,   0,   0,   3,   6,   1,  17, 468]])  Part 2: \u57fa\u4e8e\u9884\u8bad\u7ec3\u6a21\u578b\u7684CNN\u6587\u672c\u5206\u7c7b\u65b9\u6cd5-Keras \u6587\u672c\u5206\u7c7b\u4e3b\u8981\u5206\u4e3a\u4ee5\u4e0b\u6b65\u9aa4\uff1a  \u8bfb\u53d6\u8bed\u6599 \u52a0\u8f7d\u4e2d\u6587\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b \u4f7f\u7528Keras\u8fdb\u884c\u6570\u636e\u9884\u5904\u7406\uff0c\u751f\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 \u5b9a\u4e49\u8bcd\u5d4c\u5165\u77e9\u9635 \u6784\u5efa\u6a21\u578b  1. \u8bfb\u53d6\u8bed\u6599 \u8fd9\u91cc\u8bfb\u53d6\u539f\u59cb\u8bed\u6599\uff0c\u5212\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u653e\u5728\u4e86\u540e\u9762\u9884\u5904\u7406\u90e8\u5206\u3002 texts, labels = load_raw_datasets() \u4e0b\u8868\u662f\u8f6c\u6362\u540e\u7684\u6807\u7b7e\u8868\u793a\uff1a    \u5e8f\u53f7 \u6807\u7b7e \u540d\u79f0 \u5206\u7c7b\u7f16\u7801     0 C000008 Finance [1, 0, 0, 0, 0, 0, 0, 0, 0]   1 C000010 IT [0, 1, 0, 0, 0, 0, 0, 0, 0]   2 C000013 Health [0, 0, 1, 0, 0, 0, 0, 0, 0]   3 C000014 Sports [0, 0, 0, 1, 0, 0, 0, 0, 0]   4 C000016 Travel [0, 0, 0, 0, 1, 0, 0, 0, 0]   5 C000020 Education [0, 0, 0, 0, 0, 1, 0, 0, 0]   6 C000022 Recruit [0, 0, 0, 0, 0, 0, 1, 0, 0]   7 C000023 Culture [0, 0, 0, 0, 0, 0, 0, 1, 0]   8 C000024 Military [0, 0, 0, 0, 0, 0, 0, 0, 1]    2. \u52a0\u8f7d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b \u89e3\u538b\u4e4b\u540e\u7684\u4e2d\u6587\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b\u7684\u6587\u4ef6\u683c\u5f0f\u662f\u6587\u672c\u6587\u4ef6\uff0c\u9996\u884c\u53ea\u6709\u4e24\u4e2a\u7a7a\u683c\u9694\u5f00\u7684\u6570\u5b57\uff1a\u8bcd\u7684\u4e2a\u6570\u548c\u8bcd\u5411\u91cf\u7684\u7ef4\u5ea6\uff0c\u4ece\u7b2c\u4e8c\u884c\u5f00\u59cb\u683c\u5f0f\u4e3a\uff1a\u8bcd \u6570\u5b571 \u6570\u5b572 \u2026\u2026 \u6570\u5b57300\uff0c\u5f62\u5f0f\u5982\u4e0b\uff1a  364180 300 \u4eba\u6c11\u6587\u5b66\u51fa\u7248\u793e 0.003146 0.582671 0.049029 -0.312803 0.522986 0.026432 -0.097115 0.194231 -0.362708 ...... ......  embeddings_index = load_pre_trained() Found 364180 word vectors, dimension 300  3. \u4f7f\u7528Keras\u5bf9\u8bed\u6599\u8fdb\u884c\u5904\u7406 from keras.preprocessing.text import Tokenizer from keras.preprocessing.sequence import pad_sequences from keras.utils import to_categorical import numpy as np  MAX_SEQUENCE_LEN = 1000  # \u6587\u6863\u9650\u5236\u957f\u5ea6 MAX_WORDS_NUM = 20000  # \u8bcd\u5178\u7684\u4e2a\u6570 VAL_SPLIT_RATIO = 0.2 # \u9a8c\u8bc1\u96c6\u7684\u6bd4\u4f8b  tokenizer = Tokenizer(num_words=MAX_WORDS_NUM) tokenizer.fit_on_texts(texts) sequences = tokenizer.texts_to_sequences(texts)  word_index = tokenizer.word_index print(len(word_index)) # all token found # print(word_index.get('\u65b0\u95fb')) # get word index dict_swaped = lambda _dict: {val:key for (key, val) in _dict.items()} word_dict = dict_swaped(word_index) # swap key-value data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LEN)  labels_categorical = to_categorical(np.asarray(labels)) print('Shape of data tensor:', data.shape) print('Shape of label tensor:', labels_categorical.shape)  indices = np.arange(data.shape[0]) np.random.shuffle(indices) data = data[indices] labels_categorical = labels_categorical[indices]  # split data by ratio val_samples_num = int(VAL_SPLIT_RATIO * data.shape[0])  x_train = data[:-val_samples_num] y_train = labels_categorical[:-val_samples_num] x_val = data[-val_samples_num:] y_val = labels_categorical[-val_samples_num:] \u4ee3\u7801\u4e2dword_index\u8868\u793a\u53d1\u73b0\u7684\u6240\u6709\u8bcd\uff0c\u5f97\u5230\u7684\u6587\u672c\u5e8f\u5217\u53d6\u7684\u662fword_index\u4e2d\u524d\u976220000\u4e2a\u8bcd\u5bf9\u5e94\u7684\u7d22\u5f15\uff0c\u6587\u672c\u5e8f\u5217\u96c6\u5408\u4e2d\u7684\u6240\u6709\u8bcd\u7684\u7d22\u5f15\u53f7\u90fd\u572820000\u4e4b\u524d\uff1a len(data[data>=20000]) 0 \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u751f\u6210\u7684\u8bcd\u7d22\u5f15\u5e8f\u5217\u548c\u5bf9\u5e94\u7684\u7d22\u5f15\u8bcd\u5178\u67e5\u770b\u539f\u59cb\u6587\u672c\u548c\u5bf9\u5e94\u7684\u6807\u7b7e\uff1a # convert from index to origianl doc for w_index in data[0]:     if w_index != 0:         print(word_dict[w_index], end=' ') \u6606\u866b \u5927\u81ea\u7136 \u6b4c\u624b \u6606\u866b \u53e3\u8154 \u53d1\u51fa \u6606\u866b \u754c \u8457\u540d \u8179\u90e8 \u4e00\u5bf9 \u662f\u4ece \u53d1\u51fa \u5916\u9762 \u4e00\u5bf9 \u5f39\u6027 \u79f0\u4f5c \u58f0 \u808c \u76f8\u8fde \u53d1\u97f3 \u808c \u6536\u7f29 \u632f\u52a8 \u58f0\u97f3 \u7a7a\u95f4 \u54cd\u4eae \u4f20\u5230 \uff15 \uff10 \uff10 \u7c73 \u6c42\u5a5a \u542c\u5230 \u53d1\u97f3 \u90e8\u4f4d \u53d1\u97f3 \u58f0\u97f3 \u4e24 \u5f20\u5f00 \u868a\u5b50 \u4e00\u5bf9 \u8fb9\u7f18 \u652f\u6491 \u4e24\u53ea \u6bcf\u79d2 \uff12 \uff15 \uff10 \uff5e \uff16 \uff10 \uff10 \u6b21 \u63a8\u52a8 \u7a7a\u6c14 \u5f80\u8fd4 \u8fd0\u52a8 \u53d1\u51fa \u5fae\u5f31 \u58f0 \u6765\u6e90 \u8bed\u6587   category_labels[dict_swaped(labels_index)[argmax(labels_categorical[0])]] '_20_Education'  4. \u5b9a\u4e49\u8bcd\u5d4c\u5165\u77e9\u9635 \u4e0b\u9762\u521b\u5efa\u4e00\u4e2a\u8bcd\u5d4c\u5165\u77e9\u9635\uff0c\u7528\u6765\u4f5c\u4e3a\u4e0a\u8ff0\u6587\u672c\u96c6\u5408\u8bcd\u5178\uff08\u53ea\u53d6\u5e8f\u53f7\u5728\u524dMAX_WORDS_NUM\u7684\u8bcd\uff0c\u5bf9\u5e94\u4e86\u6bd4\u8f83\u5e38\u89c1\u7684\u8bcd\uff09\u7684\u8bcd\u5d4c\u5165\u77e9\u9635\uff0c\u77e9\u9635\u7ef4\u5ea6\u662f(MAX_WORDS_NUM, EMBEDDING_DIM)\u3002\u77e9\u9635\u7684\u6bcf\u4e00\u884ci\u4ee3\u8868\u8bcd\u5178word_index\u4e2d\u7b2ci\u4e2a\u8bcd\u7684\u8bcd\u5411\u91cf\u3002\u8fd9\u4e2a\u8bcd\u5d4c\u5165\u77e9\u9635\u662f\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u7684\u4e00\u4e2a\u5b50\u96c6\u3002\u6211\u4eec\u7684\u65b0\u95fb\u8bed\u6599\u4e2d\u5f88\u53ef\u80fd\u6709\u7684\u8bcd\u4e0d\u5728\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u4e2d\uff0c\u8fd9\u6837\u7684\u8bcd\u5728\u8fd9\u4e2a\u8bcd\u5411\u91cf\u77e9\u9635\u4e2d\u5bf9\u5e94\u7684\u5411\u91cf\u5143\u7d20\u90fd\u8bbe\u4e3a\u96f6\u3002\u8fd8\u8bb0\u5f97\u4e0a\u9762\u7528pad_sequence\u8865\u5145\u76840\u5143\u7d20\u4e48\uff0c\u5b83\u5bf9\u5e94\u5728\u8bcd\u5d4c\u5165\u77e9\u9635\u7684\u5411\u91cf\u4e5f\u90fd\u662f\u96f6\u3002\u5728\u672c\u4f8b\u4e2d\uff0c20000\u4e2a\u8bcd\u670992.35%\u5728\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u4e2d\u3002 EMBEDDING_DIM = 300 # embedding dimension embedding_matrix = np.zeros((MAX_WORDS_NUM+1, EMBEDDING_DIM)) # row 0 for 0 for word, i in word_index.items():     embedding_vector = embeddings_index.get(word)     if i < MAX_WORDS_NUM:         if embedding_vector is not None:             # Words not found in embedding index will be all-zeros.             embedding_matrix[i] = embedding_vector nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1)) nonzero_elements / MAX_WORDS_NUM 0.9235  Embedding Layer \u5d4c\u5165\u5c42\u7684\u8f93\u5165\u6570\u636esequence\u5411\u91cf\u7684\u6574\u6570\u662f\u6587\u672c\u4e2d\u8bcd\u7684\u7f16\u7801\uff0c\u524d\u9762\u770b\u5230\u8fd9\u4e2a\u83b7\u53d6\u5e8f\u5217\u7f16\u7801\u7684\u6b65\u9aa4\u4f7f\u7528\u4e86Keras\u7684Tokenizer API\u6765\u5b9e\u73b0\uff0c\u5982\u679c\u4e0d\u4f7f\u7528\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b\uff0c\u5d4c\u5165\u5c42\u662f\u7528\u968f\u673a\u6743\u91cd\u8fdb\u884c\u521d\u59cb\u5316\uff0c\u5728\u8bad\u7ec3\u4e2d\u5c06\u5b66\u4e60\u5230\u8bad\u7ec3\u96c6\u4e2d\u7684\u6240\u6709\u8bcd\u7684\u6743\u91cd\uff0c\u4e5f\u5c31\u662f\u8bcd\u5411\u91cf\u3002\u5728\u5b9a\u4e49Embedding\u5c42\uff0c\u9700\u8981\u81f3\u5c113\u4e2a\u8f93\u5165\u6570\u636e\uff1a  input_dim\uff1a\u6587\u672c\u8bcd\u5178\u7684\u5927\u5c0f\uff0c\u672c\u4f8b\u4e2d\u5c31\u662fMAX_WORDS_NUM + 1\uff1b output_dim\uff1a\u8bcd\u5d4c\u5165\u7a7a\u95f4\u7684\u7ef4\u5ea6\uff0c\u5c31\u662f\u8bcd\u5411\u91cf\u7684\u957f\u5ea6\uff0c\u672c\u4f8b\u4e2d\u5bf9\u5e94EMBEDDING_DIM\uff1b input_length\uff1a\u8fd9\u662f\u8f93\u5165\u5e8f\u5217\u7684\u957f\u5ea6\uff0c\u672c\u4f8b\u4e2d\u5bf9\u5e94MAX_SEQUENCE_LEN\u3002  \u672c\u6587\u4e2d\u8fd8\u591a\u4e86\u4e24\u4e2a\u8f93\u5165\u53c2\u6570weights=[embedding_matrix]\u548ctrainable=False\uff0c\u524d\u8005\u8bbe\u7f6e\u8be5\u5c42\u7684\u5d4c\u5165\u77e9\u9635\u4e3a\u4e0a\u9762\u6211\u4eec\u5b9a\u4e49\u597d\u7684\u8bcd\u5d4c\u5165\u77e9\u9635\uff0c\u5373\u4e0d\u9002\u7528\u968f\u673a\u521d\u59cb\u5316\u7684\u6743\u91cd\uff0c\u540e\u8005\u8bbe\u7f6e\u4e3a\u672c\u5c42\u53c2\u6570\u4e0d\u53ef\u8bad\u7ec3\uff0c\u5373\u4e0d\u4f1a\u968f\u7740\u540e\u9762\u6a21\u578b\u7684\u8bad\u7ec3\u800c\u66f4\u6539\u3002\u8fd9\u91cc\u6d89\u53ca\u4e86Embedding\u5c42\u7684\u51e0\u79cd\u4f7f\u7528\u65b9\u5f0f\uff1a  \u4ece\u5934\u5f00\u59cb\u8bad\u7ec3\u51fa\u4e00\u4e2a\u8bcd\u5411\u91cf\uff0c\u4fdd\u5b58\u4e4b\u540e\u53ef\u4ee5\u7528\u5728\u5176\u4ed6\u7684\u8bad\u7ec3\u4efb\u52a1\u4e2d\uff1b \u5d4c\u5165\u5c42\u4f5c\u4e3a\u6df1\u5ea6\u5b66\u4e60\u7684\u7b2c\u4e00\u4e2a\u9690\u85cf\u5c42\uff0c\u672c\u8eab\u5c31\u662f\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bad\u7ec3\u7684\u4e00\u90e8\u5206\uff1b \u52a0\u8f7d\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\u6a21\u578b\uff0c\u8fd9\u662f\u4e00\u79cd\u8fc1\u79fb\u5b66\u4e60\uff0c\u672c\u6587\u5c31\u662f\u8fd9\u6837\u7684\u793a\u4f8b\u3002  5. \u6784\u5efa\u6a21\u578b Keras\u652f\u6301\u4e24\u79cd\u7c7b\u578b\u7684\u6a21\u578b\u7ed3\u6784\uff1a  Sequential\u7c7b\uff0c\u987a\u5e8f\u6a21\u578b\uff0c\u8fd9\u4e2a\u4ec5\u7528\u4e8e\u5c42\u7684\u7ebf\u6027\u5806\u53e0\uff0c\u6700\u5e38\u89c1\u7684\u7f51\u7edc\u67b6\u6784 Functional API\uff0c\u51fd\u6570\u5f0fAPI\uff0c\u7528\u4e8e\u5c42\u7ec4\u6210\u7684\u6709\u5411\u65e0\u73af\u56fe\uff0c\u53ef\u4ee5\u6784\u5efa\u4efb\u610f\u5f62\u5f0f\u7684\u67b6\u6784  \u4e3a\u4e86\u6709\u4e2a\u5bf9\u6bd4\uff0c\u6211\u4eec\u5148\u4e0d\u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\uff0c\u8ba9\u6a21\u578b\u81ea\u5df1\u8bad\u7ec3\u8bcd\u6743\u91cd\u5411\u91cf\u3002Flatten\u5c42\u7528\u6765\u5c06\u8f93\u5165\u201c\u538b\u5e73\u201d\uff0c\u5373\u628a\u591a\u7ef4\u7684\u8f93\u5165\u4e00\u7ef4\u5316\uff0c\u8fd9\u662f\u5d4c\u5165\u5c42\u7684\u8f93\u51fa\u8f6c\u5165\u5168\u8fde\u63a5\u5c42(Dense)\u7684\u5fc5\u9700\u7684\u8fc7\u6e21\u3002 from keras.models import Sequential from keras.layers import Dense, Flatten  input_dim = x_train.shape[1]  model1 = Sequential() model1.add(Embedding(input_dim=MAX_WORDS_NUM+1,                      output_dim=EMBEDDING_DIM,                      input_length=MAX_SEQUENCE_LEN)) model1.add(Flatten()) model1.add(Dense(64, activation='relu', input_shape=(input_dim,))) model1.add(Dense(64, activation='relu')) model1.add(Dense(len(labels_index), activation='softmax'))  model1.compile(optimizer='rmsprop',               loss='categorical_crossentropy',               metrics=['accuracy'])  history1 = model1.fit(x_train,                      y_train,                     epochs=30,                     batch_size=128,                     validation_data=(x_val, y_val)) Train on 14328 samples, validate on 3582 samples Epoch 1/30 14328/14328 [==============================] - 59s 4ms/step - loss: 3.1273 - acc: 0.2057 - val_loss: 1.9355 - val_acc: 0.2510 Epoch 2/30 14328/14328 [==============================] - 56s 4ms/step - loss: 2.0853 - acc: 0.3349 - val_loss: 1.8037 - val_acc: 0.3473 Epoch 3/30 14328/14328 [==============================] - 56s 4ms/step - loss: 1.7210 - acc: 0.4135 - val_loss: 1.2498 - val_acc: 0.5731 ...... Epoch 29/30 14328/14328 [==============================] - 56s 4ms/step - loss: 0.5843 - acc: 0.8566 - val_loss: 1.3564 - val_acc: 0.6516 Epoch 30/30 14328/14328 [==============================] - 56s 4ms/step - loss: 0.5864 - acc: 0.8575 - val_loss: 0.5970 - val_acc: 0.8501  \u6bcf\u4e2aKeras\u5c42\u90fd\u63d0\u4f9b\u4e86\u83b7\u53d6\u6216\u8bbe\u7f6e\u672c\u5c42\u6743\u91cd\u53c2\u6570\u7684\u65b9\u6cd5\uff1a  layer.get_weights()\uff1a\u8fd4\u56de\u5c42\u7684\u6743\u91cd\uff08numpy array\uff09 layer.set_weights(weights)\uff1a\u4ecenumpy array\u4e2d\u5c06\u6743\u91cd\u52a0\u8f7d\u5230\u8be5\u5c42\u4e2d\uff0c\u8981\u6c42numpy array\u7684\u5f62\u72b6\u4e0elayer.get_weights()\u7684\u5f62\u72b6\u76f8\u540c  embedding_custom = model1.layers[0].get_weights()[0] embedding_custom array([[ 0.39893672, -0.9062594 ,  0.35500282, ..., -0.73564297,          0.50492775, -0.39815223],        [ 0.10640696,  0.18888871,  0.05909824, ..., -0.1642032 ,         -0.02778293, -0.15340094],        [ 0.06566656, -0.04023357,  0.1276007 , ...,  0.04459211,          0.08887506,  0.05389333],        ...,        [-0.12710813, -0.08472785, -0.2296919 , ...,  0.0468552 ,          0.12868881,  0.18596107],        [-0.03790742,  0.09758633,  0.02123675, ..., -0.08180046,          0.10254312,  0.01284804],        [-0.0100647 ,  0.01180602,  0.00446023, ...,  0.04730382,         -0.03696882,  0.00119566]], dtype=float32)  get_weights\u65b9\u6cd5\u5f97\u5230\u7684\u5c31\u662f\u8bcd\u5d4c\u5165\u77e9\u9635\uff0c\u5982\u679c\u672c\u4f8b\u4e2d\u53d6\u7684\u8bcd\u5178\u8db3\u591f\u5927\uff0c\u8fd9\u6837\u7684\u8bcd\u5d4c\u5165\u77e9\u9635\u5c31\u53ef\u4ee5\u4fdd\u5b58\u4e0b\u6765\uff0c\u4f5c\u4e3a\u5176\u4ed6\u4efb\u52a1\u7684\u9884\u8bad\u7ec3\u6a21\u578b\u4f7f\u7528\u3002\u901a\u8fc7get_config()\u53ef\u4ee5\u83b7\u53d6\u6bcf\u4e00\u5c42\u7684\u914d\u7f6e\u4fe1\u606f\uff1a model1.layers[0].get_config() {'activity_regularizer': None,  'batch_input_shape': (None, 1000),  'dtype': 'float32',  'embeddings_constraint': None,  'embeddings_initializer': {'class_name': 'RandomUniform',   'config': {'maxval': 0.05, 'minval': -0.05, 'seed': None}},  'embeddings_regularizer': None,  'input_dim': 20001,  'input_length': 1000,  'mask_zero': False,  'name': 'embedding_13',  'output_dim': 300,  'trainable': True}  \u53ef\u4ee5\u5c06\u6a21\u578b\u8bad\u7ec3\u7684\u7ed3\u679c\u6253\u5370\u51fa\u6765 plot_history(history1)  \u7b2c\u4e00\u4e2a\u6a21\u578b\u8bad\u7ec3\u65f6\u95f4\u82b1\u4e86\u5927\u7ea630\u5206\u949f\u8bad\u7ec3\u5b8c30\u4e2aepoch\uff0c\u8fd9\u662f\u56e0\u4e3a\u6a21\u578b\u9700\u8981\u8bad\u7ec3\u5d4c\u5165\u5c42\u7684\u53c2\u6570\uff0c\u4e0b\u9762\u7b2c\u4e8c\u4e2a\u6a21\u578b\u5728\u7b2c\u4e00\u4e2a\u6a21\u578b\u57fa\u7840\u4e0a\u52a0\u8f7d\u8bcd\u5d4c\u5165\u77e9\u9635\uff0c\u5e76\u5c06\u8bcd\u5d4c\u5165\u77e9\u9635\u8bbe\u4e3a\u4e0d\u53ef\u8bad\u7ec3\uff0c\u770b\u662f\u5426\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u7684\u6548\u7387\u3002 from keras.models import Sequential from keras.layers import Dense, Flatten  input_dim = x_train.shape[1]  model2 = Sequential() model2.add(Embedding(input_dim=MAX_WORDS_NUM+1,                      output_dim=EMBEDDING_DIM,                      weights=[embedding_matrix],                     input_length=MAX_SEQUENCE_LEN,                     trainable=False)) model2.add(Flatten()) model2.add(Dense(64, activation='relu', input_shape=(input_dim,))) model2.add(Dense(64, activation='relu')) model2.add(Dense(len(labels_index), activation='softmax'))  model2.compile(optimizer='rmsprop',               loss='categorical_crossentropy',               metrics=['accuracy'])  history2 = model2.fit(x_train,                      y_train,                     epochs=10,                     batch_size=128,                     validation_data=(x_val, y_val)) Train on 14328 samples, validate on 3582 samples Epoch 1/10 14328/14328 [==============================] - 37s 3ms/step - loss: 1.3124 - acc: 0.6989 - val_loss: 0.7446 - val_acc: 0.8088 Epoch 2/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.2831 - acc: 0.9243 - val_loss: 0.5712 - val_acc: 0.8551 Epoch 3/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.1183 - acc: 0.9704 - val_loss: 0.6261 - val_acc: 0.8624 Epoch 4/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0664 - acc: 0.9801 - val_loss: 0.6897 - val_acc: 0.8607 Epoch 5/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0549 - acc: 0.9824 - val_loss: 0.7199 - val_acc: 0.8660 Epoch 6/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0508 - acc: 0.9849 - val_loss: 0.7261 - val_acc: 0.8582 Epoch 7/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0513 - acc: 0.9865 - val_loss: 0.8251 - val_acc: 0.8585 Epoch 8/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0452 - acc: 0.9858 - val_loss: 0.7891 - val_acc: 0.8707 Epoch 9/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0469 - acc: 0.9865 - val_loss: 0.8663 - val_acc: 0.8680 Epoch 10/10 14328/14328 [==============================] - 35s 2ms/step - loss: 0.0418 - acc: 0.9867 - val_loss: 0.9048 - val_acc: 0.8640  plot_history(history2)  \u4ece\u7b2c\u4e8c\u4e2a\u6a21\u578b\u8bad\u7ec3\u7ed3\u679c\u53ef\u4ee5\u770b\u5230\u9884\u8bad\u7ec3\u6a21\u578b\u7684\u52a0\u8f7d\u53ef\u4ee5\u5927\u5e45\u63d0\u9ad8\u6a21\u578b\u8bad\u7ec3\u7684\u6548\u7387\uff0c\u6a21\u578b\u7684\u9a8c\u8bc1\u51c6\u786e\u5ea6\u4e5f\u63d0\u5347\u7684\u6bd4\u8f83\u5feb\uff0c\u4f46\u662f\u540c\u65f6\u53d1\u73b0\u5728\u8bad\u7ec3\u96c6\u4e0a\u51fa\u73b0\u4e86\u8fc7\u62df\u5408\u7684\u60c5\u51b5\u3002 \u7b2c\u4e09\u4e2a\u6a21\u578b\u7684\u7ed3\u6784\u6765\u81ea\u4e8eKeras\u4f5c\u8005\u7684\u535a\u5ba2\u793a\u4f8b\uff0c\u8fd9\u662fCNN\u7528\u4e8e\u6587\u672c\u5206\u7c7b\u7684\u4f8b\u5b50\u3002 from keras.layers import Dense, Input, Embedding from keras.layers import Conv1D, MaxPooling1D, Flatten from keras.models import Model  embedding_layer = Embedding(input_dim=MAX_WORDS_NUM+1,                             output_dim=EMBEDDING_DIM,                             weights=[embedding_matrix],                             input_length=MAX_SEQUENCE_LEN,                             trainable=False)   sequence_input = Input(shape=(MAX_SEQUENCE_LEN,), dtype='int32') embedded_sequences = embedding_layer(sequence_input) x = Conv1D(128, 5, activation='relu')(embedded_sequences) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation='relu')(x) x = MaxPooling1D(5)(x) x = Conv1D(128, 5, activation='relu')(x) x = MaxPooling1D(35)(x)  # global max pooling x = Flatten()(x) x = Dense(128, activation='relu')(x) preds = Dense(len(labels_index), activation='softmax')(x)  model3 = Model(sequence_input, preds) model3.compile(loss='categorical_crossentropy',               optimizer='rmsprop',               metrics=['acc'])  history3 = model3.fit(x_train,                      y_train,                     epochs=6,                     batch_size=128,                     validation_data=(x_val, y_val)) Train on 14328 samples, validate on 3582 samples Epoch 1/6 14328/14328 [==============================] - 77s 5ms/step - loss: 0.9943 - acc: 0.6719 - val_loss: 0.5129 - val_acc: 0.8582 Epoch 2/6 14328/14328 [==============================] - 76s 5ms/step - loss: 0.4841 - acc: 0.8571 - val_loss: 0.3929 - val_acc: 0.8841 Epoch 3/6 14328/14328 [==============================] - 77s 5ms/step - loss: 0.3483 - acc: 0.8917 - val_loss: 0.4022 - val_acc: 0.8724 Epoch 4/6 14328/14328 [==============================] - 77s 5ms/step - loss: 0.2763 - acc: 0.9100 - val_loss: 0.3441 - val_acc: 0.8942 Epoch 5/6 14328/14328 [==============================] - 76s 5ms/step - loss: 0.2194 - acc: 0.9259 - val_loss: 0.3014 - val_acc: 0.9107 Epoch 6/6 14328/14328 [==============================] - 77s 5ms/step - loss: 0.1749 - acc: 0.9387 - val_loss: 0.3895 - val_acc: 0.8788  plot_history(history3)  \u901a\u8fc7\u52a0\u5165\u6c60\u5316\u5c42MaxPooling1D\uff0c\u964d\u4f4e\u4e86\u8fc7\u62df\u5408\u7684\u60c5\u51b5\u3002\u9a8c\u8bc1\u96c6\u4e0a\u7684\u51c6\u5907\u5ea6\u8d85\u8fc7\u4e86\u524d\u4e24\u4e2a\u6a21\u578b\uff0c\u4e5f\u8d85\u8fc7\u4e86\u4f20\u7edf\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\u3002 \u53c2\u8003\u8d44\u6599  Deep Learning, NLP, and Representations Keras Embedding Layers API How to Use Word Embedding Layers for Deep Learning with Keras Practical Text Classification With Python and Keras Francois Chollet: Using pre-trained word embeddings in a Keras model  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": ["text-classification", "keras", "cnn", "scikit-learn", "machine-learning", "deep-learning", "nlp", "svm", "embedding", "pretrained", "tf-idf", "naive-bayes", "logistic-regression", "text-cnn", "sogou", "corpus", "word2vec"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.sogou.com/labs/resource/list_news.php", "http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a773"}, "repo_url": "https://github.com/kttl15/EE_with_NN_on_Loan_Data", "repo_name": "EE_with_NN_on_Loan_Data", "repo_full_name": "kttl15/EE_with_NN_on_Loan_Data", "repo_owner": "kttl15", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-20T23:10:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T22:40:18Z", "homepage": null, "size": 540, "language": "Python", "has_wiki": false, "license": null, "open_issues_count": 0, "github_id": 187722827, "is_fork": false, "readme_text": "EE_with_NN_on_Loan_Data This is my implementation of entity embedding of categorical data along with the normal numerical data using keras' Functional API. Data is from https://www.kaggle.com/roshansharma/loan-default-prediction The purpose of this kernal is predict, given certain features, whether the loan will default or not. The data is first split into categorical data and numerical data. The numerical data undergoes normalisation and is concatenated with the categorical data to form the featureset. The label is obtained from the original dataframe. The featureset is then vectorised. The neural network has 27 input nodes, each corresponding to a feature, that are all concatenated together, then introduced into the first FC-layer with 16 neurons. The second FC-layer has 10 neurons. Lastly, the output has a single node with a sigmoid activation function. Binary crossentropy was used as the loss function and gradient descent was done by using RMSprop. ###################################################################### EE_with_NN.py is the kernal. model.png is the computation graph. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a774"}, "repo_url": "https://github.com/williamejelias/Nim-RL", "repo_name": "Nim-RL", "repo_full_name": "williamejelias/Nim-RL", "repo_owner": "williamejelias", "repo_desc": "Nim Reinforcement Learning Agents", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T14:25:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T20:32:52Z", "homepage": null, "size": 1264, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187909410, "is_fork": false, "readme_text": "Nim Reinforcement Learning Agents Introduction This repository contains the code that was developed as part of my 3rd year Computer Science research thesis titled \"Analysis of Deep Reinforcement Learning on Nim\". Nim is a two-player combinatorial game with a trivial solution that allows us to know the optimal moves available at each game state with a simple function. The overall goal of the project was to gain a better understanding of why certain reinforcement learning algorithms are successful/unsuccessful by analysing how they learn optimal transitions and partition states into a win/lose classification. Implementation with Keras and the TensorFlow backend of various Reinforcement Learning algorithms for the game of Nim. Agent Algorithms:  Tabular Q-Learning Algorithm Deep Q-Network Deep SARSA Double-Deep Q-Network  Also implemented is a classifier network along with a complete and partial dataset of environment transitions and the classified optimal moves for those game states as labels. This allows a comparison between Reinforcement and Supervised Learning. Usage To use the trainer program, you must have the Nim Environment installed - refer to my other repository gym-nim for installation instructions. cd Nim-RL python3 Trainer.py To tweak experiments, change values within the Trainer program - within this program you can train two agents against each other and evaluate both at regular intervals against a third agent. A self play setting is also given, allowing an agent to train against itself and thus learn both sides of the interactions with the environment ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a775"}, "repo_url": "https://github.com/ninpnin/agglutinative-language-models", "repo_name": "agglutinative-language-models", "repo_full_name": "ninpnin/agglutinative-language-models", "repo_owner": "ninpnin", "repo_desc": "Trying out different agglutinative language models.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T14:38:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T11:06:50Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188021795, "is_fork": false, "readme_text": "Agglutinative Language Models Introduction Due to a plethora of inflections, agglutinative languages, eg. Finnish, tend to have more words than analytic languages, eg. English. What is conveyed in word order and choice in an analytic language is the word changing up a bit in agglutinative languages: to (the) cup  kuppi-in  Even though these two phrases have the same meaning, they have a different number of words. This translates into larger vocabulary sizes of the training datasets, and thus a more difficult classification task. Objectives  Produce meaningful, well structured text Understand words that are outside common vocabulary Be able to include words outside the training data  Solution approaches The core problem of too large vocabularies can be tackled with several strategies. Syllable level solutions kup-piin sii-vo-an  A suitable number of syllables should be chosen as a set of core syllables. The choice of syllables should minimize the redundance of the data: some letter combinations are more common than others, and some are impossible. The better this information can be directly included into the set of syllables, the more resources the neural network can use for understanding higher level patterns Options to consider  Syllable embedding using word2vec or similar Language specific properties that can be included in the syllable embedding What to do with uncommon syllables  Stem level solutions kuppi-in siivo-an  Word level solutions kuppiin siivoan  Implementation and performance metrics All of the solutions are implemented as an LSTM neural network in keras.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a776"}, "repo_url": "https://github.com/flztiii/flower_similarity", "repo_name": "flower_similarity", "repo_full_name": "flztiii/flower_similarity", "repo_owner": "flztiii", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T03:50:45Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T13:36:55Z", "homepage": null, "size": 21, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188046131, "is_fork": false, "readme_text": "flower_similiarity environment requirment python 3.5 cuda 9.0 tensorflow 1.8.0 kera 2.2.0 data prepare refer to the code trainning python3 ImageSimilarity.py test python3 test.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a777"}, "repo_url": "https://github.com/PacktPublishing/Artificial-Intelligence-with-Python-Heuristic-Search", "repo_name": "Artificial-Intelligence-with-Python-Heuristic-Search", "repo_full_name": "PacktPublishing/Artificial-Intelligence-with-Python-Heuristic-Search", "repo_owner": "PacktPublishing", "repo_desc": "Artificial Intelligence with Python \u2013 Heuristic Search, Packt Publishing", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T06:31:00Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-22T11:16:00Z", "homepage": null, "size": 241, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188023123, "is_fork": false, "readme_text": "Artificial Intelligence with Python - Heuristic Search [Video] This is the code repository for Artificial Intelligence with Python - Heuristic Search [Video], published by Packt. It contains all the supporting project files necessary to work through the video course from start to finish. About the Video Course This course is a go-to guide for the topics of logic programming, heuristic search, genetic algorithms, and building games with AI. It will help you learn to program with AI. We start with basic puzzles, parsing trees, and expression matching. Then we\u2019ll build solutions for region coloring and maze solving. The course also has fun-filled videos on building bots to play Tic-tac-toe, Connect Four, and Hexapawn. What You Will Learn   Understand logic programming and how to use it  Build automatic speech recognition systems  Understand the basics of heuristic search and genetic programming  Develop games using Artificial Intelligence  Instructions and Navigation Assumed Knowledge To fully benefit from the coverage included in this course, you will need: Prior knowledge on Python is necessary Technical Requirements This course has the following software requirements: Python 3. Related Products   Artificial Intelligence with Python - Deep Neural Networks [Video]   Artificial Intelligence with Python - Sequence Learning [Video]   Hands-On Artificial Intelligence with Keras and Python [Video]   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a778"}, "repo_url": "https://github.com/SamratPyaraka/ATB", "repo_name": "ATB", "repo_full_name": "SamratPyaraka/ATB", "repo_owner": "SamratPyaraka", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T05:28:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T12:32:42Z", "homepage": null, "size": 2950, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187636902, "is_fork": false, "readme_text": "Overview This is the code for this video on Youtube by Siraj Raval. The author of this code is edwardhdlu . It's implementation of Q-learning applied to (short-term) stock trading. The model uses n-day windows of closing prices to determine if the best action to take at a given time is to buy, sell or sit. As a result of the short-term state representation, the model is not very good at making decisions over long-term trends, but is quite good at predicting peaks and troughs. Results Some examples of results on test sets: !^GSPC 2015 S&P 500, 2015. Profit of $431.04.  Alibaba Group Holding Ltd, 2015. Loss of $351.59.  Apple, Inc, 2016. Profit of $162.73.  Google, Inc, August 2017. Profit of $19.37. Running the Code To train the model, download a training and test csv files from Yahoo! Finance into data/ mkdir model python train ^GSPC 10 1000  Then when training finishes (minimum 200 episodes for results): python evaluate.py ^GSPC_2011 model_ep1000  References Deep Q-Learning with Keras and Gym - Q-learning overview and Agent skeleton code ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a779"}, "repo_url": "https://github.com/RiptideBo/cnnframe", "repo_name": "cnnframe", "repo_full_name": "RiptideBo/cnnframe", "repo_owner": "RiptideBo", "repo_desc": "a simple CNN framework", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T09:03:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T06:28:32Z", "homepage": null, "size": 302, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187775126, "is_fork": false, "readme_text": "cnnframe A simple CNN framework built based on numpy.  How to use Check Here for example Easy to build a CNN or BP model the way like Keras from cnnframe.cnn_frame import Model from cnnframe.layers.conv2d import Conv2D,PoolingLayer,Flatten from cnnframe.layers.dense import Dense  model = Model()  model.add_layer(Conv2D(kernel=[4, 3, 3, 1], stride=[2, 2], padding='same',activation='relu')) model.add_layer(PoolingLayer(kernel_size=(2,2))) model.add_layer(Flatten()) model.add_layer(Dense(30,activation='relu')) model.add_layer(Dense(ydata2.shape[1],activation='sigmoid'))  model.build(xdata2.shape[1:]) model.summary() model.train(xdata2, ydata2, train_round=100,plot_loss=True)  Layers All layers imlement methods:  forward_propagation backward_propagation  The layers:   Conv2D: layer for convolution (kernel, stride ...)   PoolingLayer: layer for pooling   Flatten: layer for flattening, connecting convolution layer and dense layer(common bp layer)   Dense: layer for common BP network    Activation Functions  sigmoid: sigmoid function relu: sure you know what it is softmax: usually used for the last layer  Or add your function in /cnnframe/acfuns.py. oh!  The backward process. It means for now it is not quite  extremely easy to expand more functions. Maybe update this some other days.  Other test Convolution effect How would a input Imgae change after the convolution layers, example here (dependency: PIL, numpy)  Stephen Lee, 245885195@qq.com, 2019,5,21 ", "has_readme": true, "readme_language": "English", "repo_tags": ["cnn", "python", "deep-learning", "neural-network"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a77a"}, "repo_url": "https://github.com/ArthDubey/Predict-Marks", "repo_name": "Predict-Marks", "repo_full_name": "ArthDubey/Predict-Marks", "repo_owner": "ArthDubey", "repo_desc": "This program tries to predict marks of a student based ONLY on a few factors. It uses a Dataset of students with their attributes and their marks. Warning- This is only indicative of the inclination", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T15:29:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T15:05:07Z", "homepage": null, "size": 62, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187861301, "is_fork": false, "readme_text": "Predict-Marks This program tries to predict marks of a student based ONLY on a few factors. It uses a Dataset of students with their attributes and their marks. Warning- This is only indicative of the inclination. Aim This project aims to build a software which can predict marks of a student purely based on statistical basis by employing a neural network to learn from the dataset of students with their attributes and marks. Getting Started Clone the repository in your system and follow these steps-  Install requirements by  pip install -r requirements.txt    Run the file nn.py to train the neural network. (You can also skip this step to use already given weights and biases in repository)  python nn.py    Run the program nnc.py.  python nnc.py    Enjoy your results.  Requirements 1.SciPy 2.Numpy 3.Tensorflow Note- Most of these can be easily installed via pip pip install numpy pip install tensorflow  pip install keras  Installing Clone the repository in your system and follow these steps-  Follow getting started.  Running the tests  Simply install requirements.txt by running  pip install -r requirements.txt    Then run nnc.py.  Contributing Janhavi Subedarpage Authors  Arth Dubey  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ArthDubey/Predict-Marks/blob/6a1796ef32b9f10d73f105274deb4fe3f7fe200f/arthstugrade.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a77b"}, "repo_url": "https://github.com/debreczenidonat/Dice_value_recognition", "repo_name": "Dice_value_recognition", "repo_full_name": "debreczenidonat/Dice_value_recognition", "repo_owner": "debreczenidonat", "repo_desc": "A simple image recognition project", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T11:04:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T10:39:16Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187817115, "is_fork": false, "readme_text": "Dice value recognition A simple image recognition project The experiment is about training a model, that recognizes the value of a thrown dice real-time. The \"eye\" of the system is a raspberry pi camera module, fixed above the dice. The pictures are streamed to another pc, where the classification and visualization of the prediction takes place. The model gives the probability of 7 possible outcomes: one for each face of the dice (from 1 to 6) and an extra 0 value, which is active when no dice is present. You can see the probabilities predicted by the model on the bar chart. The final prediction is the value with the highest bar. For this project, I collected my own training data, by taking photos of the dice with different alignments and face values. Then, I used Keras functional API to define a convolutional neural network (CNN), and train it on the collected pictures. Experimenting with different CNN architectures was way more interesting, than collecting the data. When the system is running, the raspberry pi unit streams the pictures constantly to my pc, where the trained model is deployed. The simultaneously made predictions are shown on the bar chart. Check the video: https://youtu.be/wM-BXt1r3Hs About the files: Dice.py contains my worksheet of the model training stream.py script is the for the pc, which makes the classification during the process streaming_for_rasp.py is the script that runs on the raspberry pi modul ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a77c"}, "repo_url": "https://github.com/uzimahealth/UzimaHealthTensorFlowTrainer", "repo_name": "UzimaHealthTensorFlowTrainer", "repo_full_name": "uzimahealth/UzimaHealthTensorFlowTrainer", "repo_owner": "uzimahealth", "repo_desc": "The Uzima Health Tensor Flow Trainer allows you to be able to train models with a language of your choice, save the trained modes and query the models via a REST api. Read the README file to get started. This part of the project uses Python, Keras, TensorFlow and Flask.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T09:49:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T08:34:09Z", "homepage": null, "size": 42172, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187997045, "is_fork": false, "readme_text": "#1. Install Python install python 3.5 or 3.6 64 bit version & Dependancies https://pip.pypa.io/en/stable/installing/ https://virtualenv.pypa.io/en/stable/installation/ #2. Install Tensorflow https://www.tensorflow.org/install/pip In Windows you may have to install: pip install virtualenvwrapper-win #3. Install Keras #4. To train a language, you can open the luo.txt file and feed in the english phrases and the language you want to translate to separated by a tab space. #5. Run the python files as named in the steps. i.e. 1st step, 2nd step, 3rd step. and the other steps #6. Train and Export the Model in step 3. #7. Host/serve the model using Tensorflow-model server, you can follow steps below for Ubuntu apt-get update apt-get install tensorflow-model-server tensorflow_model_server --version #8. Run the tensorflow model server: a). save the variables and saved_model.pb in Version 1 folder. i.e. your folder structure can be exported_luo_model/1/.... This is done to ensure that the tensorflow model server is able to host and serve the model tensorflow_model_server --model_base_path=/home/ubuntu/exported_luo_model/ --rest_api_port=9000 --model_name=LanguageClassifier #9 Install Python Flask and Flask rest pip install flask pip install flask-restful Startup the flask Server: python api.py Send POST requests to the api with preferred transations http://127.0.0.1:5000 data: 'phrase to translate' ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/uzimahealth/UzimaHealthTensorFlowTrainer/blob/73c8192c6c9f683de7e91ae2f6c1196c7781e9ed/uzimatransator/model.h5"], "see_also_links": ["http://127.0.0.1:5000"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a77d"}, "repo_url": "https://github.com/yecharlie/convnet3d", "repo_name": "convnet3d", "repo_full_name": "yecharlie/convnet3d", "repo_owner": "yecharlie", "repo_desc": "A two-stage 3d convolutional network for object detection in medical image processing...", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T03:37:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T03:15:57Z", "homepage": "", "size": 86, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187954757, "is_fork": false, "readme_text": "convnet3d A two stage 3d convolutional network for object detection in medical image processing... Basically, this project is an (non official) reimplementation of the paper \"Automated Pulmonary Nodule Detection via 3D ConvNets with Online Sample Filtering and Hybrid-Loss Residual Learning\" . However this project varies from what the paper states in the flowing aspects:  Instead of using two batch in the second stage, with one batch for classifiacation, another for regresssion, this model don't perform regression for proposals (thus only one batch left in the second stage model). This model adopts a different input size, (15, 30, 30) on the first stage, (25, 60, 60) on the second stage, which is intened to aneurysm detection. The data augmentation strategy.  If you want to develop a framework exactly as the paper states, you may find all the neccessary components already shiping in. Thus you should merely rewrite a training script and a evaluation script. One suggestion is you should be careful the OOM (Out of Memory) error when combining the evaluation callback with training process. This project is developed on keras and tensorflow and tested on python3.6. In the future, It may add python2.7 support. ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "tensorflow", "object-detection", "ohem", "nms", "3d"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["http://arxiv.org/abs/1708.03867"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a77e"}, "repo_url": "https://github.com/ZeusDee/100ML", "repo_name": "100ML", "repo_full_name": "ZeusDee/100ML", "repo_owner": "ZeusDee", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T08:31:58Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T08:31:02Z", "homepage": null, "size": 10940, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187795209, "is_fork": false, "readme_text": "100-Days-Of-ML-Code 100 Days of Machine Learning Coding as proposed by Siraj Raval Get the datasets from here Data PreProcessing | Day 1 Check out the code from here.    Simple Linear Regression | Day 2 Check out the code from here.    Multiple Linear Regression | Day 3 Check out the code from here.    Logistic Regression | Day 4    Logistic Regression | Day 5 Moving forward into #100DaysOfMLCode today I dived into the deeper depth of what Logistic Regression actually is and what is the math involved behind it. Learned how cost function is calculated and then how to apply gradient descent algorithm to cost function to minimize the error in prediction. Due to less time I will now be posting an infographic on alternate days. Also if someone wants to help me out in documentaion of code and already has some experince in the field and knows Markdown for github please contact me on LinkedIn :) . Implementing Logistic Regression | Day 6 Check out the Code here K Nearest Neighbours | Day 7    Math Behind Logistic Regression | Day 8 #100DaysOfMLCode To clear my insights on logistic regression I was searching on the internet for some resource or article and I came across this article (https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) by Saishruthi Swaminathan. It gives a detailed description of Logistic Regression. Do check it out. Support Vector Machines | Day 9 Got an intution on what SVM is and how it is used to solve Classification problem. SVM and KNN | Day 10 Learned more about how SVM works and implementing the K-NN algorithm. Implementation of K-NN | Day 11 Implemented the K-NN algorithm for classification. #100DaysOfMLCode Support Vector Machine Infographic is halfway complete. Will update it tomorrow. Support Vector Machines | Day 12    Naive Bayes Classifier | Day 13 Continuing with #100DaysOfMLCode today I went through the Naive Bayes classifier. I am also implementing the SVM in python using scikit-learn. Will update the code soon. Implementation of SVM | Day 14 Today I implemented SVM on linearly related data. Used Scikit-Learn library. In Scikit-Learn we have SVC classifier which we use to achieve this task. Will be using kernel-trick on next implementation. Check the code here. Naive Bayes Classifier and Black Box Machine Learning | Day 15 Learned about different types of naive bayes classifiers. Also started the lectures by Bloomberg. First one in the playlist was Black Box Machine Learning. It gives the whole overview about prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning. Implemented SVM using Kernel Trick | Day 16 Using Scikit-Learn library implemented SVM algorithm along with kernel function which maps our data points into higher dimension to find optimal hyperplane. Started Deep learning Specialization on Coursera | Day 17 Completed the whole Week 1 and Week 2 on a single day. Learned Logistic regression as Neural Network. Deep learning Specialization on Coursera | Day 18 Completed the Course 1 of the deep learning specialization. Implemented a neural net in python. The Learning Problem , Professor Yaser Abu-Mostafa | Day 19 Started Lecture 1 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. It was basically an introduction to the upcoming lectures. He also explained Perceptron Algorithm. Started Deep learning Specialization Course 2 | Day 20 Completed the Week 1 of Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization. Web Scraping | Day 21 Watched some tutorials on how to do web scraping using Beautiful Soup in order to collect data for building a model. Is Learning Feasible? | Day 22 Lecture 2 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. Learned about Hoeffding Inequality. Decision Trees | Day 23    Introduction To Statistical Learning Theory | Day 24 Lec 3 of Bloomberg ML course introduced some of the core concepts like input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces. Implementing Decision Trees | Day 25 Check the code here. Jumped To Brush up Linear Algebra | Day 26 Found an amazing channel on youtube 3Blue1Brown. It has a playlist called Essence of Linear Algebra. Started off by completing 4 videos which gave a complete overview of Vectors, Linear Combinations, Spans, Basis Vectors, Linear Transformations and Matrix Multiplication. Link to the playlist here. Jumped To Brush up Linear Algebra | Day 27 Continuing with the playlist completed next 4 videos discussing topics 3D Transformations, Determinants, Inverse Matrix, Column Space, Null Space and Non-Square Matrices. Link to the playlist here. Jumped To Brush up Linear Algebra | Day 28 In the playlist of 3Blue1Brown completed another 3 videos from the essence of linear algebra. Topics covered were Dot Product and Cross Product. Link to the playlist here. Jumped To Brush up Linear Algebra | Day 29 Completed the whole playlist today, videos 12-14. Really an amazing playlist to refresh the concepts of Linear Algebra. Topics covered were the change of basis, Eigenvectors and Eigenvalues, and Abstract Vector Spaces. Link to the playlist here. Essence of calculus | Day 30 Completing the playlist - Essence of Linear Algebra by 3blue1brown a suggestion popped up by youtube regarding a series of videos again by the same channel 3Blue1Brown. Being already impressed by the previous series on Linear algebra I dived straight into it. Completed about 5 videos on topics such as Derivatives, Chain Rule, Product Rule, and derivative of exponential. Link to the playlist here. Essence of calculus | Day 31 Watched 2 Videos on topic Implicit Diffrentiation and Limits from the playlist Essence of Calculus. Link to the playlist here. Essence of calculus | Day 32 Watched the remaining 4 videos covering topics Like Integration and Higher order derivatives. Link to the playlist here. Random Forests | Day 33    Implementing Random Forests | Day 34 Check the code here. But what is a Neural Network? | Deep learning, chapter 1  | Day 35 An Amazing Video on neural networks by 3Blue1Brown youtube channel. This video gives a good understanding of Neural Networks and uses Handwritten digit dataset to explain the concept. Link To the video. Gradient descent, how neural networks learn | Deep learning, chapter 2 | Day 36 Part two of neural networks by 3Blue1Brown youtube channel. This video explains the concepts of Gradient Descent in an interesting way. 169 must watch and highly recommended. Link To the video. What is backpropagation really doing? | Deep learning, chapter 3 | Day 37 Part three of neural networks by 3Blue1Brown youtube channel. This video mostly discusses the partial derivatives and backpropagation. Link To the video. Backpropagation calculus | Deep learning, chapter 4 | Day 38 Part four of neural networks by 3Blue1Brown youtube channel. The goal here is to represent, in somewhat more formal terms, the intuition for how backpropagation works and the video moslty discusses the partial derivatives and backpropagation. Link To the video. Deep Learning with Python, TensorFlow, and Keras tutorial | Day 39 Link To the video. Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2 | Day 40 Link To the video. Convolutional Neural Networks - Deep Learning basics with Python, TensorFlow and Keras p.3 | Day 41 Link To the video. Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4 | Day 42 Link To the video. K Means Clustering | Day 43 Moved to Unsupervised Learning and studied about Clustering. Working on my website check it out avikjain.me Also found a wonderful animation that can help to easily understand K - Means Clustering Link    K Means Clustering Implementation | Day 44 Implemented K Means Clustering. Check the code here. Digging Deeper | NUMPY  | Day 45 Got a new book \"Python Data Science HandBook\" by JK VanderPlas Check the Jupyter notebooks here. Started with chapter 2 : Introduction to Numpy. Covered topics like Data Types, Numpy arrays and Computations on Numpy arrays. Check the code - Introduction to NumPy Understanding Data Types in Python The Basics of NumPy Arrays Computation on NumPy Arrays: Universal Functions Digging Deeper | NUMPY | Day 46 Chapter 2 : Aggregations, Comparisions and Broadcasting Link to Notebook: Aggregations: Min, Max, and Everything In Between Computation on Arrays: Broadcasting Comparisons, Masks, and Boolean Logic Digging Deeper | NUMPY | Day 47 Chapter 2 : Fancy Indexing, sorting arrays, Struchered Data Link to Notebook: Fancy Indexing Sorting Arrays Structured Data: NumPy's Structured Arrays Digging Deeper | PANDAS | Day 48 Chapter 3 : Data Manipulation with Pandas  Covered Various topics like Pandas Objects, Data Indexing and Selection, Operating on Data, Handling Missing Data, Hierarchical Indexing, ConCat and Append. Link To the Notebooks: Data Manipulation with Pandas Introducing Pandas Objects Data Indexing and Selection Operating on Data in Pandas Handling Missing Data Hierarchical Indexing Combining Datasets: Concat and Append Digging Deeper | PANDAS | Day 49 Chapter 3: Completed following topics- Merge and Join, Aggregation and grouping and Pivot Tables. Combining Datasets: Merge and Join Aggregation and Grouping Pivot Tables Digging Deeper | PANDAS | Day 50 Chapter 3: Vectorized Strings Operations, Working with Time Series Links to Notebooks: Vectorized String Operations Working with Time Series High-Performance Pandas: eval() and query() Digging Deeper | MATPLOTLIB | Day 51 Chapter 4: Visualization with Matplotlib Learned about Simple Line Plots, Simple Scatter Plotsand Density and Contour Plots. Links to Notebooks: Visualization with Matplotlib Simple Line Plots Simple Scatter Plots Visualizing Errors Density and Contour Plots Digging Deeper | MATPLOTLIB | Day 52 Chapter 4: Visualization with Matplotlib Learned about Histograms, How to customize plot legends, colorbars, and buliding Multiple Subplots. Links to Notebooks: Histograms, Binnings, and Density Customizing Plot Legends Customizing Colorbars Multiple Subplots Text and Annotation Digging Deeper | MATPLOTLIB | Day 53 Chapter 4: Covered Three Dimensional Plotting in Mathplotlib. Links to Notebooks: Three-Dimensional Plotting in Matplotlib Hierarchical Clustering | Day 54 Studied about Hierarchical Clustering. Check out this amazing Visualization.    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.avikjain.me/", "http://shabal.in/visuals/kmeans/6.html"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a77f"}, "repo_url": "https://github.com/schumali/https-github.com-Avik-Jain-100-Days-Of-ML-Code", "repo_name": "https-github.com-Avik-Jain-100-Days-Of-ML-Code", "repo_full_name": "schumali/https-github.com-Avik-Jain-100-Days-Of-ML-Code", "repo_owner": "schumali", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-22T08:13:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T08:10:49Z", "homepage": null, "size": 10940, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187993079, "is_fork": false, "readme_text": "100-Days-Of-ML-Code 100 Days of Machine Learning Coding as proposed by Siraj Raval Get the datasets from here Data PreProcessing | Day 1 Check out the code from here.    Simple Linear Regression | Day 2 Check out the code from here.    Multiple Linear Regression | Day 3 Check out the code from here.    Logistic Regression | Day 4    Logistic Regression | Day 5 Moving forward into #100DaysOfMLCode today I dived into the deeper depth of what Logistic Regression actually is and what is the math involved behind it. Learned how cost function is calculated and then how to apply gradient descent algorithm to cost function to minimize the error in prediction. Due to less time I will now be posting an infographic on alternate days. Also if someone wants to help me out in documentaion of code and already has some experince in the field and knows Markdown for github please contact me on LinkedIn :) . Implementing Logistic Regression | Day 6 Check out the Code here K Nearest Neighbours | Day 7    Math Behind Logistic Regression | Day 8 #100DaysOfMLCode To clear my insights on logistic regression I was searching on the internet for some resource or article and I came across this article (https://towardsdatascience.com/logistic-regression-detailed-overview-46c4da4303bc) by Saishruthi Swaminathan. It gives a detailed description of Logistic Regression. Do check it out. Support Vector Machines | Day 9 Got an intution on what SVM is and how it is used to solve Classification problem. SVM and KNN | Day 10 Learned more about how SVM works and implementing the K-NN algorithm. Implementation of K-NN | Day 11 Implemented the K-NN algorithm for classification. #100DaysOfMLCode Support Vector Machine Infographic is halfway complete. Will update it tomorrow. Support Vector Machines | Day 12    Naive Bayes Classifier | Day 13 Continuing with #100DaysOfMLCode today I went through the Naive Bayes classifier. I am also implementing the SVM in python using scikit-learn. Will update the code soon. Implementation of SVM | Day 14 Today I implemented SVM on linearly related data. Used Scikit-Learn library. In Scikit-Learn we have SVC classifier which we use to achieve this task. Will be using kernel-trick on next implementation. Check the code here. Naive Bayes Classifier and Black Box Machine Learning | Day 15 Learned about different types of naive bayes classifiers. Also started the lectures by Bloomberg. First one in the playlist was Black Box Machine Learning. It gives the whole overview about prediction functions, feature extraction, learning algorithms, performance evaluation, cross-validation, sample bias, nonstationarity, overfitting, and hyperparameter tuning. Implemented SVM using Kernel Trick | Day 16 Using Scikit-Learn library implemented SVM algorithm along with kernel function which maps our data points into higher dimension to find optimal hyperplane. Started Deep learning Specialization on Coursera | Day 17 Completed the whole Week 1 and Week 2 on a single day. Learned Logistic regression as Neural Network. Deep learning Specialization on Coursera | Day 18 Completed the Course 1 of the deep learning specialization. Implemented a neural net in python. The Learning Problem , Professor Yaser Abu-Mostafa | Day 19 Started Lecture 1 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. It was basically an introduction to the upcoming lectures. He also explained Perceptron Algorithm. Started Deep learning Specialization Course 2 | Day 20 Completed the Week 1 of Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization. Web Scraping | Day 21 Watched some tutorials on how to do web scraping using Beautiful Soup in order to collect data for building a model. Is Learning Feasible? | Day 22 Lecture 2 of 18 of Caltech's Machine Learning Course - CS 156 by Professor Yaser Abu-Mostafa. Learned about Hoeffding Inequality. Decision Trees | Day 23    Introduction To Statistical Learning Theory | Day 24 Lec 3 of Bloomberg ML course introduced some of the core concepts like input space, action space, outcome space, prediction functions, loss functions, and hypothesis spaces. Implementing Decision Trees | Day 25 Check the code here. Jumped To Brush up Linear Algebra | Day 26 Found an amazing channel on youtube 3Blue1Brown. It has a playlist called Essence of Linear Algebra. Started off by completing 4 videos which gave a complete overview of Vectors, Linear Combinations, Spans, Basis Vectors, Linear Transformations and Matrix Multiplication. Link to the playlist here. Jumped To Brush up Linear Algebra | Day 27 Continuing with the playlist completed next 4 videos discussing topics 3D Transformations, Determinants, Inverse Matrix, Column Space, Null Space and Non-Square Matrices. Link to the playlist here. Jumped To Brush up Linear Algebra | Day 28 In the playlist of 3Blue1Brown completed another 3 videos from the essence of linear algebra. Topics covered were Dot Product and Cross Product. Link to the playlist here. Jumped To Brush up Linear Algebra | Day 29 Completed the whole playlist today, videos 12-14. Really an amazing playlist to refresh the concepts of Linear Algebra. Topics covered were the change of basis, Eigenvectors and Eigenvalues, and Abstract Vector Spaces. Link to the playlist here. Essence of calculus | Day 30 Completing the playlist - Essence of Linear Algebra by 3blue1brown a suggestion popped up by youtube regarding a series of videos again by the same channel 3Blue1Brown. Being already impressed by the previous series on Linear algebra I dived straight into it. Completed about 5 videos on topics such as Derivatives, Chain Rule, Product Rule, and derivative of exponential. Link to the playlist here. Essence of calculus | Day 31 Watched 2 Videos on topic Implicit Diffrentiation and Limits from the playlist Essence of Calculus. Link to the playlist here. Essence of calculus | Day 32 Watched the remaining 4 videos covering topics Like Integration and Higher order derivatives. Link to the playlist here. Random Forests | Day 33    Implementing Random Forests | Day 34 Check the code here. But what is a Neural Network? | Deep learning, chapter 1  | Day 35 An Amazing Video on neural networks by 3Blue1Brown youtube channel. This video gives a good understanding of Neural Networks and uses Handwritten digit dataset to explain the concept. Link To the video. Gradient descent, how neural networks learn | Deep learning, chapter 2 | Day 36 Part two of neural networks by 3Blue1Brown youtube channel. This video explains the concepts of Gradient Descent in an interesting way. 169 must watch and highly recommended. Link To the video. What is backpropagation really doing? | Deep learning, chapter 3 | Day 37 Part three of neural networks by 3Blue1Brown youtube channel. This video mostly discusses the partial derivatives and backpropagation. Link To the video. Backpropagation calculus | Deep learning, chapter 4 | Day 38 Part four of neural networks by 3Blue1Brown youtube channel. The goal here is to represent, in somewhat more formal terms, the intuition for how backpropagation works and the video moslty discusses the partial derivatives and backpropagation. Link To the video. Deep Learning with Python, TensorFlow, and Keras tutorial | Day 39 Link To the video. Loading in your own data - Deep Learning basics with Python, TensorFlow and Keras p.2 | Day 40 Link To the video. Convolutional Neural Networks - Deep Learning basics with Python, TensorFlow and Keras p.3 | Day 41 Link To the video. Analyzing Models with TensorBoard - Deep Learning with Python, TensorFlow and Keras p.4 | Day 42 Link To the video. K Means Clustering | Day 43 Moved to Unsupervised Learning and studied about Clustering. Working on my website check it out avikjain.me Also found a wonderful animation that can help to easily understand K - Means Clustering Link    K Means Clustering Implementation | Day 44 Implemented K Means Clustering. Check the code here. Digging Deeper | NUMPY  | Day 45 Got a new book \"Python Data Science HandBook\" by JK VanderPlas Check the Jupyter notebooks here. Started with chapter 2 : Introduction to Numpy. Covered topics like Data Types, Numpy arrays and Computations on Numpy arrays. Check the code - Introduction to NumPy Understanding Data Types in Python The Basics of NumPy Arrays Computation on NumPy Arrays: Universal Functions Digging Deeper | NUMPY | Day 46 Chapter 2 : Aggregations, Comparisions and Broadcasting Link to Notebook: Aggregations: Min, Max, and Everything In Between Computation on Arrays: Broadcasting Comparisons, Masks, and Boolean Logic Digging Deeper | NUMPY | Day 47 Chapter 2 : Fancy Indexing, sorting arrays, Struchered Data Link to Notebook: Fancy Indexing Sorting Arrays Structured Data: NumPy's Structured Arrays Digging Deeper | PANDAS | Day 48 Chapter 3 : Data Manipulation with Pandas  Covered Various topics like Pandas Objects, Data Indexing and Selection, Operating on Data, Handling Missing Data, Hierarchical Indexing, ConCat and Append. Link To the Notebooks: Data Manipulation with Pandas Introducing Pandas Objects Data Indexing and Selection Operating on Data in Pandas Handling Missing Data Hierarchical Indexing Combining Datasets: Concat and Append Digging Deeper | PANDAS | Day 49 Chapter 3: Completed following topics- Merge and Join, Aggregation and grouping and Pivot Tables. Combining Datasets: Merge and Join Aggregation and Grouping Pivot Tables Digging Deeper | PANDAS | Day 50 Chapter 3: Vectorized Strings Operations, Working with Time Series Links to Notebooks: Vectorized String Operations Working with Time Series High-Performance Pandas: eval() and query() Digging Deeper | MATPLOTLIB | Day 51 Chapter 4: Visualization with Matplotlib Learned about Simple Line Plots, Simple Scatter Plotsand Density and Contour Plots. Links to Notebooks: Visualization with Matplotlib Simple Line Plots Simple Scatter Plots Visualizing Errors Density and Contour Plots Digging Deeper | MATPLOTLIB | Day 52 Chapter 4: Visualization with Matplotlib Learned about Histograms, How to customize plot legends, colorbars, and buliding Multiple Subplots. Links to Notebooks: Histograms, Binnings, and Density Customizing Plot Legends Customizing Colorbars Multiple Subplots Text and Annotation Digging Deeper | MATPLOTLIB | Day 53 Chapter 4: Covered Three Dimensional Plotting in Mathplotlib. Links to Notebooks: Three-Dimensional Plotting in Matplotlib Hierarchical Clustering | Day 54 Studied about Hierarchical Clustering. Check out this amazing Visualization.    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.avikjain.me/", "http://shabal.in/visuals/kmeans/6.html"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a780"}, "repo_url": "https://github.com/sinyyl/Neural_Network_Behavioral_Cloning", "repo_name": "Neural_Network_Behavioral_Cloning", "repo_full_name": "sinyyl/Neural_Network_Behavioral_Cloning", "repo_owner": "sinyyl", "repo_desc": "In this project, I used CNN to train the model to imitate the driving behavioral of human driving the vehicle in simulator.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T23:20:16Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T06:03:41Z", "homepage": null, "size": 59237, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187577063, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/sinyyl/Neural_Network_Behavioral_Cloning/blob/40de046426104aeb4c6cd69c6685e5a943c2d373/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a781"}, "repo_url": "https://github.com/jmhummel/MNIST-digits", "repo_name": "MNIST-digits", "repo_full_name": "jmhummel/MNIST-digits", "repo_owner": "jmhummel", "repo_desc": "Training a Convolutional Neural Net on the MNIST digits data set, using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T06:46:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T06:26:14Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187976099, "is_fork": false, "readme_text": "MNIST-digits ", "has_readme": true, "readme_language": "Occitan (post 1500)", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a782"}, "repo_url": "https://github.com/luulinh90s/NPRF-python3", "repo_name": "NPRF-python3", "repo_full_name": "luulinh90s/NPRF-python3", "repo_owner": "luulinh90s", "repo_desc": "NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval, a fork from https://github.com/ucasir/NPRF", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T06:45:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T06:41:00Z", "homepage": null, "size": 2252, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 187582158, "is_fork": false, "readme_text": "NPRF NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval [pdf]    If you use the code, please cite the following paper: @inproceedings{li2018nprf,   title={NPRF: A Neural Pseudo Relevance Feedback Framework for Ad-hoc Information Retrieval},   author={Li, Canjia and Sun, Yingfei and He, Ben and Wang, Le and Hui, Kai and Yates, Andrew and Sun, Le and Xu, Jungang},   booktitle={Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},   year={2018} }  Requirement  Tensorflow Keras gensim numpy  Getting started Training data preparation To capture the top-k terms from top-n documents, one needs to extract term document frequency from index. Afterwards, you are required to generate the similarity matrix upon the query and document given the pre-trained word embedding (e.g. word2vec). Related functions can be found in preprocess/prepare_d2d.py. Training meta data preparation We introduce two classes for the ease of training. The class Relevance incorporates the relevance information from the baseline and qrels file. The class Result simplify the write/read operation on standard TREC result file. Other information like query idf is dumped as a pickle file. Model training Configure the MODEL_config.py file, then run python MODEL.py --fold fold_number temp_file_path  You need to run 5-fold cross valiation, which can be automatically done by running the runfold.sh script. The temp file is a temporary file to write the result of the validation set in TREC format. A training log sample on the first fold of TREC 1-3 dataset is provided for reference, see sample_log. Evaluation After training, the evaluation result of each fold is retained in the result path as you specify in the MODEL_config.py file. One can simply run cat *res >> merge_file to merge results from all folds. Thereafter, run the trec_eval script to evaluate your model. Reference Some snippets of the code follow the implementation of K-NRM, MatchZoo. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://aclweb.org/anthology/D18-1478"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a783"}, "repo_url": "https://github.com/XJay18/QuickDraw-pytorch", "repo_name": "QuickDraw-pytorch", "repo_full_name": "XJay18/QuickDraw-pytorch", "repo_owner": "XJay18", "repo_desc": "This is a demo for CNN models training on quickdraw dataset. Implemented with pytorch.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T05:00:07Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T13:54:25Z", "homepage": null, "size": 24, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 187650450, "is_fork": false, "readme_text": "QuickDraw-pytorch This is a demo for CNN models training on Quick, Draw! dataset. Implemented with pytorch \ud83d\udd25. Quick, Draw! Quick, Draw! dataset is a collection of 50 million drawings across 345 categories, provided by googlecreativelab. The demo only uses at most 5000 samples from each of the 345 categories. In total, it is trained with 1380000 samples. Pytorch Implementation \ud83d\udc47 Here are the step-by-step tutorials.   Clone the repo to your local device. git clone https://github.com/XJay18/QuickDraw-pytorch.git   Download data from google and generate train&test dataset. You can run this command for example: python ./DataUtils/prepare_data.py -c 10 -d 1 -show   \ud83d\udca1 hint:  -c for how many categories you want to download, available choices: 10, 30, 100, all. Note that all is 345 categories. -d 1 means that download the data from internet, and -d 0 means that not download data and just generate train and test dataset from your pre-download data. -show means that show some random images while generating the dataset.    Start training and evaluating for example. python main.py --ngpu 0 -m convnet -e 5   \ud83d\udd11 Please refer to main.py to see the detailed usage. Reference   Train a model in tf.keras with Colab, and run it in the browser with TensorFlow.js   tfjs-converter   pytorch2keras (may be used in future since the current demo is not deployed on web using the first Reference)   TODO    Devise or revise the current model to achieve higher accuracy on Quick, Draw!.    Enlarge the used dataset (i.e, choose more samples from each categories of the dataset).    Deploy the demo on web.   Purpose \ud83d\ude03 I started this project with the purpose of improving my ability in coding quickly \ud83d\ude80. Also the project will serve as a push on my way to learning more knowledge and experience from others\u2b50\ufe0f. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a784"}, "repo_url": "https://github.com/Warfnir/Dog-Breed-Recognizer", "repo_name": "Dog-Breed-Recognizer", "repo_full_name": "Warfnir/Dog-Breed-Recognizer", "repo_owner": "Warfnir", "repo_desc": "Dog Breed Recognizer made with Keras. Dataset from  kaggle Stanford Dogs Dataset.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T11:46:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T11:34:47Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187628383, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a785"}, "repo_url": "https://github.com/matthew-mcateer/loss_landscapes_tf-keras", "repo_name": "loss_landscapes_tf-keras", "repo_full_name": "matthew-mcateer/loss_landscapes_tf-keras", "repo_owner": "matthew-mcateer", "repo_desc": "Tensorflow/Keras-compatible implementation of https://github.com/tomgoldstein/loss-landscape", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T13:51:38Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T21:21:21Z", "homepage": null, "size": 37891, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188115474, "is_fork": false, "readme_text": "Visualizing the Loss Landscape of Neural Nets This repository contains the tf.Keras implementation of the code for the paper  Hao Li, Zheng Xu, Gavin Taylor, Christoph Studer and Tom Goldstein. Visualizing the Loss Landscape of Neural Nets. NIPS, 2018.  An interactive 3D visualizer for loss surfaces has been provided by telesens. Given a network architecture and its pre-trained parameters, this tool calculates and visualizes the loss surface along random direction(s) near the optimal parameters. The calculation can be done in parallel with multiple GPUs per node, and multiple nodes. The random direction(s) and loss surface values are stored in HDF5 (.h5) files after they are produced. Setup Environment: One or more multi-GPU node(s) with the following software/libraries installed:  Tensorflow 1.13 numpy 1.15.1 h5py 2.7.0 matplotlib 2.0.2 scipy 0.19  Data preprocessing: The data pre-processing method used for visualization should be consistent with the one used for model training. No data augmentation (random cropping or horizontal flipping) is used in calculating the loss values. Visualizing 1D loss curve Creating 1D linear interpolations The 1D linear interpolation method [1] evaluates the loss values along the direction between two minimizers of the same network loss function. This method has been used to compare the flatness of minimizers trained with different batch sizes [2]. A 1D linear interpolation plot is produced using the plot_surface.py method. mpirun -n 4 python plot_surface.py --mpi --cuda --model vgg9 --x=-0.5:1.5:401 --dir_type states \\ --model_file cifar10/trained_nets/vgg9_sgd_lr=0.1_bs=128_wd=0.0_save_epoch=1/model_300.t7 \\ --model_file2 cifar10/trained_nets/vgg9_sgd_lr=0.1_bs=8192_wd=0.0_save_epoch=1/model_300.t7 --plot   --x=-0.5:1.5:401 sets the range and resolution for the plot.  The x-coordinates in the plot will run from -0.5 to 1.5 (the minimizers are located at 0 and 1), and the loss value will be evaluated at 401 locations along this line. --dir_type states indicates the direction contains dimensions for all parameters as well as the statistics of the BN layers (running_mean and running_var). Note that ignoring running_mean and running_var cannot produce correct loss values when plotting two solutions togeather in the same figure. The two model files contain network parameters describing the two distinct minimizers of the loss function.  The plot will interpolate between these two minima.   Producing plots along random normalized directions A random direction with the same dimension as the model parameters is created and \"filter normalized.\" Then we can sample loss values along this direction. mpirun -n 4 python plot_surface.py --mpi --cuda --model vgg9 --x=-1:1:51 \\ --model_file cifar10/trained_nets/vgg9_sgd_lr=0.1_bs=128_wd=0.0_save_epoch=1/model_300.t7 \\ --dir_type weights --xnorm filter --xignore biasbn --plot   --dir_type weights indicates the direction has the same dimensions as the learned parameters, including bias and parameters in the BN layers. --xnorm filter normalizes the random direction at the filter level. Here, a \"filter\" refers to the parameters that produce a single feature map.  For fully connected layers, a \"filter\" contains the weights that contribute to a single neuron. --xignore biasbn ignores the direction corresponding to bias and BN parameters (fill the corresponding entries in the random vector with zeros).   We can also customize the appearance of the 1D plots by calling plot_1D.py once the surface file is available. Visualizing 2D loss contours To plot the loss contours, we choose two random directions and normalize them in the same way as the 1D plotting. mpirun -n 4 python plot_surface.py --mpi --cuda --model resnet56 --x=-1:1:51 --y=-1:1:51 \\ --model_file cifar10/trained_nets/resnet56_sgd_lr=0.1_bs=128_wd=0.0005/model_300.t7 \\ --dir_type weights --xnorm filter --xignore biasbn --ynorm filter --yignore biasbn  --plot   Once a surface is generated and stored in a .h5 file, we can produce and customize a contour plot using the script plot_2D.py. python plot_2D.py --surf_file path_to_surf_file --surf_name train_loss   --surf_name specifies the type of surface. The default choice is train_loss, --vmin and --vmax sets the range of values to be plotted. --vlevel sets the step of the contours.  Visualizing 3D loss surface plot_2D.py can make a basic 3D loss surface plot with matplotlib. If you want a more detailed rendering that uses lighting to display details, you can render the loss surface with ParaView.   To do this, you must  Convert the surface .h5 file to a .vtp file.  python h52vtp.py --surf_file path_to_surf_file --surf_name train_loss --zmax  10 --log  This will generate a VTK file containing the loss surface with max value 10 in the log scale.  Open the .vtp file with ParaView. In ParaView, open the .vtp file with the VTK reader. Click the eye icon in the Pipeline Browser to make the figure show up. You can drag the surface around, and change the colors in the Properties window.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/matthew-mcateer/loss_landscapes_tf-keras/blob/0874ba2dbccb2522d72e6abc8aa103218a5a5c76/mnist_shallow_cnn.h5_weights_xignore=biasbn_xnorm=filter_yignore=biasbn_ynorm=filter.h5_%5B-1.0,1.0,26%5Dx%5B-1.0,1.0,26%5D.h5", "https://github.com/matthew-mcateer/loss_landscapes_tf-keras/blob/0874ba2dbccb2522d72e6abc8aa103218a5a5c76/mnist_shallow_cnn.h5_weights_xignore=biasbn_xnorm=filter_yignore=biasbn_ynorm=filter.h5_%5B-1.0,1.0,51%5Dx%5B-1.0,1.0,51%5D.h5", "https://github.com/matthew-mcateer/loss_landscapes_tf-keras/blob/0874ba2dbccb2522d72e6abc8aa103218a5a5c76/mnist_shallow_cnn.h5", "https://github.com/matthew-mcateer/loss_landscapes_tf-keras/blob/0874ba2dbccb2522d72e6abc8aa103218a5a5c76/mnist_shallow_cnn.h5_weights_xignore=biasbn_xnorm=filter_yignore=biasbn_ynorm=filter.h5"], "see_also_links": ["http://paraview.org", "http://www.telesens.co/2019/01/16/neural-network-loss-visualization/", "http://www.telesens.co/loss-landscape-viz/viewer.html", "http://docs.h5py.org/en/stable/build.html#install"], "reference_list": ["https://arxiv.org/abs/1712.09913"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a786"}, "repo_url": "https://github.com/agukwt/Keras_Basic", "repo_name": "Keras_Basic", "repo_full_name": "agukwt/Keras_Basic", "repo_owner": "agukwt", "repo_desc": "Keras\u306e\u4f7f\u3044\u3053\u306a\u3057\u306e\u305f\u3081\u306e\u81ea\u5df1\u5b66\u7fd2\u30b3\u30fc\u30c9", "description_language": "Japanese", "repo_ext_links": null, "repo_last_mod": "2019-05-21T15:50:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T15:50:06Z", "homepage": null, "size": 67, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187869239, "is_fork": false, "readme_text": "\u300c\u76f4\u611fDeepLearning\u300d\u306b\u95a2\u3059\u308b\u5b66\u7fd2 \u5b66\u7fd2\u65b9\u6cd5  [1] \u5185\u5bb9\u306b\u3064\u3044\u3066\u8aad\u4e86 [2] \u30b3\u30fc\u30c9\u3092\u307e\u305a\u5b9f\u884c [3] \u30b3\u30fc\u30c9\u5185\u3067\u6c17\u306b\u306a\u308b\u3053\u3068\u30e1\u30e2 [4] \u6c17\u306b\u306a\u308b\u3053\u3068\u3092\u30c6\u30fc\u30de\u5316\u3057\u3066\u81ea\u3089\u89e3\u304f <- \u30b3\u30b3\u3092UP   \uff11\u30c6\u30fc\u30de \\ \uff11\u30c7\u30a3\u30ec\u30af\u30c8\u30ea  ", "has_readme": true, "readme_language": "Japanese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a787"}, "repo_url": "https://github.com/nialloh23/county-web-app", "repo_name": "county-web-app", "repo_full_name": "nialloh23/county-web-app", "repo_owner": "nialloh23", "repo_desc": "A simple web UI for demoing a keras image classification model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T17:25:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T16:18:04Z", "homepage": null, "size": 9038, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187675304, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a788"}, "repo_url": "https://github.com/afabijanska/CornealEndothelium", "repo_name": "CornealEndothelium", "repo_full_name": "afabijanska/CornealEndothelium", "repo_owner": "afabijanska", "repo_desc": "Segmentation of Corneal Endothelium Images with U-Net", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T16:22:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T22:03:45Z", "homepage": null, "size": 53, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188119783, "is_fork": false, "readme_text": "Corneal Endothelium Image Segmentation This repopsitory contains a source code of algorithm for corneal endothelium image segmentation with U-Net based convolutional neural network. The source code may be used for non-commercial research provided you acknowledge the source by citing the following paper:   Fabija\u0144ska A.: Segmentation of Corneal Endothelium Images Using a U-Net-based Convolutional Neural Network, Artificial Intelligence In Medicine, vol. 88, pp. 1-13, 2018, doi:10.1016/j.artmed.2018.04.004  @article{Fabijanska2018,   author  = {Anna Fabija\\'{n}ska},    title   = {Segmentation of corneal endothelium images using a U-Net-based convolutional neural network},   journal = {Artificial Intelligence in Medicine},   volume  = {88},   number  = {},   pages   = {1-13},   year    = {2018},   note    = {},   issn    = {0933-3657},   doi    = {https://doi.org/10.1016/j.artmed.2018.04.004},    url    = {https://www.sciencedirect.com/science/article/pii/S0933365718300575} } Running the code Prerequisites Python 3.6, Tensorflow, Keras Data organization Organize your data as below. For training, keep the filenames consistent (an original image and the correspongind ground truth should be files of the same name saved in diifferent locations).  \u251c\u2500\u2500\u2500project_dir     \u2514\u2500\u2500\u2500data                    # data directory         \u2514\u2500\u2500\u2500test                # test images         |   \u2514\u2500\u2500\u2500org             # original images of corneal endothelium         |   |   testFile1.png          |   |   testFile2.png          |   |   testFile3.png          |   \u2514\u2500\u2500\u2500preds           # images predicted by the network         \u2514\u2500\u2500\u2500train               # train images             \u2514\u2500\u2500\u2500org             # original images of corneal endothelium             |   trainFile1.png              |   trainFile2.png              |   trainFile3.png              \u2514\u2500\u2500\u2500bw              # ground truths (black = 0, white = 255)                 trainFile1.png                  trainFile2.png                  trainFile3.png   Repository content   configuration.txt - file to be edited; contains data paths and train/test setings     prepare_train_set.py - script for extracting random patches from train images and saving them as hdf5 files; patches are extracted from images from an indicated directory ensuring an equal number of patches sampled from each image (to be run first)    training.py - script for training U-Net with patches loaded from hdf5 files (to be run second)  predict.py - script for performing image segmentation; segmentation is performed for images from an indicated directory (to be run third)    helpers.py - some helper functions for reading/writing data  Contact Anna Fabija\u0144ska  Institute of Applied Computer Science  Lodz University of Technology  e-mail: anna.fabijanska@p.lodz.pl  WWW: http://an-fab.kis.p.lodz.pl ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://an-fab.kis.p.lodz.pl"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a789"}, "repo_url": "https://github.com/471417367/Roy_func", "repo_name": "Roy_func", "repo_full_name": "471417367/Roy_func", "repo_owner": "471417367", "repo_desc": "\u4e2a\u4eba\u5c01\u88c5\u7684\u4e00\u4e9b\u6570\u636e\u5206\u6790\u5e38\u7528\u65b9\u6cd5", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-20T12:03:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T11:55:41Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187631257, "is_fork": false, "readme_text": "\u62c9\u683c\u6717\u65e5\u63d2\u503c\u6cd5\u5904\u7406NaN\u503c from lagrange_nan import lagrange_df, lagrange_df_col lagrange_df(df) \uff1a\u6240\u6709\u5217\u7684NaN\u8fdb\u884clagrange\u8fdb\u884c\u63d2\u503c lagrange_df_col(df_col) \uff1a\u6307\u5b9a\u5217\u7684NaN\u8fdb\u884clagrange\u8fdb\u884c\u63d2\u503c \u65e0\u8fd4\u56de\u503c  \u5c5e\u6027\u538b\u7f29\uff0c\u5f52\u4e00\u5316\u548c\u6807\u51c6\u5316 from standard_minmax_scaler import standard, minmax standard(df) \uff1a\u5c06df\u4e2d\u7684\u6240\u6709\u6570\u636e\u6807\u51c6\u5316\uff0c\u5e76\u8fd4\u56de minmax(df) \uff1a\u5c06df\u4e2d\u7684\u6240\u6709\u6570\u636e\u5f52\u4e00\u5316\uff0c\u5e76\u8fd4\u56de  \u5212\u5206\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6 from train_test_split import train_test train_test(df, size) :x_train, x_test, y_train, y_test size = 0.2 ,\u8868\u793a\u6d4b\u8bd5\u96c620%\uff0c\u8bad\u7ec3\u96c680%  \u4ea4\u53c9\u9a8c\u8bc1 from cross_val_score import cross_vs cross_vs(clf, x, y, cv) \uff1aclf\u4e3a\u5206\u7c7b\u5668\uff08\u8fd8\u6ca1\u6709\u8bad\u7ec3fit\uff09\uff0cx,y\u662f\u9a8c\u8bc1\u96c6\uff0ccv\u662f\u51e0\u6298\u4ea4\u53c9\u9a8c\u8bc1\u3002\u8fd4\u56de\u4e00\u4e2a\u6570\u7ec4[cv\u4e2a\u9a8c\u8bc1\u51c6\u786e\u7387\u7ed3\u679c] cross_k \uff1a\u4f7f\u7528\u6b64\u51fd\u6570\u9700\u8981\u7ec6\u770b\u3002\u5229\u7528\u4ea4\u53c9\u9a8c\u8bc1\u627e\u6700\u4f73\u53c2\u6570  \u53ef\u89c6\u5316\u6df7\u6dc6\u77e9\u9635 from confusion_matrix import cm_plt, cm_plt_net cm_plt(clf, x_train, y_train) :\u903b\u8f91\u56de\u5f52\uff0cSVM\uff0c\u51b3\u7b56\u6811\u3002\u65e0\u8fd4\u56de\u503c\uff0c\u76f4\u63a5\u51fa\u56fe cm_plt_net(net, x_train, y_train) \uff1a\u795e\u7ecf\u5143\u7f51\u7edc\u6a21\u578b\u3002\u65e0\u8fd4\u56de\u503c\uff0c\u76f4\u63a5\u51fa\u56fe  \u795e\u7ecf\u7f51\u7edc\u6a21\u578b from kares_model import keras_seq keras_seq(input_d, x_train, y_train) :\u8fd4\u56de\u6a21\u578b net  KMeans\u805a\u7c7b\u6a21\u578b from k_means import k_means k_means(df, k, feature) \uff1a\u805a\u7c7bK\u4e2a\uff0c\u65e0\u8fd4\u56de\u503c  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a78a"}, "repo_url": "https://github.com/PacktPublishing/Tensorflow-Deep-Learning-Solutions-for-Images", "repo_name": "Tensorflow-Deep-Learning-Solutions-for-Images", "repo_full_name": "PacktPublishing/Tensorflow-Deep-Learning-Solutions-for-Images", "repo_owner": "PacktPublishing", "repo_desc": "Tensorflow Deep Learning Solutions for Images, Published by Packt", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T06:39:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T05:46:24Z", "homepage": null, "size": 9, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187769627, "is_fork": false, "readme_text": "Tensorflow Deep Learning Solutions for Images This is the code repository for Tensorflow Deep Learning Solutions for Images, published by Packt. It contains all the supporting project files necessary to work through the video course from start to finish. About the Video Course Tensorflow is Google\u2019s popular offering for machine learning and deep learning. It has quickly become a popular choice of tool for performing fast, efficient, and accurate deep learning. This course presents the implementation of practical, real-world projects, teaching you how to leverage Tensforflow\u2019s capabilties to perform efficient deep learning. In this video, you will be acquainted with the different paradigms of performing deep learning such as deep neural nets, convolutional neural networks, recurrent neural networks, and more, and how they can be implemented using Tensorflow. This will be demonstrated with the help of end-to-end implementations of three real-world projects on popular topic areas such as natural language processing, image classification, fraud detection, and more. By the end of this course, you will have mastered all the concepts of deep learning and their implementation with Tensorflow and Keras. What You Will Learn   Understand what TensorFlow is, how TensorFlow works, from basics to advanced level with case-study based approach.  Understand neural networks and how to implement them with TensorFlow via Churn Prediction Case Study.  Implement a convolution neural network in TensorFlow for pneumonia detection from the x-ray case study.  Implement a recurrent neural network for stock price prediction case study and improving accuracy with long short-term memory network.  Learn about TensorBoard for monitoring, transformer, eager execution and debugging code with TensorFlow.  Build Transfer learning in Tensorflow using TFlearn via object detection and opinion mining model.  Instructions and Navigation Assumed Knowledge To fully benefit from the coverage included in this course, you will need: This course is for application developers looking to integrate machine learning into application software and master deep learning by implementing practical projects in Tensorflow. Knowledge of Python programming and the basics of deep learning are required to get the best out of this video. Technical Requirements This course has the following software requirements:  CUDA Toolkit 16.04 4GB RAM 2GB Graphic memory 500 GB Hard disk storage  Related Products   Hands-On Deep Learning with TensorFlow 2.0 [Video]   Hands-on Deep Learning with TensorFlow [Video]   Hands-On Deep Learning with TensorFlow 2.0 [Video]   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a78b"}, "repo_url": "https://github.com/kahotsang/image-captioning", "repo_name": "image-captioning", "repo_full_name": "kahotsang/image-captioning", "repo_owner": "kahotsang", "repo_desc": "Simple image-captioning model using Flickr8K dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T02:15:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T01:29:51Z", "homepage": null, "size": 1754417, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 1, "github_id": 187548089, "is_fork": false, "readme_text": "Image-captioning model based on Flickr8K dataset Objectives: To build a simple image-captioning model using pre-trained CNN model and LSTM model, based on the Flickr8K dataset. Please note that this project is not striving for a well-optimized and industrial-level model (thus some of the generated captions are not making sense), but just as an illustration on how to construct a deep image-captioning model (for learning purposes). Dataset Descriptions: Flickr8K dataset contains 8000 images and their reference captions. Originally, you should download a request form to request for the dataset. However, the official site seems to have been taken down, thus I also include the dataset in this project for downloading. Sample Captions: The followings are some sample captions generated by my sample_model.     Structure: build_model: It contains the files for constructing, training and evaluating the deep-learning model. caption_generator.py: It contains the code to use the built model to generate captions for the images in sample_images. Installation Requirement: In this project, I use python and Tensorflow (Keras) to build the deep-learning model. Please make sure that you have Tensorflow (no need to have GPU version) installed in your computer. Methodology For the architecture of the image-captioning model, I stick with the merge model implementation proposed by Tanti, et al. (2017). Please refer to this paper for more details. Some Possible Extensions   Use an image-captioning dataset with a larger size.   Not only use the pre-trained CNN model as feature extractor, but also fine-tune it during training.   Consider using other pre-trained CNN model.   Use pre-trained word2vec model instead of training the embedding layer from scratch.   References M. Tanti, A. Gatt and K.P. Camilleri (2017). Where to put the Image in an Image Caption Generator. arXiv: https://arxiv.org/abs/1703.09137 M. Hodosh, P. Young and J. Hockenmaier (2013). Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics. Journal of Artificial Intelligence Research, Volume 47, pages 853-899. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/kahotsang/image-captioning/blob/904818d0291283ffd8d3f1f62d85d68a80216f95/sample_model.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v0_devloss_4.12_trainloss_4.35.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v10_devloss_3.46_trainloss_3.27.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v11_devloss_3.38_trainloss_3.25.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v12_devloss_3.45_trainloss_3.23.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v13_devloss_3.37_trainloss_3.2.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v14_devloss_3.37_trainloss_3.18.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v15_devloss_3.4_trainloss_3.15.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v16_devloss_3.4_trainloss_3.15.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v17_devloss_3.4_trainloss_3.12.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v18_devloss_3.38_trainloss_3.11.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v19_devloss_3.38_trainloss_3.1.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v1_devloss_3.79_trainloss_3.97.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v2_devloss_3.65_trainloss_3.75.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v3_devloss_3.58_trainloss_3.64.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v4_devloss_3.54_trainloss_3.55.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v5_devloss_3.5_trainloss_3.48.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v6_devloss_3.49_trainloss_3.42.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v7_devloss_3.47_trainloss_3.37.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v8_devloss_3.43_trainloss_3.34.h5", "https://github.com/kahotsang/image-captioning/blob/2e446d762e10ca2d615c50b5c7425fe44c31f7ec/model/model0_v9_devloss_3.46_trainloss_3.3.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1703.09137", "https://arxiv.org/abs/1703.09137"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a78c"}, "repo_url": "https://github.com/Sakib-Abrar/Keras-Tensorflow-Predictions", "repo_name": "Keras-Tensorflow-Predictions", "repo_full_name": "Sakib-Abrar/Keras-Tensorflow-Predictions", "repo_owner": "Sakib-Abrar", "repo_desc": "A machine learning project using Keras. The target is to predict cricket match scores based on different variables", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T18:17:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T18:14:39Z", "homepage": null, "size": 142, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187890776, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Sakib-Abrar/Keras-Tensorflow-Predictions/blob/ab21d7edf2d07758984543189d5bdfc2f6e993f1/model/trained_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a78d"}, "repo_url": "https://github.com/Lornatang/Generative-Adversarial-Networks", "repo_name": "Generative-Adversarial-Networks", "repo_full_name": "Lornatang/Generative-Adversarial-Networks", "repo_owner": "Lornatang", "repo_desc": "Tensorflow implements the most primitive GAN", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T23:58:04Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T11:27:09Z", "homepage": null, "size": 12373, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188024780, "is_fork": false, "readme_text": "Generative-Adversarial-Networks paper Author: Lorna Email: shiyipaisizuo@gmail.com Chinese version Requirements  GPU: A TiTAN V or later. Disk: 128G SSD. Python version: python3.5 or later. CUDA: cuda10. CUDNN: cudnn7.4.5 or later. Tensorflow-gpu: 2.0.0-alpla0.  Run this command. pip install -r requirements.txt  What are GANs? Generative Adversarial Networks (GANs) are one of the most interesting ideas in computer science today. Two models are trained simultaneously by an adversarial process. A generator (\"the artist\") learns to create images that look real, while a discriminator (\"the art critic\") learns to tell real images apart from fakes.  During training, the generator progressively becomes better at creating images that look real, while the discriminator becomes better at telling them apart. The process reaches equilibrium when the discriminator can no longer distinguish real images from fakes.  The following animation shows a series of images produced by the generator as it was trained for 50 epochs. The images begin as random noise, and increasingly resemble hand written digits over time.  1.Introduction 1.1 Theory This is the flow chart of GAN.  GAN's main source of inspiration is zero-sum game thoughts in game theory, is applied to the deep learning neural network, is the by generating network G (Generator) and discriminant D (Discriminator) network game constantly, thus make G learn data distribution, if used on the image to generate the training is completed, G can generate lifelike image from a random number. The main functions of G and D are:  G is a generating network, it receives a random noise z (random number), through the noise to generate images. D is a network for judging whether an image is \"real\". Its input parameter is x, x represents a picture, and the output D (x) represents the probability that x is a real picture. If it is 1, it represents 100% real picture, while if it is 0, it represents an impossible picture.  In the process of training, the goal of generating network G is to generate real images as much as possible to cheat network D. And the goal of D is to try to distinguish the fake image generated by G from the real one. In this way, G and D constitute a dynamic \"game process\", and the final equilibrium point is the Nash equilibrium point.. 1.2 Architecture By optimizing the target, we can adjust the parameter of the probability generation model, so that the probability distribution and the real data distribution can be as close as possible. So how do you define an appropriate optimization goal or a loss? In the traditional generation model, the likelihood of data is generally adopted as the optimization target, but GAN innovatively USES another optimization target.  Firstly, it introduces a discriminant model (common ones include support vector machine and multi-layer neural network). Secondly, its optimization process is to find a Nash equilibrium between the generative model and the discriminant model.  A learning framework established by GAN is actually a simulation game between generating model and discriminating model. The purpose of generating models is to imitate, model and learn the distribution law of real data as much as possible. The discriminant model is to determine whether an input data obtained by itself comes from a real data distribution or a generated model. Through the continuous competition between these two internal models, the ability to generate and distinguish the two models is improved. When a model has very strong ability to distinguish. if the generated data of the model can still be confused and cannot be judged correctly, then we think that the generated model has actually learned the distribution of real data. 1.3 GAN characteristics characteristics:   low compared to the traditional model, there are two different networks, rather than a single network, USES a confrontation training methods and training ways.   low GAN gradient G in the update information from discriminant D, rather than from sample data.   advantages:   low GAN is an emergent model, compared to other generation model (boltzmann machine and GSNs) only by back propagation, without the need for a complicated markov chain.   low compared to all other model, GAN can produce more clearly, the real sample   low GAN is a kind of unsupervised learning training, and can be widely used in the field of a semi-supervised learning and unsupervised learning.   Compared with the variational self-encoder, GANs does not introduce any deterministic bias, and the variational methods introduce deterministic bias, because they optimize the lower bound of logarithmic likelihood rather than the likelihood itself, which seems to cause the instance generated by VAEs to be more fuzzy than GANs.   low compared with VAE, GANs variational lower bound, if the discriminator training is good, then the generator can learn to perfect the training sample distribution. In other words, GANs, gradual consistent, but the VAE is biased.   GAN applied to some scenes, such as picture style transfer, super resolution, image completion, noise removal, to avoid the loss of function design difficulties, regardless of three seven and twenty-one, as long as there is a benchmark, directly on the discriminator, the rest of the training to the confrontation.   disadvantages:   training GAN needs to reach Nash equilibrium, sometimes it can be achieved by gradient descent method, sometimes it can't. We haven't found a good method to achieve Nash equilibrium, so training GAN is unstable compared with VAE or PixelRNN, but I think it is more stable than training boltzmann machine in practice.   GAN is not suitable for processing discrete data, such as text.   GAN has the problems of unstable training, gradient disappearance and mode collapse.   2.Implements 2.1 Load and prepare the dataset You will use the MNIST dataset to train the generator and the discriminator. The generator will generate handwritten digits resembling the MNIST data. import tensorflow as tf   def load_dataset(mnist_size, mnist_batch_size, cifar_size, cifar_batch_size,):   \"\"\" load mnist and cifar10 dataset to shuffle.    Args:     mnist_size: mnist dataset size.     mnist_batch_size: every train dataset of mnist.     cifar_size: cifar10 dataset size.     cifar_batch_size: every train dataset of cifar10.    Returns:     mnist dataset, cifar10 dataset    \"\"\"   # load mnist data   (mnist_train_images, mnist_train_labels), (_, _) = tf.keras.datasets.mnist.load_data()    # load cifar10 data   (cifar_train_images, cifar_train_labels), (_, _) = tf.keras.datasets.cifar10.load_data()    mnist_train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1).astype('float32')   mnist_train_images = (mnist_train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]    cifar_train_images = cifar_train_images.reshape(cifar_train_images.shape[0], 32, 32, 3).astype('float32')   cifar_train_images = (cifar_train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]    # Batch and shuffle the data   mnist_train_dataset = tf.data.Dataset.from_tensor_slices(mnist_train_images)   mnist_train_dataset = mnist_train_dataset.shuffle(mnist_size).batch(mnist_batch_size)    cifar_train_dataset = tf.data.Dataset.from_tensor_slices(cifar_train_images)   cifar_train_dataset = cifar_train_dataset.shuffle(cifar_size).batch(cifar_batch_size)    return mnist_train_dataset, cifar_train_dataset 2.2 Create the models Both the generator and discriminator are defined using the Keras Sequential API. 2.2.1 Make Generator model Only the most basic form of full connection is used here for the neural network architecture. Except the first layer which does not use normalization, the other layers are all defined by the linear structure of full connection -> normalization ->LeakReLU, and the specific parameters are explained in the code below. import tensorflow as tf from tensorflow.python.keras import layers   def make_generator_model(dataset='mnist'):   \"\"\" implements generate.    Args:     dataset: mnist or cifar10 dataset. (default='mnist'). choice{'mnist', 'cifar'}.    Returns:     model.    \"\"\"   model = tf.keras.models.Sequential()   model.add(layers.Dense(256, input_dim=100))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(512))   model.add(layers.BatchNormalization())   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(1024))   model.add(layers.BatchNormalization())   model.add(layers.LeakyReLU(alpha=0.2))    if dataset == 'mnist':     model.add(layers.Dense(28 * 28 * 1, activation='tanh'))     model.add(layers.Reshape((28, 28, 1)))   elif dataset == 'cifar':     model.add(layers.Dense(32 * 32 * 3, activation='tanh'))     model.add(layers.Reshape((32, 32, 3)))    return model 2.2.2 Make Discriminator model The discriminator is a CNN-based image classifier. import tensorflow as tf from tensorflow.python.keras import layers   def make_discriminator_model(dataset='mnist'):   \"\"\" implements discriminate.    Args:     dataset: mnist or cifar10 dataset. (default='mnist'). choice{'mnist', 'cifar'}.    Returns:     model.    \"\"\"   model = tf.keras.models.Sequential()   if dataset == 'mnist':     model.add(layers.Flatten(input_shape=[28, 28, 1]))   elif dataset == 'cifar':     model.add(layers.Flatten(input_shape=[32, 32, 3]))    model.add(layers.Dense(1024))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(512))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(256))   model.add(layers.LeakyReLU(alpha=0.2))    model.add(layers.Dense(1, activation='sigmoid'))    return model 2.3 Define the loss and optimizers 2.3.1 Define loss functions and optimizers for both models. cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)  2.3.2 Discriminator loss This method quantifies how well the discriminator is able to distinguish real images from fakes. It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions on fake (generated) images to an array of 0s. def discriminator_loss(real_output, fake_output):   \"\"\" This method quantifies how well the discriminator is able to distinguish real images from fakes.       It compares the discriminator's predictions on real images to an array of 1s, and the discriminator's predictions       on fake (generated) images to an array of 0s.    Args:     real_output: origin pic.     fake_output: generate pic.    Returns:     real loss + fake loss    \"\"\"   real_loss = cross_entropy(tf.ones_like(real_output), real_output)   fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)   total_loss = real_loss + fake_loss    return total_loss  2.3.3 Generator loss The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1). Here, we will compare the discriminators decisions on the generated images to an array of 1s. def generator_loss(fake_output):   \"\"\" The generator's loss quantifies how well it was able to trick the discriminator.       Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (or 1).       Here, we will compare the discriminators decisions on the generated images to an array of 1s.    Args:     fake_output: generate pic.    Returns:     loss    \"\"\"   return cross_entropy(tf.ones_like(fake_output), fake_output)  2.3.4 optimizer The discriminator and the generator optimizers are different since we will train two networks separately. def generator_optimizer():   \"\"\" The training generator optimizes the network.    Returns:     optim loss.    \"\"\"   return tf.keras.optimizers.Adam(lr=1e-4)   def discriminator_optimizer():   \"\"\" The training discriminator optimizes the network.    Returns:     optim loss.    \"\"\"   return tf.keras.optimizers.Adam(lr=1e-4)  2.4 Save checkpoints This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted. import os import tensorflow as tf   def save_checkpoints(generator, discriminator, generator_optimizer, discriminator_optimizer, save_path):   \"\"\" save gan model    Args:     generator: generate model.     discriminator: discriminate model.     generator_optimizer: generate optimizer func.     discriminator_optimizer: discriminator optimizer func.     save_path: save gan model dir path.    Returns:     checkpoint path    \"\"\"   checkpoint_dir = save_path   checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")   checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,                                    discriminator_optimizer=discriminator_optimizer,                                    generator=generator,                                    discriminator=discriminator)    return checkpoint_dir, checkpoint, checkpoint_prefix 2.5 train 2.5. 1 Define the training loop The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator. from dataset.load_dataset import load_dataset from network.generator import make_generator_model from network.discriminator import make_discriminator_model from util.loss_and_optim import generator_loss, generator_optimizer from util.loss_and_optim import discriminator_loss, discriminator_optimizer from util.save_checkpoints import save_checkpoints from util.generate_and_save_images import generate_and_save_images  import tensorflow as tf import time import os import argparse  parser = argparse.ArgumentParser() parser.add_argument('--dataset', default='mnist', type=str,                     help='use dataset {mnist or cifar}.') parser.add_argument('--epochs', default=50, type=int,                     help='Epochs for training.') args = parser.parse_args() print(args)  # define model save path save_path = 'training_checkpoint'  # create dir if not os.path.exists(save_path):   os.makedirs(save_path)  # define random noise noise = tf.random.normal([16, 100])  # load dataset mnist_train_dataset, cifar_train_dataset = load_dataset(60000, 128, 50000, 64)  # load network and optim paras generator = make_generator_model(args.dataset) generator_optimizer = generator_optimizer()  discriminator = make_discriminator_model(args.dataset) discriminator_optimizer = discriminator_optimizer()  checkpoint_dir, checkpoint, checkpoint_prefix = save_checkpoints(generator,                                                                  discriminator,                                                                  generator_optimizer,                                                                  discriminator_optimizer,                                                                  save_path)   # This annotation causes the function to be \"compiled\". @tf.function def train_step(images):   \"\"\" break it down into training steps.    Args:     images: input images.    \"\"\"   with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:     generated_images = generator(noise, training=True)      real_output = discriminator(images, training=True)     fake_output = discriminator(generated_images, training=True)      gen_loss = generator_loss(fake_output)     disc_loss = discriminator_loss(real_output, fake_output)    gradients_of_generator = gen_tape.gradient(gen_loss,                                              generator.trainable_variables)   gradients_of_discriminator = disc_tape.gradient(disc_loss,                                                   discriminator.trainable_variables)    generator_optimizer.apply_gradients(     zip(gradients_of_generator, generator.trainable_variables))   discriminator_optimizer.apply_gradients(     zip(gradients_of_discriminator, discriminator.trainable_variables))   def train(dataset, epochs):   \"\"\" train op    Args:     dataset: mnist dataset or cifar10 dataset.     epochs: number of iterative training.    \"\"\"   for epoch in range(epochs):     start = time.time()      for image_batch in dataset:       train_step(image_batch)      # Produce images for the GIF as we go     generate_and_save_images(generator,                              epoch + 1,                              noise,                              save_path)      # Save the model every 15 epochs     if (epoch + 1) % 15 == 0:       checkpoint.save(file_prefix=checkpoint_prefix)      print(f'Time for epoch {epoch+1} is {time.time()-start:.3f} sec.')    # Generate after the final epoch   generate_and_save_images(generator,                            epochs,                            noise,                            save_path)   if __name__ == '__main__':   if args.dataset == 'mnist':     train(mnist_train_dataset, args.epochs)   else:     train(cifar_train_dataset, args.epochs) 2.6 Generate and save images from matplotlib import pyplot as plt   def generate_and_save_images(model, epoch, test_input):   # Notice `training` is set to False.   # This is so all layers run in inference mode (batchnorm).   predictions = model(test_input, training=False)    fig = plt.figure(figsize=(4,4))    for i in range(predictions.shape[0]):       plt.subplot(4, 4, i+1)       plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')       plt.axis('off')    plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))   plt.show() 3.Common problems 3.1 why do optimizers in GAN not often use SGD  SGD is easy to shake, easy to make GAN training unstable.  -The purpose of GAN is to find the Nash equilibrium point in the higher-dimensional non-convex parameter space. The Nash equilibrium point of GAN is a saddle point, but SGD will only find the local minimum value, because SGD solves the problem of finding the minimum value, and GAN is a game problem. 3.2 Why GAN is not suitable for processing text data   Compared text data are discrete image data, because for text, usually need to map a word as a high dimensional vector, and finally forecasts the output is a one - hot vector, assuming softmax output is (0.2, 0.3, 0.1, 0.2, 0.15, 0.05) then becomes onehot,1,0,0,0,0 (0), if the softmax output is (0.2, 0.25, 0.2, 0.1, 0.15, 0.1), one - is still hot (0, 1, 0, 0, 0, 0). Therefore, for the generator, G outputs different results, but D gives the same discriminant result, and cannot well transmit the gradient update information to G, so the discriminant of D's final output is meaningless.   In addition, the loss function of GAN is JS divergence, which is not suitable for measuring the distance between distributions that do not want to intersect.   3.3 some skills to train GAN   Input normalized to (-1, 1), last level of activation function using tanh BEGAN (exception)   Using wassertein GAN's loss function,   If you have label data, try to use labels. Some people suggest that it is good to use inverted labels, and use label smoothing, unilateral label smoothing or bilateral label smoothing   Using mini-batch norm, if you do not use batch norm, you can use instance norm or weight norm   Avoid using RELU and pooling layers to reduce the possibility of sparse gradient, and leakrelu activation function can be used   The optimizer chooses ADAM as far as possible, and the learning rate should not be too large. The initial 1e-4 can be referred to. In addition, the learning rate can be continuously reduced as the training goes on.   Adding gaussian noise to the network layer of D is equivalent to a kind of regularization   3.4 Model collapse reason Generally, GAN is not stable in training, and the results are very poor. However, even if the training time is extended, it cannot be well improved. The specific reasons can be explained as follows: Is against training methods used by GAN, G gradient update from D, G generated is good, so D what to say to me. Specifically, G will generate a sample and give it to D for evaluation. D will output the probability (0-1) that the generated false sample is a true sample, which is equivalent to telling G how authentic the generated sample is. G will improve itself and improve the probability value of D's output according to this feedback. But if one G generated samples may not be true, but D gives the correct evaluation, or is the result of a G generated some characteristics have been the recognition of D, then G output will think I'm right, so I so output D surely will also give a high evaluation, G actually generated is not how, but they are two so self-deception, lead to the resulting results lack some information, characteristics. 4. GAN in the application of life   GAN itself is a generative model, so data generation is the most common, the most common is image generation, commonly used DCGAN WGAN BEGAN, personal feeling in BEGAN the best and the most simple.   GAN itself is also a model of unsupervised learning. So it is widely used in unsupervised learning and semi-supervised learning.   GAN not only plays a role in the generation field, but also plays a role in the classification field. To put it simply, it is to replace the discriminator as a classifier and do multiple classification tasks, while the generator still does generation tasks and assists the classifier training.   GAN can be combined with reinforcement learning. A good example is seq-gan.   At present, GAN is an interesting application in image style transfer, image noise reduction and restoration, and image super resolution, all of which have good results.   TODO  Write metrics code. Create GIF.  Reference Sakura55 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1406.2661v1.pdf"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a78e"}, "repo_url": "https://github.com/metriczulu/massvecpy", "repo_name": "massvecpy", "repo_full_name": "metriczulu/massvecpy", "repo_owner": "metriczulu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T06:35:48Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T22:08:33Z", "homepage": null, "size": 23, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187920385, "is_fork": false, "readme_text": "#BELOW IS OUTDATED AND DEPRECATED.  NEW FUNCTIONALITY ADDED AND USE CHANGED #WILL UPDATE WHEN I GET TIME. MassiveWordVec massvecpy is a package for converting a large corpus into word embeddings using pre-trained word vectors.  Corpora that consistent of an array, list, or DataFrame of tokenized docs with shape-(num_docs,) are converted into an array of shape-(num_docs, max_doc_length, vector_dimensions). Corpora that are too large to store the entire word embedding matrix into memory can be turned into a series of n - 'slices' with shape-(num_docs/num_slices, max_doc_length, vector_dimensions).  These slices are saved to local harddrive to save memory and loaded into your program individually (or multiple together if memory allows) for use as mini-batch to train models. Corpora can be sliced, converted, and saved with or without training labels.  With training labels, both corpus and labels are shuffled together and sliced into corresponding matrices that are automatically saved and loaded together. This package currently only provides support for pymagnitude pre-trained word vectors.  If you have a suggestion for word vecs in other packages, please let me know. Installation Use the package manager pip to install MassiveWordVec. pip install massvecpy Usage import massvecpy import pymagnitude  #Load your pretrained word vectors from pymagnitude vector_directory = '~/Word Embeddings/' vector_dict = mag.Magnitude(vector_directory+'glove.6B.200d.magnitude')  #Define the corpus to split and convert. #If entire corpus embedding matrix can fit in memory then leave #the number of slices at 1. corpus = massvecpy.DocVectorizer(corpus_name, tokenized_corpus, labels, vector_dict,          vector_dimension, number_of_slices, file_directory, random_state)  #convert slice 0 to word embedding matrix with associated labels x, y = corpus.fit(0, verbose = True)  #save slice 0 to harddrive corpus.save()  #define and fit a model (can be anything, most useful to use model that allows mini-batch training) clf = Model() clf.fit(x, y)  #convert the rest of the slices and save them to their own files.  Provide updates to track process. corpus.fit_and_save(range(1, number_of_slices), verbose - True)  #clear currently stored embeddings from memory corpus.clear_memory()  #************  corpus = massvecpy.LoadVectorizedDoc(corpus_name, file_directory)  #load first slice (0) into local arrays x, y = corpus.load(0)  #figure out the dimensions of our slices num_docs_in_slice = x.shape[0] max_length = x.shape[1] vec_dims = x.shape[2]  #alternatively, process all slices in current file directory with name corpus_name clf = SGDClassifier(loss='log')  for i in corpus.all_indices_available:      x, y = corpus.load(index)      x_f = np.reshape(x, (num_docs_in_slice, max_length * vec_dims))      clf.partial_fit(x_f, y, [0, 1]) Future Plans In the future I will build support for just generating a word vector lookup dictionary for all (or some specified number of) words in the corpus.  Additionally, plan to use this dictionary to generate matrix embeddings in the format Keras uses to load weights into an embedding layer. License None brah ", "has_readme": true, "readme_language": "English", "repo_tags": ["wordvectors", "embedding-vectors", "embedding-python", "pymagnitude", "python3", "large-dataset"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a78f"}, "repo_url": "https://github.com/lukasturcani/dnn_tensorflow", "repo_name": "dnn_tensorflow", "repo_full_name": "lukasturcani/dnn_tensorflow", "repo_owner": "lukasturcani", "repo_desc": "Implementations of deep network architectures in TensorFlow.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T10:23:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T08:47:33Z", "homepage": "", "size": 68, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187999486, "is_fork": false, "readme_text": "  Author: Lukas Turcani   I no longer maintain this repo, and the code in it is out of date. For example the user of TensorFlow layers should be replaced with the TensorFlow keras api. I maintain https://www.github.com/lukasturcani/dnn instead. This repo provides a number of network architectures, implemented in TensorFlow as well as scripts used to run them. To train a network, run the appropriate run script. For example: $ python -m dnn_tensorflow.train_scripts.cifar100.simple_cnn  Notice that the scripts are run from the module level using the python -m flag. Networks can be modified using command line arguments, for example: $ python -m dnn_tensorflow.train_scripts.mnist.simple_cnn --conv_in_channels 1 20 50 --conv_out_channels 20 50 60 --conv_kernel_size 5 5 5 --conv_strides 1 1 1 --conv_paddings 0 0 0 --conv_dilations 1 1 1 --pool_kernel_sizes 2 2 2 --pool_strides 2 2 2 --pool_paddings 0 0 0 --pool-dilations 1 1 1 --train_batch_size 100 --label_smoothing 0.5 --epochs 10  changes the default values for training batch size, label smoothing and the number of training epochs and adds a new convolution layer and pooling layer vs the default network. Any number of layers can be added / removed via the command line in this fashion. Each script can have options viewed by: $ python -m path.to.train.script --help   Results This is a short summary of some of the nice results from this repo. Not all implemented architectures are listed here.  Image Inpainting DCGAN FCGAN   Image Inpainting This is a task where the generator is provided with an image that is missing some pixels and it is asked to fill them in. In this example, I cover up the bottom half of MNIST images and get the generator to fill them in.  TensorFlow implementation Run with: $ python -m dnn.tensorflow.train_scripts.mnist.image_inpainting  Results:  DCGAN A more advanced GAN architecture, which is fully convolutional. Run with: $ python -m dnn_tensorflow.train_scripts.mnist.dcgan  To generate results: $ python -m dnn_tensorflow.train_scripts.mnist.dcgan --sample_generator    DCGAN with labels. This network can also be trained using the labelling information present in the MNIST dataset. This leads to better results and each digit can be sampled explicitly. Run with: $ python -m dnn.tensorflow.train_scripts.mnist.dcgan --labels  To generate results: $ python -m dnn.tensorflow.train_scripts.mnist.dcgan --labels --sample_generator    FCGAN This is a vanilla GAN using feed-forward networks as both the generator and discriminator. Run with: $ python -m dnn_tensorflow.train_scripts.mnist.fcgan  To generate results: $ python -m dnn_tensorflow.train_scripts.mnist.fcgan --sample_generator    FCGAN with labels. This network can also be trained using the labelling information present in the MNIST dataset. This leads to better results and each digit can be sampled explicitly. Run with: $ python -m dnn_tensorflow.train_scripts.mnist.dcgan --labels  To generate results: $ python -m dnn_tensorflow.train_scripts.mnist.dcgan --labels --sample_generator   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a790"}, "repo_url": "https://github.com/leopeng1995/neuralsql", "repo_name": "neuralsql", "repo_full_name": "leopeng1995/neuralsql", "repo_owner": "leopeng1995", "repo_desc": "Make MongoDB More Intelligent", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T10:34:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T06:27:22Z", "homepage": "", "size": 15859, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187580175, "is_fork": false, "readme_text": "NeuralSQL - Make MongoDB More Intelligent 2019 MongoDB Hackathon Project What is NeuralSQL? It is a simple query engine using SQL-like syntax. Only supported Python 3.x. Motivation SQL is an intuitive way to query data. Nowadays, more and more data use MongoDB as their storage databases which are benefited by MongoDB NoSQL Features. We want to build a SQL-like query engine to train machine learning or deep learning models using data in the MongoDB. This experimental project also demonstrates my idea on serverless function to predict using pretrained model stored in MongoDB. Features  Use SQL-like syntax to query MongoDB and get data insight. Use multiple backends, such as TensorFlow, Keras, or scikit-learn. Computation Engine for generates MongoDB pipeline/map-reduce operator to reduce data transmission. Auto-completion and syntax highlighting CLI. Pretrained Model Auto Serving.  Examples Tutorial 0: WordCount Simplest NeuralSQL Example. In fact, the SQL executor will generates execution operator using MongoDB map-reduce. Prepare data uses neuralsql -L wordcount or neuralsql --load-sample wordcount. SELECT content FROM text8.train TRAINER WordCount;  Word occurrences top 500: SELECT content FROM text8.train TRAINER WordCount LIMIT 500;  Tutorial 1: Word2Vec SELECT content FROM text8.train TRAINER Word2Vec;  We can use this example to build keyword recommendation engine. In next example, we will show you how to use MongoDB Stitch to serve word similarity request. Tutorial 2: Twenty News Classifier Prepare data uses neuralsql -L classifier or neuralsql -L classifier[twenty_news]. SELECT content FROM twenty_news.train TRAINER classifier.MultinomialNB;  Or you can use a classifier attached by parameters: SELECT content FROM twenty_news.train TRAINER classifier.SGDClassifier WITH loss='hinge', penalty='l2', random_state=42, max_iter=5, tol=None;  Tutorial 3: Chatbot We can use MongoDB data to build a simple chatbot! Configuration If you want to use MongoDB Altas and MongoDB Stitch, you can set your username and password in config.ini. Serverless We can use serverless to query pretrained models. In chatbot (MongoDB Stitch) example, we will use MongoDB Stitch function to query pretrained model stored in MongoDB. You can connect MongoDB Stitch using mongo command. mongo \"mongodb://<username>:<password>@stitch.mongodb.com:27020/?authMechanism=PLAIN&authSource=%24external&ssl=true&appName=todo-tutorial1-uhdox:mongodb-atlas:local-userpass\" Then you can create a function in your Stitch app console. exports = async function(text) {   let res = await context.http.post({     url: \"http://www.der.ai/chatbot\",     form: {       user_id: context.user.id,       text: text     }   });      return EJSON.parse(res.body.text()); };  You should deploy the simple chatbot service in your server. We deployed one in our server (maybe slow or down). cd samples/chatbot_service ./run.sh Finally, you can call this function in mongo shell. db.runCommand({     callFunction: \"chatfunc\",     arguments: [\"Hello World!\"] })  TODO Lots of things to do. This is just a demo project. Do not used in production environment! However, welcome all of you to propose suggestions.  TF-IDF Using MongoDB Map-Reduce  RoadMap  Improve SQL Parser Add more common machine learning / deep learning models support  Thanks  CLI is inspired by mycli. Presto  ", "has_readme": true, "readme_language": "English", "repo_tags": ["mongodb", "data-analysis", "sql"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a791"}, "repo_url": "https://github.com/harrybolingot/mymaskrcnn", "repo_name": "mymaskrcnn", "repo_full_name": "harrybolingot/mymaskrcnn", "repo_owner": "harrybolingot", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-22T03:58:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T21:31:02Z", "homepage": null, "size": 129219, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 187916323, "is_fork": false, "readme_text": "# Mask R-CNN for Object Detection and Segmentation  This is an implementation of [Mask R-CNN](https://arxiv.org/abs/1703.06870) on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.  ![Instance Segmentation Sample](assets/street.png)  The repository includes: * Source code of Mask R-CNN built on FPN and ResNet101. * Training code for MS COCO * Pre-trained weights for MS COCO * Jupyter notebooks to visualize the detection pipeline at every step * ParallelModel class for multi-GPU training * Evaluation on MS COCO metrics (AP) * Example of training on your own dataset   The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released [Matterport3D](https://matterport.com/blog/2017/09/20/announcing-matterport3d-research-dataset/) dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples [here](https://matterport.com/gallery/).  # Getting Started * [demo.ipynb](samples/demo.ipynb) Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.  * [train_shapes.ipynb](samples/shapes/train_shapes.ipynb) shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.  * ([model.py](mrcnn/model.py), [utils.py](mrcnn/utils.py), [config.py](mrcnn/config.py)): These files contain the main Mask RCNN implementation.    * [inspect_data.ipynb](samples/coco/inspect_data.ipynb). This notebook visualizes the different pre-processing steps to prepare the training data.  * [inspect_model.ipynb](samples/coco/inspect_model.ipynb) This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.  * [inspect_weights.ipynb](samples/coco/inspect_weights.ipynb) This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.   # Step by Step Detection To help with debugging and understanding the model, there are 3 notebooks  ([inspect_data.ipynb](samples/coco/inspect_data.ipynb), [inspect_model.ipynb](samples/coco/inspect_model.ipynb), [inspect_weights.ipynb](samples/coco/inspect_weights.ipynb)) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples:    ## 1. Anchor sorting and filtering Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement. ![](assets/detection_anchors.png)  ## 2. Bounding Box Refinement This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage. ![](assets/detection_refinement.png)  ## 3. Mask Generation Examples of generated masks. These then get scaled and placed on the image in the right location.  ![](assets/detection_masks.png)  ## 4.Layer activations Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).  ![](assets/detection_activations.png)  ## 5. Weight Histograms Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.  ![](assets/detection_histograms.png)  ## 6. Logging to TensorBoard TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.  ![](assets/detection_tensorboard.png)  ## 6. Composing the different pieces into a final result  ![](assets/detection_final.png)   # Training on MS COCO We're providing pre-trained weights for MS COCO to make it easier to start. You can use those weights as a starting point to train your own variation on the network. Training and evaluation code is in `samples/coco/coco.py`. You can import this module in Jupyter notebook (see the provided notebooks for examples) or you can run it directly from the command line as such:  ``` # Train a new model starting from pre-trained COCO weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  # Train a new model starting from ImageNet weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet  # Continue training a model that you had trained earlier python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5  # Continue training the last model you trained. This will find # the last trained weights in the model directory. python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last ```  You can also run the COCO evaluation code with: ``` # Run COCO evaluation on the last trained model python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last ```  The training schedule, learning rate, and other parameters should be set in `samples/coco/coco.py`.   # Training on Your Own Dataset  Start by reading this [blog post about the balloon color splash sample](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46). It covers the process starting from annotating images to training to using the results in a sample application.  In summary, to train the model on your own dataset you'll need to extend two classes:  ```Config``` This class contains the default configuration. Subclass it and modify the attributes you need to change.  ```Dataset``` This class provides a consistent way to work with any dataset.  It allows you to use new datasets for training without having to change  the code of the model. It also supports loading multiple datasets at the same time, which is useful if the objects you want to detect are not  all available in one dataset.   See examples in `samples/shapes/train_shapes.ipynb`, `samples/coco/coco.py`, `samples/balloon/balloon.py`, and `samples/nucleus/nucleus.py`.  ## Differences from the Official Paper This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.  * **Image Resizing:** To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px. * **Bounding Boxes**: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation.      To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more,  and only 0.01% differed by 10px or more.  * **Learning Rate:** The paper uses a learning rate of 0.02, but we found that to be too high, and often causes the weights to explode, especially when using a small batch size. It might be related to differences between how Caffe and TensorFlow compute  gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively. We found that smaller learning rates converge faster anyway so we go with that.  ## Citation Use this bibtex to cite this repository: ``` @misc{matterport_maskrcnn_2017,   title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},   author={Waleed Abdulla},   year={2017},   publisher={Github},   journal={GitHub repository},   howpublished={\\url{https://github.com/matterport/Mask_RCNN}}, } ```  ## Contributing Contributions to this repository are welcome. Examples of things you can contribute: * Speed Improvements. Like re-writing some Python code in TensorFlow or Cython. * Training on other datasets. * Accuracy Improvements. * Visualizations and examples.  You can also [join our team](https://matterport.com/careers/) and help us build even more projects like this one.  ## Requirements Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in `requirements.txt`.  ### MS COCO Requirements: To train or test on MS COCO, you'll also need: * pycocotools (installation instructions below) * [MS COCO Dataset](http://cocodataset.org/#home) * Download the 5K [minival](https://dl.dropboxusercontent.com/s/o43o90bna78omob/instances_minival2014.json.zip?dl=0)   and the 35K [validation-minus-minival](https://dl.dropboxusercontent.com/s/s3tw5zcg7395368/instances_valminusminival2014.json.zip?dl=0)   subsets. More details in the original [Faster R-CNN implementation](https://github.com/rbgirshick/py-faster-rcnn/blob/master/data/README.md).  If you use Docker, the code has been verified to work on [this Docker container](https://hub.docker.com/r/waleedka/modern-deep-learning/).   ## Installation 1. Clone this repository 2. Install dependencies    ```bash    pip3 install -r requirements.txt    ``` 3. Run setup from the repository root directory     ```bash     python3 setup.py install     ```  3. Download pre-trained COCO weights (mask_rcnn_coco.h5) from the [releases page](https://github.com/matterport/Mask_RCNN/releases). 4. (Optional) To train or test on MS COCO install `pycocotools` from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).      * Linux: https://github.com/waleedka/coco     * Windows: https://github.com/philferriere/cocoapi.     You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)  # Projects Using this Model If you extend this model to other datasets or build projects that use it, we'd love to hear from you.  ### [4K Video Demo](https://www.youtube.com/watch?v=OOT3UIXZztE) by Karol Majek. [![Mask RCNN on 4K Video](assets/4k_video.gif)](https://www.youtube.com/watch?v=OOT3UIXZztE)  ### [Images to OSM](https://github.com/jremillard/images-to-osm): Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.  ![Identify sport fields in satellite images](assets/images_to_osm.png)  ### [Splash of Color](https://engineering.matterport.com/splash-of-color-instance-segmentation-with-mask-r-cnn-and-tensorflow-7c761e238b46). A blog post explaining how to train this model from scratch and use it to implement a color splash effect. ![Balloon Color Splash](assets/balloon_color_splash.gif)   ### [Segmenting Nuclei in Microscopy Images](samples/nucleus). Built for the [2018 Data Science Bowl](https://www.kaggle.com/c/data-science-bowl-2018) Code is in the `samples/nucleus` directory.  ![Nucleus Segmentation](assets/nucleus_segmentation.png)  ### [Detection and Segmentation for Surgery Robots](https://github.com/SUYEgit/Surgery-Robot-Detection-Segmentation) by the NUS Control & Mechatronics Lab. ![Surgery Robot Detection and Segmentation](https://github.com/SUYEgit/Surgery-Robot-Detection-Segmentation/raw/master/assets/video.gif)  ### [Reconstructing 3D buildings from aerial LiDAR](https://medium.com/geoai/reconstructing-3d-buildings-from-aerial-lidar-with-ai-details-6a81cb3079c0) A proof of concept project by [Esri](https://www.esri.com/), in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher. ![3D Building Reconstruction](assets/project_3dbuildings.png)  ### [Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy](https://github.com/oist/usiigaci) A project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.  ![](assets/project_usiigaci1.gif) ![](assets/project_usiigaci2.gif)  ### [Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery](http://www.mdpi.com/2072-4292/10/9/1487) Research project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy. ![image](assets/project_ice_wedge_polygons.png)  ### [Mask-RCNN Shiny](https://github.com/huuuuusy/Mask-RCNN-Shiny) A computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results. ![](assets/project_shiny1.jpg)  ### [Mapping Challenge](https://github.com/crowdAI/crowdai-mapping-challenge-mask-rcnn): Convert satellite imagery to maps for use by humanitarian organisations. ![Mapping Challenge](assets/mapping_challenge.png)  ### [GRASS GIS Addon](https://github.com/ctu-geoforall-lab/i.ann.maskrcnn) to generate vector masks from geospatial imagery. Based on a [Master's thesis](https://github.com/ctu-geoforall-lab-projects/dp-pesek-2018) by Ond\u0159ej Pe\u0161ek. ![GRASS GIS Image](assets/project_grass_gis.png) # MyMaskRCNN  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.mdpi.com/2072-4292/10/9/1487", "http://cocodataset.org/#home"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a792"}, "repo_url": "https://github.com/yinyuanlai/CNN", "repo_name": "CNN", "repo_full_name": "yinyuanlai/CNN", "repo_owner": "yinyuanlai", "repo_desc": "\u5377\u79ef\u795e\u7ecf\u7f51\u7edc", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-22T06:54:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T06:53:28Z", "homepage": null, "size": 103, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 187980263, "is_fork": false, "readme_text": "Tensorflow-CNN-Tutorial \u8fd9\u662f\u4e00\u4e2a\u624b\u628a\u624b\u6559\u4f60\u7528Tensorflow\u6784\u5efa\u5377\u673a\u7f51\u7edc\uff08CNN\uff09\u8fdb\u884c\u56fe\u50cf\u5206\u7c7b\u7684\u6559\u7a0b\u3002\u5b8c\u6574\u4ee3\u7801\u53ef\u5728Github\u4e2d\u4e0b\u8f7d\uff1ahttps://github.com/hujunxianligong/Tensorflow-CNN-Tutorial\u3002\u6559\u7a0b\u5e76\u6ca1\u6709\u4f7f\u7528MNIST\u6570\u636e\u96c6\uff0c\u800c\u662f\u4f7f\u7528\u4e86\u771f\u5b9e\u7684\u56fe\u7247\u6587\u4ef6\uff0c\u5e76\u4e14\u6559\u7a0b\u4ee3\u7801\u5305\u542b\u4e86\u6a21\u578b\u7684\u4fdd\u5b58\u3001\u52a0\u8f7d\u7b49\u529f\u80fd\uff0c\u56e0\u6b64\u5e0c\u671b\u5728\u65e5\u5e38\u9879\u76ee\u4e2d\u4f7f\u7528Tensorflow\u7684\u670b\u53cb\u53ef\u4ee5\u53c2\u8003\u8fd9\u7bc7\u6559\u7a0b\u3002 \u6982\u8ff0  \u4ee3\u7801\u5229\u7528\u5377\u79ef\u7f51\u7edc\u5b8c\u6210\u4e00\u4e2a\u56fe\u50cf\u5206\u7c7b\u7684\u529f\u80fd \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u6a21\u578b\u4fdd\u5b58\u5728model\u6587\u4ef6\u4e2d\uff0c\u53ef\u76f4\u63a5\u4f7f\u7528\u6a21\u578b\u8fdb\u884c\u7ebf\u4e0a\u5206\u7c7b \u540c\u4e00\u4e2a\u4ee3\u7801\u5305\u62ec\u4e86\u8bad\u7ec3\u548c\u6d4b\u8bd5\u9636\u6bb5\uff0c\u901a\u8fc7\u4fee\u6539train\u53c2\u6570\u4e3aTrue\u548cFalse\u63a7\u5236\u8bad\u7ec3\u548c\u6d4b\u8bd5  \u6570\u636e\u51c6\u5907 \u6559\u7a0b\u7684\u56fe\u7247\u4eceCifar\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\uff0cdownload_cifar.py\u4eceKeras\u81ea\u5e26\u7684Cifar\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u4e86\u90e8\u5206Cifar\u6570\u636e\u96c6\uff0c\u5e76\u5c06\u5176\u8f6c\u6362\u4e3ajpg\u56fe\u7247\u3002 \u9ed8\u8ba4\u4eceCifar\u6570\u636e\u96c6\u4e2d\u9009\u53d6\u4e863\u7c7b\u56fe\u7247\uff0c\u6bcf\u7c7b50\u5f20\u56fe\uff0c\u5206\u522b\u662f  0 => \u98de\u673a 1 => \u6c7d\u8f66 2 => \u9e1f  \u56fe\u7247\u90fd\u653e\u5728data\u6587\u4ef6\u5939\u4e2d\uff0c\u6309\u7167label_id.jpg\u8fdb\u884c\u547d\u540d\uff0c\u4f8b\u59822_111.jpg\u4ee3\u8868\u56fe\u7247\u7c7b\u522b\u4e3a2\uff08\u9e1f\uff09\uff0cid\u4e3a111\u3002  \u5bfc\u5165\u76f8\u5173\u5e93 \u9664\u4e86Tensorflow\uff0c\u672c\u6559\u7a0b\u8fd8\u9700\u8981\u4f7f\u7528pillow(PIL)\uff0c\u5728Windows\u4e0bPIL\u53ef\u80fd\u9700\u8981\u4f7f\u7528conda\u5b89\u88c5\u3002 \u5982\u679c\u4f7f\u7528download_cifar.py\u81ea\u5df1\u6784\u5efa\u6570\u636e\u96c6\uff0c\u8fd8\u9700\u8981\u5b89\u88c5keras\u3002 import os #\u56fe\u50cf\u8bfb\u53d6\u5e93 from PIL import Image #\u77e9\u9635\u8fd0\u7b97\u5e93 import numpy as np import tensorflow as tf \u914d\u7f6e\u4fe1\u606f \u8bbe\u7f6e\u4e86\u4e00\u4e9b\u53d8\u91cf\u589e\u52a0\u7a0b\u5e8f\u7684\u7075\u6d3b\u6027\u3002\u56fe\u7247\u6587\u4ef6\u5b58\u653e\u5728data_dir\u6587\u4ef6\u5939\u4e2d\uff0ctrain\u8868\u793a\u5f53\u524d\u6267\u884c\u662f\u8bad\u7ec3\u8fd8\u662f\u6d4b\u8bd5\uff0cmodel-path\u7ea6\u5b9a\u4e86\u6a21\u578b\u5b58\u653e\u7684\u8def\u5f84\u3002 # \u6570\u636e\u6587\u4ef6\u5939 data_dir = \"data\" # \u8bad\u7ec3\u8fd8\u662f\u6d4b\u8bd5 train = True # \u6a21\u578b\u6587\u4ef6\u8def\u5f84 model_path = \"model/image_model\" \u6570\u636e\u8bfb\u53d6 \u4ece\u56fe\u7247\u6587\u4ef6\u5939\u4e2d\u5c06\u56fe\u7247\u8bfb\u5165numpy\u7684array\u4e2d\u3002\u8fd9\u91cc\u6709\u51e0\u4e2a\u7ec6\u8282\uff1a  pillow\u8bfb\u53d6\u7684\u56fe\u50cf\u50cf\u7d20\u503c\u57280-255\u4e4b\u95f4\uff0c\u9700\u8981\u5f52\u4e00\u5316\u3002 \u5728\u8bfb\u53d6\u56fe\u50cf\u6570\u636e\u3001Label\u4fe1\u606f\u7684\u540c\u65f6\uff0c\u8bb0\u5f55\u56fe\u50cf\u7684\u8def\u5f84\uff0c\u65b9\u4fbf\u540e\u671f\u8c03\u8bd5\u3002  # \u4ece\u6587\u4ef6\u5939\u8bfb\u53d6\u56fe\u7247\u548c\u6807\u7b7e\u5230numpy\u6570\u7ec4\u4e2d # \u6807\u7b7e\u4fe1\u606f\u5728\u6587\u4ef6\u540d\u4e2d\uff0c\u4f8b\u59821_40.jpg\u8868\u793a\u8be5\u56fe\u7247\u7684\u6807\u7b7e\u4e3a1 def read_data(data_dir):     datas = []     labels = []     fpaths = []     for fname in os.listdir(data_dir):         fpath = os.path.join(data_dir, fname)         fpaths.append(fpath)         image = Image.open(fpath)         data = np.array(image) / 255.0         label = int(fname.split(\"_\")[0])         datas.append(data)         labels.append(label)      datas = np.array(datas)     labels = np.array(labels)      print(\"shape of datas: {}\\tshape of labels: {}\".format(datas.shape, labels.shape))     return fpaths, datas, labels   fpaths, datas, labels = read_data(data_dir)  # \u8ba1\u7b97\u6709\u591a\u5c11\u7c7b\u56fe\u7247 num_classes = len(set(labels)) \u5b9a\u4e49placeholder(\u5bb9\u5668) \u9664\u4e86\u56fe\u50cf\u6570\u636e\u548cLabel\uff0cDropout\u7387\u4e5f\u8981\u653e\u5728placeholder\u4e2d\uff0c\u56e0\u4e3a\u5728\u8bad\u7ec3\u9636\u6bb5\u548c\u6d4b\u8bd5\u9636\u6bb5\u9700\u8981\u8bbe\u7f6e\u4e0d\u540c\u7684Dropout\u7387\u3002 # \u5b9a\u4e49Placeholder\uff0c\u5b58\u653e\u8f93\u5165\u548c\u6807\u7b7e datas_placeholder = tf.placeholder(tf.float32, [None, 32, 32, 3]) labels_placeholder = tf.placeholder(tf.int32, [None])  # \u5b58\u653eDropOut\u53c2\u6570\u7684\u5bb9\u5668\uff0c\u8bad\u7ec3\u65f6\u4e3a0.25\uff0c\u6d4b\u8bd5\u65f6\u4e3a0 dropout_placeholdr = tf.placeholder(tf.float32) \u5b9a\u4e49\u5377\u57fa\u7f51\u7edc\uff08\u5377\u79ef\u548cPooling\u90e8\u5206\uff09 # \u5b9a\u4e49\u5377\u79ef\u5c42, 20\u4e2a\u5377\u79ef\u6838, \u5377\u79ef\u6838\u5927\u5c0f\u4e3a5\uff0c\u7528Relu\u6fc0\u6d3b conv0 = tf.layers.conv2d(datas_placeholder, 20, 5, activation=tf.nn.relu) # \u5b9a\u4e49max-pooling\u5c42\uff0cpooling\u7a97\u53e3\u4e3a2x2\uff0c\u6b65\u957f\u4e3a2x2 pool0 = tf.layers.max_pooling2d(conv0, [2, 2], [2, 2])  # \u5b9a\u4e49\u5377\u79ef\u5c42, 40\u4e2a\u5377\u79ef\u6838, \u5377\u79ef\u6838\u5927\u5c0f\u4e3a4\uff0c\u7528Relu\u6fc0\u6d3b conv1 = tf.layers.conv2d(pool0, 40, 4, activation=tf.nn.relu) # \u5b9a\u4e49max-pooling\u5c42\uff0cpooling\u7a97\u53e3\u4e3a2x2\uff0c\u6b65\u957f\u4e3a2x2 pool1 = tf.layers.max_pooling2d(conv1, [2, 2], [2, 2]) \u5b9a\u4e49\u5168\u8fde\u63a5\u90e8\u5206 # \u5c063\u7ef4\u7279\u5f81\u8f6c\u6362\u4e3a1\u7ef4\u5411\u91cf flatten = tf.layers.flatten(pool1)  # \u5168\u8fde\u63a5\u5c42\uff0c\u8f6c\u6362\u4e3a\u957f\u5ea6\u4e3a100\u7684\u7279\u5f81\u5411\u91cf fc = tf.layers.dense(flatten, 400, activation=tf.nn.relu)  # \u52a0\u4e0aDropOut\uff0c\u9632\u6b62\u8fc7\u62df\u5408 dropout_fc = tf.layers.dropout(fc, dropout_placeholdr)  # \u672a\u6fc0\u6d3b\u7684\u8f93\u51fa\u5c42 logits = tf.layers.dense(dropout_fc, num_classes)  predicted_labels = tf.arg_max(logits, 1) \u5b9a\u4e49\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668 \u8fd9\u91cc\u6709\u4e00\u4e2a\u6280\u5de7\uff0c\u6ca1\u6709\u5fc5\u8981\u7ed9Optimizer\u4f20\u9012\u5e73\u5747\u7684\u635f\u5931\uff0c\u76f4\u63a5\u5c06\u672a\u5e73\u5747\u7684\u635f\u5931\u51fd\u6570\u4f20\u7ed9Optimizer\u5373\u53ef\u3002 # \u5229\u7528\u4ea4\u53c9\u71b5\u5b9a\u4e49\u635f\u5931 losses = tf.nn.softmax_cross_entropy_with_logits(     labels=tf.one_hot(labels_placeholder, num_classes),     logits=logits ) # \u5e73\u5747\u635f\u5931 mean_loss = tf.reduce_mean(losses)  # \u5b9a\u4e49\u4f18\u5316\u5668\uff0c\u6307\u5b9a\u8981\u4f18\u5316\u7684\u635f\u5931\u51fd\u6570 optimizer = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(losses) \u5b9a\u4e49\u6a21\u578b\u4fdd\u5b58\u5668/\u8f7d\u5165\u5668 \u5982\u679c\u5728\u6bd4\u8f83\u5927\u7684\u6570\u636e\u96c6\u4e0a\u8fdb\u884c\u957f\u65f6\u95f4\u8bad\u7ec3\uff0c\u5efa\u8bae\u5b9a\u671f\u4fdd\u5b58\u6a21\u578b\u3002 # \u7528\u4e8e\u4fdd\u5b58\u548c\u8f7d\u5165\u6a21\u578b saver = tf.train.Saver() \u8fdb\u5165\u8bad\u7ec3/\u6d4b\u8bd5\u6267\u884c\u9636\u6bb5 with tf.Session() as sess: \u5728\u6267\u884c\u9636\u6bb5\u6709\u4e24\u6761\u5206\u652f\uff1a  \u5982\u679ctrian\u4e3aTrue\uff0c\u8fdb\u884c\u8bad\u7ec3\u3002\u8bad\u7ec3\u9700\u8981\u4f7f\u7528sess.run(tf.global_variables_initializer())\u521d\u59cb\u5316\u53c2\u6570\uff0c\u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u9700\u8981\u4f7f\u7528saver.save(sess, model_path)\u4fdd\u5b58\u6a21\u578b\u53c2\u6570\u3002 \u5982\u679ctrain\u4e3aFalse\uff0c\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6d4b\u8bd5\u9700\u8981\u4f7f\u7528saver.restore(sess, model_path)\u8bfb\u53d6\u53c2\u6570\u3002  \u8bad\u7ec3\u9636\u6bb5\u6267\u884c if train:        print(\"\u8bad\u7ec3\u6a21\u5f0f\")        # \u5982\u679c\u662f\u8bad\u7ec3\uff0c\u521d\u59cb\u5316\u53c2\u6570        sess.run(tf.global_variables_initializer())        # \u5b9a\u4e49\u8f93\u5165\u548cLabel\u4ee5\u586b\u5145\u5bb9\u5668\uff0c\u8bad\u7ec3\u65f6dropout\u4e3a0.25        train_feed_dict = {            datas_placeholder: datas,            labels_placeholder: labels,            dropout_placeholdr: 0.25        }        for step in range(150):            _, mean_loss_val = sess.run([optimizer, mean_loss], feed_dict=train_feed_dict)            if step % 10 == 0:                print(\"step = {}\\tmean loss = {}\".format(step, mean_loss_val))        saver.save(sess, model_path)        print(\"\u8bad\u7ec3\u7ed3\u675f\uff0c\u4fdd\u5b58\u6a21\u578b\u5230{}\".format(model_path)) \u6d4b\u8bd5\u9636\u6bb5\u6267\u884c else:     print(\"\u6d4b\u8bd5\u6a21\u5f0f\")     # \u5982\u679c\u662f\u6d4b\u8bd5\uff0c\u8f7d\u5165\u53c2\u6570     saver.restore(sess, model_path)     print(\"\u4ece{}\u8f7d\u5165\u6a21\u578b\".format(model_path))     # label\u548c\u540d\u79f0\u7684\u5bf9\u7167\u5173\u7cfb     label_name_dict = {         0: \"\u98de\u673a\",         1: \"\u6c7d\u8f66\",         2: \"\u9e1f\"     }     # \u5b9a\u4e49\u8f93\u5165\u548cLabel\u4ee5\u586b\u5145\u5bb9\u5668\uff0c\u6d4b\u8bd5\u65f6dropout\u4e3a0     test_feed_dict = {         datas_placeholder: datas,         labels_placeholder: labels,         dropout_placeholdr: 0     }     predicted_labels_val = sess.run(predicted_labels, feed_dict=test_feed_dict)     # \u771f\u5b9elabel\u4e0e\u6a21\u578b\u9884\u6d4blabel     for fpath, real_label, predicted_label in zip(fpaths, labels, predicted_labels_val):         # \u5c06label id\u8f6c\u6362\u4e3alabel\u540d         real_label_name = label_name_dict[real_label]         predicted_label_name = label_name_dict[predicted_label]         print(\"{}\\t{} => {}\".format(fpath, real_label_name, predicted_label_name)) ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a793"}, "repo_url": "https://github.com/gyadam/behavioral-cloning", "repo_name": "behavioral-cloning", "repo_full_name": "gyadam/behavioral-cloning", "repo_owner": "gyadam", "repo_desc": "Behavioral Cloning: Project for Udacity Self-Driving Car Nanodegree", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T20:43:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T18:50:45Z", "homepage": null, "size": 17984, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187696275, "is_fork": false, "readme_text": "Behavioral Cloning Project #4 for Udacity Self-Driving Car Nanodegree By Adam Gyarmati   Overview: Behavioral Cloning The goals of this project were the following:  Use the simulator build by Udacity to collect data of good driving behavior Build a convolution neural network in Keras that predicts steering angles from images Train and validate the model with a training and validation set Test that the model successfully drives around track one without leaving the road  1. Files The project includes the following files:  model.py containing the script to create and train the model drive.py for driving the car in autonomous mode  Additionally, other files which were not uploaded due to their large size:  model.h5 containing a trained convolution neural network video_tr1.mp4 showing a video recording of the vehicle driving autonomously with model.h5 on track 1 video_tr2.mp4 showing a video recording of the vehicle driving autonomously with model.h5 on track 2  2. Code Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing python drive.py model.h5 The model.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works. Model Architecture and Training Strategy 1. Solution Design Approach Since I wanted to make my model work on both tracks, I collected data from both tracks. Because I wanted the vehicle to keep its lane on the second track, I decided to teach it to always keep to the right, even on the first track. Therefore I also recorded recoveries from the left or center back to the right side. The final dataset consisted of:  1 + 1 lap on track 1 in both directions keeping to the right 2 laps on track 2 in the same direction staying in the right lane recordings of recovery from the side of the lanes for both tracks (8-10 recoveries/track) multiple recordings of \"tricky parts\", e.g. sharp turns  2. Model architecture I got to my final model architecture after a lot of experiments using the NVIDIA model suggested by Udacity and modifying it to get better results. The model accepts preprocessed images of the road made by the center camera. An example of a raw camera image from the recording is provided below:  Preprocessing consists of:  cropping the images to delete irrelevant sections like trees and the front of the vehicle (see example images below) normalizing the pixel values from the range (0, 255) to (-0.5, 0.5)  Example of a cropped image (this may be a lot of cropping, but I found it useful especially for the second track):  The convolutional neutal network model is based on NVIDIA's end-to-end deep learning model, which uses the following architecture (image sizes are different in my network due to the different input size):  I made the following modifications to make the model work for me:  added Batch Normalization after all convolutional layers and the first three dense layers used ELU (Exponential Linear Unit) activation functions after Batch Normalization added Dropout layers (with 50% dropout rate) after the first two dense layers  3. Training the model After the collection process, I had about 57000 number of data points. I randomly shuffled the data set and put 5% of the data into a validation set. I used the training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was 10 as this provided the best results in the simulator (although the validation loss was not always the lowest). I used an adam optimizer so that manually training the learning rate wasn't necessary. 4. Attempts to reduce overfitting in the model The model contains dropout layers in order to reduce overfitting (model.py lines 91 and 96). The model was trained and validated on different data sets to ensure that the model was not overfitting (model.py line 114). The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track. 5. Results and further improvement potential My model is able to output the correct angle to steer the car autonomously on both tracks, and keeps to the right on both tracks, which I think is more realistic than driving in the middle even in the case of the first track. However there is still room for improvement:  driving could be smoother, and in some cases the vehicle keeps to much to the right, risking hitting a curb or going off track since I trained the model using images from the simulator in the fastest (= lower quality graphics) mode, there are features in the high quality mode (e.g. shadows) that it never saw during training this is not an issue for the first track, but the model fails on the second track in high quality mode therefore, data should be collected from high quality mode to train the model with the additional features (most importantly the shadows)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a794"}, "repo_url": "https://github.com/ebucheli/SimpleSpeechCommandsPaper", "repo_name": "SimpleSpeechCommandsPaper", "repo_full_name": "ebucheli/SimpleSpeechCommandsPaper", "repo_owner": "ebucheli", "repo_desc": "This repo contains the code for the paper \"A Review of Simple Speech Detection Using Deep Learning\". Currently only published in spanish.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T20:03:58Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T04:51:52Z", "homepage": null, "size": 16739, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 187965082, "is_fork": false, "readme_text": "Simple Speech Commands Detection This repo presents various architectures to solve the Keyword Detection Problem using Deep Learning and the Simple Speech Commands Detection dataset from TensorFlow. This code was generated as part of a research project to be published at COMIA 2019 in Mexico, currently only available in spanish. The name of the paper is \"Deteccion de Comandos de Voz con Modelos Compactos de Aprendizaje Profundo\" (A Review of Small-Footprint Speech Commands Detection Using Deep Learning) Structure This research was a comparative study of audio representations and artificial neural networks. You can find four basic types of Neural Network Architectures in this repository.  MLP (in FFNetworks.py) 1D Convnets (in CNNetworks1D.py) 2D Convnets (in CNNetworks2D.py) CRNN (in RNNetworks.py)  There are also four basic input representations for audio. 2, 3 and 4 are all based in the Fourier Transform and are all time-frequency representations.  Waveforms Power Spectrograms Mel Spectrograms Mel Frequency Cepstral Coefficients (MFCC)  Using main.py you can create a model with a combination of these two aspects. However, architectures made for waveforms are only compatible with waveforms. Usage You can execute using python main.py [OPTIONS] PATH from the command line. You need to specify PATH to point where the data set is in your system. It expects the directory with the subfolders for every word i.e. /yes,/no, etc. You can use the --help flag to learn about the usage. Below is a description of the available options.  --problem [INTEGER]: Select the version of the problem, here you can choose between classifying 10 words (plus unknown and silence) (0), 20 words (plus unknown and silence) (1) and 2Words (Left/Right) (plus unknown and silence) (2). If you choose 10 Words you need v0.01 of the dataset. --transformation [INTEGER]: Select the input representation; Waveform (0), Power Spectrogram (1), Mel Spectrogram (2), or MFCC (3). If no transformation is specified, Waveforms will be used for Networks 0, 1 and 2 and MS-40 for the rest. --mels [INTEGER]: Choose the Frequency resolution for Mel Spectrograms and MFCC (either 40, 80 or 120). --network [INTEGER]: Choose the Architecture, check next section or use --help for a breakdown. --train/--no_train:  If you wish to use pre-trained weights, use --no_train. If so please specify the file using --weights_file. --weights_file [TEXT]: Name of the file with the pre-trained weights, the package assumes it is in trained_weights. Use --no_train. --epochs [INTEGER]: Specify the number of epochs. --save_w: Use this flag to save the weights after the model has been trained. Please specify the name of the file using --outfile --outfile [TEXT]: Specify the name of the output file with the weights.  Networks We have provided 10 different Artificial Neural Network architectures. The first three are meant for waveforms and thus can't be used with time-frequency representations, the inverse is true for the final seven networks.  CNN 1D: Taken from here. CRNN 1D: This network was inspired by this paper but applied to waveforms. attRNN 1D: Same as before but with this paper. FCNN: Baseline model described here O'Malley: This model was proposed by the second place winner of the TensorFlow Speech Recognition Challenge in Kaggle, Thomas O'Malley. He explains it here. CNN_TRAD_FPOOL3: From the same paper as FCNN. CNN_ONE_FSTRIDE4: From the same paper as FCNN and CNN_TRAD_FPOOL3. CRNN1 2D: This is on of the networks from the same paper as the one referenced in CRNN 1D. CRNN2 2D: Same as CRNN1 2D. attRNN 2D: This network was the one proposed in the paper referenced in attRNN 1D and you can also find it here.  Examples There are defaults for all the options, if you run, python main.py the model will run using waveforms and the architecture CNN1D on the 10 Words problem. The model will be trained but the weights will not be saved. If you want to use a Mel Spectrogram with a frequency resolution of 80 on the 20 word problem and the CRNN1-2D architecture and save the weights you can run, python main.py --problem 1 --transformation 2 --mels 80 --network 7 --save_w --outfile \"example_weights.h5\" You can also use pre-trained weights either created by you or from some of our previously generated one in trained_weights. The package assumes the file is in said directory; python main.py --no_train --weights_file 'WF_CNN1D_10Words.h5' Requirements The models were created using Python 3.6 with Keras using the TensorFlow backend. You will also need the Librosa package. Other dependencies includes the tqdm package. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/e16b07793da73e30352408bb4230b31067b429c6/trained_weights/WF_CNN1D_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/e16b07793da73e30352408bb4230b31067b429c6/trained_weights/WF_CRNN1D_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/MS40_CRNN2DV2_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/MS40_Malley_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/MS40_attRNN2D_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/MS80_CRNN2-2D_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/MS80_Malley_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/MS80_attRNN-2D_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/PS257_CRNN2DV2_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/PS257_attRNN2D_10Words.h5", "https://github.com/ebucheli/SimpleSpeechCommandsPaper/blob/85b14cc2bb929b47925af786cb37dd294bfc0087/trained_weights/WF_attRNN1D_10Words.h5"], "see_also_links": ["http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz"], "reference_list": ["https://arxiv.org/abs/1703.05390", "https://arxiv.org/abs/1808.08929"]}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a795"}, "repo_url": "https://github.com/minoh0201/DeepMicro", "repo_name": "DeepMicro", "repo_full_name": "minoh0201/DeepMicro", "repo_owner": "minoh0201", "repo_desc": "Deep representation learning for prediction of disease state based on microbiome data", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T04:19:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-22T19:11:02Z", "homepage": null, "size": 15848, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188098686, "is_fork": false, "readme_text": "DeepMicro DeepMicro is a deep representation learning framework exploiting various autoencoders to learn robust low-dimensional representations from high-dimensional data and training classification models based on the learned representation. Quick Setup Guide Step 1: Change the current working directory to the location where you want to install DeepMicro. Step 2: Clone the repository using git command ~$ git clone https://github.com/minoh0201/DeepMicro ~$ cd DeepMicro  Step 3: Create virtual environment using Anaconda3 (Read Anaconda3 install guide) and activate the virtual environment ~$ conda create --name deep_env python=3.5  ~$ conda activate deep_env  Step 4: Install required packages, then install tensorflow. ~$ pip install --upgrade pip && pip install numpy==1.16.2 && pip install pandas==0.24.2 && pip install scipy==1.2.1 && pip install sklearn==0.20.3 && pip install scikit-learn==0.20.3 && pip install matplotlib==3.0.3 && pip install psutil==5.6.1 && pip install keras==2.2.4   If your machine is not equipped with GPU, install tensorflow CPU version ~$ pip install tensorflow==1.13.1   If it is equipped with GPU, then install tensorflow GPU version ~$ pip install tensorflow-gpu==1.13.1    Step 5: Run DeepMicro, printing out its usage. ~$ python DM.py -h  Quick Start Guide Make sure you have already gone through the Quick Setup Guide above. Learning representation with your own data 1. Copy your data under the /data directory. Your data should be a comma separated file without header and index, where each row represents a sample and each column represents a microbe. We are going to assume that your file name is UserDataExample.csv which is already provided. 2. Check your data can be successfully loaded and verify its shape with the following command. ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv  The output will show the number of rows and columns right next to X_train.shape. Our data UserDataExample.csv contains 80 rows and 200 columns. Using TensorFlow backend. Namespace(act='relu', ae=False, ae_lact=False, ae_oact=False, aeloss='mse', cae=False, custom_data='UserDataExample.csv', custom_data_labels=None, data=None, dataType='float64', data_dir='', dims='50', max_epochs=2000, method='all', no_clf=True, numFolds=5, numJobs=-2, patience=20, pca=False, repeat=1, rf_rate=0.1, rp=False, save_rep=False, scoring='roc_auc', seed=0, st_rate=0.25, svm_cache=1000, vae=False, vae_beta=1.0, vae_warmup=False, vae_warmup_rate=0.01) X_train.shape:  (80, 200) Classification task has been skipped.  3. Suppose that we want to reduce the number of dimensions of our data to 20 from 200 using a shallow autoencoder. Note that --save_rep argument will save your representation under the /results folder. ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv --ae -dm 20 --save_rep  4. Suppose that we want to use deep autoencoder with 2 hidden layers which has 100 units and 40 units, respectively. Let the size of latent layer to be 20. We are going to see the structure of deep autoencoder first. ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv --ae -dm 100,40,20 --no_trn  It looks fine. Now, run the model and get the learned representation. ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv --ae -dm 100,40,20 --save_rep  5. We can try variational autoencoder and * convolutional autoencoder* as well. Note that you can see detailed argument description by using -h argument. ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv --vae -dm 100,20 --save_rep  ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv --cae -dm 100,50,1 --save_rep  Conducting binary classification after Learning representation with your own data 1. Copy your data file and label file under the /data directory. Your data file should be a comma separated value (CSV) format without header and index, where each row represents a sample and each column represents a microbe. Your label file should contain a binary value (0 or 1) in each line and the number of lines should be equal to that in your data file. We are going to assume that your data file name is UserDataExample.csv and label file name is UserLabelExample.csv which are already provided. 2. Check your data can be successfully loaded and verify its shape with the following command. ~$ python DM.py -r 1 --no_clf -cd UserDataExample.csv -cl UserLabelExample.csv  Our data UserDataExample.csv consists of 80 samples each of which has 200 features. The data will be split into the training set and the test set (in 8:2 ratio). The output will show the number of rows and columns for each data set. Namespace(act='relu', ae=False, ae_lact=False, ae_oact=False, aeloss='mse', cae=False, custom_data='UserDataExample.csv', custom_data_labels='UserLabelExample.csv', data=None, dataType='float64', data_dir='', dims='50', max_epochs=2000, method='all', no_clf=True, no_trn=False, numFolds=5, numJobs=-2, patience=20, pca=False, repeat=1, rf_rate=0.1, rp=False, save_rep=False, scoring='roc_auc', seed=0, st_rate=0.25, svm_cache=1000, vae=False, vae_beta=1.0, vae_warmup=False, vae_warmup_rate=0.01) X_train.shape:  (64, 200) y_train.shape:  (64,) X_test.shape:  (16, 200) y_test.shape:  (16,) Classification task has been skipped.  3. Suppose that we want to directly apply SVM algorithm on our data without representation learning.  Remove --no_clf command and specify classification method with -m svm argument (If you don't specify classification algorithm, all three algorithms will be running). ~$ python DM.py -r 1 -cd UserDataExample.csv -cl UserLabelExample.csv -m svm  The result will be saved under /results folder as a UserDataExample_result.txt. The resulting file will be growing as you conduct more experiments. 4. You can learn representation first, and then apply SVM algorithm on the learned representation. ~$ python DM.py -r 1 -cd UserDataExample.csv -cl UserLabelExample.csv --ae -dm 20 -m svm  5. You can repeat the same experiment by changing seeds for random partitioning of training and test set.  Suppose we want to repeat classfication task five times. You can do it by put 5 into -r argument. ~$ python DM.py -r 5 -cd UserDataExample.csv -cl UserLabelExample.csv --ae -dm 20 -m svm  Reproducing the experiments described in our paper 1. Unzip abundance.zip and marker.zip files under the /data directory. ~$ cd data ~$ unzip abundance.zip && unzip marker.zip ~$ cd ..  2. Specify dataset name to run. Choose dataset you want to run. You can choose one of the followings: abundance_Cirrhosis, abundance_Colorectal, abundance_IBD, abundance_Obesity, abundance_T2D, abundance_WT2D, marker_Cirrhosis, marker_Colorectal, marker_IBD, marker_Obesity, marker_T2D, marker_WT2D. Note that WT2D indicates European Women cohort (EW-T2D) and T2D indicates Chinese cohort (C-T2D). 3. Run experiments, specifying autoencoder details. Suppose we are going to run the best representation model on marker profile of EW-T2D dataset as shown in Table S1. Then, all three classification algorithms are trained and evaluated. We are going to repeat this process 5 times with the following command: ~$ python DM.py -d marker_WT2D --ae -dm 256  Note that if you don't specify -r argument, it will repeat five times by default. We can use all available CPU cores when we train classification models by introducing -t -1 argument. Here are another examples using a single classification algorithm. ~$ python DM.py -d marker_T2D --cae -dm 4,2 -m mlp  ~$ python DM.py -d abundance_Obesity --cae -dm 4,2 -m rf  ~$ python DM.py -d marker_Colorectal --dae -dm 512,256,128 -m mlp  The result will be saved under /results folder in a file whose name is ended with _results.txt (e.g. marker_WT2D_result.txt) Citation TBA ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a796"}, "repo_url": "https://github.com/datadrivenempathy/who-wrote-this-training", "repo_name": "who-wrote-this-training", "repo_full_name": "datadrivenempathy/who-wrote-this-training", "repo_owner": "datadrivenempathy", "repo_desc": "Logic for building and training the models for whowrotethis.com", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T22:38:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-20T19:22:15Z", "homepage": null, "size": 116, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 187700358, "is_fork": false, "readme_text": "Who Wrote This Training Code and reference configurations used in research for https://whowrotethis.com and \"Machine Learning Techniques for Detecting Identifying Linguistic Patterns in the News Media\" by A. Samuel Pottinger. Released under the MIT license. For data, please see https://whowrotethis.com.   Purpose This repository provides a harness based on the Template Method pattern to train a neural network to predict from which news agency an article was published given only a short snippet of text. When used from the command line, it takes in a JSON file describing how to run the harness and typically a SQLite database with article information. These scripts can also optionally report results out to Weights and Biases (referred to as W&B).   Environment Setup These scripts will require Python 3, pip, and a number of supporting libraries. Setup instructions for the language are platform specific. Note that users may choose to use a virtual environment though the specifics of that setup are not discussed here. Finally, users will require a database of articles which is available from https://whowrotethis.com. After having Python, pip, optionally a virtual environment set up, and a copy of the articles database, one can install the supporting libraries by executing $ pip install -r requirements.txt.   Usage While these scripts can be imported to other Python code, typical usage will come from command line operation. This can be done either by running many scripts or a single configuration. Those configurations are described below.  Running a single configuration Running a single configuration will cause a new model to be trained and predictions using that model to be persisted into the input sqlite database. The required script can be run from the command line with $ python run_single.py [path to json config] [project name] [run name] [path to sqlite] [write predictions] [optional output path for accuracy] [optional output path for source performance] [optional path for predictions output]. The arguments are as follows:  The path to the JSON configuration (as described below) with which to execute the pipeline. The name of the project. This will be used with W&B if enabled. The name to give to this specific execution. This will be used with W&B if enabled. Path to the sqlite database from which article information will be read and into which predictions will be written. If 't' (case-insensitive), will write predictions to SQLite database. Otherwise, predictions will not be persisted. Optional path to where accuracy statistics (test, train, and validation) should be written. Optional path to where source by source performance statistics (precision, recall) should be written. Optional path to where predictions should be written.  An example of a single configuration file is at config/selected_network.json.  Running multiple configurations Running multiple configurations will train multiple models, optionally writing the results to W&B. This will not cause predictions to be persisted into the sqlite database. The required script can be run from the command line with $ python run_set.py [json_file] [db loc]. The arguments are as follows:  The path to the JSON configurations (as described below) with which to execute the pipeline. Path to the sqlite database from which article information will be read.  An example of a multiple configuration file is at config/configs_combined.json.  Configuration options The available configuration settings for a single configuration are as follows:  corpusCol: The name of the column like description from the database from which the model will be trained. denseSize1: If using the feed forward network occurrence network, this the number of units to appear in the first dense layer. denseSize2: If using the feed forward network occurrence network, this the number of units to appear in the second dense layer. dropoutRate: The dropout rate to use for internal layers of a network from 0 to 1. kernelRegPenalty: The L2 activation penalty to apply for internal layers of a network from 0 to 1. lstmSize: If using the recurrent sequence LSTM network, the number of LSTM units to appear in the model. maxSeqLen: If using the recurrent sequence LSTM network, the maximum sequence length to feed into the network. Sequences will be cut to [0,n) tokens or padded to be of length n where n equals this value. method: The type of network to use. The value of \"occurrence\" will cause the pipeline to train with the feed forward network operating on word occurrences / co-occurrences. The value of \"sequence\" will cause the pipeline to train with the LSTM recurrent structure. numWords: The maximum number of words with which to train. The top most frequent \"n\" words will be retained where n equals the value of this parameter. sourceCol: The name of the column from the database indicating which news agency published an article. sourceIdCol: The data frame column in which the unique numeric ID of an agency should be written. This value is not persisted but is useful when debugging. sourceIdVectorCol: The data frame column in which a vectorization of an agency should be written. This value is not persisted but is useful when debugging. tokenVectorCol: The data frame column in which the vectorization of the article content should be written. This value is not persisted but is useful when debugging. tokensCol: The data frame column in which the tokenization of the article content should be written. This value is not persisted but is useful when debugging.  There are the following optional parameters as well:  foxWeight: The amount of resampling to apply for Fox between 0 (no resampling) and 1 (every article duplicated). This only impacts the training set. Defaults to 1. useWandb: Flag indicating if results should be reported to Weights and Biases. Defaults to True. epochs: The number of epochs for which the model should be trained. Defaults to 30.   Database structure Note that the scripts expect a table or view in the sqlite database called articles_clean_assigned which can be created using script/create_sets.py and has the following columns:  source: String column indicating which agency published the article (represented by the row). title: The text of the tile for the article (represented by the row). description: The description provided the for the article (represented by the row) by the publishing agency. setAssignemnt: Strings 'test', 'train', or 'validation' indicating in which set the article was assigned.  If persisting results of the model (using run_single.py), the predictions will be saved to a new table called predictions.  File Formats For the JSON input files, running run_single.py expects a single JSON object with the configuration as demonstrated in condfig/selected_network.json. Meanwhile, running run_set.py expects a root object with the attribute configs which itself contains an array of objects with attributes name (the name of the configuration, will be reported as the \"run name\" in W&B) and config (the configuration as described above). See config/configs_combined.json for an example. One may also optionally include project in the root object for run_set.py which will override the name of the W&B project with which the run will be associated.   Testing Automated tests are provided via the Python standard unittest library. Simply execute nosetests to run: $ nosetests ........................ ---------------------------------------------------------------------- Ran 24 tests in 9.934s  OK    Release This model itself does not have a release to production step but the sqlite database after running run_single.py can be used with the Who Wrote This open source web application or can be evaluated using SQL scripts provided in the sql directory.   Coding Standards Please provide unit tests where possible and conform to the Google Python and documentation style where possible.   Related Projects Note that this is in a series of related projects as linked:  who-wrote-this-training: logic for machine learning. who-wrote-this-server: web application to demo the model. who-wrote-this-news-crawler: crawler to record RSS feeds.    License and Open Source Libraries Used This project is made available under the MIT license as described in LICENSE.txt. It uses the following open source libraries internally:  beautifulsoup4 used under the MIT license. gensim used under the LGPLv2 license. Keras used under the MIT license. lxml used under the BSD license. numpy used under the BSD license. pandas used under the BSD license. scikit_learn used under the BSD license. tabulate used under the MIT license. tensorflow v1 used under the Apache v2 License. textblob used under the MIT license. wandb used under the MIT license.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html#license"], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a797"}, "repo_url": "https://github.com/gwyxjtu/Deep_Q_Learning", "repo_name": "Deep_Q_Learning", "repo_full_name": "gwyxjtu/Deep_Q_Learning", "repo_owner": "gwyxjtu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T04:38:36Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-21T08:51:05Z", "homepage": null, "size": 500, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 1, "github_id": 187798880, "is_fork": false, "readme_text": "Deep_Q_Learning \u5927\u81f4\u4ecb\u7ecd \u8fd9\u4e2a\u9879\u76ee\u7684\u6765\u6e90\u662f\u524d\u4e0d\u4e45\u7684\u4e24\u7bc7\u8bba\u6587\uff0cPlaying Atari with Deep Reinforcement Learning\uff0c\u548cDeepMind-DQN-nature\u3002\u8bba\u6587\u4e2d\uff0c\u4f5c\u8005\u4f7f\u7528\u5f3a\u5316\u5b66\u4e60\u7684\u65b9\u6cd5\u8ba9\u4eba\u5de5\u667a\u80fd\u5b66\u4f1a\u4e86\u6253\u6e38\u620f\uff0c\u5305\u62ecatria\u4e2d\u7684\u591a\u6b3e\u6e38\u620f\u3002\u6709\u90e8\u5206\u7684\u6e38\u620f\u6548\u679c\u5f88\u597d\uff0c\u4e5f\u6709\u90e8\u5206\u7684\u6548\u679c\u4e0d\u597d\uff0c\u8fd9\u4e2a\u4ee3\u7801\u7684\u7528\u5904\u5c31\u662f\u590d\u73b0\u4e86\u4e00\u4e0b\u4f7f\u7528CNN\u548c\u5f3a\u5316\u5b66\u4e60\u6765\u8bad\u7ec3agent\u6765\u73a9\u592a\u7a7a\u5165\u4fb5\u8005\u8fd9\u6b3e\u6e38\u620f\u3002  \u524d\u671f\u7684\u51c6\u5907\u5de5\u4f5c \u5b89\u88c5gym \u9996\u5148\u5f97\u5b89\u88c5gym\u4e2d\u7684atari\u90e8\u5206\uff0c\u5982\u679c\u6709\u9519\u8bef\uff0c\u53ef\u4ee5\u53c2\u8003\u6211\u4e4b\u524d\u5199\u7684\u535a\u5ba2 \u4f20\u9001\u95e8 \u5b89\u88c5opencv opencv\u662f\u7528\u6765\u5bf9\u56fe\u50cf\u8fdb\u884c\u5904\u7406\u7684\uff0c\u53ef\u4ee5\u7701\u4e0b\u5f88\u591a\u529f\u592b\u3002 \u53ef\u4ee5\u76f4\u63a5pip install opencv-python \u5b89\u88c5keras keras\u7c7b\u4f3c\u4e8epytorch\uff0c\u6784\u5efa\u7f51\u7edc\u5341\u5206\u65b9\u4fbf\u3002\u4f46\u662f\u524d\u63d0\u662f\u5df2\u7ecf\u5b89\u88c5\u597dtensorflow\u3002 \u8bba\u6587\u4e2d\u6570\u5b66\u601d\u60f3 \u8fd9\u91cc\u4ee5\u8bba\u6587\u4e3a\u57fa\u7840\uff0c\u8bb2\u4e00\u4e0b\u6211\u7684\u7406\u89e3\uff0c\u96be\u514d\u4f1a\u9519\u8bef\uff0c\u8fd8\u8bf7\u5927\u4f6c\u4eec\u63d0\u51fa\u3002 Playing Atari with Deep Reinforcement Learning  \u8fd9\u7bc7\u8bba\u6587\u53ef\u4ee5\u89e3\u51b3\u7684\u95ee\u9898  \u5927\u591a\u6570\u6df1\u5ea6\u5b66\u4e60\u90fd\u4f9d\u9760\u5927\u91cf\u7684\u624b\u5de5\u6570\u636e\u96c6\u6807\u8bb0 \u6df1\u5ea6\u5b66\u4e60\u7684\u6837\u672c\u90fd\u662f\u72ec\u7acb\u7684\uff0cRL\u4e2d\u786e\u5b9e\u6709\u8bb0\u5fc6\u6027\u7684\uff0c\u524d\u540e\u7684\u72b6\u6001\u4f1a\u4e92\u76f8\u5f71\u54cd\u3002 \u6df1\u5ea6\u5b66\u4e60\u7684\u76ee\u6807\u5206\u5e03\u662f\u56fa\u5b9a\u7684\uff0c\u5f3a\u5316\u5b66\u4e60\u7684\u76ee\u6807\u4e0d\u56fa\u5b9a\uff0c\u6bd4\u5982\u6e38\u620f\u5f80\u540e\u9762\u4f1a\u5207\u6362\u573a\u666f\uff0c\u5c31\u5f97\u5206\u5f00\u8bad\u7ec3\u6216\u8005\u5207\u6362\u6a21\u578b\u3002   \u6a21\u62df\u5668\u7684\u5185\u90e8\u662f\u4e0d\u80fd\u88abagent\u89c2\u5bdf\u7684\uff0c\u89c2\u5bdf\u7684\u662f\u56fe\u7247\u3002 agent\u7684\u76ee\u7684\u662f\u4e0e\u6a21\u62df\u5668\u4ea4\u4e92\uff0c\u9009\u62e9reward\u6700\u5927\u7684action \u4e0eTD-Gammon\u548c\u7c7b\u4f3c\u7684\u5728\u7ebf\u65b9\u6cd5\u5f62\u6210\u5bf9\u6bd4\uff0c\u6211\u4eec\u4f7f\u7528\u4e00\u79cd\u88ab\u79f0\u4e3a\u7ecf\u9a8c\u91cd\u653e\u7684\u6280\u672f\uff0c\u6211\u4eec\u5728\u6bcf\u4e00\u4e2a\u65f6\u95f4\u6b65\u957f\u4e2d\u5b58\u50a8agent\u7684\u7ecf\u9a8c\uff0c\u628a\u6bcf\u4e00\u6b65\u7684\u7ecf\u9a8c\u5b58\u50a8\u5728\u6570\u636e\u96c6\u4e2d\u3002\u5728\u7b97\u6cd5\u7684\u5185\u90e8\u5faa\u73af\u4e2d\uff0c\u6211\u4eec\u4ece\u5b58\u50a8\u7684\u6837\u672c\u6c60\u4e2d\u968f\u673a\u62bd\u53d6e \u223c D\uff0c\u5e76\u7528Q-learning\u66f4\u65b0\u6216\u5c0f\u6279\u91cf\u66f4\u65b0\u3002\u5728\u6267\u884c \u7ecf\u9a8c\u91cd\u653e \u540e\uff0cagent\u6839\u636e \u8d2a\u5a6a\u7b56\u7565 \u9009\u62e9\u5e76\u6267\u884c\u4e00\u4e2aaction\u3002\u7531\u4e8e\u4f7f\u7528\u4efb\u610f\u957f\u5ea6\u7684\u5386\u53f2\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u53ef\u80fd\u5f88\u56f0\u96be\uff0c\u53d6\u800c\u4ee3\u4e4b\uff0c\u6211\u4eec\u7684q\u51fd\u6570\u4f5c\u7528\u4e8e\u56fa\u5b9a\u957f\u5ea6\u7684\u8868\u793a\uff0c\u8fd9\u4e2a\u8868\u793a\u662f\u7531 \u51fd\u6570 \u63d0\u4f9b\u7684\u3002\u7b97\u6cd51\u7ed9\u51fa\u4e86\u5b8c\u6574\u7b97\u6cd5\uff0c\u6211\u4eec\u79f0\u4e4b\u4e3a\u6df1\u5ea6Q-learning\u3002 \u8fd9\u79cd\u65b9\u6cd5\u6bd4\u6807\u51c6\u7684\u5728\u7ebfQ-learning[23]\u6709\u51e0\u4e2a\u4f18\u70b9\u3002  \u9996\u5148\uff0c\u7ecf\u9a8c\u7684\u6bcf\u4e00\u6b65\u90fd\u6709\u53ef\u80fd\u7528\u4e8e\u8bb8\u591a\u6743\u91cd\u66f4\u65b0\uff0c\u4ece\u800c\u63d0\u9ad8\u6570\u636e\u6548\u7387\u3002 \u7b2c\u4e8c\uff0c\u76f4\u63a5\u4ece\u8fde\u7eed\u7684\u6837\u672c\u4e2d\u5b66\u4e60\u6548\u7387\u5f88\u4f4e\uff0c\u56e0\u4e3a\u6837\u672c\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5f88\u5f3a\uff1b\u968f\u673a\u5316\u6837\u672c\u6253\u7834\u4e86\u8fd9\u4e9b\u76f8\u5173\u6027\uff0c\u56e0\u6b64\u51cf\u5c11\u4e86\u66f4\u65b0\u7684\u65b9\u5dee\u3002 \u7b2c\u4e09\uff0c\u5f53\u5728\u7b56\u7565\u4e0a\u5b66\u4e60\u65f6\uff0c\u5f53\u524d\u53c2\u6570\u51b3\u5b9a\u4e86\u4e0b\u4e00\u4e2a\u53c2\u6570\u6240\u8bad\u7ec3\u7684\u6570\u636e\u6837\u672c\u3002\u4f8b\u5982\uff0c\u5982\u679c\u51fd\u6570\u503c\u6700\u5927\u7684\u52a8\u4f5c\u662f\u5de6\u79fb\uff0c\u5219\u8bad\u7ec3\u6837\u672c\u5c06\u7531\u6765\u81ea\u5de6\u624b\u4fa7\u7684\u6837\u672c\u4e3b\u5bfc\uff1b\u5982\u679c\u6700\u5927\u5316\u7684\u52a8\u4f5c\u5207\u6362\u5230\u53f3\u8fb9\uff0c\u90a3\u4e48\u8bad\u7ec3\u7684\u5206\u5e03\u4e5f\u4f1a\u6539\u53d8\u3002\u56e0\u6b64\uff0c\u53ef\u80fd\u51fa\u73b0\u4e0d\u5e0c\u671b\u6709\u7684\u53cd\u9988\u56de\u8def\uff0c\u4e14\u8fd9\u4e9b\u53c2\u6570\u53ef\u80fd\u4f1a\u9677\u5165\u4e00\u79cd\u8f83\u5dee\u7684\u5c40\u90e8\u6700\u5c0f\u503c\uff0c\u6216\u8005\u751a\u81f3\u662f\u707e\u96be\u6027\u7684\u3002\u901a\u8fc7\u4f7f\u7528\u7ecf\u9a8c\u91cd\u653e\uff0c\u884c\u4e3a\u5206\u5e03\u5728\u5b83\u4ee5\u524d\u7684\u8bb8\u591a\u72b6\u6001\u4e0a\u662f\u5e73\u6ed1\u7684\uff0c\u907f\u514d\u53c2\u6570\u7684\u632f\u8361\u6216\u53d1\u6563\u3002   \u539f\u59cb\u7684Atari\u6846\u67b6\uff08210160\u50cf\u7d20\uff0c128\u4e2a\u989c\u8272\uff09\u5bf9\u8ba1\u7b97\u8981\u6c42\u592a\u9ad8\uff0c\u56e0\u6b64\uff0c\u6211\u4eec\u5e94\u7528\u4e86\u4e00\u4e2a\u9884\u5904\u7406\u6b65\u9aa4\u6765\u964d\u4f4e\u8f93\u5165\u7684\u7ef4\u6570\u3002\u539f\u59cb\u5e27\u88ab\u9884\u5904\u7406\uff0c\u9996\u5148\u5c06\u5b83\u4eec\u7684RGB\u8f6c\u6362\u6210\u7070\u5ea6\uff0c\u5e76\u5c06\u5176\u964d\u91c7\u6837\u523011084\u7684\u56fe\u50cf\u3002\u6700\u540e\u7684\u8f93\u5165\uff1a\u88c1\u526a\u6210\u4e00\u4e2a84*84\u7684\u56fe\u50cf\u533a\u57df\uff0c\u8fd9\u4e2a\u533a\u57df\u5927\u81f4\u53ef\u4ee5\u6355\u6349\u5230\u6e38\u620f\u533a\u57df\u3002\u6700\u540e\u7684\u88c1\u526a\u9636\u6bb5\u662f\u5fc5\u9700\u7684\uff0c\u56e0\u4e3a\u6211\u4eec\u4f7f\u7528\u7684\u662f2D\u5377\u79ef\u7684GPU\u5b9e\u73b0\uff0c\u5b83\u671f\u671b\u7684\u662f\u65b9\u5f62\u8f93\u5165\u3002\u5bf9\u4e8e\u672c\u6587\u7684\u5b9e\u9a8c\uff0c\u7b97\u6cd51\u7684\u51fd\u6570\u5c06\u6b64\u9884\u5904\u7406\u5e94\u7528\u4e8e\u5386\u53f2\u7684\u6700\u540e4\u5e27\uff0c\u5e76\u5c06\u5b83\u4eec\u5806\u53e0\u8d77\u6765\u751f\u6210q\u51fd\u6570\u7684\u8f93\u5165\u3002 \u5b58\u5728\u51e0\u79cd\u201c\u7528\u795e\u7ecf\u7f51\u7edc\u5c06Q\u8fdb\u884c\u53c2\u6570\u5316\u201d\u7684\u65b9\u5f0f\u3002\u7531\u4e8eQ\u5c06\u5386\u53f2-\u52a8\u4f5c\u5bf9\u6620\u5c04\u5230\u5b83\u4eec\u7684Q\u503c\uff0c\u5386\u53f2\u548c\u52a8\u4f5c\u5728\u4ee5\u524d\u4e00\u4e9b\u65b9\u6cd5\u5df2\u7ecf\u88ab\u7528\u4f5c\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u3002\u8fd9\u79cd\u67b6\u6784\u7684\u4e3b\u8981\u7f3a\u70b9\u662f\uff1a\u9700\u8981\u5355\u72ec\u7684\u524d\u5411\u4f20\u9012\u6765\u8ba1\u7b97\u6bcf\u4e2a\u64cd\u4f5c\u7684q\u503c\uff0c\u8fd9\u5bfc\u81f4\u8ba1\u7b97\u6210\u672c\u4e0e\u64cd\u4f5c\u7684\u6570\u91cf\u6210\u7ebf\u6027\u5173\u7cfb\u3002\u8f93\u51fa\u5bf9\u5e94\u4e8e\u8f93\u5165\u72b6\u6001\u4e0b\u5355\u4e2a\u52a8\u4f5c\u7684\u9884\u6d4b\u503c\u3002\u8fd9\u79cd\u67b6\u6784\u7684\u4e3b\u8981\u4f18\u70b9\u662f\uff1a\u53ea\u901a\u8fc7\u4e00\u4e2a\u524d\u5411\u7f51\u7edc\uff0c\u80fd\u591f\u8ba1\u7b97\u7ed9\u5b9a\u72b6\u6001\u4e0b\u6240\u6709\u53ef\u80fd\u52a8\u4f5c\u7684q\u503c\u3002\u6211\u4eec\u73b0\u5728\u63cf\u8ff0\u4e86\u6240\u67097\u6b3e\u96c5\u8fbe\u5229\u6e38\u620f\u7684\u67b6\u6784\u3002\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165\u662f\u4e00\u4e2a84844\u7684\u56fe\u50cf\u3002\u7b2c\u4e00\u4e2a\u9690\u85cf\u5c42\uff1a16\u4e2a8 \u00d7 8 filters with stride 4 \uff0c\u5e76\u8fdb\u884c\u975e\u7ebf\u6027\u5904\u7406\u3002\u7b2c\u4e8c\u4e2a\u9690\u85cf\u5c42\uff1a\u5377\u79ef\u4e8632\u4e2a4 \u00d7 4 filters with stride 2\uff0c\u5e76\u8fdb\u884c\u975e\u7ebf\u6027\u5904\u7406\u3002\u6700\u540e\u7684\u9690\u85cf\u5c42\u662f\u5168\u8fde\u63a5\u7684\uff0c\u7531256\u4e2a\u6574\u6d41\u5355\u5143\u7ec4\u6210\u3002\u8f93\u51fa\u5c42\u662f\u4e00\u4e2a\u5168\u8fde\u63a5\u7684\u7ebf\u6027\u5c42\uff0c\u5bf9\u4e8e\u6bcf\u4e2a\u6709\u6548\u52a8\u4f5c\u53ea\u6709\u4e00\u4e2a\u8f93\u51fa\u3002\u5728\u6211\u4eec\u8003\u8651\u7684\u6e38\u620f\u4e2d\uff0c\u6709\u6548\u52a8\u4f5c\u7684\u6570\u91cf\u57284\u523018\u4e4b\u95f4\u53d8\u5316\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u6211\u4eec\u7684\u65b9\u6cd5\u8bad\u7ec3\u7684\u5377\u79ef\u7f51\u7edc\u79f0\u4e3a\u6df1\u5ea6q\u7f51\u7edc(Deep Q-Networks, DQN)\u3002 \u6309\u7167\u4ee5\u524d\u7684\u65b9\u6cd5\u6765\u73a9Atari\u6e38\u620f\uff0c\u6211\u4eec\u4e5f\u4f7f\u7528\u4e86\u4e00\u4e2a\u7b80\u5355\u7684\u5e27\u8df3\u8fc7\u6280\u672f\u3002\u66f4\u51c6\u786e\u7684\u8bf4\uff0c\u667a\u80fd\u4f53\u6bcfk\u5e27\u9009\u62e9\u4e00\u6b21\uff0c\u5b83\u7684\u6700\u540e\u4e00\u4e2a\u52a8\u4f5c\u5728\u8df3\u8fc7\u7684\u5e27\u4e0a\u91cd\u590d\u3002\u7531\u4e8e\u5728\u4e00\u4e2a\u6b65\u9aa4\u4e0a\u8fd0\u884c\u6a21\u62df\u5668\u9700\u8981\u6bd4\u8ba9\u667a\u80fd\u4f53\u9009\u62e9\u4e00\u4e2a\u52a8\u4f5c\u8981\u5c11\u5f97\u591a\u7684\u8ba1\u7b97\uff0c\u8fd9\u79cd\u6280\u672f\u5141\u8bb8\u667a\u80fd\u4f53\u5728\u4e0d\u663e\u8457\u589e\u52a0\u8fd0\u884c\u65f6\u7684\u60c5\u51b5\u4e0b\uff0c\u73a9\u5927\u7ea6k\u6b21\u7684\u6e38\u620f\u3002\u6211\u4eec\u5728\u6240\u6709\u6e38\u620f\u4e2d\u90fd\u4f7f\u7528k = 4\uff0c\u9664\u4e86\u300a\u592a\u7a7a\u5165\u4fb5\u8005\u300b\uff0c\u6211\u4eec\u6ce8\u610f\u5230\u4f7f\u7528k = 4\u4f1a\u4f7f\u6fc0\u5149\u770b\u4e0d\u89c1\uff0c\u6211\u4eec\u7528k = 3\u4f7f\u6fc0\u5149\u53ef\u89c1\uff0c\u8fd9\u4e2a\u53d8\u5316\u662f\u6240\u6709\u6e38\u620f\u4e2d\u8d85\u53c2\u6570\u503c\u7684\u552f\u4e00\u533a\u522b\u3002 \u5728\u8fd9\u4e9b\u5b9e\u9a8c\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e86RMSProp\u7b97\u6cd5\u3002mini-batch\u5927\u5c0f\u4e3a32\u3002\u5728\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\uff0c\u884c\u4e3a\u7b56\u7565\u662f\u8d2a\u5a6a\u7684\uff0c\u4ece1\u52300.1\u7ebf\u6027\u9000\u706b\uff0c\u5728\u7b2c\u4e00\u767e\u4e07\u5e27\u4e2d\uff0c\u6b64\u540e\u56fa\u5b9a\u4e3a0.1.\u6211\u4eec\u603b\u5171\u8bad\u7ec3\u4e861000\u4e07\u5e27\uff0c\u5e76\u4f7f\u7528\u4e86100\u4e07\u5f20\u6700\u65b0\u5e27\u7684\u91cd\u653e\u8bb0\u5fc6\u3002 \u53cc\u91cdDQN\u7684\u601d\u60f3\u662f\uff0c\u5728\u5efa\u7acb\u4e00\u4e2a\u6a21\u578b\u7684\u540c\u65f6\uff0c\u5efa\u7acb\u4e00\u4e2a\u6709\u5ef6\u8fdf\u7684\u6a21\u578b\u7528\u6765\u8fdb\u884c\u9a8c\u8bc1\uff0c\u56e0\u4e3a\u5982\u679c\u4f7f\u7528\u540c\u4e00\u4e2a\u6a21\u578b\u6765\u5c3d\u5fc3\u9884\u6d4b\u548c\u9a8c\u8bc1\u7684\u8bdd\uff0c\u8bf4\u670d\u529b\u4f1a\u66f4\u5c0f\u3002  Q-Learning\u7b97\u6cd5 Q-Learning\u7b97\u6cd5\u4e0b\uff0c\u76ee\u6807\u662f\u8fbe\u5230\u76ee\u6807\u72b6\u6001(Goal State)\u5e76\u83b7\u53d6\u6700\u9ad8\u6536\u76ca\uff0c\u4e00\u65e6\u5230\u8fbe\u76ee\u6807\u72b6\u6001\uff0c\u6700\u7ec8\u6536\u76ca\u4fdd\u6301\u4e0d\u53d8\u3002\u56e0\u6b64\uff0c\u76ee\u6807\u72b6\u6001\u53c8\u79f0\u4e4b\u4e3a\u5438\u6536\u6001\u3002 \u901a\u5e38\uff0c\u6211\u4eec\u9700\u8981\u6784\u5efa\u4e00\u4e2a\u5373\u65f6\u5956\u52b1\u77e9\u9635R\uff0c\u7528\u4e8e\u8868\u793a\u4ece\u72b6\u6001s\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001s\u2019\u7684\u52a8\u4f5c\u5956\u52b1\u503c\u3002 \u7531\u5373\u65f6\u5956\u52b1\u77e9\u9635R\u8ba1\u7b97\u5f97\u51fa\u6307\u5bfcagent\u884c\u52a8\u7684Q\u77e9\u9635\u3002 $Q(s,a)=R(s,a)+\\gamma max[Q(s',all action)]$ \u5176\u4e2d$s'$\u8868\u793a\u4e0b\u4e00\u4e2a\u72b6\u6001\u3002\u901a\u8fc7\u8fd9\u4e2a\u63a8\u5230\u516c\u5f0f\u53ef\u4ee5\u8ba1\u7b97\u51faQ\u77e9\u9635\u7684\u5143\u7d20\u3002 \u4e00\u4e9b\u53c2\u6570  s:\u5f53\u524d\u7684\u72b6\u6001 a:\u5f53\u524d\u7684\u884c\u52a8 s':\u884c\u52a8\u540e\u4ea7\u751f\u7684\u65b0\u4e00\u8f6estate a':\u4e0b\u6b21\u7684action r:\u672c\u6b21\u884c\u52a8\u7684\u5956\u52b1 $\\gamma$:\u6298\u6263\u56e0\u6570\uff0c\u8868\u793a\u727a\u7272\u5f53\u524d\u6536\u76ca\uff0c\u6362\u53d6\u957f\u8fdc\u6536\u76ca\u7684\u7a0b\u5ea6\u3002  \u7b97\u6cd5\u601d\u60f3  \u8bbe\u7f6e$\\gamma$\u4ee5\u53ca\u77e9\u9635$R$ \u521d\u59cb\u5316Q\u77e9\u9635\u5168\u90e8\u4e3a0 for each episode:  \u968f\u673a\u9009\u62e9\u4e00\u4e2a\u521d\u59cb\u72b6\u6001 while(\u6700\u7ec8\u76ee\u6807\u6ca1\u6709\u8fbe\u6210):  \u4ece\u5f53\u524d\u53ef\u80fd\u7684action\u4e2d\u9009\u62e9\u4e00\u4e2a \u6267\u884c\u5230\u4e0b\u4e00\u4e2astate \u627e\u5230\u4e0b\u4e00\u4e2a\u72b6\u6001\u4e2d\u7684$Q_{max}$ \u8ba1\u7b97$Q(s,a)=R(s,a)+\\gamma max[Q(s',all action)]$ \u5c06\u4e0b\u4e00\u6b21\u7684state\u8bbe\u7f6e\u4e3a\u5f53\u524d\u7684state   end while   end for  \u6bcf\u6b21\u5b66\u4e60\u540eQ\u77e9\u9635\u90fd\u4f1a\u66f4\u65b0\uff0c\u591a\u6b21\u8bad\u7ec3\u540e\u53ef\u4ee5\u5f97\u5230\u8bad\u7ec3\u597d\u7684Q\u77e9\u9635\uff0c\u4e4b\u540e\u5c31\u662f\u4f7f\u7528Q\u77e9\u9635\uff0c\u5c31\u662f\u6bcf\u6b21\u90fd\u4ece\u5176\u4e2d\u9009\u62e9\u5956\u52b1\u6700\u5927\u7684\u503c\u5c31\u53ef\u4ee5 \u8bba\u6587\u4e2d\u7684Q-Learning reward\u7684\u8868\u8fbe\u5f0f $R_t=\\sum_{t'=t}^T\\gamma^{t'-t}r_{t'}$ \u4ece\u516c\u5f0f\u4e2d\u53ef\u4ee5\u770b\u51fa\u6765\uff0creward\u662f\u5c06\u672a\u6765T-t\u4e2a\u65f6\u523b\u7684reward\u52a0\u5728\u4e86\u4e00\u8d77\uff0cT\u662f\u6e38\u620f\u7ec8\u6b62\u7684\u65f6\u523b\u3002 Q\u503c\u7684\u8868\u8fbe\u5f0f(\u6700\u4f18\u4ef7\u503c\u51fd\u6570) $Q^(s,a)= max_\\pi\\mathbb{E}[R_t|s_t=s,a_t=a,\\pi]$ \u8fd9\u4e2a\u662f\u671f\u671b\uff08\u627e\u4e86\u534a\u5929\u624d\u53d1\u73b0\u662f\u671f\u671b\uff09\uff0c$\\pi$\u662f\u4e00\u4e2a\u7b56\u7565\uff0c\u80fd\u591f\u4eces\u6620\u5c04\u5230a\u3002 \u6700\u4f18\u503cQ(s\u2019, a\u2019)\u5728\u4e0d\u540c\u7684a\u2019\u7684\u9009\u62e9\u4e0b\u90fd\u77e5\u9053\u4e86\uff0c\u6700\u4f18\u7684\u7b56\u7565\u5c31\u662f\u9009\u62e9\u80fd\u591f\u4f7f\u4e0b\u5f0f\u6700\u5927\u7684a\u2019\u3002 $$Q^(s,a)= \\mathbb{E}{s'\\xi}[r+\\gamma \\max{a'}Q^(s',a')|s,a]$$ \u8bb8\u591a\u5f3a\u5316\u5b66\u4e60\u7b97\u6cd5\u80cc\u540e\u7684\u57fa\u672c\u601d\u60f3\u662f\uff1a\u901a\u8fc7\u4f7f\u7528Bellman\u65b9\u7a0b\u4f5c\u4e3a\u8fed\u4ee3\u66f4\u65b0\uff0c\u6765\u4f30\u8ba1action-value\u51fd\u6570\u7684\u503c\u3002 $$Q_{i+1}(s,a)= \\mathbb{E}[r+\\gamma \\max_{a'}Q^(s',a')|s,a]$$ \u8fd9\u4e2a\u65f6\u5019 $$Q_i\\rightarrow Q^\\quad as\\quad i\\rightarrow\\infin$$ \u4f46\u662f\u5728\u5b9e\u8df5\u4e2d\uff0c\u8fd9\u79cd\u57fa\u672c\u65b9\u6cd5\u662f\u5b8c\u5168\u4e0d\u5207\u5b9e\u9645\u7684\uff0c\u56e0\u4e3a\u6bcf\u4e2a\u5e8f\u5217\u7684action-value\u51fd\u6570\u90fd\u662f\u5355\u72ec\u4f30\u8ba1\u7684\uff0c\u6ca1\u6709\u4efb\u4f55\u4e00\u822c\u5316\u3002\u53d6\u800c\u4ee3\u4e4b\uff0c\u901a\u5e38\u4f7f\u7528\u201c\u51fd\u6570\u903c\u8fd1\u5668\u201d\u6765\u4f30\u8ba1action-value\u51fd\u6570\uff0c$Q(s,a,\\theta)\\approx Q^(s,a)$\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u8fd9\u901a\u5e38\u662f\u4e00\u4e2a\u7ebf\u6027\u51fd\u6570\u903c\u8fd1\u5668\uff0c\u5982\u679c\u662f\u975e\u7ebf\u6027\u7684\u903c\u8fd1\u5c31\u662f\u795e\u7ecf\u7f51\u7edc\u3002 \u6211\u4eec\u5c06\u6743\u503c\u4e3a$\\theta$\u7684\u795e\u7ecf\u7f51\u7edc\u51fd\u6570\u903c\u8fd1\u5668\u79f0\u4e3aQ-network\u3002 \u635f\u5931\u51fd\u6570 $$L_i(\\theta i)= \\mathbb{E}{s,a}[(y_i-Q(s,a;\\theta_i))^2]$$ \u5176\u4e2d$y_i$\u5c31\u662f\u7b2ci\u8f6e\u8fed\u4ee3\u7684\u76ee\u6807\u771f\u5b9e\u503c\uff0c\u4e4b\u540e\u8fdb\u884c\u968f\u673a\u68af\u5ea6\u4e0b\u964d\u51fd\u6570\u8fdb\u884c\u4f18\u5316\u3002 \u8be6\u7ec6\u7684\u601d\u8def\uff08\u753b\u4e00\u4e2a\u6d41\u7a0b\u56fe\uff09  \u4ee3\u7801\u8bb2\u89e3 \u4e4b\u524d\u5199\u8fc7\u4e00\u7bc7\u535a\u5ba2\u8bb2\u89e3\u76f8\u5173\u7684\u4ee3\u7801\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u8003 \u4f20\u9001\u95e8 \u5177\u4f53\u7684\u5206\u5de5 \u90ed\u4e3b\u8981\u8fdb\u884cgym\u76f8\u5173\u63a5\u53e3\u5b66\u4e60\uff0creward\u5956\u52b1\u7b56\u7565\u7b49\u3002\u5b59\u4e3b\u8981\u8d1f\u8d23CNN\u7684\u642d\u5efa\u4ee5\u53ca\u8c03\u53c2\uff0cQlearning\u7b97\u6cd5\u5b66\u4e60\u673a\u5236\u7b49\u3002\u674e\u4e3b\u8981\u8d1f\u8d23tensorflow\u7248\u672c\u7684\u7f16\u5199\uff0c\u4ece\u5e95\u5c42\u5165\u624b\u5b66\u4e60\u8fd9\u4e2a\u7b97\u6cd5\u3002 \u4ee3\u7801\u4f7f\u7528\u65b9\u6cd5 \u6362\u6e38\u620f\u7684\u65b9\u6cd5 \u5728\u4ee3\u7801\u7684124\u884c\uff0c\u53ef\u4ee5\u66f4\u6539\u6e38\u620f\u7684\u540d\u5b57 \u5177\u4f53\u7684\u53c2\u6570 \u5728DQNAgent\u7684\u7c7b\u91cc\u9762\uff0c\u53ef\u4ee5\u81ea\u884c\u8c03\u6574\u8fde\u63a5\u5c42\u7684\u53c2\u6570 \u6a21\u578b\u4fdd\u5b58\u548c\u8bfb\u53d6 load\u548csave\uff0c\u4e0d\u591a\u8bf4\u4e86 \u4ee3\u7801\u4e0d\u8db3\u4ee5\u53ca\u4e4b\u540e\u6539\u8fdb\u7684\u5730\u65b9 \u7531\u4e8e\u8bad\u7ec3\u7684\u89c4\u6a21\u4e0d\u591f\uff0c\u5f97\u5206\u4e0d\u662f\u5f88\u9ad8\u3002\u5927\u7ea6\u96c6\u4e2d\u5728\u4e09\u767e\u5206\u5de6\u53f3\uff0c\u5076\u5c14\u7531\u4e8e\u6253\u4e2d\u4e86boss\uff0c\u5f97\u5206\u4f1a\u6bd4\u8f83\u9ad8\u3002\u8fd8\u6709\u5c31\u662f\u6ca1\u6709\u5b9e\u73b0\u6570\u636e\u9884\u5904\u7406\u4e2d\u7684\u95ea\u70c1\u64cd\u4f5c\uff0c\u4e4b\u540e\u6709\u65f6\u95f4\u53ef\u4ee5\u8865\u4e0a\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424b87eb8d64a58c1a798"}, "repo_url": "https://github.com/duythien0912/Attendance-using-ml", "repo_name": "Attendance-using-ml", "repo_full_name": "duythien0912/Attendance-using-ml", "repo_owner": "duythien0912", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T18:05:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-21T17:27:48Z", "homepage": null, "size": 166077, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 3, "github_id": 187884153, "is_fork": false, "readme_text": "Face_recognition_attendance_system The Basic Approach This is my attempt to make a Face recognition system for classroom or office attendance. The system is based on a special type of cnn architecture known as a siamese network. Such a network is trained to generate a very accurate and almost unique 128 vector given that the images of face which a are fed to the network are properly aligned and cropped.  Then another dense neural network is trained taking input these embeddings. The second neural network is only for classification purposes. Then the person who is identified by the system, his/her attendance in the system is incremented by 1. When the system is closed, a excel file consisting of attendance of all the students is generated.  taken from DeepLearning.ai. You can watch these videos. Professor Andrew Ng gives an excellent explanation to these networks. Embedding Generator I have download the pretrained facenet model from  nyoki-mtl githubu  This network is pretrained on a pretty large dataset, and produces a unique 128 dimensional vector for a particular face given the images fed to it are cropped to only the face region and are alligned. The input size of image for this netowrk is 160X160X3 Face Detection Face detection is acheived by using haar cascades of opencv. Face detection haarcascade is used to detect the face and this detected region is fed to the embedding generator. The second neural net The second neural network has a dense architecture and is used for classification. The second neural network take input the 128 dimensional vector and ouputs the probability of the face to be one of the student.The architecture of the second neural network is  Updation of attendance The database used is mongodb. Pymongo is used to add, delete records and also increment the attendance of the particular student.  csv file generation After the application is closed, an excel file is generated. This excel file contains the attendance of all the student. Requiremnents Installing the requirements   Start your terminal of cmd depending on your os.   If you have a NVidia GPU then make sure you have the prerequisites for Tensorflow GPU installation (Refer to official site). Then use this commmand pip install -r requirements_gpu.txt   In case you do not have a GPU then use this command   pip install -r requirements_cpu.txt    Apart from all this you also have to install mongodb in your system. Want to run it on your own 1)Install all the requirements 2)Make a folder named \"people\" without quotes 3)Now run Generating_training_data.py, when this runs enter the name of the person followed by a index beginning from zero for example, if I want to generate data for \"ravi\", I will write \"ravi0\" and for the next name write \"secondname1\", just make sure the index given to everybody is in increasing order. Now put all this folders into the people folder  4)Now in trainer.py change the number of classes according to number of folder and then run trainer.py 5)The model will be trained. 6)Now create a database using mongodb. Enter all the names with their attendance. This can be acheived by  a)create a data base named \"new\" b)create a collection named \"pa\" c)add the enteries. For eg db.pa.insert({\"name\":\"satinder\",\"attendance\":0})  7)Now open recognizer.py and change the dictionary \"a\" and people according to your data. The key of array \"a\" is the index of the people and the data is a indicating variable which is used to indicate that in a particular session, if the person attendance has been taken. 8)Dictionary \"people\" is self explanatory. 9)Run recognizer.py to recognize people. Their attendance will be registered in the mongodb database. Results  Updated attendance in the database  Liked it If you liked it you will surely like my other repos as well. You can also have a look at my youtube channel \"reactor science\". If you have any doubts you can contact me on my facebook page \"reactor science\" References 1)Deep learning with python by Francois Chollet 2)keras.io 3)Deeplearning.ai by coursera(prof Andrew Ng) 4)CS231n by stanford 5)Pyimagesearch.com(Adrian Rosenberg) 6)Brandon Amos(github:https://github.com/bamos) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/duythien0912/Attendance-using-ml/blob/d854aa80a88a281d2e8f464dcaca7e90baccb66a/facenet_keras.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbef"}, "repo_url": "https://github.com/qubvel/efficientnet", "repo_name": "efficientnet", "repo_full_name": "qubvel/efficientnet", "repo_owner": "qubvel", "repo_desc": "Implementation on EfficientNet model. Keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T17:29:54Z", "repo_watch": 176, "repo_forks": 8, "private": false, "repo_created_at": "2019-05-30T20:21:09Z", "homepage": "https://arxiv.org/abs/1905.11946", "size": 778, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 5, "github_id": 189477120, "is_fork": false, "readme_text": "EfficientNet-Keras This repository contains Keras reimplementation of EfficientNet, the new convolutional neural network architecture from EfficientNet (TensorFlow implementation). Table of content  About EfficientNets Examples Models Installation  About EfficientNet Models  If you're new to EfficientNets, here is an explanation straight from the official TensorFlow implementation: EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models. EfficientNets are based on AutoML and Compound Scaling. In particular, AutoML Mobile framework have been used to develop a mobile-size baseline network, named as EfficientNet-B0; Then, the compound scaling method is used to scale up this baseline to obtain EfficientNet-B1 to B7.           EfficientNets achieve state-of-the-art accuracy on ImageNet with an order of magnitude better efficiency:   In high-accuracy regime, EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet with 66M parameters and 37B FLOPS, being 8.4x smaller and 6.1x faster on CPU inference than previous best Gpipe.   In middle-accuracy regime, EfficientNet-B1 is 7.6x smaller and 5.7x faster on CPU inference than ResNet-152, with similar ImageNet accuracy.   Compared with the widely used ResNet-50, EfficientNet-B4 improves the top-1 accuracy from 76.3% of ResNet-50 to 82.6% (+6.3%), under similar FLOPS constraint.   Examples  Two lines to create model: from efficientnet import EfficientNetB0  model = EfficientNetB0(weights='imagenet')  Inference example inference_example.ipynb Models  Available architectures and pretrained weights (converted from original repo):    Architecture @top1* @top5* Weights     EfficientNetB0 0.7668 0.9312 +   EfficientNetB1 0.7863 0.9418 +   EfficientNetB2 0.7968 0.9475 +   EfficientNetB3 0.8083 0.9531 +   EfficientNetB4 - - -   EfficientNetB5 - - -   EfficientNetB6 - - -   EfficientNetB7 - - -    \"*\" - topK accuracy score for converted models (imagenet val set) Weights for B4-B7 are not released yet (issue). Installation  Requirements:  keras >= 2.2.0 (tensorflow) scikit-image  Source: $ pip install git+https://github.com/qubvel/efficientnet PyPI: $ pip install efficientnet ", "has_readme": true, "readme_language": "English", "repo_tags": ["classification", "imagenet", "efficientnet", "pretrained-models", "image-classification", "efficient", "nasnetmobile", "mobilenet", "deep-learning"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1905.11946", "https://arxiv.org/abs/1811.06965", "https://arxiv.org/abs/1512.03385", "https://arxiv.org/abs/1512.03385"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf0"}, "repo_url": "https://github.com/vietnguyen91/Super-mario-bros-A3C-pytorch", "repo_name": "Super-mario-bros-A3C-pytorch", "repo_full_name": "vietnguyen91/Super-mario-bros-A3C-pytorch", "repo_owner": "vietnguyen91", "repo_desc": "Asynchronous Advantage Actor-Critic (A3C) algorithm for Super Mario Bros", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T18:48:39Z", "repo_watch": 215, "repo_forks": 44, "private": false, "repo_created_at": "2019-05-29T16:50:54Z", "homepage": "", "size": 250542, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 189264557, "is_fork": false, "readme_text": "[PYTORCH] Asynchronous Advantage Actor-Critic (A3C) for playing Super Mario Bros Introduction Here is my python source code for training an agent to play super mario bros. By using Asynchronous Advantage Actor-Critic (A3C) algorithm introduced in the paper Asynchronous Methods for Deep Reinforcement Learning paper.           Sample results  Motivation Before I implemented this project, there are several repositories reproducing the paper's result quite well, in different common deep learning frameworks such as Tensorflow, Keras and Pytorch. In my opinion, most of them are great. However, they seem to be overly complicated in many parts including image's pre-processing, environtment setup and weight initialization, which distracts user's attention from more important matters. Therefore, I decide to write a cleaner code, which simplifies unimportant parts, while still follows the paper strictly. As you could see, with minimal setup and simple network's initialization, as long as you implement the algorithm correctly, an agent will teach itself how to interact with environment and gradually find out the way to reach the final goal. Explanation in layman's term If you are already familiar to reinforcement learning in general and A3C in particular, you could skip this part. I write this part for explaining what is A3C algorithm, how and why it works, to people who are interested in or curious about A3C or my implementation, but do not understand the mechanism behind. Therefore, you do not need any prerequiste knowledge for reading this part \u263a\ufe0f If you search on the internet, there are numerous article introducing or explaining A3C, some even provide sample code. However, I would like to take another approach: Break down the name Asynchronous Actor-Critic Agents into smaller parts and explain in an aggregated manner. Actor-Critic Your agent has 2 parts called actor and critic, and its goal is to make both parts perfom better over time by exploring and exploiting the environment. Let imagine a small mischievous child (actor) is discovering the amazing world around him, while his dad (critic) oversees him, to make sure that he does not do anything dangerous. Whenever the kid does anything good, his dad will praise and encourage him to repeat that action in the future. And of course, when the kid do anything harmful, he will get warning from his dad. The more the kid interact to the world, and take different actions, the more feedback, both positive and negative, he gets from his dad. The goal of the kid is, to collect as many positive feedback as possible from his dad, while the goal of the dad is to evaluate his son's action better. In other word, we have a win-win relationship between the kid and his dad, or equivalently between actor and critic. Advantage Actor-Critic To make the kid learn faster, and more stable, the dad, instead of telling his son how good his action is, will tell him how better or worse his action in compared to other actions (or a \"virtual\" average action). An example is worth a thousand words. Let's compare 2 pairs of dad and son. The first dad gives his son 10 candies for grade 10 and 1 candy for grade 1 in school. The second dad, on the other hand, gives his son 5 candies for grade 10, and \"punish\" his son by not allowing him to watch his favorite TV series for a day when he gets grade 1. How do you think? The second dad seems to be a little bit smarter, right? Indeed, you could rarely prevent bad actions, if you still \"encourage\" them with small reward. Asynchronous Advantage Actor-Critic If an agent discovers environment alone, the learning process would be slow. More seriously, the agent could be possibly bias to a particular suboptimal solution, which is undesirable. What happen if you have a bunch of agents which simultaneously discover different part of the environment and update their new obtained knowledge to one another periodically? It is exactly the idea of Asynchronous Advantage Actor-Critic. Now the kid and his mates in kindergarten have a trip to a beautiful beach (with their teacher, of course). Their task is to build a great sand castle. Different child will build different parts of the castle, supervised by the teacher. Each of them will have different task, with the same final goal is a strong and eye-catching castle. Certainly, the role of the teacher now is the same as the dad in previous example. The only difference is that the former is busier \ud83d\ude05 How to use my code With my code, you can:  Train your model by running python train.py Test your trained model by running python test.py  Trained models You could find some trained models I have trained in Super Mario Bros A3C trained models Requirements  python 3.6 gym cv2 pytorch numpy  ", "has_readme": true, "readme_language": "English", "repo_tags": ["reinforcement-learning", "a3c", "pytorch", "gym", "python", "deep-learning"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1602.01783"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf1"}, "repo_url": "https://github.com/yuxiaowww/NLP-Basic-Learning", "repo_name": "NLP-Basic-Learning", "repo_full_name": "yuxiaowww/NLP-Basic-Learning", "repo_owner": "yuxiaowww", "repo_desc": "NLP-\u57fa\u7840\u5b66\u4e60\uff08\u5305\u62ec\uff1a#\u57fa\u7840\u7684\u6587\u672c\u9884\u5904\u7406\u64cd\u4f5c\uff1b#NLP\u76f8\u5173\u7b97\u6cd5\u5b9e\u73b0\uff1b)", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-06-02T10:33:46Z", "repo_watch": 9, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T11:31:02Z", "homepage": "", "size": 22, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189212589, "is_fork": false, "readme_text": "NLP-Basic Learning \u4efb\u52a11: \u6570\u636e\u96c6\u63a2\u7d22-\u8bcd\u4e0eid\u4e92\u8f6c \u539f\u6587\u53c2\u8003\uff1a\u53c2\u8003\u94fe\u63a5\u3002 \u4efb\u52a11\u4e2d\u5f53\u524d\u76ee\u5f55cnews\u6587\u4ef6\u5939\u4e2d\u5b58\u653e\u6570\u636e\u96c6\uff1a \u6570\u636e\u4e0b\u8f7d\u94fe\u63a5 \u63d0\u53d6\u7801: qfud  \u4efb\u52a12: \u5206\u8bcd-\u53bb\u505c\u7528\u8bcd-\u8bcd\u9891-\u6587\u672c\u5411\u91cf\u5316 \u539f\u6587\u53c2\u8003\uff1a\u53c2\u8003\u94fe\u63a5\u3002 \u4efb\u52a12\u4e2d\u5f53\u524d\u76ee\u5f55cnews\u6587\u4ef6\u5939\u4e2d\u5b58\u653e\u6570\u636e\u96c6\uff1a \u6570\u636e\u4e0b\u8f7d\u94fe\u63a5\u540c\u4efb\u52a11,\u505c\u7528\u8bcd\u5df2\u4e0a\u4f20  \u4efb\u52a13: TFIDF-Word2Vec-\u4e92\u4fe1\u606f \u539f\u6587\u53c2\u8003\uff1a\u53c2\u8003\u94fe\u63a5\u3002\u51e0\u79cd\u65b9\u6cd5\u7684\u7b80\u5355\u793a\u4f8b\u3002  \u5176\u4ed6\u4efb\u52a1: keras\u7b80\u5355\u793a\u4f8b  keras\u6587\u672c\u5904\u7406\u793a\u4f8b\uff1bkeras\u6587\u672c\u5206\u7c7b\u793a\u4f8b ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf2"}, "repo_url": "https://github.com/DableUTeeF/keras-efficientnet", "repo_name": "keras-efficientnet", "repo_full_name": "DableUTeeF/keras-efficientnet", "repo_owner": "DableUTeeF", "repo_desc": "keras-efficientnet: A Keras implementation of EfficientNet", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T07:21:40Z", "repo_watch": 5, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-31T10:43:07Z", "homepage": "", "size": 20561, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189577575, "is_fork": false, "readme_text": "A Keras implementation of EfficientNet EfficientNets [1] Mingxing Tan and Quoc V. Le.  EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks. ICML 2019. Arxiv link: https://arxiv.org/abs/1905.11946. 1. About EfficientNet Models EfficientNets are a family of image classification models, which achieve state-of-the-art accuracy, yet being an order-of-magnitude smaller and faster than previous models. We develop EfficientNets based on AutoML and Compound Scaling. In particular, we first use AutoML Mobile framework to develop a mobile-size baseline network, named as EfficientNet-B0; Then, we use the compound scaling method to scale up this baseline to obtain EfficientNet-B1 to B7.           EfficientNets achieve state-of-the-art accuracy on ImageNet with an order of magnitude better efficiency:   In high-accuracy regime, our EfficientNet-B7 achieves state-of-the-art 84.4% top-1 / 97.1% top-5 accuracy on ImageNet with 66M parameters and 37B FLOPS, being 8.4x smaller and 6.1x faster on CPU inference than previous best Gpipe.   In middle-accuracy regime, our EfficientNet-B1 is 7.6x smaller and 5.7x faster on CPU inference than ResNet-152, with similar ImageNet accuracy.   Compared with the widely used ResNet-50, our EfficientNet-B4 improves the top-1 accuracy from 76.3% of ResNet-50 to 82.6% (+6.3%), under similar FLOPS constraint.   2. Get the weights In order to get the TF official weights.  Create a Colab file in your Google Drive. Mount the drive with  from google.colab import drive drive.mount('/content/drive/')   Run this command in your created Colab  !gsutil cp -r gs://cloud-tpu-checkpoints/efficientnet \"/content/drive/My Drive/effnets/\"   Download the effnets folder from your drive and extract somewhere.   Extract the weights from each layer to a directory by.   from extract_weights import extract_tensors_from_checkpoint_file  extract_tensors_from_checkpoint_file('efficientnet-b0/model.ckpt-109400')  # change this line to your extracted directory  Each TF weights directory should be like.  best_eval.txt checkpoint model.ckpt-12345.data-00000-of-00001 model.ckpt-12345.index model.ckpt-12345.meta  Use model.ckpt-12345 in this case.   Create the hdf5 weights by run the load_weights.py, make sure to change WEIGHTS_DIR and model_name first.   The example of testing a panda image is in main.py   Credit Layers and utils from https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet load_weights and extract_weights implemented from https://github.com/yuyang-huang/keras-inception-resnet-v2 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/DableUTeeF/keras-efficientnet/blob/7fab5fad5eeda2a37059b750bcd5020e49ed11bc/keras_efficientnet/models/efficientnet_b0_weights_tf_dim_ordering_tf_kernels.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1905.11946", "https://arxiv.org/abs/1811.06965", "https://arxiv.org/abs/1512.03385", "https://arxiv.org/abs/1512.03385"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf3"}, "repo_url": "https://github.com/braekevelt/KerasWrappers", "repo_name": "KerasWrappers", "repo_full_name": "braekevelt/KerasWrappers", "repo_owner": "braekevelt", "repo_desc": "Wrappers around the Sequential and Functional API of Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T14:59:57Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T21:10:06Z", "homepage": null, "size": 58, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189658844, "is_fork": false, "readme_text": "KerasWrappers Wrappers around the Sequential and Functional API of Keras by Alexander Braekevelt. Importing the wrappers from keras_wrappers import SequentialWrapper, ModelWrapper Creating a model Sequential API class MyModel(SequentialWrapper):      def __init__(self):         model = super().__init__(name='my_model')  model.add(Dense(32, input_dim=784))  model.add(Activation('softmax'))  model.compile(optimizer='adam',               loss='categorical_crossentropy',               metrics=['accuracy'])      @Override     def preprocess_x(self, data):  # Optional preprocessing         return super().preprocess_x(data)      @Override     def preprocess_y(self, data):  # Optional preprocessing         return super().preprocess_y(data)      @Override     def postprocess(self, data):  # Optional postprocessing         return super().postprocess(data)   my_model = MyModel() Functional API class MyModel(ModelWrapper):      def __init__(self):  a = Input(shape=(32,))  b = Dense(32)(a)         model = super().__init__(inputs=a, outputs=b, name='my_model')  model.compile(optimizer='adam',               loss='categorical_crossentropy',               metrics=['accuracy'])      @Override     def preprocess_x(self, data):  # Optional preprocessing         return super().preprocess_x(data)      @Override     def preprocess_y(self, data):  # Optional preprocessing         return super().preprocess_y(data)      @Override     def postprocess(self, data):  # Optional postprocessing         return super().postprocess(data)   my_model = MyModel() Training Training saves epochs (if not interrupted) and applies both types of preprocessing. my_model.train(x, y) from keras.preprocessing.image import ImageDataGenerator generator = ImageDataGenerator(             width_shift_range=0.1,             height_shift_range=0.1,             horizontal_flip=True           ) my_model.train_generator(generator, x, y, batch_size=64, epochs=5) Plotting Plots loss and accuracy of all epochs combined. my_model.plot_history(log_y=False)  Predicting Predicting applies preprocessing and postprocessing. prediction = my_model.predict_one(single_y) predictions = my_model.predict_all(multiple_y) Saving Saves both the model weights and the history. my_model.save_model() Loading Restores the model weights and the history. (Requires same model architecture.) my_model.load_model('my_model_35_epochs') Other methods All methods of the original model also exist for the wrappers. ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "deep-learning", "neural-network", "python3", "matplotlib"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf4"}, "repo_url": "https://github.com/Talasta/42AI-atelier-keras", "repo_name": "42AI-atelier-keras", "repo_full_name": "Talasta/42AI-atelier-keras", "repo_owner": "Talasta", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T15:14:13Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T10:17:52Z", "homepage": null, "size": 910, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 1, "github_id": 189392570, "is_fork": false, "readme_text": "42AI-atelier-keras Keras Keras information and ressources. https://keras.io/layers/core/ Keras InputGenerator Keras input generator implementation example. https://medium.com/@ensembledme/writing-custom-keras-generators-fe815d992c5a Dataset Dataset to work with, 360 fruit dataset. https://www.kaggle.com/moltean/fruits Resnet Model to try to implement. http://cs231n.stanford.edu/reports/2017/pdfs/12.pdf ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cs231n.stanford.edu/reports/2017/pdfs/12.pdf"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf5"}, "repo_url": "https://github.com/dohee0203z/hairshop", "repo_name": "hairshop", "repo_full_name": "dohee0203z/hairshop", "repo_owner": "dohee0203z", "repo_desc": "Change hairstyle in a picture", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T03:12:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T02:33:12Z", "homepage": null, "size": 314, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189136671, "is_fork": false, "readme_text": "Download \"pspnet_resnet101_sgd_lr_0.002_epoch_100_test_iou_0.918.pth\" at https://github.com/YBIGTA/pytorch-hair-segmentation/tree/master/models Keras: Deep Learning for humans    You have just found Keras. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. Being able to go from idea to result with the least possible delay is key to doing good research. Use Keras if you need a deep learning library that:  Allows for easy and fast prototyping (through user friendliness, modularity, and extensibility). Supports both convolutional networks and recurrent networks, as well as combinations of the two. Runs seamlessly on CPU and GPU.  Read the documentation at Keras.io. Keras is compatible with: Python 2.7-3.6.  Guiding principles   User friendliness. Keras is an API designed for human beings, not machines. It puts user experience front and center. Keras follows best practices for reducing cognitive load: it offers consistent & simple APIs, it minimizes the number of user actions required for common use cases, and it provides clear and actionable feedback upon user error.   Modularity. A model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible. In particular, neural layers, cost functions, optimizers, initialization schemes, activation functions, regularization schemes are all standalone modules that you can combine to create new models.   Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples. To be able to easily create new modules allows for total expressiveness, making Keras suitable for advanced research.   Work with Python. No separate models configuration files in a declarative format. Models are described in Python code, which is compact, easier to debug, and allows for ease of extensibility.    Getting started: 30 seconds to Keras The core data structure of Keras is a model, a way to organize layers. The simplest type of model is the Sequential model, a linear stack of layers. For more complex architectures, you should use the Keras functional API, which allows to build arbitrary graphs of layers. Here is the Sequential model: from keras.models import Sequential  model = Sequential() Stacking layers is as easy as .add(): from keras.layers import Dense  model.add(Dense(units=64, activation='relu', input_dim=100)) model.add(Dense(units=10, activation='softmax')) Once your model looks good, configure its learning process with .compile(): model.compile(loss='categorical_crossentropy',               optimizer='sgd',               metrics=['accuracy']) If you need to, you can further configure your optimizer. A core principle of Keras is to make things reasonably simple, while allowing the user to be fully in control when they need to (the ultimate control being the easy extensibility of the source code). model.compile(loss=keras.losses.categorical_crossentropy,               optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) You can now iterate on your training data in batches: # x_train and y_train are Numpy arrays --just like in the Scikit-Learn API. model.fit(x_train, y_train, epochs=5, batch_size=32) Alternatively, you can feed batches to your model manually: model.train_on_batch(x_batch, y_batch) Evaluate your performance in one line: loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) Or generate predictions on new data: classes = model.predict(x_test, batch_size=128) Building a question answering system, an image classification model, a Neural Turing Machine, or any other model is just as fast. The ideas behind deep learning are simple, so why should their implementation be painful? For a more in-depth tutorial about Keras, you can check out:  Getting started with the Sequential model Getting started with the functional API  In the examples folder of the repository, you will find more advanced models: question-answering with memory networks, text generation with stacked LSTMs, etc.  Installation Before installing Keras, please install one of its backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend.  TensorFlow installation instructions. Theano installation instructions. CNTK installation instructions.  You may also consider installing the following optional dependencies:  cuDNN (recommended if you plan on running Keras on GPU). HDF5 and h5py (required if you plan on saving Keras models to disk). graphviz and pydot (used by visualization utilities to plot model graphs).  Then, you can install Keras itself. There are two ways to install Keras:  Install Keras from PyPI (recommended):  sudo pip install keras If you are using a virtualenv, you may want to avoid using sudo: pip install keras  Alternatively: install Keras from the GitHub source:  First, clone Keras using git: git clone https://github.com/keras-team/keras.git Then, cd to the Keras folder and run the install command: cd keras sudo python setup.py install  Configuring your Keras backend By default, Keras will use TensorFlow as its tensor manipulation library. Follow these instructions to configure the Keras backend.  Support You can ask questions and join the development discussion:  On the Keras Google group. On the Keras Slack channel. Use this link to request an invitation to the channel.  You can also post bug reports and feature requests (only) in GitHub issues. Make sure to read our guidelines first.  Why this name, Keras? Keras (\u03ba\u03ad\u03c1\u03b1\u03c2) means horn in Greek. It is a reference to a literary image from ancient Greek and Latin literature, first found in the Odyssey, where dream spirits (Oneiroi, singular Oneiros) are divided between those who deceive men with false visions, who arrive to Earth through a gate of ivory, and those who announce a future that will come to pass, who arrive through a gate of horn. It's a play on the words \u03ba\u03ad\u03c1\u03b1\u03c2 (horn) / \u03ba\u03c1\u03b1\u03af\u03bd\u03c9 (fulfill), and \u1f10\u03bb\u03ad\u03c6\u03b1\u03c2 (ivory) / \u1f10\u03bb\u03b5\u03c6\u03b1\u03af\u03c1\u03bf\u03bc\u03b1\u03b9 (deceive). Keras was initially developed as part of the research effort of project ONEIROS (Open-ended Neuro-Electronic Intelligent Robot Operating System).  \"Oneiroi are beyond our unravelling --who can be sure what tale they tell? Not all that men look for comes to pass. Two gates there are that give passage to fleeting Oneiroi; one is made of horn, one of ivory. The Oneiroi that pass through sawn ivory are deceitful, bearing a message that will not be fulfilled; those that come out through polished horn have truth behind them, to be accomplished for men who see them.\" Homer, Odyssey 19. 562 ff (Shewring translation).   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://docs.h5py.org/en/latest/build.html", "http://deeplearning.net/software/theano/install.html#install"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf6"}, "repo_url": "https://github.com/as1067/KerasRL", "repo_name": "KerasRL", "repo_full_name": "as1067/KerasRL", "repo_owner": "as1067", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T21:34:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T21:30:11Z", "homepage": null, "size": 7473, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189485042, "is_fork": false, "readme_text": "Deep Reinforcement Learning for Keras            What is it? keras-rl implements some state-of-the art deep reinforcement learning algorithms in Python and seamlessly integrates with the deep learning library Keras. Furthermore, keras-rl works with OpenAI Gym out of the box. This means that evaluating and playing around with different algorithms is easy. Of course you can extend keras-rl according to your own needs. You can use built-in Keras callbacks and metrics or define your own. Even more so, it is easy to implement your own environments and even algorithms by simply extending some simple abstract classes. Documentation is available online. What is included? As of today, the following algorithms have been implemented:   Deep Q Learning (DQN) [1], [2]  Double DQN [3]  Deep Deterministic Policy Gradient (DDPG) [4]  Continuous DQN (CDQN or NAF) [6]  Cross-Entropy Method (CEM) [7], [8]  Dueling network DQN (Dueling DQN) [9]  Deep SARSA [10]  Asynchronous Advantage Actor-Critic (A3C) [5]  Proximal Policy Optimization Algorithms (PPO) [11]  You can find more information on each agent in the doc. Installation  Install Keras-RL from Pypi (recommended):  pip install keras-rl   Install from Github source:  git clone https://github.com/keras-rl/keras-rl.git cd keras-rl python setup.py install  Examples If you want to run the examples, you'll also have to install:  gym by OpenAI: Installation instruction h5py: simply run pip install h5py  For atari example you will also need:  Pillow: pip install Pillow gym[atari]: Atari module for gym. Use pip install gym[atari]  Once you have installed everything, you can try out a simple example: python examples/dqn_cartpole.py This is a very simple example and it should converge relatively quickly, so it's a great way to get started! It also visualizes the game during training, so you can watch it learn. How cool is that? Some sample weights are available on keras-rl-weights. If you have questions or problems, please file an issue or, even better, fix the problem yourself and submit a pull request! External Projects  Starcraft II Learning Environment  You're using Keras-RL on a project? Open a PR and share it! Citing If you use keras-rl in your research, you can cite it as follows: @misc{plappert2016kerasrl,     author = {Matthias Plappert},     title = {keras-rl},     year = {2016},     publisher = {GitHub},     journal = {GitHub repository},     howpublished = {\\url{https://github.com/keras-rl/keras-rl}}, }  References  Playing Atari with Deep Reinforcement Learning, Mnih et al., 2013 Human-level control through deep reinforcement learning, Mnih et al., 2015 Deep Reinforcement Learning with Double Q-learning, van Hasselt et al., 2015 Continuous control with deep reinforcement learning, Lillicrap et al., 2015 Asynchronous Methods for Deep Reinforcement Learning, Mnih et al., 2016 Continuous Deep Q-Learning with Model-based Acceleration, Gu et al., 2016 Learning Tetris Using the Noisy Cross-Entropy Method, Szita et al., 2006 Deep Reinforcement Learning (MLSS lecture notes), Schulman, 2016 Dueling Network Architectures for Deep Reinforcement Learning, Wang et al., 2016 Reinforcement learning: An introduction, Sutton and Barto, 2011 Proximal Policy Optimization Algorithms, Schulman et al., 2017  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://keras-rl.readthedocs.io/en/latest/agents/overview/", "http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.81.6579&rep=rep1&type=pdf", "http://keras-rl.readthedocs.org", "http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf", "http://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf", "http://keras.io", "http://keras-rl.readthedocs.io/"], "reference_list": ["http://arxiv.org/abs/1312.5602", "http://arxiv.org/abs/1509.06461", "http://arxiv.org/abs/1509.02971", "http://arxiv.org/abs/1603.00748", "https://arxiv.org/abs/1511.06581", "http://arxiv.org/abs/1602.01783", "https://arxiv.org/abs/1707.06347"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf7"}, "repo_url": "https://github.com/elenita1221/Localization", "repo_name": "Localization", "repo_full_name": "elenita1221/Localization", "repo_owner": "elenita1221", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T09:24:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T08:55:37Z", "homepage": null, "size": 3891, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189188012, "is_fork": false, "readme_text": "ResNetCAM-keras Keras implementation of a ResNet-CAM model Motivation The original Matlab implementation and paper (for AlexNet, GoogLeNet, and VGG16) can be found here.  A Keras implementation of VGG-CAM can be found here. This implementation is written in Keras and uses ResNet-50, which was not explored in the original paper. Requirements  keras with tensorflow backend (keras version 2.0.0 or later) numpy ast scipy matplotlib opencv3  Feel free to try out your own image by replacing images/dog.png with a file path to another image! :) Example plots  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf8"}, "repo_url": "https://github.com/XianglongTan/keras-graph-convolution-GRU", "repo_name": "keras-graph-convolution-GRU", "repo_full_name": "XianglongTan/keras-graph-convolution-GRU", "repo_owner": "XianglongTan", "repo_desc": "keras graph convolution GRU", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T03:00:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T07:51:27Z", "homepage": "", "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189370278, "is_fork": false, "readme_text": "This is the keras implementation of DCRNN with GRU cell and graph convolution Thanks to: https://github.com/liyaguang/DCRNN ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbf9"}, "repo_url": "https://github.com/manojCoe/CNN_keras_mnist", "repo_name": "CNN_keras_mnist", "repo_full_name": "manojCoe/CNN_keras_mnist", "repo_owner": "manojCoe", "repo_desc": "Python implementation of Convolutional Neural Networks for 2-D image data using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T09:36:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T09:33:04Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189385870, "is_fork": false, "readme_text": "CNN_keras_mnist Python implementation of Convolutional Neural Networks for 2-D image data using Keras Dependencies: 1.tensorflow pip3 install tensorflow 2.keras pip3 install keras 3.matplotlib pip3 install matplotlib Dataset: mnist dataset from keras.datasets ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbfa"}, "repo_url": "https://github.com/magicwifi/zh-NER-keras", "repo_name": "zh-NER-keras", "repo_full_name": "magicwifi/zh-NER-keras", "repo_owner": "magicwifi", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T15:11:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T12:31:15Z", "homepage": null, "size": 38682, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189590599, "is_fork": false, "readme_text": "zh-NER-keras  this project is a sample for Chinese Named Entity Recognition(NER) by Keras 2.1.4  requirements  keras=>2.1.4 keras contribute 2.0.8 (https://github.com/keras-team/keras-contrib) h5py pickle  demo python val.py  input: \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u56fd\u52a1\u9662\u603b\u7406\u5468\u6069\u6765\u5728\u5916\u4ea4\u90e8\u957f\u9648\u6bc5, \u526f\u90e8\u957f\u738b\u4e1c\u7684\u966a\u540c\u4e0b\uff0c \u8fde\u7eed\u8bbf\u95ee\u4e86\u57c3\u585e\u4fc4\u6bd4\u4e9a\u7b49\u975e\u6d3210\u56fd\u4ee5\u53ca\u963f\u5c14\u5df4\u5c3c\u4e9a  output: ['person: \u5468\u6069\u6765 \u9648\u6bc5, \u738b\u4e1c', 'location: \u57c3\u585e\u4fc4\u6bd4\u4e9a \u975e\u6d32 \u963f\u5c14\u5df4\u5c3c\u4e9a', 'organzation: \u4e2d\u534e\u4eba\u6c11\u5171\u548c\u56fd\u56fd\u52a1\u9662 \u5916\u4ea4\u90e8']  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/magicwifi/zh-NER-keras/blob/be3c280d2954c1fe7cbeedb2b23b14952f6bd697/model/crf.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbfb"}, "repo_url": "https://github.com/Xin-tian/npKeras", "repo_name": "npKeras", "repo_full_name": "Xin-tian/npKeras", "repo_owner": "Xin-tian", "repo_desc": "Keras model inference in pure numpy", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T08:33:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T13:57:20Z", "homepage": null, "size": 12305, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189236169, "is_fork": false, "readme_text": "npKeras mini framework Keras model inference in pure numpy Train Keras CNN, export the trained model weights and run inference in pure Numpy npKeras uses standard Sequential model syntax definition and provides set of layer types. Layers:  Conv2D Dense MaxPooling2D Flatten ReLU softmax and special SparseCategory layer (onehot to category label) ... [easily expandable]  Model supports standard function:  add predict evaluate set_weights load_weights summary  Beside Sequential() Keras model implemetation the separate layers may be executed separately using layer.forward() function as the building blocks of more complicated models like U-net. The inference can be splited into batches for managable memory consumption, and also could run in paralel using multprocessing - some platform restrictions may apply. Runtime statistics is available in verbose mode. To optimize execution npKeras use NCHW data format which may differ from Keras NHWC. Input data could be easily converted if necessary:  np.reshape(X, (-1, 1, 28, 28)) A dedicated function for exporting trained Keras model weights is provided for transport into npKeras in pickle format. During import the layers are identified by names between Keras and npKeras, and should have identical input shape and output shape. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbfc"}, "repo_url": "https://github.com/carlfu127/Keras-NTS-Net", "repo_name": "Keras-NTS-Net", "repo_full_name": "carlfu127/Keras-NTS-Net", "repo_owner": "carlfu127", "repo_desc": "This is a Keras implementation of the ECCV2018 paper \"Learning to Navigate for Fine-grained Classification\"", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T08:09:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T07:24:10Z", "homepage": "", "size": 127, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189548014, "is_fork": false, "readme_text": "Keras-NTS-Net This is an implementation of NTS-Net(https://arxiv.org/pdf/1809.00287.pdf) on Python 3, Keras, and TensorFlow. Requirements  python 3+ keras 2.2.4+ tensorflow-gpu 1.9+ numpy opencv-python 3.4+ datetime  Datasets Download the CUB-200-2011 datasets and put it in the dataset directory named CUB_200_2011, You can also try other fine-grained datasets. Train the model You may need to change the configurations in config.py, and just run train.py. Acknowledgement Original implementation NTS-Net Third Party Libs NTS-Net-keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz"], "reference_list": ["https://arxiv.org/pdf/1809.00287.pdf"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbfd"}, "repo_url": "https://github.com/buiduchanh/Convert_model", "repo_name": "Convert_model", "repo_full_name": "buiduchanh/Convert_model", "repo_owner": "buiduchanh", "repo_desc": "#convert model #keras #tensorflow #fronzen model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T08:24:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T07:37:15Z", "homepage": "", "size": 13, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189368205, "is_fork": false, "readme_text": "Convert model Abstract This project is use for converting tensorfow and keras model to frozen model Requirements 1\u3001tensorflow 2\u3001cuda 3\u3001python Usage When convert model to different types, we need to know the input and output of model so we must process the first step below :   Check input/output node of model in /check_node   Convert keras to frozen model in /keras_to_pb  python convert_h5_to_pb.py    Convert keras to tensorflow model in /keras_to_tensorflow  python keras_to_tensorflow.py --input_model <file_h5> --input_model_json <model json> --output_model <frozen model path    Convert tensorflow model to frozen model tensor_to_pb  python convert_to_pb.py --model_dir <model dir> --output_node_names <node output>    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbfe"}, "repo_url": "https://github.com/mrkswlsn/cnn-gan", "repo_name": "cnn-gan", "repo_full_name": "mrkswlsn/cnn-gan", "repo_owner": "mrkswlsn", "repo_desc": "cnn plus generative adversarial network", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T04:42:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T03:34:42Z", "homepage": "", "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189144868, "is_fork": false, "readme_text": "INCOMPLETE Contents Requirements Experiment Configuration Files Training Testing [GPU Utilization](#gpu utilization) Cloning repo on VDL  Requirements Tensorflow 1.4.0 (tested in 1.4.1) Keras 2.1.2 (tested in 2.1.3) Training Train.py trains a CNN on a training dataset and evaluates performance on a validation dataset. Train.py is controlled by a yaml experiment configuration file. The configuration file has four sections:  Data, Models, Learning Config, and Data Augmentation. Train.py produces three main outputs: 1.) A monitor.csv which details training progress by epoch. 2.) Validation results and metrics.  After each epoch, the CNN is used to evaluate each sample in the validation set.  The results of this evaluation are stored in a csv file and a text based summary report is saved in a metrics file.  Two sets of these files are maintained during training:  the results after the last epoch and the best results. 3.) Saved Keras models of the best performer (in terms of validation set classification accuracy), and the last model from after the most recent training epoch. Params: --experiment=$DATA/path/to/experiment.yml Returns/Outputs:     monitor.csv                      # epoch, val set classification accuracy, loss, date/time    seed.txt                         # seed used during training    training_classes.pickle          # training dataset classnames-2-indices dict (used in test.py)    experiment.csv                   # last evaluation results: val filenames, truth labels, predicted labels    experiment_metrics.txt           # last classification report: classname-to-index table, confusion matrix     experiment.h5                    # last CNN model saved after last epoch    experiment_best.csv              # best evaluation results: val filenames, truth labels, predicted labels     experiment_metrics_best.txt      # best classification report: classname-to-index table, confusion matrix     experiment_best.h5               # best CNN model saved after last epoch  Experiment Configuration File A training session with train.py is controlled by the --experiment=experiment.yml argument.  Here is an example config.yml file: ################################## # Data # image_path: path of keras style dataset folder. # This folder contains subfolders, named after each image class. # Each sub-folder contains all of the examples for each class. # The 'train' and 'validation' datasets may each be composed of multiple datasets # which are combined prior to training  or validation. ################################## data:      train:          image_path:               - $DATA/fromAFIT/png_versions/train_png_k              - path/to/keras/dataset              - path/to/another/keras/dataset      validation:          image_path:               - $DATA/fromAFIT/png_versions/test_png_k              - path/to/keras/dataset              - path/to/another/keras/dataset    ################################## # Models  # # From Keras Applicatioons (https://keras.io/applications/) # inception_v3, mobilenet, inception_resent_v2, inception_v3,  vgg16, vgg19, xception # resnet50, densenet121, densenet169, densenet201, NASNetMobile, NASNetLarge ################################## model_name: inception_v3    ################################### #Learning Config ################################### #finetuning: `False', `final_layer', or `all' finetune: all #False results in glorot_uniform random initialization.  # if finetuning, one can specify a checkpoint to use in lieu of the Imagenet default: #checkpoint: $DATA/dl_irad/experiments/keras/example_experiment_finetune_k.h5  epochs: 30 batch_size: 64 num_gpus: 2 learning_rate: 0.0001 debug: False #If debug is set to True, prints out a model.summary() showing the network structure,  #number of parameters, trainable parameters, etc.  # set seed value to False to generate a random seed and record to disk.  Or specify an integer value # if absent, seed is False seed: False  ################################### # Data Augmentation ################################### # Data Augmentation is performed in Keras's ImageDataGenerator Method:  # specification is at: https://faroit.github.io/keras-docs/1.0.3/preprocessing/image/  # The da_args dictionary formed below directly parameterizes the method call. # Simply create the dictionary with any non-default data augmentation parameters  # that you want applied to the training data  da_args:     channel_shift_range: 15    #random channel-wise color shift (integer)     zoom_range: 0.1     rotation_range: 20     fill_mode: mirror     horizontal_flip: True     vertical_flip: True     width_shift_range: 0.1     height_shift_range: 0.1     rescale: 0.0039215   #1.0 / 255  note, scaling occurs after other transforms  # If present, keep_da_images_folder: dumps *ALL* of the augmented images into the given folder. # Useful for performing a reality check on your data augmentation. # WARNING:  this option will create millions of files if left unattended.  For brief testing only! # this is a separate argument from da_args.  Therefore, it should not be indented. # keep_da_images_folder: $DATA/dl_irad/experiments/keras/temp1   #if the da_args (in total) is unspecified, a default DA will be applied: #da_args: #    channel_shift_range: 15    #random channel-wise color shift (integer) #    zoom_range: 0.1 #    rotation_range: 20 #    fill_mode: mirror #    horizontal_flip: True #    vertical_flip: True #    width_shift_range: 0.1 #    height_shift_range: 0.1 #    rescale: 0.0039215   #1.0 / 255  note, scaling occurs after other transforms  # Validation set does not undergo data augmentation.  (only scaling to 0-1) Example of how to train a model for a given experiment. #!/bin/bash  module load keras/2.1.2 #keras/2.1.2 loads tensorflow/1.4.0 module list cd ~/code/eo_rf_cnn_classification  TF_CPP_MIN_LOG_LEVEL=1 python train.py --experiment=$path/to/config.yml Testing Test.py is used after training to evaluate a CNN's performance on a test dataset(s).  Description: test.py loads a keras cnn model (*.h5) from an experiment folder created by train.py,  evaluates test dataset(s) samples matching the training classes (non-matching classes discarded) and writes an output.csv file and a metrics.txt file  params: --experiment=path/to/experiment.yml  #(num_gpus, image_size) --datasets path/to/keras/dataset1 path/to/keras/dataset2    # no equals sign or commas --output_prefix=test                                        # default is 'test' --batch_size=xx                                             # default is 32 --best_or_last=best                                         # 'best' or 'last'.  default is 'best' --seed=int                                                  # default from experiment.yml  returns: output_prefix_output.csv        #contains test filenames, predictions, labels output_prefix_metrics.txt       #confusion matrix, classification report, and classnames-2-indices  requirements: train.pickle in experiment folder.  Dict of class names vs indices from training (written by train.py)   Example of how to test a model's performance on a test dataset. #!/bin/bash  module load keras/2.1.2 #keras/2.1.2 loads tensorflow/1.4.0 module list cd ~/code/eo_rf_cnn_classification  TF_CPP_MIN_LOG_LEVEL=1 python test.py --experiment=$path/to/config.yml  --datasets $DATA/path/to/dataset1 $DATA/path/to/dataset2 GPU Utilization Use the CUDA_VISIBLE_DEVICES environment variable to control which GPUs are used.  This can be specified in the command line as shown above or set in the Run Configuration menu in the Pycharm IDE.  (Or the equivalent in other IDEs.) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dbff"}, "repo_url": "https://github.com/pshar33/Malaria-CELL-Detection-KERAS-CNN", "repo_name": "Malaria-CELL-Detection-KERAS-CNN", "repo_full_name": "pshar33/Malaria-CELL-Detection-KERAS-CNN", "repo_owner": "pshar33", "repo_desc": "Link to my KAGGLE KERNEL :- https://www.kaggle.com/parthsharma5795/malaria-detection-keras-cnn-95-accuracy", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T20:55:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T15:25:25Z", "homepage": "", "size": 62, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189616893, "is_fork": false, "readme_text": "Malaria-CELL-Detection-KERAS-CNN  The data can be downloaded from the following Kaggle dataset  https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria . Link to my Kaggle kernel https://www.kaggle.com/parthsharma5795/malaria-detection-keras-cnn-95-accuracy  Code Requirements  Numpy Pandas cv2 Seaborn,matplotlib Keras  Description This is an image classification problem on Kaggle Datasets.The dataset contains 2 folders - Infected - Uninfected and has been originally taken from a government data website  https://ceb.nlm.nih.gov/repositories/malaria-datasets/ .  Breakdown of the code:  Loading the dataset : Load the data and import the libraries. Data Preprocessing :  Reading the images,labels stored in 2 folders(Parasitized,Uninfected). Plotting the Uninfected and Parasitized images with their respective labels. Normalizing the image data. Train,test split   Data Augmentation : Augment the train and validation data using ImageDataGenerator Creating and Training the Model: Create a cnn model in KERAS. Evaluation: Display the plots from the training history. Submission: Run predictions with model.predict, and create confusion matrix.  Accuracy and loss plots  Results:  The accuracy for the test dataset came out to be 97%. The link to my kaggle kernel is https://www.kaggle.com/parthsharma5795/malaria-detection-keras-cnn-95-accuracy which is a jupyter notebook.Please feel free to leave a feedback or comment and leave an upvote if you found it to be helpful !!  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc00"}, "repo_url": "https://github.com/GansaikhanShur/Machine-Learning-Codes", "repo_name": "Machine-Learning-Codes", "repo_full_name": "GansaikhanShur/Machine-Learning-Codes", "repo_owner": "GansaikhanShur", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T19:01:37Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-31T15:22:32Z", "homepage": null, "size": 14, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189616439, "is_fork": false, "readme_text": "Machine Learning example Codes by Gansaikhan Shur (TF, KERAS) I used TensorFlow and Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc01"}, "repo_url": "https://github.com/sharmachaitanya/stock", "repo_name": "stock", "repo_full_name": "sharmachaitanya/stock", "repo_owner": "sharmachaitanya", "repo_desc": "The proposed model is composed of LSTM and a CNN, which are utilized for extracting temporal features and image features. We measure the performance of the proposed model relative to those of single models (CNN and LSTM) .", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T07:26:58Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T07:25:24Z", "homepage": "", "size": 29, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189172910, "is_fork": false, "readme_text": "Stock-Price-Prediction-using-Keras-and-Recurrent-Neural-Networ Stock Price Prediction case study using Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc02"}, "repo_url": "https://github.com/ThomasWangWeiHong/Context-Aggregation-Network-for-Semantic-Labeling-in-Aerial-Images", "repo_name": "Context-Aggregation-Network-for-Semantic-Labeling-in-Aerial-Images", "repo_full_name": "ThomasWangWeiHong/Context-Aggregation-Network-for-Semantic-Labeling-in-Aerial-Images", "repo_owner": "ThomasWangWeiHong", "repo_desc": "Python implementation of Convolutional Neural Network (CNN) proposed in academia", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T06:47:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T06:40:19Z", "homepage": null, "size": 14, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189541711, "is_fork": false, "readme_text": "Context-Aggregation-Network-for-Semantic-Labeling-in-Aerial-Images Python implementation of Convolutional Neural Network (CNN) proposed in academia This repository includes functions to preprocess the input images and their respective polygons so as to create the input image patches and mask patches to be used for model training. The CNN used here is the Context Aggregation Network model implemented in the paper 'Context Aggregation Network for Semantic Labeling in Aerial Images' by Cheng W., Yang W., Wang M., Wang G., Chen J. (2019). The main differences between the implementations in the paper and the implementation in this repository is as follows:   Group Normalization is used instead of Batch Normalization, since it is envisaged that very small batch sizes would be used for training this model with consumer - level Graphics Processing Unit (GPU) in view of memory constraints, and it has been shown in academia that Group Normalization outperforms Batch Normalization for very small batch sizes.   Transpose convolutions are used instead of bilinear interpolation to produce the final resolution classification map, as it is believed that such convolutions would produce a smoother result than bilinear interpolation of class probabilities   The group normalization implementation in Keras used in this repository is the exact same class object defined in the group_norm.py file located in titu1994's Keras-Group-Normalization github repository at https://github.com/titu1994/Keras-Group-Normalization. Please ensure that the group_norm.py file is placed in the correct directory before use. Requirements:  cv2 glob json numpy rasterio group_norm (downloaded from https://github.com/titu1994/Keras-Group-Normalization) keras (tensorflow backend)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc03"}, "repo_url": "https://github.com/ChellyD65/KerasGAN", "repo_name": "KerasGAN", "repo_full_name": "ChellyD65/KerasGAN", "repo_owner": "ChellyD65", "repo_desc": "A GAN implemented in Keras", "description_language": "Indonesian", "repo_ext_links": null, "repo_last_mod": "2019-05-31T03:10:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T02:34:02Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189514494, "is_fork": false, "readme_text": "KerasGAN A GAN implemented in Keras.  This is essentially a fork of https://github.com/eriklindernoren/Keras-GAN/tree/master/wgan_gp with some tweaks (e.g. flexible image sizes, etc). Tested in this environment gcc/6.2.0 conda2/4.2.13 cuda/9.0 Conda environment info: channels:  anaconda conda-forge bioconda defaults dependencies: _tflow_select=2.1.0=gpu absl-py=0.7.1=py36_0 anaconda=custom=py36hbbc8b67_0 astor=0.7.1=py36_0 blas=1.0=mkl c-ares=1.15.0=h7b6447c_1 ca-certificates=2019.1.23=0 certifi=2019.3.9=py36_0 cudatoolkit=10.0.130=0 cudnn=7.3.1=cuda10.0_0 cupti=10.0.130=0 freetype=2.9.1=h8a8886c_1 gast=0.2.2=py36_0 grpcio=1.16.1=py36hf8bcb03_1 h5py=2.9.0=py36h7918eee_0 hdf5=1.10.4=hb1b8bf9_0 intel-openmp=2019.3=199 jpeg=9b=habf39ab_1 keras-applications=1.0.7=py_0 keras-base=2.2.4=py36_0 keras-gpu=2.2.4=0 keras-preprocessing=1.0.9=py_0 libffi=3.2.1=he1b5a44_1006 libgcc-ng=8.2.0=hdf63c60_1 libgfortran-ng=7.3.0=hdf63c60_0 libpng=1.6.37=hbc83047_0 libprotobuf=3.6.1=hd408876_0 libstdcxx-ng=8.2.0=hdf63c60_1 libtiff=4.0.10=h2733197_2 markdown=3.1=py36_0 mkl=2019.3=199 mkl_fft=1.0.10=py36ha843d7b_0 mkl_random=1.0.2=py36hd81dba3_0 mock=2.0.0=py36_0 ncurses=6.1=hf484d3e_1002 numpy=1.16.3=py36h7e9f1db_0 numpy-base=1.16.3=py36hde5b4d6_0 olefile=0.46=py36_0 openssl=1.1.1=h7b6447c_0 pbr=5.1.3=py_0 pillow=6.0.0=py36h34e0f95_0 pip=19.1=py36_0 protobuf=3.6.1=py36he6710b0_0 python=3.6.7=h381d211_1004 pyyaml=5.1=py36h7b6447c_0 readline=7.0=hf8c457e_1001 scipy=1.2.1=py36h7c811a0_0 setuptools=41.0.1=py36_0 six=1.12.0=py36_0 sqlite=3.26.0=h67949de_1001 tensorboard=1.13.1=py36hf484d3e_0 tensorflow=1.13.1=gpu_py36h3991807_0 tensorflow-base=1.13.1=gpu_py36h8d69cac_0 tensorflow-estimator=1.13.0=py_0 tensorflow-gpu=1.13.1=h0d30ee6_0 termcolor=1.1.0=py36_1 tk=8.6.9=h84994c4_1001 werkzeug=0.15.2=py_0 wheel=0.33.1=py36_0 xz=5.2.4=h14c3975_1001 yaml=0.1.7=h96e3832_1 zlib=1.2.11=h14c3975_1004 zstd=1.3.7=h0b5b093_0  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc04"}, "repo_url": "https://github.com/wuliytTaotao/PyTorch2Keras", "repo_name": "PyTorch2Keras", "repo_full_name": "wuliytTaotao/PyTorch2Keras", "repo_owner": "wuliytTaotao", "repo_desc": "Convert the pre-trained AlexNet from PyTorch to keras by MMdnn", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T11:33:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T01:39:47Z", "homepage": null, "size": 511, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189325310, "is_fork": false, "readme_text": "PyTorch2Keras  Author: wuliytTaotao blog link: https://www.cnblogs.com/wuliytTaotao/p/10942877.html  \u5c06 PyTorch \u4e2d\u9884\u8bad\u7ec3\u597d\u7684 AlexNet \u6a21\u578b\u8f6c\u5316\u6210 tf.keras \u53ef\u8bfb\u7684 h5 \u6587\u4ef6\u3002 \u6587\u4ef6 model/keras_alexnet.h5 \u5373\u662f\u901a\u8fc7 GitHub - microsoft/MMdnn \u751f\u6210\u7684 keras \u53ef\u4ee5\u52a0\u8f7d\u7684 AlexNet \u9884\u8bad\u7ec3\u6a21\u578b\u53c2\u6570\u3002 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/wuliytTaotao/PyTorch2Keras/blob/be7f71482bbbc9f57e8d9e431ffcc0c235acb064/model/keras_alexnet.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc05"}, "repo_url": "https://github.com/manojCoe/FNN_Multiclass_keras", "repo_name": "FNN_Multiclass_keras", "repo_full_name": "manojCoe/FNN_Multiclass_keras", "repo_owner": "manojCoe", "repo_desc": "A Python implementation of feed forward neural network for classification using keras library.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T09:12:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T07:55:53Z", "homepage": null, "size": 19283, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189370935, "is_fork": false, "readme_text": "FNN_Multiclass_keras A Python implementation of feed forward neural network for classification using keras library. Dependencies:  Pandas: To work with DataFrames numpy: For Lianear Algebra keras: Deep learning library  Installation: ->pip3 install pandas ->pip3 install keras ->pip3 install tensorflow Dataset: 'reuters' from keras.datasets X_train[0] [1, 2, 2, 8, 43, 10, 447, 5, 25, 207, 270, 5, 3095, 111, 16, 369, 186, 90, 67, 7, 89, 5,  19, 102, 6, 19, 124, 15, 90, 67, 84, 22, 482, 26, 7, 48, 4, 49, 8, 864, 39, 209, 154, 6, 151, 6, 83, 11, 15, 22, 155, 11, 15, 7, 48, 9, 4579, 1005, 504, 6, 258, 6, 272, 11, 15, 22, 134, 44, 11, 15, 16, 8, 197, 1245, 90, 67, 52, 29, 209, 30, 32, 132, 6, 109, 15, 17, 12] X_test[0] [1, 4, 1378, 2025, 9, 697, 4622, 111, 8, 25, 109, 29, 3650, 11, 150, 244, 364, 33, 30, 30, 1398, 333, 6, 2, 159, 9, 1084, 363, 13, 2, 71, 9, 2, 71, 117, 4, 225, 78, 206, 10, 9, 1214, 8, 4, 270, 5, 2, 7, 748, 48, 9, 2, 7, 207, 1451, 966, 1864, 793, 97, 133, 336, 7, 4, 493, 98, 273, 104, 284, 25, 39, 338, 22, 905, 220, 3465, 644, 59, 20, 6, 119, 61, 11, 15, 58, 579, 26, 10, 67, 7, 4, 738, 98, 43, 88, 333, 722, 12, 20, 6, 19, 746, 35, 15, 10, 9, 1214, 855, 129, 783, 21, 4, 2280, 244, 364, 51, 16, 299, 452, 16, 515, 4, 99, 29, 5, 4, 364, 281, 48, 10, 9, 1214, 23, 644, 47, 20, 324, 27, 56, 2, 2, 5, 192, 510, 17, 12] y_train[:10] array([ 3,  4,  3,  4,  4,  4,  4,  3,  3, 16], dtype=int64) Code is explained with the comments for every crucial operation. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/manojCoe/FNN_Multiclass_keras/blob/ff1f66befb1c6218a3e83d3fd515ebbd287e7ba6/weights-improvement-01-0.75.hdf5", "https://github.com/manojCoe/FNN_Multiclass_keras/blob/ff1f66befb1c6218a3e83d3fd515ebbd287e7ba6/weights-improvement-02-0.79.hdf5", "https://github.com/manojCoe/FNN_Multiclass_keras/blob/ff1f66befb1c6218a3e83d3fd515ebbd287e7ba6/weights-improvement-03-0.80.hdf5", "https://github.com/manojCoe/FNN_Multiclass_keras/blob/ff1f66befb1c6218a3e83d3fd515ebbd287e7ba6/weights-improvement-04-0.81.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc06"}, "repo_url": "https://github.com/sharmahr/Smart_Trial_Mirror", "repo_name": "Smart_Trial_Mirror", "repo_full_name": "sharmahr/Smart_Trial_Mirror", "repo_owner": "sharmahr", "repo_desc": "Smart Mirror to try various things from specs to clothes.( Version 1)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T13:03:36Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T07:32:04Z", "homepage": null, "size": 3991, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189174083, "is_fork": false, "readme_text": "Smart_Trial_Mirror Smart Mirror to try various things from specs to clothes.( Version 1) This deep learning application in python can put various sunglasses on a detected face by finding the Facial Keypoints (15 unique points). These keypoints mark important areas of the face - the eyes, corners of the mouth, the nose, etc. Data Description OpenCV is often used in practice with other machine learning and deep learning libraries to produce interesting results. Employing Convolutional Neural Networks (CNN) in Keras along with OpenCV. Working Example Link to the Video!! Installation To install opencv 4 -- pip install opencv-python To install tensorflow -- pip install tensorflow/ pip install tensorflow-gpu (for cuda enabled gpu's, faster results) To install keras -- pip install keras Execution Order of Execution is as follows: Step 0 - Download the 'training.zip' file from here and extract it into the 'data' folder. Step 1 - Execute python model_builder.py Step 2 - This could take a while, so feel free to take a break. Step 3 - Execute python shades.py Future Version Coming Soon... ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/sharmahr/Smart_Trial_Mirror/blob/80eb8adb31cc711310865b0c480469f49719bec1/Smart_Trial_Mirror/my_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc07"}, "repo_url": "https://github.com/Dawning23/Automatic-driving-car-based-on-Raspberry-Pi", "repo_name": "Automatic-driving-car-based-on-Raspberry-Pi", "repo_full_name": "Dawning23/Automatic-driving-car-based-on-Raspberry-Pi", "repo_owner": "Dawning23", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T08:11:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T05:58:45Z", "homepage": null, "size": 133770, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189354540, "is_fork": false, "readme_text": "\u6811\u8393\u6d3e\u81ea\u52a8\u9a7e\u9a76\u5c0f\u8f66\u5b89\u88c5\u6559\u7a0b \u8c28\u8bb0\uff1a\u4e0a\u4f4d\u673a\u7528\u4e8e\u8bad\u7ec3\u6a21\u578b\u7684\u73af\u5883\u5fc5\u987b\u4e0e\u6811\u8393\u6d3e\u73af\u5883\u5b8c\u5168\u76f8\u540c\uff0c\u672c\u6559\u7a0b\u5b89\u88c5\u7248\u672c\u5982\u4e0b\uff1a    \u540d\u79f0 \u7248\u672c\u53f7     python 3.5.3   tensorflow 1.10.0   keras 2.0.5   pandas 0.23.4   scikit-learn 0.20.0    \u4e0a\u4f4d\u673a\u73af\u5883\u5b89\u88c5\uff08\u57fa\u4e8e Windows 10\uff09   \u5b98\u7f51\u4e0b\u8f7d anaconda  \u4e0b\u8f7d\u5b8c\u6210\u540e\u4e0a\u7f51\u641c\u5bfb\u5b89\u88c5\u6559\u7a0b   \u6253\u5f00\u7ec8\u7aef\u5e76\u4f9d\u6b21\u8f93\u5165\u4e0b\u5217\u547d\u4ee4    # 1. \u521b\u5efa\u865a\u62df\u73af\u5883 conda create -n Car python==3.5.3 # 2. \u6fc0\u6d3b\u5b89\u88c5\u597d\u7684\u865a\u62df\u73af\u5883 conda activate Car # 3. \u5b89\u88c5\u6240\u9700\u5e93 conda install tensorflow==1.10.0 keras==2.0.8 pandas==0.23.4 scikit-learn==0.20.0 # 4. \u5207\u6362\u5230\u5de5\u4f5c\u76ee\u5f55\uff0c\u4f8b\uff1a\u6211\u7684\u8bad\u7ec3\u6240\u9700\u6587\u4ef6\u4fdd\u5b58\u5728\u684c\u9762 #    Automatic-driving-car-based-on-Raspberry-Pi \u6587\u4ef6\u5939\u4e0b cd C:\\Users\\\u4f60\u7684\u7528\u6237\u540d\\Desktop\\Automatic-driving-car-based-on-Raspberry-Pi\\   \u5f00\u59cb\u8bad\u7ec3 \u6ce8\uff1a\u2460\u6bcf\u6b21\u8bad\u7ec3\u524d\u6fc0\u6d3b\u865a\u62df\u73af\u5883\u2461\u5207\u6362\u5230\u5de5\u4f5c\u76ee\u5f55  python train_model.py   \u4ee5\u4e0b\u90e8\u5206\u672a\u5b8c\u6210\uff0c\u5b9e\u9645\u5e94\u7528\u8fc7\u7a0b\u53ef\u80fd\u51fa\u9519  \u6811\u8393\u6d3e\u73af\u5883\u5b89\u88c5  \u4e0b\u8f7d\u538b\u7f29\u5305\u5e76\u89e3\u538b    \u5b89\u88c5\u524d\u51c6\u5907(\u786e\u5b9a python \u7248\u672c)  # \u6253\u5f00\u7ec8\u7aef\u5feb\u6377\u952e Ctrl + Alt + T  TODO \u63d2\u5165\u56fe\u7247  \u9000\u51fa python  exit()   \u8fdb\u5165\u89e3\u538b\u540e\u6587\u4ef6\u6240\u5728\u76ee\u5f55  cd \u89e3\u538b\u540e\u6587\u4ef6\u6240\u5728\u76ee\u5f55   \u8f93\u5165\u547d\u4ee4\u5b89\u88c5\u4e0b\u8f7d\u597d\u7684\u5305  # keras \u5305\u672a\u786e\u8ba4\u662f\u5426\u80fd\u7528 sudo pip3 install tensorflow-1.10.0-cp35-none-linux_armv7l.whl Keras-2.0.5-py3-none-any.whl numpy-1.15.2-cp35-cp35m-linux_armv7l.whl pandas-0.23.4-cp35-cp35m-linux_armv7l.whl scikit_learn-0.20.0-cp35-cp35m-linux_armv7l.whl   \u62a5\u9519\u603b\u7ed3  TypeError: unsupported operand type(s) for -=: 'Retry' and 'int'   \u89e3\u51b3\u529e\u6cd5: \u591a\u8bd5\u51e0\u6b21  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc08"}, "repo_url": "https://github.com/csaiprashant/ocr_mnist_keras", "repo_name": "ocr_mnist_keras", "repo_full_name": "csaiprashant/ocr_mnist_keras", "repo_owner": "csaiprashant", "repo_desc": "Optical Character Recongintion on the MNIST dataset using a CNN", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T16:16:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T02:01:31Z", "homepage": null, "size": 41, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189510256, "is_fork": false, "readme_text": "Handwritten Digits Classifier on the MNIST dataset using a CNN In this repo we built a handwritten digits classifier using Keras. Keras is a high\u2013level neural networks API capable of running on top of TensorFlow (which is an open source machine learning framework). Convolutional neural networks are more sophisticated and offer higher accuracy than a simple multi-layer perceptron model. Keras provides many functionalities for creating convolutional neural networks easily. The code for can be found in \u201cmnist.py\u201d. The first step is to import the classes and functions needed. Then, we need to load the MNIST dataset and reshape it so that it is suitable for use training a CNN. In Keras, the layers used for two-dimensional convolutions expect pixel values with the dimensions [pixels][width][height]. MNIST consists of grey scale images, hence the pixel dimension is set to 1. The pixel values are normalized to the range of 0 and 1 by dividing them by 255. Next, we define our sequential neural network model.  The first layer is called Convolution2D. This layer has 32 feature maps, each with the size of 5\u00d75 and a rectifier activation function. This is the input layer, expecting images with the structure outline above [pixels][width][height]. Then, we define a pooling layer called MaxPooling2D. It is configured with a pool size of 2\u00d72. We add another convolutional layer with 64 feature maps, each with a size 3x3 and having a rectifier activation function followed by a pooling layer with a pool size 2x2. The next layer converts the 2D matrix data to a vector called Flatten. It allows the output to be processed by standard fully connected layers. The next layer is a fully connected layer with 128 neurons and a rectifier activation function. Finally, the output layer has 10 neurons for the 10 classes and a softmax activation function to output probability-like predictions for each class.  The CNN is fit over 10 epochs with a batch size of 200. Running the example, the accuracy on the training and validation test is printed each epoch and at the end of the classification error rate is printed. The figure below shows a screenshot of the result. We could achieve an accuracy of 99.19%.  ", "has_readme": true, "readme_language": "English", "repo_tags": ["mnist", "keras", "ocr", "handwritten-digit-recognition", "python", "convolutional-neural-networks"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc09"}, "repo_url": "https://github.com/dw-dui/yolov3", "repo_name": "yolov3", "repo_full_name": "dw-dui/yolov3", "repo_owner": "dw-dui", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T20:34:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T20:24:18Z", "homepage": null, "size": 45896, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189654163, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc0a"}, "repo_url": "https://github.com/njordsir/Clustering-Faces", "repo_name": "Clustering-Faces", "repo_full_name": "njordsir/Clustering-Faces", "repo_owner": "njordsir", "repo_desc": "This project provides a high level wrapper for detecting, recognizing and clustering faces.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T12:37:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T12:33:06Z", "homepage": null, "size": 6334, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189590829, "is_fork": false, "readme_text": "Clustering-faces This project provides a high level wrapper for detecting faces in photos, generating facenet encodings and clustering faces belonging to recognized individuals, using tensorflow and keras. Introduction  This project can be considered as Part I of a two-part series for Face Clustering on Android. Pre-trained models from facenet are converted to keras models and wrapper scripts are provided to cluster all faces in a collection of photos. tflite models are created using the keras models (This seemed like the most reliable way to port facenet to android). Also contains scripts for creating interactive visualizations of generated clusters.  Major Requirements tensorflow keras scikit-learn cv2 imageio dlib (for chinese whispers) google_drive_downloader (for downloading models) MulticoreTSNE (for optional speed-up)  Usage 1. Obtain pre-trained models. $ python models/prepare_models.py -e 512  parameters: -e/--emb_dim | size of the embedding generated by the model | one of [128, 512] -lite/--tflite | create tflite model | one of [0, 1] To possibly speed-up the process, models can be directly downloaded from 128, 512 and placed in the models folder, followed by script execution as above. 2. Detect faces and generate encodings for all photos in a folder. $ python get_encodings.py -f ~/folder  paramters: -f/--folder | folder containing photos to be processed -e/--emb_dim | use the 128 or 512 dim model to generate encodings | one of [128, 512] 3. Cluster the encodings $python clustering.py -f ~/folder -c kmeans -k 7 -v 1 -s 1  $python clustering.py -f ~/folder -c chinese_whispers -cw -v 1 -s 1  parameters: -f/--folder | folder containing photos to be processed -c/--cluster_method | method to be used for clustering | one of [kmeans, chinese_whispers, dbscan] -k/--kmeans_k | no. of clusters for kmeans clustering -eps/--dbscan_eps | epsilon value for dbscan clustering -ms/--dbscan_min_samples | minimum sample count for a cluster in dbScan clustering -cw/--chinese_whispers_eps | epsilon value or chinese whispers clustering -v/--visualize | generate interactive vizualization for clusters | one of [0, 1] -s/--save | save clusters as folders | one of [0, 1] Vizualization Sample  Notes  Chinese whispers works best for large collections with large no. of individuals and is the overall winner. Kmeans works best for small datasets with equal photo count per class Kmeans with k = 2 can be used to identify an individual with relatively high no. of photos. For personal photo collections, this would mean identifying your own photos vs rest. For this specific usage, kmeans might beat chinese whispers. For Kmeans, set k to be 1 + expected no. of classes, to separarate out outliers. Parameters for dbscan and chinese whispers may require a lot of trial and error before decent results are obtained. I found it tricky to identify parameters that generalize well across different datasets. clustering.py prints out the no. of identified persons and the params can be tweaked till a satisfactory count is obtained.  TODO  Will add scripts to identify best paramters for any given dataset. Will add Rank Order Clustering which is known to perform well for face datasets.  References  https://github.com/davidsandberg/facenet https://github.com/nyoki-mtl/keras-facenet http://dlib.net/face_clustering.py.html https://arxiv.org/abs/1503.03832 https://arxiv.org/abs/1604.02878  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://dlib.net/face_clustering.py.html"], "reference_list": ["https://arxiv.org/abs/1503.03832", "https://arxiv.org/abs/1604.02878"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc0b"}, "repo_url": "https://github.com/rayleegit/scrapeImages", "repo_name": "scrapeImages", "repo_full_name": "rayleegit/scrapeImages", "repo_owner": "rayleegit", "repo_desc": "Python module that scrapes Google Image Search images and stores them to specified local directory", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T02:52:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T15:54:50Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189442196, "is_fork": false, "readme_text": "scrapeImages Python module that scrapes Google Image Search images, stores them to specified local directory, and optionally adds VGG image classification model top prediction (using pre-trained ImageNet weights) to the filename. Requirements: Python 3.7 w/ the following packages: bs4, requests, re, urllib, os, argparse, sys, json optional packages for running VGG image classification model: tensorflow, keras, matplotlib, numpy, pil, io Instructions To run this module, download 'scrapeImages.py' to your local drive, open up command line, then enter: python scrapeImages.py --search [IMAGE SEARCH TERM] --num_images [# OF IMAGES TO SCRAPE] --directory [DIRECTORY TO SAVE IMAGE FILES TO] --prediction ['1' if you'd like VGG predictions to be included in filename; otherwise '0'] note: code inspired by https://stackoverflow.com/questions/20716842/python-download-images-from-google-image-search and https://www.learnopencv.com/keras-tutorial-using-pre-trained-imagenet-models/ ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc0c"}, "repo_url": "https://github.com/Puneet-Shivanand/keras-learn", "repo_name": "keras-learn", "repo_full_name": "Puneet-Shivanand/keras-learn", "repo_owner": "Puneet-Shivanand", "repo_desc": "Getting started with Keras for Dummies", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T10:52:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T06:19:29Z", "homepage": "", "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189538821, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc0d"}, "repo_url": "https://github.com/rickyHong/DeepUNet-repl", "repo_name": "DeepUNet-repl", "repo_full_name": "rickyHong/DeepUNet-repl", "repo_owner": "rickyHong", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T06:06:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T06:06:27Z", "homepage": null, "size": 52, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189160742, "is_fork": false, "readme_text": "DeepUNet using Tensorflow and Keras This project was forked from zizhaozhang: https://github.com/zizhaozhang/unet-tensorflow-keras.git His code uses tensorflow + keras to train a U-Net model. I re-used this framework to implement the DeepUNet model presented in the following paper by R. Li et al.: Li, R., Liu, W., Yang, L., Sun, S., Hu, W., Zhang, F., & Li, W. (2018). Deepunet: A deep fully convolutional network for pixel-level sea-land segmentation. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. Usage   See loader.py to organize your train/test data hierarchy   Set necessary hpyerparameters and run train.py python train.py --data_path ./datasets/your_dataset_folder/ --checkpoint_path ./checkpoints/unet_example/   Visualize the train loss, dice score, learning rate, output mask, and first layer convolutional kernels per iteration in tensorboard tensorboard --logdir=train_log/    When checkpoints are saved, you can use eval.py to test an input image with an arbitrary size.   Evaluate your model python eval.py --data_path ./datasets/your_dataset_folder/ --load_from_checkpoint ./checkpoints/unet_example/model-0 --batch_size 1   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc0e"}, "repo_url": "https://github.com/yuht4/PAS", "repo_name": "PAS", "repo_full_name": "yuht4/PAS", "repo_owner": "yuht4", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T16:11:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T08:14:51Z", "homepage": null, "size": 21316, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189181193, "is_fork": false, "readme_text": "Run the ResPolyA analysis program 1. Clone the repository     git clone https://github.com/yuht4/PAS.git  The programme file is in the folder PAS/ 2. Install prerequisite tools   keras: Using the installation guide of keras, from https://keras.io/#installation   3. Prepare input files for the program   Positive samples csv file: A csv file with 2 columns for: DNA sequence, label, separated by ';' for example:   AATTGGATAGGGAGAAGCC...GATGTAGCTGATTCTAGCAAGA;1;    Negative samples csv file: A csv file with 2 columns for: DNA sequence, label, separated by ';' for example:   GGTCATGTGGGTCGTGGGCGAG...GGAGGGGAGACCGTGGAAG;0;    4. Run the model script   Run the script: Use the script, PAS.py, in PAS/ to run the script.   Usage: PAS.py [OPTIONS]   OPTIONS:        --positive       positive 6mA csv        --negative       negative 6mA csv        --output         output folder        --help           help info    Model result The model output result is in the folder, user specified. For example, in outputDir/  Model(*).h5  ROC(*).png  perfromance.txt      A. Model(*).h5 These files is the model file the script built. For example: B. ROC(*).png This files is the ROC cure picutres the script built. C. perfromance.txt This file contains the performance of our method, including the accuracy, sensitivity,specificity,MCC and ROCArea.   5. Predicting PAS sequence Use the script, predict.py, in PAS/ to predict whether a DNA sequence contains the PAS.     Usage: predict6mA.py [OPTIONS]     OPTIONS:          --h5File       the model h5 file          --sequence     the input DNA sequence      The script will output the probability of the input DNA seqnece is a positive sample  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc0f"}, "repo_url": "https://github.com/dtczhl/dtc-pointpillars-keras", "repo_name": "dtc-pointpillars-keras", "repo_full_name": "dtczhl/dtc-pointpillars-keras", "repo_owner": "dtczhl", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T17:50:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T22:10:16Z", "homepage": null, "size": 28, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189305921, "is_fork": false, "readme_text": "dtc-pointpillars-keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc10"}, "repo_url": "https://github.com/wait1ess/keras-yolo3-voc", "repo_name": "keras-yolo3-voc", "repo_full_name": "wait1ess/keras-yolo3-voc", "repo_owner": "wait1ess", "repo_desc": "A Keras implementation of YOLOv3 (Tensorflow backend)  for  datas in Pascal VOC format", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T05:51:52Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T03:08:52Z", "homepage": null, "size": 374, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189141560, "is_fork": false, "readme_text": "keras-yolo3-voc A Keras implementation of YOLOv3 (Tensorflow backend)  for  datas in Pascal VOC format \u4e00\u3001\u57fa\u7840\u73af\u5883\u642d\u5efa\uff08ubuntu 18.04 \u5b89\u88c5GPU +CUDA+cuDNN\uff09 1\u3001\u5b89\u88c5Ubuntu 18.04 \u4e3a\u4e86\u6ee1\u8db3\u540e\u7eed\u7684\u6a21\u578b\u8bad\u7ec3\u9700\u8981\uff0c\u5728\u5b89\u88c5\u64cd\u4f5c\u7cfb\u7edf\u65f6\uff0c\u5c06\u4ea4\u6362\u533a\u8bbe\u7f6e\u5f97\u5927\u4e00\u4e9b\uff0c\u4f8b\u59828G\u621616G\uff0c\u4ee5\u9632\u6a21\u578b\u8bad\u7ec3\u65f6\u8bfb\u53d6\u5927\u91cf\u6837\u672c\u5bfc\u81f4\u5185\u5b58\u6ea2\u51fa 2\u3001\u5b89\u88c5\u663e\u5361\u9a71\u52a8 \u5b89\u88c5Ubuntu\u540e\u9ed8\u8ba4\u5b89\u88c5\u7684\u662f\u5f00\u6e90\u7248\u672c\u7684\u663e\u5361\u9a71\u52a8\uff0c\u4e3a\u4e86\u540e\u7eed\u80fd\u591f\u5728\u4f7f\u7528tensorflow-gpu\u65f6\u80fd\u66f4\u597d\u5730\u53d1\u6325GPU\u7684\u6027\u80fd\uff0c\u63a8\u8350\u5b89\u88c5NVIDIA\u5b98\u65b9\u7248\u672c\u7684\u9a71\u52a8 \u5728Ubuntu\u91cc\u9762\uff0c\u6253\u5f00\u201c\u8f6f\u4ef6\u548c\u66f4\u65b0\u201d\uff0c\u70b9\u51fb\u91cc\u9762\u7684\u201c\u9644\u52a0\u9a71\u52a8\u201d\u6807\u7b7e\u9875\uff0c\u9009\u62e9\u4f7f\u7528NVIDIA driver\uff0c\u7136\u540e\u70b9\u51fb\u201c\u5e94\u7528\u66f4\u6539\u201d\u8fdb\u884c\u5b98\u65b9\u9a71\u52a8\u7684\u5b89\u88c5\uff0c\u5b89\u88c5\u540e\u91cd\u542f\u7535\u8111\u5373\u53ef \u91cd\u542f\u7535\u8111\u540e\uff0c\u53ea\u8981\u5728\u7535\u8111\u7684\u8bbe\u5907\u4fe1\u606f\u91cc\u9762\u770b\u5230\u201c\u56fe\u5f62\u201d\u662f\u663e\u793a\u4e86\u662f\u81ea\u5df1\u663e\u5361\u578b\u53f7\uff0c\u5219\u8bf4\u660eNVIDIA\u5b98\u65b9\u663e\u5361\u9a71\u52a8\u5b89\u88c5\u6210\u529f\u4e86\uff0c\u4e4b\u540e\u5c31\u80fd\u7528nvidia-smi\u547d\u4ee4\u4e86 3.\u5b89\u88c5CUDA \u8fdb\u5165 https://developer.nvidia.com/cuda-downloads  \uff0c\u4f9d\u6b21\u9009\u62e9 CUDA \u7c7b\u578b\u7136\u540e\u4e0b\u8f7d\u5bf9\u5e94\u7684CUDA\u5373\u53ef 4.\u5b89\u88c5Anaconda\uff0c\u4eceAnaconda\u5b98\u7f51\uff08https://www.continuum.io/downloads\uff09  \u4e0a\u4e0b\u8f7d\u5b89\u88c5\u5305\uff0c\u9009\u62e9Linux\u7cfb\u7edf\uff0c\u5b89\u88c5\u57fa\u4e8ePython 3.6\u7248\u672c \u5bf9\u4e0b\u8f7d\u7684\u6587\u4ef6\u6388\u4e88\u53ef\u6267\u884c\u6743\u9650\uff0c\u7136\u540e\u8fdb\u884c\u5b89\u88c5\uff1a bash Anaconda3-5.2.0-Linux-x86_64.sh    \u5f53\u8be2\u95ee\u662f\u5426\u628aAnaconda\u7684bin\u6dfb\u52a0\u5230\u7528\u6237\u7684\u73af\u5883\u53d8\u91cf\u4e2d\uff0c\u9009\u62e9yes 5.\u4f7f\u7528conda create\u547d\u4ee4\u521b\u5efa\u865a\u62df\u73af\u5883\u5230\u6307\u5b9a\u8def\u5f84\uff0c\u5e76\u6307\u5b9aPython\u7248\u672c\uff0c\u540c\u65f6\u53ef\u4ee5\u5c06\u9700\u8981\u4e00\u8d77\u5b89\u88c5\u7684\u5305\u4e5f\u4e00\u8d77\u6307\u5b9a\uff1a conda create \u2013n KerasYolo3Demo python=3.6 numpy scipy matplotlib jupyter  \u5176\u4e2d-n\u6307\u5b9a\u865a\u62df\u73af\u5883\u7684\u540d\u79f0\uff08\u4e0a\u9762\u7684\u662fKerasYolo3Demo\u6587\u4ef6\u5939\u91cc\uff09 \u9ed8\u8ba4\u5b89\u88c5\u7684\u8def\u5f84\u4f4d\u4e8eanaconda\u5b89\u88c5\u76ee\u5f55\u4e0b\u7684envs\u6587\u4ef6\u5939\u91cc\u9762\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528\u2014prefix\u53c2\u6570\u6765\u91cd\u65b0\u6307\u5b9a\u865a\u62df\u73af\u5883\u8def\u5f84 \u5982\u679c\u8981\u67e5\u770b\u6709\u54ea\u4e9b\u865a\u62df\u73af\u5883\uff0c\u5219\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\uff1a conda info -envis  \u5982\u679c\u5728\u521b\u5efaconda\u865a\u62df\u73af\u5883\u65f6\u6ca1\u6709\u6307\u5b9apython\u7684\u7248\u672c\uff0c\u5219\u9ed8\u8ba4\u662f\u4f7f\u7528anaconda\u5b89\u88c5\u76ee\u5f55\u4e0bbin\u4e2d\u7684python\u7248\u672c\u3002\u4e3a\u4e86\u5b9e\u73b0\u865a\u62df\u73af\u5883\u7684\u9694\u79bb\uff0c\u5fc5\u987b\u6307\u5b9apython\u7248\u672c 6.\u6fc0\u6d3b\u865a\u62df\u73af\u5883 \u521b\u5efa\u597dconda\u865a\u62df\u73af\u5883\u540e\uff0c\u5728\u4f7f\u7528\u4e4b\u524d\u5fc5\u987b\u5148\u8fdb\u884c\u6fc0\u6d3b\u3002\u4e0b\u9762\u6fc0\u6d3b\u521a\u521b\u5efa\u7684KerasYolo3Demo\u865a\u62df\u73af\u5883\uff0c\u547d\u4ee4\u5982\u4e0b\uff1a conda source activate KerasYolo3Demo  \u5982\u679c\u8981\u6ce8\u9500\u9000\u51fa\u5f53\u524d\u7684\u865a\u62df\u73af\u5883\uff0c\u5219\u6267\u884c\u547d\u4ee4\uff1a conda source deactivate tensorflow  7.\u5b89\u88c5tensorflow-gpu conda source activate tensorflow conda install tensorflow-gpu  conda\u5c06\u4f1a\u68c0\u6d4btensorflow-gpu\u7684\u6700\u65b0\u7248\u672c\u4ee5\u53ca\u76f8\u5173\u7684\u4f9d\u8d56\u5305\uff0c\u5305\u62ec\u8c03\u7528NVIDIA\u663e\u5361\u6240\u9700\u8981\u7684Cuda\u3001Cudnn\u7b49\u4f9d\u8d56\u73af\u5883\uff0c\u90fd\u4f1a\u81ea\u52a8\u6309\u987a\u5e8f\u8fdb\u884c\u5b89\u88c5 keras\u7248\u672c\u7684yolo3\u8fd8\u4f9d\u8d56\u4e8ePIL\u5de5\u5177\u5305\uff0c\u5982\u679c\u4e4b\u524d\u6ca1\u5b89\u88c5\u7684\uff0c\u4e5f\u8981\u5728anaconda\u4e2d\u5b89\u88c5 conda install keras-gpu conda install pillow  8.\u5b89\u88c5OpenCV\uff08\u65e0\u6cd5\u76f4\u63a5\u5b89\u88c5\u7684\uff0c\u9700\u8981\u6307\u5b9a\u5b89\u88c5\u6e90\uff09 conda install --channel https://conda.anaconda.org/menpo opencv3 9.\u5b89\u88c5PyCharm\uff08Python\u5f00\u53d1IDE\u73af\u5883\uff0c\u793e\u533a\u7248\u514d\u8d39\u4f7f\u7528\uff09 \u5728Ubuntu\u91cc\u9762\u5b89\u88c5PyCharm\u975e\u5e38\u7b80\u5355\uff0c\u5728Ubuntu\u8f6f\u4ef6\u5546\u57ce\u91cc\u9762\u641c\u7d22\u201cpycharm\u201d\uff0c\u7136\u540e\u9009\u62e9\u793e\u533a\u7248\u201cPyCharm CE\u201d\u8fdb\u884c\u5b89\u88c5\u5373\u53ef \u4e3a\u4e86\u80fd\u591f\u5728PyCharm\u4e2d\u4f7f\u7528\u6211\u4eec\u81ea\u5df1\u521b\u5efa\u7684conda\u865a\u62df\u73af\u5883\uff0c\u9700\u8981\u8fdb\u884c\u4e0b\u914d\u7f6e\u3002\u5728Pycharm \u7684Files>>settings>>Project Interpreter>>Add local \u91cc\u9762\u6dfb\u52a0\u521a\u624d\u521b\u5efa\u7684conda\u865a\u62df\u73af\u5883\u7684\u76ee\u5f55\u4e0b\u6240\u5728\u7684Python 3.6\u7a0b\u5e8f\uff0c\u5e94\u7528\u4e4b\u540e\u5c31\u53ef\u4ee5\u4f7f\u7528\u6211\u4eec\u81ea\u5df1\u4f7f\u7528\u7684\u865a\u62df\u73af\u5883\u4e86 \u4e8c\u3001\u914d\u7f6eYOLO 1.\u4e0b\u8f7dYOLOv3\u6e90\u4ee3\u7801\uff0c\u540e\u7eed\u53ef\u5728\u6b64\u57fa\u7840\u4e0a\u8fdb\u884c\u6539\u5199\uff1a https://github.com/wait1ess/keras-yolo3-voc 2.\u6253\u5f00PyCharm\uff0c\u65b0\u5efa\u9879\u76ee\uff0c\u5c06keras-yolo3\u7684\u6e90\u4ee3\u7801\u5bfc\u5165\u5230PyCharm\u4e2d 3.YOLO\u5b98\u7f51\u4e0a\u63d0\u4f9b\u4e86YOLOv3\u6a21\u578b\u8bad\u7ec3\u597d\u7684\u6743\u91cd\u6587\u4ef6\uff0c\u628a\u5b83\u4e0b\u8f7d\u4fdd\u5b58\u5230\u7535\u8111\u4e0a\uff0c\u4e0b\u8f7d\u5730\u5740\u4e3a https://pjreddie.com/media/files/yolov3.weights \u4e09\u3001\u4f7f\u7528\u9884\u8bad\u7ec3\u6743\u91cd\u8fdb\u884c\u76ee\u6807\u68c0\u6d4b 1.\u8f6c\u6362\u6743\u91cd\u6587\u4ef6\uff0c\u5c06\u524d\u9762\u4e0b\u8f7d\u7684yolo\u6743\u91cd\u6587\u4ef6yolov3.weights\u8f6c\u6362\u6210\u9002\u5408Keras\u7684\u6a21\u578b\u6587\u4ef6\uff0c\u8f6c\u6362\u4ee3\u7801\u5982\u4e0b\uff1a source activate tensorflow python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5  2.\u4fee\u6539yolo.py\u91cc\u9762\u7684\u76f8\u5173\u8def\u5f84\u914d\u7f6e\uff0c\u4e3b\u8981\u662fmodel_path,classes_path\u548cgpu_num class YOLO(object):     _defaults = {         \"model_path\": 'model_data/yolo_weights.h5',         # \"model_path\":\"model_data/trained_weights_final.h5\",         \"anchors_path\": 'model_data/yolo_anchors.txt',         \"classes_path\":   'model_data/coco_classes.txt',         # \"classes_path\": 'model_data/voc_classes.txt',         \"score\" : 0.7,         \"iou\" : 0.45,         \"model_image_size\" : (416, 416),         \"gpu_num\" : 0,     }  3\u3001\u521b\u5efayolo\u5b9e\u4f8b\uff0c\u9884\u6d4b\u56fe\u7247\u6216\u89c6\u9891 \u56fe\u7247\u5982\u4e0b\uff1a if __name__ == '__main__':     yolo=YOLO()     path = 'D:/VOCtrainval_06-Nov-2007/yoloV3conf/keras-yolo3-master/test001.jpg'     try:         image = Image.open(path)     except:         print('Open Error! Try again!')     else:         r_image = yolo.detect_image(image)         r_image.show()      yolo.close_session()  \u89c6\u9891\u5982\u4e0b\uff1a if __name__ == '__main__':     video_path=\"D:/VOCtrainval_06-Nov-2007/yoloV3conf/keras-yolo3-master/test002.mp4\"     output_path=\"D:/VOCtrainval_06-Nov-2007/yoloV3conf/keras-yolo3-master/result002.mp4\"     detect_video(YOLO(), video_path, output_path)  \u56db\u3001\u8bad\u7ec3\u81ea\u5df1\u7684\u76ee\u6807\u68c0\u6d4b\u6a21\u578b\uff08\u5e94\u7528\u4e8eVOC\u683c\u5f0f\u6570\u636e\uff09 1.\u4ee5\u73af\u5883\u6587\u4ef6\u5939\u4e3a\u6839\u76ee\u5f55\u521b\u5efa\u4ee5\u4e0b\u6570\u636e\u96c6  2\u3001\u5b89\u88c5\u6807\u6ce8\u5de5\u5177labelImg\uff0c\u6807\u6ce8\u6570\u636e http://host.robots.ox.ac.uk/pascal/VOC/ \u4f7f\u7528\u65b9\u6cd5\u8be6\u89c1\uff1ahttps://blog.csdn.net/weixin_41065383/article/details/90637205 3\u3001\u4f7f\u7528make_main_txt.py\u5212\u5206\u8bad\u7ec3\u6570\u636e\u96c6\uff0c\u9a8c\u8bc1\u96c6\u4ee5\u53ca\u6d4b\u8bd5\u96c6\uff0c\u4fdd\u5b58\u4e8eMain\u4e2d 4\u3001\u8f6c\u6362\u6570\u636e\u96c6\u6807\u6ce8\u6587\u4ef6\uff08VOC\u683c\u5f0f\u5230YOLO\u683c\u5f0f\uff09 YOLO\u91c7\u7528\u7684\u6807\u6ce8\u6570\u636e\u6587\u4ef6\uff0c\u6bcf\u4e00\u884c\u7531\u6587\u4ef6\u6240\u5728\u8def\u5f84\u3001\u6807\u6ce8\u6846\u7684\u4f4d\u7f6e\uff08\u5de6\u4e0a\u89d2\u3001\u53f3\u4e0b\u89d2\uff09\u3001\u7c7b\u522bID\u7ec4\u6210\uff0c\u683c\u5f0f\u4e3a\uff1aimage_file_path x_min,y_min,x_max,y_max,class_id \u4f8b\u5b50\u5982\u4e0b\uff1a path/00001.jpg xmin,ymin,xmax,ymax,class path/00002.jpg xmin,ymin,xmax,ymax,class xmin,ymin,xmax,ymax,class... \u8fd9\u79cd\u6587\u4ef6\u683c\u5f0f\u8ddf\u524d\u9762\u5236\u4f5c\u597d\u7684VOC_2007\u6807\u6ce8\u6587\u4ef6\uff08Main\u4e2d\u7684txt\uff09\u7684\u683c\u5f0f\u4e0d\u4e00\u6837\uff0cKeras-yolo3\u91cc\u9762\u63d0\u4f9b\u4e86voc\u683c\u5f0f\u8f6cyolo\u683c\u5f0f\u7684\u8f6c\u6362\u811a\u672c voc_annotation.py \u5728\u8f6c\u6362\u683c\u5f0f\u4e4b\u524d\uff0c\u5148\u6253\u5f00voc_annotation.py\u6587\u4ef6\uff0c\u4fee\u6539\u91cc\u9762\u7684classes\u7684\u503c\u4e3a\u81ea\u5df1\u8bad\u7ec3\u6570\u636e\u7684label\uff0c\u7136\u540e\u6267\u884c\u5373\u53ef\uff0c\u65b0\u7684\u6570\u636e\u96c6\u6807\u6ce8\u6587\u4ef6\u4fdd\u5b58\u4e8emodel_data\u76ee\u5f55 source activate tensorflow python voc_annotation.py  5\u3001\u521b\u5efa\u7c7b\u522b\u6587\u4ef6voc_classes.txt 6\u3001\u4fee\u6539train.py\u91cc\u9762\u7684\u76f8\u5173\u8def\u5f84\u914d\u7f6e\uff0c\u4e3b\u8981\u6709\uff1aannotation_path\u3001classes_path\u3001weights_path     annotation_path = 'model_data/2007_train.txt'     log_dir = 'model_data/logs/000/'     classes_path = 'model_data/voc_classes.txt'     anchors_path = 'model_data/yolo_anchors.txt'  7\u3001\u8bad\u7ec3\uff0c\u8bad\u7ec3\u540e\u7684\u6a21\u578b\uff0c\u9ed8\u8ba4\u4fdd\u5b58\u8def\u5f84\u4e3alogs/000/trained_weights_final.h5 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://host.robots.ox.ac.uk/pascal/VOC/"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc11"}, "repo_url": "https://github.com/1977763153/car_logo_identify", "repo_name": "car_logo_identify", "repo_full_name": "1977763153/car_logo_identify", "repo_owner": "1977763153", "repo_desc": "This is a simple program using keras to identify car logo.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T10:06:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T02:40:15Z", "homepage": null, "size": 13470, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189137663, "is_fork": false, "readme_text": "car_logo_identify This is a simple program using keras to identify car logo. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc12"}, "repo_url": "https://github.com/nli276/data_science_pipeline_py", "repo_name": "data_science_pipeline_py", "repo_full_name": "nli276/data_science_pipeline_py", "repo_owner": "nli276", "repo_desc": "Data processing and machine learning pipeline (pandas, sklearn, xgboost, keras, tensorflow, etc.)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T02:48:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T02:25:02Z", "homepage": null, "size": 440, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189135533, "is_fork": false, "readme_text": "data_science_pipeline_py Data processing and machine learning pipeline (pandas, sklearn, xgboost, keras, tensorflow, etc.) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc13"}, "repo_url": "https://github.com/RishabhGhosh/Convolutional_Neural_Networks", "repo_name": "Convolutional_Neural_Networks", "repo_full_name": "RishabhGhosh/Convolutional_Neural_Networks", "repo_owner": "RishabhGhosh", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T20:10:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T20:02:24Z", "homepage": null, "size": 220866, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189474714, "is_fork": false, "readme_text": "Convolutional_Neural_Networks Prerequisite: Python3.5 Tensorflow Keras Pillow Scipy Numpy ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc14"}, "repo_url": "https://github.com/mpkuse/demo_keras_to_nvidia-uff", "repo_name": "demo_keras_to_nvidia-uff", "repo_full_name": "mpkuse/demo_keras_to_nvidia-uff", "repo_owner": "mpkuse", "repo_desc": "how to convert keras model to nvidia uff for tx2", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T08:17:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T08:16:47Z", "homepage": null, "size": 54, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189555907, "is_fork": false, "readme_text": "A minimalistic demo to showcase how to convert keras model (with custom layer) to Nvidia's UFF ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc15"}, "repo_url": "https://github.com/Rblivingstone/Node2Vec", "repo_name": "Node2Vec", "repo_full_name": "Rblivingstone/Node2Vec", "repo_owner": "Rblivingstone", "repo_desc": "An simplified implementation of node2vec using networkx and keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T23:10:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T16:30:27Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189447312, "is_fork": false, "readme_text": "Node2Vec A simplified implementation of node2vec using networkx and keras More to come in terms of a readme. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc16"}, "repo_url": "https://github.com/JaydenGG/Mobilefacenet", "repo_name": "Mobilefacenet", "repo_full_name": "JaydenGG/Mobilefacenet", "repo_owner": "JaydenGG", "repo_desc": "implement mobilefacenet(including arcface layer)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T15:47:39Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T05:38:14Z", "homepage": null, "size": 10, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189157301, "is_fork": false, "readme_text": "Mobilefacenet implement mobilefacenet(including arcface layer) using keras add dataset into data folder train the model by train.py ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "mobilefacenet", "arcface"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc17"}, "repo_url": "https://github.com/UnleashTheCode/AirQualityANN", "repo_name": "AirQualityANN", "repo_full_name": "UnleashTheCode/AirQualityANN", "repo_owner": "UnleashTheCode", "repo_desc": "An artificial neural network for an AirQuality dataset using keras and pandas", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T19:17:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T19:14:06Z", "homepage": "", "size": 226, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189284264, "is_fork": false, "readme_text": "AirQualityANN An artificial neural network for an AirQuality dataset using keras and pandas. The dataset is from UCI Machine Learning Repository: dataset ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc18"}, "repo_url": "https://github.com/Milogav/Facial-gender-prediction", "repo_name": "Facial-gender-prediction", "repo_full_name": "Milogav/Facial-gender-prediction", "repo_owner": "Milogav", "repo_desc": "Gender prediction from facial images using tensorflow and keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T21:39:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T21:32:30Z", "homepage": null, "size": 434, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189485279, "is_fork": false, "readme_text": "Facial-gender-prediction Gender prediction from facial images using tensorflow and keras Dataset: UTKfaces Model: VGG19 pretrained on imagenet, fine tuned for gender prediction from facial images ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc19"}, "repo_url": "https://github.com/garry197/Image_Captioning", "repo_name": "Image_Captioning", "repo_full_name": "garry197/Image_Captioning", "repo_owner": "garry197", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T18:00:19Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T17:48:13Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189272632, "is_fork": false, "readme_text": "Image_Captioning I have used Flickr8k dataset. For References: https://github.com/yashk2810/Image-Captioning https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc1a"}, "repo_url": "https://github.com/Nyquixt/color-classifier", "repo_name": "color-classifier", "repo_full_name": "Nyquixt/color-classifier", "repo_owner": "Nyquixt", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T16:38:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T16:36:00Z", "homepage": null, "size": 135, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189262507, "is_fork": false, "readme_text": "color-classifier a re-implementation of Daniel Shiffman's color classifier in Python using Keras the database is taken from Daniel's crowdsource data ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc1b"}, "repo_url": "https://github.com/hongzzz/InferType", "repo_name": "InferType", "repo_full_name": "hongzzz/InferType", "repo_owner": "hongzzz", "repo_desc": "\u673a\u5668\u5b66\u4e60\u6062\u590d\u4e8c\u8fdb\u5236\u6587\u4ef6\u53d8\u91cf\u7c7b\u578b", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-31T01:58:17Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T01:51:20Z", "homepage": null, "size": 2913, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189509031, "is_fork": false, "readme_text": "Predict Types Train.py \u4f7f\u7528keras\u8bad\u7ec3 Predict.py \u7528\u8bad\u7ec3\u597d\u7684\u6a21\u578b\u9884\u6d4b\u7c7b\u578b ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/hongzzz/InferType/blob/5ed0218af0217ed1b10600f9e1fdfc230229ca4a/common/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc1c"}, "repo_url": "https://github.com/lodhi-jj/Fine-Tuning", "repo_name": "Fine-Tuning", "repo_full_name": "lodhi-jj/Fine-Tuning", "repo_owner": "lodhi-jj", "repo_desc": "In this project you can fine tune any pre-treined model compatible with keras to predict the significant classification results in python and MATLAB.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T07:34:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T07:21:15Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189172136, "is_fork": false, "readme_text": "Fine-Tuning In this project you can fine tune any pre-treined model compatible with keras to predict the significant classification results in python and MATLAB. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc1d"}, "repo_url": "https://github.com/jeju-sw-hackathon/emoji_board", "repo_name": "emoji_board", "repo_full_name": "jeju-sw-hackathon/emoji_board", "repo_owner": "jeju-sw-hackathon", "repo_desc": "Emoji board at JCCEI", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T14:29:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T09:29:18Z", "homepage": null, "size": 83107, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189567380, "is_fork": false, "readme_text": "Dependency  python 3.7.3 django 2.2.1 tensorflow 1.13.1 keras 2.2.4 text-unidecode 1.2  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/jeju-sw-hackathon/emoji_board/blob/a38ef3c76fdbabbbd081d67dad8d05f36003a601/board/jejuDLcamp_emotion/models/deepmoji_weights.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc1e"}, "repo_url": "https://github.com/leolv131/CNN-model", "repo_name": "CNN-model", "repo_full_name": "leolv131/CNN-model", "repo_owner": "leolv131", "repo_desc": "Applying CNN to regression problem", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T09:01:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T08:20:30Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189182099, "is_fork": false, "readme_text": "CNN-model /Applying cnn to build a mathematical model by keras /Applying CNN to regression problem /nn.py will be updated ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc1f"}, "repo_url": "https://github.com/joeyklee/geoGAN-trainer", "repo_name": "geoGAN-trainer", "repo_full_name": "joeyklee/geoGAN-trainer", "repo_owner": "joeyklee", "repo_desc": "A repo documenting the training for the ml5 DCGAN geo example", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T13:47:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T13:36:15Z", "homepage": null, "size": 13, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189599535, "is_fork": false, "readme_text": "geoGAN-trainer The following steps and code are attributed to:  YG's https://github.com/alantian/ganshowcase  Download the data Coming soon (Optional) crop images Follow along with cropping here: https://ellennickles.site/blog?category=Neural+Aesthetic (Optional) resize images from 128x128 to 64x64 for f in /Users/joeyklee/Desktop/geoGAN-trainer/images/santiago_urban/*; do          gdal_translate -of PNG -outsize 64 64 -r cubic $f \"${f%.*}-s64.png\" done   Then get all those images and put them into a folder. In this case santiago_urban64 Create a virtualenv virtualenv /geoGAN-trainer  Activate your virtualenv source /geoGAN-trainer/bin/activate  2. install deps pip install tensorflow image chainer spell pip install tensorflowjs==1.1.2 h5py numpy keras pip install --no-deps tensorflowjs pip install tensorflow_hub  3. Convert images to .npz file python datatool.py --task dir_to_npz --dir_path images/santiago_urban64 --npz_path npzdata/img_santiago_urban_64.npz --size 64 4. spell login spell login # username # password  5. run the thing on spell This will take anywhere from 2 - X hours depending on the machine and number of images spell run --machine-type V100 \\             --framework tensorflow \\             --pip image \\             --pip chainer \\             --pip cupy \\     \"python chainer_dcgan.py \\     --arch dcgan64 \\     --image_size 64 \\     --adam_alpha 0.0001 \\     --adam_beta1 0.5 \\     --adam_beta2 0.999 \\     --lambda_gp 1.0 \\     --learning_rate_anneal 0.9 \\     --learning_rate_anneal_trigger 0 \\     --learning_rate_anneal_interval 5000 \\     --max_iter 100000 --snapshot_interval 5000 \\     --evaluation_sample_interval 100 \\     --display_interval 10 \\     --npz_path npzdata/img_santiago_urban_64.npz \\     --out out_chainer_dcgan64\"  6. convert the Generator file to a tfjs model python dcgan_chainer_to_keras.py --arch dcgan64 --chainer_model_path out_chainer_dcgan64/Generator_95000.npz --keras_model_path out_chainer_dcgan64/Keras_Generator_95000.h5 --tfjs_model_path out_chainer_dcgan64/tfjs_Generator_95000  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc20"}, "repo_url": "https://github.com/geocodey/vqaTrain", "repo_name": "vqaTrain", "repo_full_name": "geocodey/vqaTrain", "repo_owner": "geocodey", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T23:33:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T21:31:41Z", "homepage": null, "size": 35145, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189660902, "is_fork": false, "readme_text": "Deep Learning for Visual Question Answering Click here to go to the accompanying blog post. This project uses Keras to train a variety of Feedforward and Recurrent Neural Networks for the task of Visual Question Answering. It is designed to work with the VQA dataset. Models Implemented:    BOW+CNN Model LSTM + CNN Model          Requirements  Keras 0.20 spaCy 0.94 scikit-learn 0.16 progressbar Nvidia CUDA 7.5 (optional, for GPU acceleration) Caffe (Optional)  Tested with Python 2.7 on Ubuntu 14.04 and Centos 7.1. Notes:  Keras needs the latest Theano, which in turn needs Numpy/Scipy. spaCy is currently used only for converting questions to a vector (or a sequence of vectors), this dependency can be easily be removed if you want to. spaCy uses Goldberg and Levy's word vectors by default, but I found the performance to be much superior with Stanford's Glove word vectors. VQA Tools is not needed. Caffe (Optional) - For using the VQA with your own images.  Installation Guide This project has a large number of dependecies, and I am yet to make a comprehensive installation guide. In the meanwhile, you can use the following guide made by @gajumaru4444:  Prepare for VQA in Ubuntu 14.04 x64 Part 1 Prepare for VQA in Ubuntu 14.04 x64 Part 2  If you intend to use my pre-trained models, you would also need to replace spaCy's default word vectors with the GloVe word vectors from Stanford. You can find more details here on how to do this. Using Pre-trained models Take a look at scripts/demo_batch.py. An LSTM-based pre-trained model has been released. It currently works only on the images of the MS COCO dataset (need to be downloaded separately), since I have pre-computed the VGG features for them. I do intend to add a pipeline for computing features for other images. Caution: Use the pre-trained model with 300D Common Crawl Glove Word Embeddings. Do not the the default spaCy embeddings (Goldberg and Levy 2014). If you try to use these pre-trained models with any embeddings except Glove, your results would be garbage. You can find more deatails here on how to do this. Using your own images Now you can use your own images with the scripts/own_image.py script. Use it like : python own_image.py --caffe /path/to/caffe For now, a Caffe installation is required. However, I'm working on a Keras based VGG Net which should be up soon. Download the VGG Caffe model weights from here and place it in the scripts folder. The Numbers Performance on the validation set and the test-dev set of the VQA Challenge:    Model val test-dev     BOW+CNN 48.46% TODO   LSTM-Language only 44.17% TODO   LSTM+CNN 51.63% 53.34%    Note: For validation set, the model was trained on the training set, while it was trained on both training and validation set for the test-dev set results. There is a lot of scope for hyperparameter tuning here. Experiments were done for 100 epochs. Training Time on various hardware:    Model GTX 760 Intel Core i7     BOW+CNN 140 seconds/epoch 900 seconds/epoch   LSTM+CNN 200 seconds/epoch 1900 seconds/epoch    The above numbers are valid when using a batch size of 128, and training on 215K examples in every epoch. Get Started Have a look at the get_started.sh script in the scripts folder. Also, have a look at the readme present in each of the folders. Feedback All kind of feedback (code style, bugs, comments etc.) is welcome. Please open an issue on this repo instead of mailing me, since it helps me keep track of things better. License MIT ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/geocodey/vqaTrain/blob/a52afe5051f73617846a3ea06d54d4bd8e1ba4e9/models/lstm_1_num_hidden_units_lstm_512_num_hidden_units_mlp_1024_num_hidden_layers_mlp_3_epoch_070.hdf5"], "see_also_links": ["http://visualqa.org", "http://www.robots.ox.ac.uk/~vgg/software/very_deep/caffe/VGG_ILSVRC_16_layers.caffemodel", "http://scikit-learn.org/", "http://visualqa.org/challenge.html", "http://spacy.io/", "http://keras.io/", "http://spacy.io/tutorials/load-new-word-vectors/", "http://nlp.stanford.edu/projects/glove/"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc21"}, "repo_url": "https://github.com/qwertpi/cifar-dcgan", "repo_name": "cifar-dcgan", "repo_full_name": "qwertpi/cifar-dcgan", "repo_owner": "qwertpi", "repo_desc": "A GAN loosely based on DCGAN trained on the CIFAR10 dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T17:34:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T10:13:31Z", "homepage": null, "size": 615, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 189201580, "is_fork": false, "readme_text": "cifar-dcgan A GAN loosely based on DCGAN trained on the CIFAR10 dataset. The output quality is not amazing but I think it's about as good as it gets for a basic DCGAN on CIFAR10. For a GAN with better output quality (and much smaller model files) see my MNIST DCGAN. WARNING: Only Linux has been tested Feedback and pull requests are very welcome Inspired by https://github.com/eriklindernoren/Keras-GAN/blob/master/dcgan/dcgan.py https://medium.com/@utk.is.here/keep-calm-and-train-a-gan-pitfalls-and-tips-on-training-generative-adversarial-networks-edd529764aa9 and https://github.com/soumith/ganhacks  Copyright Copyright \u00a9 2019  Rory Sharp All rights reserved. This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. You should have received a copy of the GNU General Public License along with this program.  If you have not received this, see http://www.gnu.org/licenses/gpl-3.0.html. For a summary of the licence go to https://tldrlegal.com/license/gnu-general-public-license-v3-(gpl-3) Prerequisites For One Liner  Curl apt-get install curl  For Manual Install  Python 3 Keras pip3 install keras Numpy pip3 install numpy TensorFlow pip3 install tensorflow h5py pip3 install h5py matplotlib pip3 install matplotlib keract (for intermediate layer visualisation only) pip3 install keract libhdf5 (only needed on some systems) sudo apt-get install libhdf5-serial-dev  One-liner install curl https://raw.githubusercontent.com/qwertpi/cifar-dcgan/master/install.bash | bash Usage 0. Download this repo Training (Optional) 1. Create a folder called images that will be used to peridocly save generated imaes to during training 2. Run train.py  for as long as you want (I recomend around 15,000 epochs), I trained using google colabatory 3. Copy the gen.h5 file and model.json file to the folder with generate.py in and rename your gen.h5 file to model.h5 Generating 4. Run generate.py 5. Marvel at what modern neural networks can do Intermideate layer visualistion 6. If you run viz.py you will be able to see the outputs of the layers inbetween the input and output however these are not particuarly intresting for this network ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/qwertpi/cifar-dcgan/blob/5b4cc927672493cecc3bb9f8c5e6d5f95cf11d82/model.h5"], "see_also_links": ["http://www.gnu.org/licenses/gpl-3.0.html"], "reference_list": ["https://arxiv.org/pdf/1511.06434.pdf"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc22"}, "repo_url": "https://github.com/parijitkedia/StockMarketPrediction", "repo_name": "StockMarketPrediction", "repo_full_name": "parijitkedia/StockMarketPrediction", "repo_owner": "parijitkedia", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T10:00:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T09:55:45Z", "homepage": null, "size": 22, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189571350, "is_fork": false, "readme_text": "How-to-Predict-Stock-Prices-Easily-Demo How to Predict Stock Prices Easily - Intro to Deep Learning #7 by Siraj Raval on Youtube ##Overview This is the code for this video on Youtube by Siraj Raval part of the Udacity Deep Learning nanodegree. We use an LSTM neural network to predict the closing price of the S&P 500 using a dataset of past prices. ##Dependencies  keras tensorflow  Install Keras from here and Tensorflow from here. ##Usage Run this using jupyter notebook. Just type jupyter notebook in the main directory and the code will pop up in a browser window. #Coding Challenge - Due Date, Thursday, March 2nd 2017 at 12 PM PST Use the price history AND two other metrics of your choice to predict the price of GOOGL stock with an LSTM network. You can find the CSV here. Metrics could be sentiment analysis from Twitter of what people have said about Google, dividends, etc. ##Credits Credits go to jaungiers. I've merely created a wrapper to get people started. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://jupyter.readthedocs.io/en/latest/install.html", "http://colah.github.io/posts/2015-08-Understanding-LSTMs/"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc23"}, "repo_url": "https://github.com/Nawal095/Stock-Price-Prediction", "repo_name": "Stock-Price-Prediction", "repo_full_name": "Nawal095/Stock-Price-Prediction", "repo_owner": "Nawal095", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T19:37:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T19:35:53Z", "homepage": null, "size": 36, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189286852, "is_fork": false, "readme_text": "Stock-Price-Prediction Stock Price Prediction of Apple Inc. Using Recurrent Neural Network OHLC Average Prediction of Apple Inc. Using LSTM Recurrent Neural Network Dataset: The dataset is taken from yahoo finace's website in CSV format. The dataset consists of Open, High, Low and Closing Prices of Apple Inc. stocks from 3rd january 2011 to 13th August 2017 - total 1664 rows. Price Indicator: Stock traders mainly use three indicators for prediction: OHLC average (average of Open, High, Low and Closing Prices), HLC average (average of High, Low and Closing Prices) and Closing price, In this project, OHLC average has been used. Data Pre-processing: After converting the dataset into OHLC average, it becomes one column data. This has been converted into two column time series data, 1st column consisting stock price of time t, and second column of time t+1. All values have been normalized between 0 and 1. Model: Two sequential LSTM layers have been stacked together and one dense layer is used to build the RNN model using Keras deep learning library. Since this is a regression task, 'linear' activation has been used in final layer. Version: Python 2.7 and latest versions of all libraries including deep learning library Keras and Tensorflow. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc24"}, "repo_url": "https://github.com/BrunoBSM/Deep-Q-Learning", "repo_name": "Deep-Q-Learning", "repo_full_name": "BrunoBSM/Deep-Q-Learning", "repo_owner": "BrunoBSM", "repo_desc": "Deep Q-Learning implementation with Keras and Tensorflow on OpenAI-Gym environments", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T18:15:11Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T14:47:53Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189245269, "is_fork": false, "readme_text": "Deep-Q-Learning Deep Q-Learning implementation with Keras and Tensorflow on OpenAI-Gym environments Based on the DQN paper Requirements  Python 3 OpenAI-Gym Keras TensorFlow  Usage This implementation is of a Deep Q-Learning for image input, in order to change the Environment, make sure it is compatible. This means it must have an image as state/observation and a discrete action space such as all ATARI 2600 environments. In order to choose a different Environment, simply change the following \"Breakout-v0\" to the name of the environment you'd like to test. env = gym.make(\"Breakout-v0\")  To change the learning parameters and the size for the Replay Memory maxlen, change these lines: self.memory = deque(maxlen=200000) # learning parameters self.gamma = 0.99 # discount rate/factor self.epsilon = 1.0 # exploration rate. Will be decreased over time self.epsilon_min = 0.1 self.epsilon_decay = 0.9995 self.learning_rate = 0.00025 # alpha  For learning purposes I encourage you to test different parameters and compare performances. Useful Links Tutorials and explainations for DQN https://becominghuman.ai/lets-build-an-atari-ai-part-1-dqn-df57e8ff3b26 https://towardsdatascience.com/welcome-to-deep-reinforcement-learning-part-1-dqn-c3cab4d41b6b https://keon.io/deep-q-learning/ Vlad Mnih's lecture on DQN ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc25"}, "repo_url": "https://github.com/Momotoculteur/HashtagRecommandeurImage", "repo_name": "HashtagRecommandeurImage", "repo_full_name": "Momotoculteur/HashtagRecommandeurImage", "repo_owner": "Momotoculteur", "repo_desc": "Permet de creer avec Tensorflow et Keras un recommandeur de hastag pour image, avec des algorithmes de deep learning.", "description_language": "French", "repo_ext_links": null, "repo_last_mod": "2019-05-30T15:19:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T14:32:51Z", "homepage": "https://deeplylearning.fr/cours-pratiques-deep-learning/recommandeur-de-hashtag-pour-image/", "size": 78076, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189429075, "is_fork": false, "readme_text": "Recommendeur de hashtag pour image Permet de creer avec Tensorflow et Keras un recommandeur de hastag pour image, avec des algorithmes de deep learning. Installer les pr\u00e9-requis Je vous laisse installer vous m\u00eame les libraires n\u00e9cessaires, par conda ou pip selon vos pr\u00e9ferences et environnements  Tensorflow Keras Pandas TQDM PIL  Customization des hyper param\u00e8tres Disponible dans le fichier trainModel.py :    Attribut Description     CLASSE Lien vers le fichier contenant la liste totales des classes avec leurs indices respectifs   NB_CLASSES Nombre de classe ; d\u00e9finie le nombre de neuronnes sur le dernier layer   NB_EPOCH Nombre d'iteration pour l'entrainement   BATCH_SIZE Nombre d'image par lot   SHUFFLE Permet de melanger notre dataset   IMG_SIZE Taille des images \u00e0 resize   TRAINSIZE_RATIO Ratio pour gerer la taille des jeux d'entrainement et de validation   DIRECTORY_DATA Chemin parent vers les images   DIRECTORY_TRAINED_MODEL Chemin ou save le modele   COLOR_MODE Choix des canaux des images : couleur ou noir/blanc    G\u00e9n\u00e9rer et entrainer un model $ python trainModel.py Pr\u00e9diction de nouvelles images $ python autoPredic.py ", "has_readme": true, "readme_language": "French", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Momotoculteur/HashtagRecommandeurImage/blob/d1eea588ee9f57fb4ccaed08a73011479887f3c1/trainedModel/model.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc26"}, "repo_url": "https://github.com/dnap512/blurLogo", "repo_name": "blurLogo", "repo_full_name": "dnap512/blurLogo", "repo_owner": "dnap512", "repo_desc": "This is blurring logos system", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T08:16:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T15:13:10Z", "homepage": null, "size": 118, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189435571, "is_fork": false, "readme_text": "Brand Logo Blurring System This system could detect 16 brand logos. Adidas  Apple  BMW  Cocacola  Ferrari  Google Heineken  McDonalds  Mini  Nike  Pepsi  Porsche Puma  RedBull  Sprite  Starbucks  Weights file link : https://nas.gclab.cs.kookmin.ac.kr:5001/sharing/tizrp8gfx Weights file path : model_data/yolo_logo.h5  Quick start Webcam mode :  python blur_logo.py --webcam Input video or image mode : python blur_logo.py --input \"File path\" --output \"File path\"  Usage Use --help to see usage of blur_logo.py: usage: blur_logo.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM]                      [--input] [--output][--webcam]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().   The original code address : https://github.com/qqwweee/keras-yolo3 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc27"}, "repo_url": "https://github.com/kevinlin311tw/neural-branchpred", "repo_name": "neural-branchpred", "repo_full_name": "kevinlin311tw/neural-branchpred", "repo_owner": "kevinlin311tw", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T06:13:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T00:21:25Z", "homepage": null, "size": 87251, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189499904, "is_fork": false, "readme_text": "neural-branchpred install pip3 install --upgrade tensorflow-gpu==1.5 pip3 install keras, m5 sudo ldconfig /usr/local/cuda/lib64 export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-9.0/lib64/ prepare data bash /data/get_1Mdata.sh quick test on minival python3 branch-minival.py run on full dataset python3 branch.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc28"}, "repo_url": "https://github.com/YaCpotato/B4", "repo_name": "B4", "repo_full_name": "YaCpotato/B4", "repo_owner": "YaCpotato", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T16:48:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T13:40:40Z", "homepage": null, "size": 1296, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189600205, "is_fork": false, "readme_text": "Deeplearning Tuning using keras[Dataset:cifar10] cifar10  consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images  CIFAR-10 official  Environment TBD Evaluation index  Accuracy Recall Precision F-metrics ::confusion_matrix   Result  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.cs.toronto.edu/~kriz/cifar.html"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc29"}, "repo_url": "https://github.com/seboo1995/RGPPfinalproject", "repo_name": "RGPPfinalproject", "repo_full_name": "seboo1995/RGPPfinalproject", "repo_owner": "seboo1995", "repo_desc": "Codes from Graduate Admisson and Housing prices dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T07:49:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T07:35:07Z", "homepage": null, "size": 288, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189367899, "is_fork": false, "readme_text": "RGPPfinalproject Codes from Graduate Admisson and Housing prices dataset Linear Regression and Keras ANN on two datasets. I have my code on Linear Regression and I used the sckit library. Also there is the non-linear regression. All the code is written in Python and I have made my own score. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc2a"}, "repo_url": "https://github.com/amhajavi/Stutter", "repo_name": "Stutter", "repo_full_name": "amhajavi/Stutter", "repo_owner": "amhajavi", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T16:31:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T16:21:02Z", "homepage": null, "size": 441, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 189624683, "is_fork": false, "readme_text": "Dependencies  Python_3 Keras Tensorflow  Training the model To train the model on the stutter dataset, you can run  python main.py --spec_len 50 --gpu 0 --lr 0.0001 --person [the number for each person to test e.g. 20] --batch_size 20 --ohem_level 2  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc2b"}, "repo_url": "https://github.com/SeaOfFrost/CPW-Impedance-Calculator", "repo_name": "CPW-Impedance-Calculator", "repo_full_name": "SeaOfFrost/CPW-Impedance-Calculator", "repo_owner": "SeaOfFrost", "repo_desc": "This repo contains the code for an impedance calculator for a coplanar waveguide, implemented with the help of a plain vanilla neural network.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T09:31:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T08:57:08Z", "homepage": null, "size": 1915, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189562269, "is_fork": false, "readme_text": "CPW-Impedance-Calculator This repo contains the code for an impedance calculator for a coplanar waveguide, implemented with the help of a plain vanilla neural network. This was originally a project idea that I had for a project for my microwaves assignment, but I didn't end up submitting it. I finished up the code a bit later. As we know, a neural network can learn the properties of a function during training. Getting Started Coplanar Waveguide Impedance Theory and Formulae The theory for the coplanar waveguide is given on Microwaves101. They also have an online calculator to find the parameters of the waveguide. However, this website gives the formulae to calculate the impedance of the waveguide, alongwith a calculator. Dataset Generation Since this is a regression problem, where the network needs to learn elliptical functions, it was essential to generate a dataset to train the network. I generated the test set using the CPW in-built functions in the RF Toolbox in Matlab. The code for the same is in the Generate.m file. I've also uploaded the ZData.csv file for the dataset. Prequisites Following packages are required: python  TensorFlow Keras scikit-learn numpy pandas tabulate  Running Code is pretty much self-explanatory. Just change the Predictions and epochs values to save your results in different files. Results In general, results improved as number of epochs was increased. The network performace could be better with a deeper network. Average error is the average of the difference between the predicted and actual values of the impedance.    Epochs Average Error     100 7.49   500 1.27   1000 0.57    Acknowledgements This was inspired by this paper. However, the source code for the same was not released so I created the network using Keras. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/SeaOfFrost/CPW-Impedance-Calculator/blob/f531b1693696c27d086815b77a50a22a18174d8d/my_model100.h5", "https://github.com/SeaOfFrost/CPW-Impedance-Calculator/blob/f531b1693696c27d086815b77a50a22a18174d8d/my_model1000.h5", "https://github.com/SeaOfFrost/CPW-Impedance-Calculator/blob/f531b1693696c27d086815b77a50a22a18174d8d/my_model500.h5"], "see_also_links": [], "reference_list": ["https://ieeexplore.ieee.org/document/4763072"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc2c"}, "repo_url": "https://github.com/Guyanqi/GMDNN", "repo_name": "GMDNN", "repo_full_name": "Guyanqi/GMDNN", "repo_owner": "Guyanqi", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T07:47:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T04:16:26Z", "homepage": null, "size": 238647, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189526423, "is_fork": false, "readme_text": "Image Segmentation combining Graphical Model and Deep Neural Network This project includes the implementation of GMDNN: combination of graphical model and Deep Neural Network for Image Segmentation Example: SegNet with CRF as RNN The project is written in python 3.6 Installation Clone this repository and inside it, run: conda env create -f GMDNN_env.yml source activate GMDNN  Compile high_dim_filter: cd cpp ./compile.sh  After that you can run train script as mentioned on Training the Model. To test with GPU: pip install --upgrade tensorflow-gpu==1.4 conda install -c menpo opencv3   keras.json content {     \"epsilon\": 1e-07,      \"floatx\": \"float32\",      \"image_data_format\": \"channels_last\",      \"backend\": \"theano\" } Visualizing the prepared data You can also visualize your prepared annotations for verification of the prepared data. python visualizeDataset.py \\  --images=\"data/dataset1/images_prepped_train/\" \\  --annotations=\"data/dataset1/annotations_prepped_train/\" \\  --n_classes=11  Training the Model TENSORFLOW_FLAGS=device=cuda0,image_data_format=channels_last,floatX=float32 python train.py \\  --save_weights_path=\"weights/ex1/\" \\  --train_images=\"path/train/\" \\  --train_annotations=\"data_semantics/trainannot/\" \\  --val_images=\"data_semantics/val/\" \\  --val_annotations=\"data_semantics/valannot/\" \\  --n_classes=8 \\  --model_name=\"segnet_res_crf\" \\  --input_height=128 \\  --input_width=128 Getting the predictions TENSORFLOW_FLAGS=device=cuda0,image_data_format=channels_last,floatX=float32 python predict.py   --output_path=\"teste/\" \\  --test_images=\"data_semantics/test/\" \\  --n_classes=8 --model_name=\"segnet_res_crf\" \\  --input_height=128 \\  --input_width=128 \\  --save_weights_path=\"weights_360_480_res_with_crf.hdf5\" Image Segmentation on Street Scene Dataset python train.py \\  --save_weights_path=\"weights/ex1/\"  \\  --train_images=\"dataset1/images_prepped_train/\" \\  --train_annotations=\"dataset1/annotations_prepped_train/\" \\  --val_images=\"dataset1/images_prepped_test/\" \\  --val_annotations=\"dataset1/annotations_prepped_test/\" \\  --n_classes=50  \\  --input_height=320 \\  --input_width=640 \\  --model_name=\"segnet_res_crf\" Reference SegNet implementation: https://github.com/divamgupta/image-segmentation-keras CRF as RNN implementation: https://github.com/sadeepj/crfasrnn_keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Guyanqi/GMDNN/blob/01293464b8dab5656b6739e9e7509301a5df98a1/weights_360_480_res_with_crf.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc2d"}, "repo_url": "https://github.com/13952522076/hpc_yolo3", "repo_name": "hpc_yolo3", "repo_full_name": "13952522076/hpc_yolo3", "repo_owner": "13952522076", "repo_desc": "an object detection framework for UNT REU 2019 project. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T18:17:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T19:09:12Z", "homepage": "", "size": 4743, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189468348, "is_fork": false, "readme_text": "hpc_yolo3  Introduction A keras (tensorflow backend) implement of YOLOV3 (2D image object detection) for UNT REU2019 PROJECT. Created by Xu Ma, xuma@my.unt.edu. Requirement UNT HPC environment, which includes python3, tensorflow, keras, numpy ... Examples  Install   Login UNT HPC sever: ssh username@talon3.hpc.unt.edu.   Create a new folder using your name. For example, mkdir Jacob .   cd Jacob.   Download hpc_yolo3 project. git install https://github.com/13952522076/hpc_yolo3.git  Make sure you download the project in your name folder.   Download weight to local computer: yolo.h5 Copy the downloaded yolo.h5 file to hpc sever 'Jacob/hpc_yolo3/model_data' folder using scp. (Only this step is in a new terminal.) scp /download/yolo.h5 username@talon3.hpc.unt.edu:/home/Jacob/hpc_yolo3/model_data  Remember 1). change the local path to yolo.h5 file, and change the 2) username, 3)Jacob. 3 modifications on this command.   You can have a look at Images folder, which includes 3 subfoloder. images: the collected image data. outputs: the detected labels, confidence, coordinate results: resulted detection images.   Remove all files in these three folders. Do not delete the folders.  rm Images/results/*  rm Images/outputs/*  rm Images/images/*  upload collected images to 'Images/images/' folder. Edit 'run_test.sh', change demo@my.unt.edu to your email,  username to hpc username, YOURNAME to your name. Run SLURM job 'run_test.sh' by sbatch run_test.sh  After this, you will see your job id, like 999444. A few seconds latter, a log file will generated, named as hpc_999444.log. Once filed or completed, you will get an email. Detected images will be in Images/results. Detected information will be in Images/outputs.   ", "has_readme": true, "readme_language": "English", "repo_tags": ["yolov3"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1804.02767"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc2e"}, "repo_url": "https://github.com/ChZ-CC/elephant", "repo_name": "elephant", "repo_full_name": "ChZ-CC/elephant", "repo_owner": "ChZ-CC", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T11:48:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T12:38:31Z", "homepage": null, "size": 14, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189222626, "is_fork": false, "readme_text": "API   \u63a5\u53e3\uff1ahttp://host/image-predict   // post   {       \"image_url\": \"http://img.9ku.com/geshoutuji/singertuji/2/2293/2293_2.jpg\",       \"key\": \"SECRET_KEY\"   }    // response   {       \"code\": 0,       \"data\": {           \"name\": \"\u82f9\u679c\",           \"num\": 0.6523614525794983       },       \"message\": \"success\"   }      // code:   // 0        success --\u6210\u529f   // 401      Unauthorized -- \u7f3a\u5c11key   // 503      Image Recognition failed -- \u6a21\u578b\u5931\u8d25   // 400      image_url required -- \u7f3a\u5c11\u53c2\u6570   \u7b97\u6cd5\u63a5\u53e3   function: image_predict.predict   model \u76ee\u5f55: models/xxx.hdf5 # \u8fdb\u5165\u9879\u76ee\u76ee\u5f55\uff0c\u542f\u52a8python\u73af\u5883 $ cd elephant $ pipenv shell  # \u6d4b\u8bd5\u63a5\u53e3 (elephant)$ python image_predict.py # output \u7701\u7565\u4e00\u5927\u7247\u63d0\u793a\u4fe1\u606f ('\u82f9\u679c', 0.61621004)     \u95ee\u9898   \u76f4\u63a5\u8fd0\u884c python app.predict.py \u53ef\u4ee5\u6b63\u786e\u6267\u884c\uff0c\u4f46\u662f\u5728 flask \u73af\u5883\u4e0b\u8c03\u7528\uff0c\u62a5\u9519\uff1aTensor Tensor(\"dense_6/Softmax:0\", shape=(?, 12), dtype=float32) is not an element of this graph. fixed!!  \u89e3\u51b3\u65b9\u6cd5\uff1a\u6bcf\u6b21\u8bf7\u6c42\u91cd\u65b0 load_model()\uff0cload \u4e4b\u524d keras.backend.clear_session()\u3002    \u73af\u5883\u95ee\u9898\uff1a  ubuntu 14.04 tensorflow \u5b89\u88c5\u6210\u529f\uff0c\u8fd0\u884c\u62a5\u9519\uff1aImportError: /lib/x86_64-linux-gnu/libm.so.6: version GLIBC_2.23' not found (required by /root/.local/share/virtualenvs/elephant-FLgR8YHo/lib/python3.7/site-packages/tensorflow/python/_pywrap_tensorflow_internal.so)` ubuntu 16.04 keras.load_model \u9519\u8bef TypeError: Unexpected keyword argument passed to optimizer: amsgrad\uff0c\u5185\u5b58\u5f00\u9500\u5927\uff0c\u5361\u6b7b\u3002 fixed by pip install keras==2.1.3    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc2f"}, "repo_url": "https://github.com/JosvanGoor/StarGan", "repo_name": "StarGan", "repo_full_name": "JosvanGoor/StarGan", "repo_owner": "JosvanGoor", "repo_desc": "Stargan for the CelebA dataset, implemented in Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T14:21:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T14:17:53Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189426615, "is_fork": false, "readme_text": "celeba_stargan Stargan for CelebA. A Deep Learning practical Keras implementation based on https://github.com/taki0112/StarGAN-Tensorflow Notes: Defaults: epochs: 20 iterations: 1000 batch_size: 16 decay_flag: True decay_epoch: 10 learn_rate: 0.0001 gradient_penalty_lambda: 10.0 adv_weigt: 1 // weight about gan rec_weight: 10 //reconstruction weight cls_weight: 10 // classification weight base_channel_per_layer: 64 number_resblock: 6 number_discriminator_layer: 6 number_critics: 5 img_size: 128 img_channels: 3 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc30"}, "repo_url": "https://github.com/mauzeyj/currency_trading_rl_agent", "repo_name": "currency_trading_rl_agent", "repo_full_name": "mauzeyj/currency_trading_rl_agent", "repo_owner": "mauzeyj", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T05:40:17Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T00:06:02Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189315952, "is_fork": false, "readme_text": "currency_trading_rl_agent The purpose of this project is to practice with reinforcement learning and to maybe make some money along the way.  The project trajectory is as follows. #1 create a simple gym environment from historical price and volume data #2 use kerasrl to get a baseline #3 create V2 of gym environment that includes news data #4 hand craft using keras an agent that can consume both price and text data ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc31"}, "repo_url": "https://github.com/deepdialog/deepdialog", "repo_name": "deepdialog", "repo_full_name": "deepdialog/deepdialog", "repo_owner": "deepdialog", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T16:00:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T08:14:16Z", "homepage": null, "size": 253, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189181079, "is_fork": false, "readme_text": "DeepDialog     \u4e00\u4e2a\u975e\u5e38\u7b80\u6613\u7684\uff0c\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u4e0e\u6570\u636e\u9a71\u52a8\u7684\u5bf9\u8bdd\u7cfb\u7edf\u539f\u578b Requirement Python >= 3.6 Python Dependency keras tensorflow pyyaml joblib scikit-learn tqdm numpy pandas scipy  \u7cfb\u7edf\u6d41\u7a0b User --> LU --> DST    \\            /      NLG <-- DP <-- DB  Train python3 -m train trainning_data_dir output_model_dir Serve python3 -m serve model_dir ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc32"}, "repo_url": "https://github.com/duydole/3wayfinal", "repo_name": "3wayfinal", "repo_full_name": "duydole/3wayfinal", "repo_owner": "duydole", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T03:57:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T02:49:05Z", "homepage": null, "size": 53638, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189516442, "is_fork": false, "readme_text": "Branch: master Code: DQN DOUBLE TARGET 60x60x1 tlc_thesis_final_v1 Dates: 12h16 27/04/2019  Old reward New action for 4 phase 10s 10s 10s 10s Implement Target network Simple simulation includes 1 vehicle type (4 simulation NS WE HIGH LOW). State 60601 for position (Duy's implementation) Model by Keras Dueling, double, prioritized,..: NOT YET IMPLEMENT  TARGET  GOOD GOOD Verify it?   Transform to To Hien Thanh Simulation 4 silumation: WE NS HIGH LOW   State: 60602 (DA's implementation) GOOD Implement it and verify.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/duydole/3wayfinal/blob/3ffac8783fa3267e5a7825d40b10c7dd455a44f5/trained_models/Agen1_95.h5", "https://github.com/duydole/3wayfinal/blob/3ffac8783fa3267e5a7825d40b10c7dd455a44f5/trained_models/Agent2_200.h5", "https://github.com/duydole/3wayfinal/blob/531b9aa80cd3b7e51538d062669f95775588d80f/trained_models/Agent2_30.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc33"}, "repo_url": "https://github.com/lambdal/TensorFlow2-tutorial", "repo_name": "TensorFlow2-tutorial", "repo_full_name": "lambdal/TensorFlow2-tutorial", "repo_owner": "lambdal", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T12:05:47Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T20:43:48Z", "homepage": null, "size": 13, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189656171, "is_fork": false, "readme_text": "TensorFlow2-tutorial Installation git clone https://github.com/lambdal/TensorFlow2-tutorial.git cd TensorFlow2-tutorial virtualenv venv-tf2 . venv-tf2/bin/activate pip install tf-nightly-gpu-2.0-preview==2.0.0.dev20190526  Tutorials Summary See individual tutorial's README for details 01 Basic Image Classification A tutorial of Image classification with ResNet.  Data pipeline with TensorFlow Dataset API Model pipeline with Keras (TensorFlow 2's offical high level API) Multi-GPU with distributed strategy Customized training with callbacks (TensorBoard, Customized learning schedule)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc34"}, "repo_url": "https://github.com/flyyufelix/donkey_pixmoving", "repo_name": "donkey_pixmoving", "repo_full_name": "flyyufelix/donkey_pixmoving", "repo_owner": "flyyufelix", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T02:59:58Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T02:28:54Z", "homepage": null, "size": 10, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189331445, "is_fork": false, "readme_text": "Code used for Pixmoving Hackathon 2019 Background Code used by Felix during Pixmoving Hackathon. This repo just contains the code but not the dataset nor the trained models. Description Here is a high level description of the important files: d2/manage.py - Default manage.py file d2/manage_enhanced.py - Include support for Stacked Frame, and Time Sequence Frames (for LSTM) imported as parts d2/manage_enhanced_cv.py - Include Image Thresholding to eliminate light glare, also implemented as parts d2/train/datasets.py - Dataloader to prepare training data for default model, stacked frame model, and LSTM model. d2/train/train.py - Train various models such as LSTM, fine-tune Q model from RL, default CNN with grayscale stacked frame, etc donkeycar/donkeycar/parts/cv.py - CV related algorithms implemented as parts. Image Thresholding, Stacked Frame, and Time Sequence Frames code can be found here donkeycar/donkeycar/parts/keras.py - Wrap Keras models (such as LSTM, Q model, default CNN) as parts Dependencies Run on Jetson Nano following this guide. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc35"}, "repo_url": "https://github.com/BrunoBSM/FlappyBirdDeepFace", "repo_name": "FlappyBirdDeepFace", "repo_full_name": "BrunoBSM/FlappyBirdDeepFace", "repo_owner": "BrunoBSM", "repo_desc": "A Flappy Bird Clone made using python-pygame with dynamic difficulty based on player's mood", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T18:10:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T18:04:20Z", "homepage": null, "size": 43076, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189274876, "is_fork": false, "readme_text": "FlapPyBirdDeepFace Flappy Bird with dynamic difficult based on face recognition Projeto baseado no conceito de dificuldade din\u00e2mica baseado em reconhecimento facial do usu\u00e1rio. Requirements  Python 3.5 Keras 2.0.3 TensorFlow 1.1.0 Pandas 0.19.1 NumPy 1.12.1 h5py 2.7 Statistics OpenCV 3.2.0  Usage  Clone project Run code: python3 flappy.py  Projects used:  Flappy Bird Clone : https://github.com/sourabhv/FlapPyBird Face Classification : https://github.com/oarriaga/face_classification  Authors Bruno Brand\u00e3o Rafael Marques ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.00-0.47.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.02-0.52.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.03-0.53.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.04-0.55.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.05-0.56.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.08-0.57.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.10-0.58.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.100-0.65.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.107-0.66.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.11-0.58.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.110-0.65.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.12-0.58.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.14-0.59.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.15-0.60.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.25-0.60.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.27-0.62.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.29-0.62.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.32-0.62.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.37-0.62.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.38-0.62.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.41-0.62.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.43-0.64.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.51-0.63.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.70-0.63.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.97-0.65.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/fer2013_mini_XCEPTION.99-0.65.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/mini_XCEPTION_KDEF.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/simple_CNN.530-0.65.hdf5", "https://github.com/BrunoBSM/FlappyBirdDeepFace/blob/3298cd9dd8e913611b397a2ef7ea3818f7cc0537/trained_models/emotion_models/simple_CNN.985-0.66.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc36"}, "repo_url": "https://github.com/jiajunhua/eriklindernoren-Keras-GAN", "repo_name": "eriklindernoren-Keras-GAN", "repo_full_name": "jiajunhua/eriklindernoren-Keras-GAN", "repo_owner": "jiajunhua", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T09:40:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T08:12:34Z", "homepage": null, "size": 90409, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189373333, "is_fork": false, "readme_text": "   Keras-GAN Collection of Keras implementations of Generative Adversarial Networks (GANs) suggested in research papers. These models are in some cases simplified versions of the ones ultimately described in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GAN varieties to implement are very welcomed. See also: PyTorch-GAN Table of Contents  Installation Implementations  Auxiliary Classifier GAN Adversarial Autoencoder Bidirectional GAN Boundary-Seeking GAN Conditional GAN Context-Conditional GAN Context Encoder Coupled GANs CycleGAN Deep Convolutional GAN DiscoGAN DualGAN Generative Adversarial Network InfoGAN LSGAN Pix2Pix PixelDA Semi-Supervised GAN Super-Resolution GAN Wasserstein GAN Wasserstein GAN GP    Installation $ git clone https://github.com/eriklindernoren/Keras-GAN $ cd Keras-GAN/ $ sudo pip3 install -r requirements.txt  Implementations AC-GAN Implementation of Auxiliary Classifier Generative Adversarial Network. Code Paper: https://arxiv.org/abs/1610.09585 Example $ cd acgan/ $ python3 acgan.py     Adversarial Autoencoder Implementation of Adversarial Autoencoder. Code Paper: https://arxiv.org/abs/1511.05644 Example $ cd aae/ $ python3 aae.py     BiGAN Implementation of Bidirectional Generative Adversarial Network. Code Paper: https://arxiv.org/abs/1605.09782 Example $ cd bigan/ $ python3 bigan.py  BGAN Implementation of Boundary-Seeking Generative Adversarial Networks. Code Paper: https://arxiv.org/abs/1702.08431 Example $ cd bgan/ $ python3 bgan.py  CC-GAN Implementation of Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks. Code Paper: https://arxiv.org/abs/1611.06430 Example $ cd ccgan/ $ python3 ccgan.py     CGAN Implementation of Conditional Generative Adversarial Nets. Code Paper:https://arxiv.org/abs/1411.1784 Example $ cd cgan/ $ python3 cgan.py     Context Encoder Implementation of Context Encoders: Feature Learning by Inpainting. Code Paper: https://arxiv.org/abs/1604.07379 Example $ cd context_encoder/ $ python3 context_encoder.py     CoGAN Implementation of Coupled generative adversarial networks. Code Paper: https://arxiv.org/abs/1606.07536 Example $ cd cogan/ $ python3 cogan.py  CycleGAN Implementation of Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. Code Paper: https://arxiv.org/abs/1703.10593    Example $ cd cyclegan/ $ bash download_dataset.sh apple2orange $ python3 cyclegan.py     DCGAN Implementation of Deep Convolutional Generative Adversarial Network. Code Paper: https://arxiv.org/abs/1511.06434 Example $ cd dcgan/ $ python3 dcgan.py     DiscoGAN Implementation of Learning to Discover Cross-Domain Relations with Generative Adversarial Networks. Code Paper: https://arxiv.org/abs/1703.05192    Example $ cd discogan/ $ bash download_dataset.sh edges2shoes $ python3 discogan.py     DualGAN Implementation of DualGAN: Unsupervised Dual Learning for Image-to-Image Translation. Code Paper: https://arxiv.org/abs/1704.02510 Example $ cd dualgan/ $ python3 dualgan.py  GAN Implementation of Generative Adversarial Network with a MLP generator and discriminator. Code Paper: https://arxiv.org/abs/1406.2661 Example $ cd gan/ $ python3 gan.py     InfoGAN Implementation of InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Code Paper: https://arxiv.org/abs/1606.03657 Example $ cd infogan/ $ python3 infogan.py     LSGAN Implementation of Least Squares Generative Adversarial Networks. Code Paper: https://arxiv.org/abs/1611.04076 Example $ cd lsgan/ $ python3 lsgan.py  Pix2Pix Implementation of Image-to-Image Translation with Conditional Adversarial Networks. Code Paper: https://arxiv.org/abs/1611.07004    Example $ cd pix2pix/ $ bash download_dataset.sh facades $ python3 pix2pix.py     PixelDA Implementation of Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks. Code Paper: https://arxiv.org/abs/1612.05424 MNIST to MNIST-M Classification Trains a classifier on MNIST images that are translated to resemble MNIST-M (by performing unsupervised image-to-image domain adaptation). This model is compared to the naive solution of training a classifier on MNIST and evaluating it on MNIST-M. The naive model manages a 55% classification accuracy on MNIST-M while the one trained during domain adaptation gets a 95% classification accuracy. $ cd pixelda/ $ python3 pixelda.py     Method Accuracy     Naive 55%   PixelDA 95%    SGAN Implementation of Semi-Supervised Generative Adversarial Network. Code Paper: https://arxiv.org/abs/1606.01583 Example $ cd sgan/ $ python3 sgan.py     SRGAN Implementation of Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network. Code Paper: https://arxiv.org/abs/1609.04802    Example $ cd srgan/ <follow steps at the top of srgan.py> $ python3 srgan.py     WGAN Implementation of Wasserstein GAN (with DCGAN generator and discriminator). Code Paper: https://arxiv.org/abs/1701.07875 Example $ cd wgan/ $ python3 wgan.py     WGAN GP Implementation of Improved Training of Wasserstein GANs. Code Paper: https://arxiv.org/abs/1704.00028 Example $ cd wgan_gp/ $ python3 wgan_gp.py     ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1610.09585", "https://arxiv.org/abs/1511.05644", "https://arxiv.org/abs/1605.09782", "https://arxiv.org/abs/1702.08431", "https://arxiv.org/abs/1611.06430", "https://arxiv.org/abs/1411.1784", "https://arxiv.org/abs/1604.07379", "https://arxiv.org/abs/1606.07536", "https://arxiv.org/abs/1703.10593", "https://arxiv.org/abs/1511.06434", "https://arxiv.org/abs/1703.05192", "https://arxiv.org/abs/1704.02510", "https://arxiv.org/abs/1406.2661", "https://arxiv.org/abs/1606.03657", "https://arxiv.org/abs/1611.04076", "https://arxiv.org/abs/1611.07004", "https://arxiv.org/abs/1612.05424", "https://arxiv.org/abs/1606.01583", "https://arxiv.org/abs/1609.04802", "https://arxiv.org/abs/1701.07875", "https://arxiv.org/abs/1704.00028"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc37"}, "repo_url": "https://github.com/shlok97/LSTM-StockPredictor", "repo_name": "LSTM-StockPredictor", "repo_full_name": "shlok97/LSTM-StockPredictor", "repo_owner": "shlok97", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T17:56:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T04:31:34Z", "homepage": null, "size": 132, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189150835, "is_fork": false, "readme_text": "Stock Price Prediction of Apple Inc. Using Recurrent Neural Network OHLC Average Prediction of Apple Inc. Using LSTM Recurrent Neural Network Dataset: The dataset is taken from yahoo finace's website in CSV format. The dataset consists of Open, High, Low and Closing Prices of Apple Inc. stocks from 3rd january 2011 to 13th August 2017 - total 1664 rows. Price Indicator: Stock traders mainly use three indicators for prediction: OHLC average (average of Open, High, Low and Closing Prices), HLC average (average of High, Low and Closing Prices) and Closing price, In this project, OHLC average has been used. Data Pre-processing: After converting the dataset into OHLC average, it becomes one column data. This has been converted into two column time series data, 1st column consisting stock price of time t, and second column of time t+1. All values have been normalized between 0 and 1. Model: Two sequential LSTM layers have been stacked together and one dense layer is used to build the RNN model using Keras deep learning library. Since this is a regression task, 'linear' activation has been used in final layer. Version: Python 2.7 and latest versions of all libraries including deep learning library Keras and Tensorflow. Training: 75% data is used for training. Adagrad (adaptive gradient algorithm) optimizer is used for faster convergence. After training starts it will look like:  Test: Test accuracy metric is root mean square error (RMSE). Results: The comparison of OHLC, HLC and Closing price:  After the training the fitted curve with original stock price:  Observation and Conclusion: Since difference among OHLC average, HLC average and closing value is not significat, so only OHLC average is used to build the model and prediction. The training and testing RMSE are: 1.24 and 1.37 respectively which is pretty good to predict future values of stock. Stock price of last day of dataset was 158.8745 and using this model and price of next two days are predicted as 160.3230 and 160.9240 - which were 159.2075 and 159.8325 on 14th and 15th August 2017 according to Yahoo Finance. However, future values for any time period can be predicted using this model. Finally, this work can greatly help the quantitative traders to take decisions. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shlok97/LSTM-StockPredictor/blob/693a813e741c3aaeb7d77d59409c175eb132921a/LSTM.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc38"}, "repo_url": "https://github.com/wilson-kaka/faceai", "repo_name": "faceai", "repo_full_name": "wilson-kaka/faceai", "repo_owner": "wilson-kaka", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T00:53:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T00:52:39Z", "homepage": null, "size": 40460, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189320227, "is_fork": false, "readme_text": "English Doc \u529f\u80fd  \u4eba\u8138\u68c0\u6d4b\u3001\u8bc6\u522b\uff08\u56fe\u7247\u3001\u89c6\u9891\uff09 \u8f6e\u5ed3\u6807\u8bc6 \u5934\u50cf\u5408\u6210\uff08\u7ed9\u4eba\u6234\u5e3d\u5b50\uff09 \u6570\u5b57\u5316\u5986\uff08\u753b\u53e3\u7ea2\u3001\u7709\u6bdb\u3001\u773c\u775b\u7b49\uff09 \u6027\u522b\u8bc6\u522b \u8868\u60c5\u8bc6\u522b\uff08\u751f\u6c14\u3001\u538c\u6076\u3001\u6050\u60e7\u3001\u5f00\u5fc3\u3001\u96be\u8fc7\u3001\u60ca\u559c\u3001\u5e73\u9759\u7b49\u4e03\u79cd\u60c5\u7eea\uff09 \u89c6\u9891\u5bf9\u8c61\u63d0\u53d6 \u56fe\u7247\u4fee\u590d\uff08\u53ef\u7528\u4e8e\u6c34\u5370\u53bb\u9664\uff09 \u56fe\u7247\u81ea\u52a8\u4e0a\u8272 \u773c\u52a8\u8ffd\u8e2a\uff08\u5f85\u5b8c\u5584\uff09 \u6362\u8138\uff08\u5f85\u5b8c\u5584\uff09  \u67e5\u770b\u529f\u80fd\u9884\u89c8\u2193\u2193\u2193 \u5f00\u53d1\u73af\u5883  Windows 10\uff08x64\uff09 Python 3.6.4 OpenCV 3.4.1 Dlib 19.8.1 face_recognition 1.2.2 keras 2.1.6 tensorflow 1.8.0 Tesseract OCR 4.0.0-beta.1  \u6559\u7a0b OpenCV\u73af\u5883\u642d\u5efa Tesseract OCR\u6587\u5b57\u8bc6\u522b \u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09 \u56fe\u7247\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09 \u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08OpenCV\u7248\uff09 \u89c6\u9891\u4eba\u8138\u68c0\u6d4b\uff08Dlib\u7248\uff09 \u8138\u90e8\u8f6e\u5ed3\u7ed8\u5236 \u6570\u5b57\u5316\u5986 \u89c6\u9891\u4eba\u8138\u8bc6\u522b \u5934\u50cf\u7279\u6548\u5408\u6210 \u6027\u522b\u8bc6\u522b \u8868\u60c5\u8bc6\u522b \u89c6\u9891\u5bf9\u8c61\u63d0\u53d6 \u56fe\u7247\u4fee\u590d \u5176\u4ed6\u6559\u7a0b Ubuntu apt-get\u548cpip\u6e90\u66f4\u6362 pip/pip3\u66f4\u6362\u56fd\u5185\u6e90\u2014\u2014Windows\u7248 OpenCV\u6dfb\u52a0\u4e2d\u6587 \u4f7f\u7528\u9f20\u6807\u7ed8\u56fe\u2014\u2014OpenCV \u529f\u80fd\u9884\u89c8 \u7ed8\u5236\u8138\u90e8\u8f6e\u5ed3   \u4eba\u813868\u4e2a\u5173\u952e\u70b9\u6807\u8bc6   \u5934\u50cf\u7279\u6548\u5408\u6210   \u6027\u522b\u8bc6\u522b   \u8868\u60c5\u8bc6\u522b   \u6570\u5b57\u5316\u5986   \u89c6\u9891\u4eba\u8138\u68c0\u6d4b   \u89c6\u9891\u4eba\u8138\u8bc6\u522b   \u89c6\u9891\u4eba\u8138\u8bc6\u522b   \u56fe\u7247\u4fee\u590d   \u56fe\u7247\u81ea\u52a8\u4e0a\u8272   \u6280\u672f\u65b9\u6848 \u6280\u672f\u5b9e\u73b0\u65b9\u6848\u4ecb\u7ecd \u4eba\u8138\u8bc6\u522b\uff1aOpenCV / Dlib  \u4eba\u8138\u68c0\u6d4b\uff1aface_recognition  \u6027\u522b\u8bc6\u522b\uff1akeras + tensorflow  \u6587\u5b57\u8bc6\u522b\uff1aTesseract OCR  TODO \u6362\u8138\u2014\u2014\u5f85\u5b8c\u5584 \u773c\u775b\u79fb\u52a8\u65b9\u5411\u68c0\u6d4b\u2014\u2014\u5f85\u5b8c\u5584 Dlib\u6027\u80fd\u4f18\u5316\u65b9\u6848 Dlib\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5 Tesseract\u6a21\u578b\u8bad\u7ec3\u65b9\u6cd5 \u8d21\u732e\u8005\u540d\u5355\uff08\u7279\u522b\u611f\u8c22\uff09 archersmind rishab-sharma \u5fae\u4fe1\u6253\u8d4f  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/wilson-kaka/faceai/blob/64b9920c650bb6beb13cf36ba078c06b6960bd76/faceai/classifier/emotion_models/simple_CNN.530-0.65.hdf5", "https://github.com/wilson-kaka/faceai/blob/64b9920c650bb6beb13cf36ba078c06b6960bd76/faceai/classifier/gender_models/gender_mini_XCEPTION.21-0.95.hdf5", "https://github.com/wilson-kaka/faceai/blob/64b9920c650bb6beb13cf36ba078c06b6960bd76/faceai/classifier/gender_models/simple_CNN.81-0.96.hdf5", "https://github.com/wilson-kaka/faceai/blob/64b9920c650bb6beb13cf36ba078c06b6960bd76/faceai/data/simple_colorize.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc39"}, "repo_url": "https://github.com/Baticsute/Facial_Expression_Recognition_FER2013", "repo_name": "Facial_Expression_Recognition_FER2013", "repo_full_name": "Baticsute/Facial_Expression_Recognition_FER2013", "repo_owner": "Baticsute", "repo_desc": "Images and Videos, Real-time Facial Expession Recognition Application with Combine CNN , deep learning features extraction incorporate SIFT, FAST feature . ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T15:42:58Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T07:37:23Z", "homepage": "", "size": 744, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189549980, "is_fork": false, "readme_text": "Facial_Expression_Recognition_FER2013 Powered by Python 3.5 Library Requirements  pip install tensorflow-gpu  pip install keras  pip install numpy  pip install sklearn  pip install pandas  pip install opencv-python==3.3.1  Dataset FER2013 More details : Kaggle Challenge - https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data Accuracy Achivement Kaggle Challenge Winner 71.2%. Model Combine  CNN1 + CNN2 + CNN_FAST + CNN_SIFT 72.89%  Confusion Matrix  CNN Architectures Build from scratch in /cnn folder . HOW ARE YOU TODAY? Application Run UI.py file to open application.  You can choose some image and video that include human faces to detect emotions Click Button \"Open Your Camera\" for real-time detecting yourself emotions from webcam. Highlight Results      Video Realtime Examples will be uploaded later. References The Main Idea From This Article. ", "has_readme": true, "readme_language": "English", "repo_tags": ["fer2013", "convolutional-neural-networks", "sift-descriptors", "sift", "cnn", "facial-expression-recognition", "emotion-recognition", "emotion-detection", "deep-neural-networks", "computer-vision"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1608.02833"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc3a"}, "repo_url": "https://github.com/gideont/tensorflow_ex", "repo_name": "tensorflow_ex", "repo_full_name": "gideont/tensorflow_ex", "repo_owner": "gideont", "repo_desc": "Tensorflow Examples", "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T06:09:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T09:45:03Z", "homepage": "", "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189196937, "is_fork": false, "readme_text": "Tensorflow Examples Commands to run example codes: Note: The following steps create python virtual environment for tensorflow and keras, activate the virtual environment, and install the dependencies using pip $ virtualenv -p /usr/bin/python3 venv $ source venv/bin/activate $ pip install -r requirements.txt Go to examples folders e.g. 02_xor/ then run the python code. The example compile, fit the model, then save it.  Next command is to load the model to the neural network and make predictions: $ python gen_save_model.py $ python load_n_predict.py Other useful pip commands: $ pip freeze > requirements.txt $ pip install -r requirements.txt $ pip install --upgrade \"tensorflow-1.12.0-cp36-cp36m-linux_x86_64.whl\" ", "has_readme": true, "readme_language": "English", "repo_tags": ["tensorflow"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc3b"}, "repo_url": "https://github.com/spalit/scType_Predict", "repo_name": "scType_Predict", "repo_full_name": "spalit/scType_Predict", "repo_owner": "spalit", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T18:11:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T17:36:45Z", "homepage": null, "size": 8086, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 189270991, "is_fork": false, "readme_text": "Introduction scType_Predict is an easy-to-use python package that can be used to annotate clusters using the marker genes obtained in the cell-type marker discovery analysis for scRNA-seq data (for example). The scType_Predict method scType_Predict is based on an artificial neural network that is trained on the entire Mouse Cell Atlas. Installation Installing scType_Predict from PyPi using:: pip install -e scType_Predict  It requires Python 3.5.4, keras 2.2.4, tensorflow 1.12.0, numpy 1.15.2, pandas 0.23.4 Usage Import scType_Predict as:: from scType_Predict.merge_mtx import predict predict()  The input genelist should be as a .txt file and placed within the folder:: /scType_Predict/scType_Predict/data/test.txt  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/spalit/scType_Predict/blob/13185595b3555283e4d9fc5e3703a3c1bac73e6b/scType_Predict/data/model_d0.4_n50_d0.4_n25_outca.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc3c"}, "repo_url": "https://github.com/bgshin/distill_demo", "repo_name": "distill_demo", "repo_full_name": "bgshin/distill_demo", "repo_owner": "bgshin", "repo_desc": "Implementation of Word Embedding Distillation with Ensemble Learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T19:03:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T18:33:22Z", "homepage": null, "size": 23, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189278948, "is_fork": false, "readme_text": "Introduction  Official Implementation of the paper, \"The Pupil Has Become the Master: Teacher-Student Model-Based Word Embedding Distillation with Ensemble Learning\", B. Shin, H. Yang, and J.D. Choi, IJCAI 2019. Author: Bonggun Shin  Download Data  Unzip Data into 'data' folder  distill_demo/data/sentiment_analysis/* distill_demo/data/w2v/*  Requirements pip install tensorflow pip install keras pip install gensim pip install sklearn  Train teachers cd src/ python train_teacher.py -ds sst5 -m cnn2 -t 0 python train_teacher.py -ds sst5 -m cnn2 -t 1 ... python train_teacher.py -ds sst5 -m cnn2 -t 9  python train_teacher.py -ds sst5 -m lstm -t 0 python train_teacher.py -ds sst5 -m lstm -t 1 ... python train_teacher.py -ds sst5 -m lstm -t 9  Train an autoencoder cd src/ python train_ae.py  Distill without ensemble cd src/ python distill.py  Distill with ensemble cd src/ python extract_logits.py python distill.py  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc3d"}, "repo_url": "https://github.com/rizandigp/keras_superconvergence", "repo_name": "keras_superconvergence", "repo_full_name": "rizandigp/keras_superconvergence", "repo_owner": "rizandigp", "repo_desc": "Super-convergence: very fast training of neural networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T03:18:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T08:08:16Z", "homepage": null, "size": 428, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189372738, "is_fork": false, "readme_text": "keras_superconvergence Super-convergence: very fast training of neural networks ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc3e"}, "repo_url": "https://github.com/kiranu1024/deep-learning", "repo_name": "deep-learning", "repo_full_name": "kiranu1024/deep-learning", "repo_owner": "kiranu1024", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T10:25:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T10:18:42Z", "homepage": null, "size": 72, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189574552, "is_fork": false, "readme_text": "Top deep learning Github repositories Here's a list of top 200 deep learning Github repositories sorted by the number of stars. The query that has been used with Github search API is:  deep-learning OR CNN OR RNN OR \"convolutional neural network\" OR \"recurrent neural network\"  Trending deep learning Github repositories can be found here. Date: 04-02-2019 compared to  07-17-2018 Hint: This will be updated regularly.     Pos Name Description Language Stars Forks     \u2796 1 tensorflow An Open Source Machine Learning Framework for Everyone C++ 124506 73403   \u2796 2 keras Deep Learning for humans Python 39683 15113   \u2796 3 opencv Open Source Computer Vision Library C++ 33226 23984   \u2b06\ufe0f1 4 TensorFlow-Examples TensorFlow Tutorial and Examples for Beginners with Latest APIs Jupyter Notebook 30459 11486   \u2b07\ufe0f1 5 caffe Caffe: a fast open framework for deep learning. C++ 27693 16655   \u2b06\ufe0f3 6 pytorch Tensors and Dynamic neural networks in Python  with strong GPU acceleration C++ 26443 6256   \u2796 7 deeplearningbook-chinese Deep Learning Book Chinese Translation TeX 23647 6939   \ud83c\udd95 8 DeepLearning-500-questions \u6df1\u5ea6\u5b66\u4e60500\u95ee\uff0c\u4ee5\u95ee\u7b54\u5f62\u5f0f\u5bf9\u5e38\u7528\u7684\u6982\u7387\u77e5\u8bc6\u3001\u7ebf\u6027\u4ee3\u6570\u3001\u673a\u5668\u5b66\u4e60\u3001\u6df1\u5ea6\u5b66\u4e60\u3001\u8ba1\u7b97\u673a\u89c6\u89c9\u7b49\u70ed\u70b9\u95ee\u9898\u8fdb\u884c\u9610\u8ff0\uff0c\u4ee5\u5e2e\u52a9\u81ea\u5df1\u53ca\u6709\u9700\u8981\u7684\u8bfb\u8005\u3002 \u5168\u4e66\u5206\u4e3a18\u4e2a\u7ae0\u8282\uff0c\u8fd130\u4e07\u5b57\u3002\u7531\u4e8e\u6c34\u5e73\u6709\u9650\uff0c\u4e66\u4e2d\u4e0d\u59a5\u4e4b\u5904\u6073\u8bf7\u5e7f\u5927\u8bfb\u8005\u6279\u8bc4\u6307\u6b63\u3002   \u672a\u5b8c\u5f85\u7eed............ \u5982\u6709\u610f\u5408\u4f5c\uff0c\u8054\u7cfbscutjy2015@163.com                     \u7248\u6743\u6240\u6709\uff0c\u8fdd\u6743\u5fc5\u7a76       Tan 2018.06 None 22408 6145   \u2b07\ufe0f1 9 Deep-Learning-Papers-Reading-Roadmap Deep Learning papers reading roadmap for anyone who are eager to learn this amazing tech! Python 22280 5021   \u2b07\ufe0f4 10 machine-learning-for-software-engineers A complete daily plan for studying to become a machine learning engineer. None 21454 4805   \ud83c\udd95 11 Algorithm_Interview_Notes-Chinese 2018/2019/\u6821\u62db/\u6625\u62db/\u79cb\u62db/\u7b97\u6cd5/\u673a\u5668\u5b66\u4e60(Machine Learning)/\u6df1\u5ea6\u5b66\u4e60(Deep Learning)/\u81ea\u7136\u8bed\u8a00\u5904\u7406(NLP)/C/C++/Python/\u9762\u8bd5\u7b14\u8bb0 Python 20767 6184   \u2b07\ufe0f1 12 Detectron FAIR's research platform for object detection research, implementing popular algorithms like Mask R-CNN and RetinaNet. Python 19765 4152   \u2b07\ufe0f3 13 awesome-deep-learning-papers The most cited deep learning papers TeX 18322 3467   \ud83c\udd95 14 practicalAI \ud83d\udcda A practical approach to learning and using machine learning. Jupyter Notebook 16909 2715   \u2b07\ufe0f2 15 incubator-mxnet Lightweight, Portable, Flexible Distributed/Mobile Deep Learning with Dynamic, Mutation-aware Dataflow Dep Scheduler; for Python, R, Julia, Scala, Go, Javascript and more C++ 16622 5923   \u2b07\ufe0f4 16 CNTK Microsoft Cognitive Toolkit (CNTK), an open source deep-learning toolkit C++ 15965 4237   \u2b07\ufe0f3 17 data-science-ipython-notebooks Data science Python notebooks: Deep learning (TensorFlow, Theano, Caffe, Keras), scikit-learn, Kaggle, big data (Spark, Hadoop MapReduce, HDFS), matplotlib, pandas, NumPy, SciPy, Python essentials, AWS, and various command lines. Python 15256 4600   \u2b07\ufe0f3 18 lectures Oxford Deep NLP 2017 course None 13632 3191   \u2b06\ufe0f9 19 handson-ml A series of Jupyter notebooks that walk you through the fundamentals of Machine Learning and Deep Learning in python using Scikit-Learn and TensorFlow. Jupyter Notebook 13620 7247   \u2b06\ufe0f32 20 fastai The fastai deep learning library, plus lessons and tutorials Jupyter Notebook 12832 4678   \u2b07\ufe0f2 21 spaCy \ud83d\udcab Industrial-strength Natural Language Processing (NLP) with Python and Cython Python 12760 2152   \u2b06\ufe0f3 22 darknet Convolutional Neural Networks C 12393 7111   \u2b07\ufe0f7 23 Qix Machine Learning\u3001Deep Learning\u3001PostgreSQL\u3001Distributed System\u3001Node.Js\u3001Golang None 12379 4647   \u2b06\ufe0f12 24 openpose OpenPose: Real-time multi-person keypoint detection library for body, face, hands, and foot estimation C++ 11963 3284   \u2b07\ufe0f7 25 openface Face recognition with deep neural networks. Lua 11846 2875   \u2b07\ufe0f6 26 cheatsheets-ai Essential Cheat Sheets for deep learning and machine learning researchers None 11773 2778   \u2b07\ufe0f4 27 awesome-deep-learning A curated list of awesome Deep Learning tutorials, projects and communities. None 11683 3585   \ud83c\udd95 28 Screenshot-to-code A neural network that transforms a design mock-up into a static website HTML 11435 1097   \u2b07\ufe0f3 29 ML-From-Scratch Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from data mining to deep learning. Python 11242 1923   \u2b06\ufe0f17 30 Mask_RCNN Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow Python 11166 4795   \u2b07\ufe0f10 31 deeplearning4j Deeplearning4j, ND4J, DataVec and more - deep learning & linear algebra for Java/Scala with GPUs + Spark Java 10562 4622   \u2b06\ufe0f8 32 DeepSpeech A TensorFlow implementation of Baidu's DeepSpeech architecture C++ 9794 1759   \u2b07\ufe0f2 33 awesome-datascience \ud83d\udcdd An awesome Data Science repository to learn and apply for real world problems. None 9625 2858   \u2b07\ufe0f10 34 convnetjs Deep Learning in Javascript. Train Convolutional Neural Networks (or ordinary ones) in your browser. JavaScript 9602 1898   \ud83c\udd95 35 100-Days-Of-ML-Code 100-Days-Of-ML-Code\u4e2d\u6587\u7248 Jupyter Notebook 9264 2325   \u2b07\ufe0f9 36 dive-into-machine-learning Dive into Machine Learning with Python Jupyter notebook and scikit-learn! None 9194 1677   \u2b07\ufe0f5 37 neural-enhance Super Resolution for images using deep learning. Python 9065 965   \u2b07\ufe0f8 38 tflearn Deep learning library featuring a higher-level API for TensorFlow. Python 9003 2265   \u2b06\ufe0f3 39 neural-networks-and-deep-learning Code samples for my book \"Neural Networks and Deep Learning\" Python 8858 4384   \u2b07\ufe0f2 40 Machine-Learning-Tutorials machine learning and deep learning tutorials, articles and other resources None 8855 2636   \ud83c\udd95 41 caffe2 Caffe2 is a lightweight, modular, and scalable deep learning framework. Shell 8447 2129   \u2b07\ufe0f5 42 tfjs-core WebGL-accelerated ML // linear algebra // automatic differentiation for JavaScript. TypeScript 8409 935   \u2b07\ufe0f2 43 Paddle PArallel Distributed Deep LEarning \uff08PaddlePaddle\u6838\u5fc3\u6846\u67b6\uff0c\u9ad8\u6027\u80fd\u5355\u673a\u3001\u5206\u5e03\u5f0f\u8bad\u7ec3\u548c\u8de8\u5e73\u53f0\u90e8\u7f72\uff09 C++ 8386 2268   \u2b06\ufe0f2 44 stanford-tensorflow-tutorials This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research. Python 8338 3837   \u2b07\ufe0f1 45 awesome-deep-vision A curated list of deep learning resources for computer vision None 8015 2349   \u2b06\ufe0f5 46 awesome-nlp \ud83d\udcd6 A curated list of resources dedicated to Natural Language Processing (NLP) None 7898 1433   \u2b06\ufe0f3 47 fast-style-transfer TensorFlow CNN for fast style transfer \u26a1\ud83d\udda5\ud83c\udfa8\ud83d\uddbc Python 7675 1820   \u2b06\ufe0f7 48 facenet Face recognition using Tensorflow Python 7618 3168   \ud83c\udd95 49 DeepCreamPy Decensoring Hentai with Deep Neural Networks Python 7551 759   \u2b07\ufe0f5 50 MLAlgorithms Minimal and clean examples of machine learning algorithms implementations Python 7356 1238   \u2b06\ufe0f13 51 tensor2tensor Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. Python 7344 1860   \ud83c\udd95 52 d2l-zh \u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\uff0c\u82f1\u6587\u7248\u5373\u4f2f\u514b\u5229\u6df1\u5ea6\u5b66\u4e60\uff08STAT 157\uff0c2019\u6625\uff09\u6559\u6750\u3002\u9762\u5411\u4e2d\u6587\u8bfb\u8005\u3001\u80fd\u8fd0\u884c\u3001\u53ef\u8ba8\u8bba\u3002 Python 7338 1973   \ud83c\udd95 53 Virgilio Your new Mentor for Data Science E-Learning. Jupyter Notebook 7016 1281   \u2796 54 dlib A toolkit for making real world machine learning and data analysis applications in C++ C++ 6911 2069   \u2b06\ufe0f22 55 labelImg \ud83e\udd18 LabelImg is a graphical image annotation tool and label object bounding boxes in images Python 6430 2258   \ud83c\udd95 56 libfacedetection An open source library for face detection in images. The face detection speed can reach 1500FPS. C++ 6377 1746   \u2796 57 pix2pix Image-to-image translation with conditional adversarial nets Lua 5971 974   \u2b06\ufe0f103 58 Awesome-pytorch-list A comprehensive list of pytorch related content on github,such as different models,implementations,helper libraries,tutorials etc. None 5938 1299   \u2b06\ufe0f9 59 ncnn ncnn is a high-performance neural network inference framework optimized for the mobile platform C++ 5923 1566   \ud83c\udd95 60 horovod Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. C++ 5839 818   \ud83c\udd95 61 stanford-cs-229-machine-learning VIP cheatsheets for Stanford's CS 229 Machine Learning None 7432 1724   \u2b07\ufe0f17 62 MLAlgorithms Minimal and clean examples of machine learning algorithms implementations Python 7356 1238   \u2b06\ufe0f1 63 tensor2tensor Library of deep learning models and datasets designed to make deep learning more accessible and accelerate ML research. Python 7344 1860   \ud83c\udd95 64 d2l-zh \u300a\u52a8\u624b\u5b66\u6df1\u5ea6\u5b66\u4e60\u300b\uff0c\u82f1\u6587\u7248\u5373\u4f2f\u514b\u5229\u6df1\u5ea6\u5b66\u4e60\uff08STAT 157\uff0c2019\u6625\uff09\u6559\u6750\u3002\u9762\u5411\u4e2d\u6587\u8bfb\u8005\u3001\u80fd\u8fd0\u884c\u3001\u53ef\u8ba8\u8bba\u3002 Python 7338 1973   \ud83c\udd95 65 Virgilio Your new Mentor for Data Science E-Learning. Jupyter Notebook 7016 1281   \u2b07\ufe0f12 66 dlib A toolkit for making real world machine learning and data analysis applications in C++ C++ 6911 2069   \u2b07\ufe0f6 67 TensorFlow-Tutorials TensorFlow Tutorials with YouTube Videos Jupyter Notebook 6787 3295   \u2b06\ufe0f17 68 ray A system for parallel and distributed Python that unifies the ML ecosystem. Python 6441 889   \u2b06\ufe0f8 69 labelImg \ud83e\udd18 LabelImg is a graphical image annotation tool and label object bounding boxes in images Python 6430 2258   \ud83c\udd95 70 libfacedetection An open source library for face detection in images. The face detection speed can reach 1500FPS. C++ 6377 1746   \u2b06\ufe0f11 71 deep-learning-with-python-notebooks Jupyter notebooks for the code samples of the book \"Deep Learning with Python\" Jupyter Notebook 6369 2943   \u2b07\ufe0f1 72 conv_arithmetic A technical report on convolution arithmetic in the context of deep learning TeX 6048 1189   \u2b07\ufe0f16 73 pix2pix Image-to-image translation with conditional adversarial nets Lua 5971 974   \u2b06\ufe0f87 74 Awesome-pytorch-list A comprehensive list of pytorch related content on github,such as different models,implementations,helper libraries,tutorials etc. None 5938 1299   \u2b07\ufe0f7 75 ncnn ncnn is a high-performance neural network inference framework optimized for the mobile platform C++ 5923 1566   \ud83c\udd95 76 horovod Distributed training framework for TensorFlow, Keras, PyTorch, and Apache MXNet. C++ 5839 818   \u2b06\ufe0f36 77 allennlp An open-source NLP research library, built on PyTorch. Python 5801 1114   \u2b06\ufe0f2 78 onnx Open Neural Network Exchange PureBasic 5769 847   \u2b06\ufe0f16 79 mit-deep-learning-book-pdf MIT Deep Learning Book in PDF format (complete and parts) by Ian Goodfellow, Yoshua Bengio and Aaron Courville Java 5572 1323   \u2b07\ufe0f17 80 py-faster-rcnn Faster R-CNN (Python implementation) -- see https://github.com/ShaoqingRen/faster_rcnn for the official MATLAB version Python 5548 3471   \u2b07\ufe0f28 81 Swift-AI The Swift machine learning library. Swift 5547 535   \ud83c\udd95 82 DeOldify A Deep Learning based project for colorizing and restoring old images Python 5455 402   \u2b06\ufe0f8 83 ml-agents Unity Machine Learning Agents Toolkit C# 5448 1374   \ud83c\udd95 84 learnopencv Learn OpenCV  : C++ and Python Examples Jupyter Notebook 5441 3449   \u2b06\ufe0f14 85 imgaug Image augmentation for machine learning experiments. Python 5428 1131   \u2b07\ufe0f30 86 BossSensor Hide screen when boss is approaching. Python 5390 1001   \u2b07\ufe0f1 87 fashion-mnist A MNIST-like fashion product database. Benchmark \ud83d\udc49 Python 5362 937   \u2b07\ufe0f30 88 awesome-rnn Recurrent Neural Network - A curated list of resources dedicated to RNN None 5327 1330   \ud83c\udd95 89 SerpentAI Game Agent Framework. Helping you create AIs / Bots to play any game you own! Jupyter Notebook 5097 539   \u2b07\ufe0f25 90 DeepLearningFlappyBird Flappy Bird hack using Deep Reinforcement Learning (Deep Q-learning). Python 5057 1588   \ud83c\udd95 91 CNN Convolutional Neural Network\uff08\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\uff09(NOT FINISHED!!!) Python 43 35   \ud83c\udd95 92 tangshi-rnn None Lua 43 19   \ud83c\udd95 93 deep-learning-nd Udacity Deep learning nanodegree projects Python 42 44   \ud83c\udd95 94 gogh-figure Fast, Lightweight Style Transfer using Deep Learning Python 42 11   \ud83c\udd95 95 AWSGPU_DeepLearning Code to setup AWS GPU instance to run Daniel Nouri's Facial Keypoints competition Python 41 18   \ud83c\udd95 96 deep-learning-cn None None 41 8   \ud83c\udd95 97 CNN-Face-Point-Detection This project is about the utilization of CNN to detect human face point. Trained with the dataset LFW and images from Internet. C++ 41 46   \ud83c\udd95 98 Selfie_Filters_OpenCV This deep learning application can detect Facial Keypoints (15 unique points).  They mark important areas of the face - the eyes, corners of the mouth, the nose, etc. Python 41 10   \ud83c\udd95 99 recnn Repository for the code of \"QCD-Aware Recursive Neural Networks for Jet Physics\" Jupyter Notebook 41 14   \ud83c\udd95 100 sangita A Natural Language Toolkit for Indian Languages Python 41 40   \ud83c\udd95 101 chainer-segnet SegNet implementation & experiments in Chainer Python 41 7   \ud83c\udd95 102 DeepLearningBook-Resources Resource files for \"Deep Learning - From Basics to Practice\" by Andrew Glassner None 40 6   \ud83c\udd95 103 rnnlib A. Grave's RNNLIB made more readable nesC 40 7   \ud83c\udd95 104 LittleConv The C++11 deep learning framework, which is very easy to use C++ 40 16   \ud83c\udd95 105 deep-fashion Proposal a new method to retrieval clothing images Python 40 15   \ud83c\udd95 106 customerml CustomerML is an open source customer science platform leveraging the power of Predictiveworks and fully integrated with Elasticsearch and Shopify. CustomerML starts with proven RFM analysis and combines the results with machine learning thereby providing a deep customer understanding. Scala 40 22   \ud83c\udd95 107 RealismCNN code for predicting and improving visual realism in composite images Matlab 39 14   \ud83c\udd95 108 bayesian-deep-learning-notes A list of notes on Bayesian deep learning papers None 39 9   \ud83c\udd95 109 deeprl Universal library for deep reinforcement learning. Python 39 15   \ud83c\udd95 110 ddx Deep Learning Dashboard JavaScript 39 5   \ud83c\udd95 111 udacity_self_driving_car Projects from Udacity Self Driving Car Nanodegree Jupyter Notebook 39 23   \ud83c\udd95 112 GCNGEMM Optimized half precision gemm assembly kernels on AMD Fiji Perl 39 11   \ud83c\udd95 113 ConvNetSwift Swift port of ConvnetJS. [Work in progress] Swift 39 5   \ud83c\udd95 114 FaceDetect Face detection with convolutional neural networks in TensorFlow. Python 38 15   \ud83c\udd95 115 MachineLearningSamples-DeepLearningforPredictiveMaintenance MachineLearningSamples-DeepLearningforPredictiveMaintenance Jupyter Notebook 38 32   \ud83c\udd95 116 resnet Implementation of Deep Residual Learning / Residual Network for MSR paper http://arxiv.org/abs/1512.03385 Python 38 27   \ud83c\udd95 117 Classify-Real-Time-Desktop Inception model used to classify camera feed on real time. Coded during the Deep Learning Hackathon 2017 San Francisco Python 38 16   \ud83c\udd95 118 transfer-mxnet transfer learning written in mxnet Python 38 8   \ud83c\udd95 119 Deep_Learning_with_Intel This is the code for \"Deep Learning with Intel\" By Siraj Raval on Youtube Python 37 9   \ud83c\udd95 120 DeepANN Theano based deep ANN learning code Python 37 16   \u2b07\ufe0f15 121 DeepLearningProject An in-depth machine learning tutorial introducing readers to a whole machine learning pipeline from scratch. HTML 3986 593   \u2b07\ufe0f33 122 deeplearning-papernotes Summaries and notes on Deep Learning research papers None 3960 850   \u2b07\ufe0f33 123 machine-learning-mindmap A mindmap summarising Machine Learning concepts, from Data Analysis to Deep Learning. None 3960 700   \u2b07\ufe0f26 124 h2o-3 Open Source Fast Scalable Machine Learning Platform For Smarter Applications: Deep Learning, Gradient Boosting & XGBoost, Random Forest, Generalized Linear Modeling (Logistic Regression, Elastic Net), K-Means, PCA, Stacked Ensembles, Automatic Machine Learning (AutoML), etc. Java 3912 1448   \u2b07\ufe0f5 125 tensorpack A Neural Net Training Interface on TensorFlow, with focus on speed + flexibility Python 3909 1214   \u2b07\ufe0f38 126 neon Intel\u00ae Nervana\u2122 reference deep learning framework committed to best performance on all hardware Python 3751 833   \u2b06\ufe0f54 127 sketch-code Keras model to generate HTML code from hand-drawn website mockups. Implements an image captioning architecture to drawn source images. Python 3746 443   \u2b07\ufe0f36 128 DeepLearningTutorials Deep Learning Tutorial notes and code. See the wiki for more info. Python 3724 2075   \ud83c\udd95 129 tensorspace Neural network 3D visualization framework, build interactive and intuitive model in browsers, support pre-trained deep learning models from TensorFlow, Keras, TensorFlow.js JavaScript 3664 297   \u2b07\ufe0f22 130 keras-rl Deep Reinforcement Learning for Keras. Python 3645 920   \u2b07\ufe0f7 131 deep-learning-coursera Deep Learning Specialization by Andrew Ng on Coursera. Jupyter Notebook 3635 2843   \u2b07\ufe0f35 132 DIGITS Deep Learning GPU Training System HTML 3612 1306   \ud83c\udd95 133 netron Visualizer for deep learning and machine learning models JavaScript 3603 382   \u2b07\ufe0f32 134 vrn \ud83d\udc68  Code for \"Large Pose 3D Face Reconstruction from a Single Image via Direct Volumetric CNN Regression\" Shell 3562 562   \u2b07\ufe0f26 135 Realtime_Multi-Person_Pose_Estimation Code repo for realtime multi-person pose estimation in CVPR'17 (Oral) Jupyter Notebook 3549 1063   \ud83c\udd95 136 wav2letter Facebook AI Research Automatic Speech Recognition Toolkit C++ 3496 527   \u2b07\ufe0f33 137 dl-docker An all-in-one Docker image for deep learning. Contains all the popular DL frameworks (TensorFlow, Theano, Torch, Caffe, etc.) Python 3466 782   \u2b07\ufe0f23 138 the-incredible-pytorch The Incredible PyTorch: a curated list of tutorials, papers, projects, communities and more relating to PyTorch. None 3444 654   \ud83c\udd95 139 pytorch-handbook pytorch handbook\u662f\u4e00\u672c\u5f00\u6e90\u7684\u4e66\u7c4d\uff0c\u76ee\u6807\u662f\u5e2e\u52a9\u90a3\u4e9b\u5e0c\u671b\u548c\u4f7f\u7528PyTorch\u8fdb\u884c\u6df1\u5ea6\u5b66\u4e60\u5f00\u53d1\u548c\u7814\u7a76\u7684\u670b\u53cb\u5feb\u901f\u5165\u95e8\uff0c\u5176\u4e2d\u5305\u542b\u7684Pytorch\u6559\u7a0b\u5168\u90e8\u901a\u8fc7\u6d4b\u8bd5\u4fdd\u8bc1\u53ef\u4ee5\u6210\u529f\u8fd0\u884c Jupyter Notebook 3403 730   \u2b07\ufe0f44 140 iOS-10-Sampler Code examples for new APIs of iOS 10. Swift 3360 348   \u2b07\ufe0f30 141 DenseNet Densely Connected Convolutional Networks, In CVPR 2017 (Best Paper Award). Lua 3355 808   \u2b07\ufe0f35 142 iGAN Interactive Image Generation via Generative Adversarial Networks Python 3328 501   \u2b06\ufe0f13 143 MMdnn MMdnn is a set of tools to help users inter-operate among different deep learning frameworks. E.g. model conversion and visualization. Convert models between Caffe, Keras, MXNet, Tensorflow, CNTK, PyTorch Onnx and CoreML. Python 3327 656   \ud83c\udd95 144 photoprism Personal Photo Management powered by Go and Google TensorFlow Go 3310 148   \ud83c\udd95 145 serving A flexible, high-performance serving system for machine learning models C++ 3264 1362   \u2b07\ufe0f43 146 DeepLearnToolbox Matlab/Octave toolbox for deep learning. Includes Deep Belief Nets, Stacked Autoencoders, Convolutional Neural Nets, Convolutional Autoencoders and vanilla Neural Nets. Each method has examples to get you started. Matlab 3255 2118   \ud83c\udd95 147 python-machine-learning-book-2nd-edition The \"Python Machine Learning (2nd edition)\" book code repository and info resource Jupyter Notebook 3239 1479   \ud83c\udd95 148 deep-learning-ocean \ud83d\udce1 All You Need to Know About Deep Learning - A kick-starter Python 3218 449   \u2b07\ufe0f35 149 MachineLearning Basic Machine Learning and Deep Learning Python 3206 2463   \u2b07\ufe0f49 150 skflow Simplified interface for TensorFlow (mimicking Scikit Learn) for Deep Learning Python 3200 467   \u2b07\ufe0f34 151 DeepLearningZeroToAll TensorFlow Basic Tutorial Labs Jupyter Notebook 3168 1925   \ud83c\udd95 152 bert-as-service Mapping a variable-length sentence to a fixed-length vector using BERT model Python 3144 582   \u2b07\ufe0f21 153 mace MACE is a deep learning inference framework optimized for mobile heterogeneous computing platforms. C++ 3141 553   \u2b06\ufe0f18 154 tvm Open deep learning compiler stack for cpu, gpu and specialized accelerators Python 3139 743   \u2b07\ufe0f20 155 pix2pixHD Synthesizing and manipulating 2048x1024 images with conditional GANs Python 3111 578   \ud83c\udd95 156 graph_nets Build Graph Nets in Tensorflow Python 3102 380   \ud83c\udd95 157 stanford-cs-230-deep-learning VIP cheatsheets for Stanford's CS 230 Deep Learning None 3066 593   \ud83c\udd95 158 pytorch_geometric Geometric Deep Learning Extension Library for PyTorch Python 3050 409   \u2b06\ufe0f40 159 faster-rcnn.pytorch A faster pytorch implementation of faster r-cnn Python 3016 918   \u2b07\ufe0f30 160 Practical_RL A course in reinforcement learning in the wild Jupyter Notebook 3013 815   \u2b07\ufe0f20 161 Augmentor Image augmentation library in Python for machine learning. Jupyter Notebook 3001 560   \u2b07\ufe0f3 162 Deep-Learning-21-Examples \u300a21\u4e2a\u9879\u76ee\u73a9\u8f6c\u6df1\u5ea6\u5b66\u4e60\u2014\u2014\u2014\u57fa\u4e8eTensorFlow\u7684\u5b9e\u8df5\u8be6\u89e3\u300b\u914d\u5957\u4ee3\u7801 Python 2980 1233   \u2b07\ufe0f20 163 SSD-Tensorflow Single Shot MultiBox Detector in TensorFlow Jupyter Notebook 2938 1363   \u2b07\ufe0f42 164 deep-learning Repo for the Deep Learning Nanodegree Foundations program. Jupyter Notebook 2936 3568   \ud83c\udd95 165 Deep-Learning-World \ud83d\udce1 Organized Resources for Deep Learning Researchers and Developers Python 2932 286   \u2b07\ufe0f12 166 DeepPavlov An open source library for deep learning end-to-end dialog systems and chatbots. Python 2920 492   \u2b07\ufe0f12 167 Tensorflow-Tutorial Tensorflow tutorial from basic to hard Python 2909 1397   \u2b07\ufe0f52 168 BigDL BigDL: Distributed Deep Learning Library for Apache Spark Scala 2880 709   \ud83c\udd95 169 PySyft A library for encrypted, privacy preserving deep learning Python 2842 634   \u2b07\ufe0f6 170 opencv4nodejs Asynchronous OpenCV 3.x nodejs bindings with JavaScript and TypeScript API, with examples for: Face Detection, Machine Learning, Deep Neural Nets, Hand Gesture Recognition, Object Tracking, Feature Matching, Image Histogram C++ 2803 430   \u2b07\ufe0f59 171 deep-learning-papers Papers about deep learning ordered by task, date. Current state-of-the-art papers are labelled. None 2794 376   \u2b07\ufe0f43 172 Tensorflow-Project-Template A best practice for tensorflow project template architecture. Python 2792 650   \u2b06\ufe0f5 173 OpenNMT-py Open Source Neural Machine Translation in PyTorch Python 2775 1088   \ud83c\udd95 174 PyTorch-Tutorial Build your neural network easy and fast Jupyter Notebook 2773 1244   \u2b07\ufe0f50 175 mlpack mlpack: a scalable C++ machine learning library -- C++ 2746 1002   \u2b07\ufe0f50 176 keras-resources Directory of tutorials and open-source code repositories for working with Keras, the Python deep learning library None 2745 781   \ud83c\udd95 177 Recommenders Recommender Systems Jupyter Notebook 2736 327   \u2b07\ufe0f13 178 machine_learning_examples A collection of machine learning examples and tutorials. Python 2721 2931   \ud83c\udd95 179 hands-on-ml-zh \ud83d\udcd6 [\u8bd1] Sklearn \u4e0e TensorFlow \u673a\u5668\u5b66\u4e60\u5b9e\u7528\u6307\u5357 CSS 2716 1000   \ud83c\udd95 180 spinningup An educational resource to help anyone learn deep reinforcement learning. Python 2707 482   \u2b07\ufe0f63 181 neural-storyteller A recurrent neural network for generating little stories about images Python 2705 484   \ud83c\udd95 182 awesome-object-detection Awesome Object Detection based on handong1587 github: https://handong1587.github.io/deep_learning/2015/10/09/object-detection.html None 2705 832   \u2b07\ufe0f62 183 easy-tensorflow Simple and comprehensive tutorials in TensorFlow Python 2686 263   \u2b07\ufe0f65 184 DeepLearning Deep Learning (Python, C, C++, Java, Scala, Go) Java 2617 1348   \u2b07\ufe0f22 185 face-alignment \ud83d\udd25 2D and 3D Face alignment library build using pytorch Python 2587 583   \u2b07\ufe0f41 186 DeepVideoAnalytics A distributed visual search and visual data analytics platform. Python 2543 609   \ud83c\udd95 187 60_Days_RL_Challenge Learn Deep Reinforcement Learning in Depth in 60 days Jupyter Notebook 2502 237   \u2b07\ufe0f51 188 deep-learning-keras-tensorflow Introduction to Deep Neural Networks with Keras and Tensorflow Jupyter Notebook 2495 1116   \u2b07\ufe0f42 189 gorgonia Gorgonia is a library that helps facilitate machine learning in Go. Go 2492 234   \u2b07\ufe0f52 190 deep-learning-book Repository for \"Introduction to Artificial Neural Networks and Deep Learning: A Practical Guide with Applications in Python\" Jupyter Notebook 2491 677   \ud83c\udd95 191 adanet Fast and flexible AutoML with learning guarantees. Python 2485 345   \u2b07\ufe0f56 192 char-rnn-tensorflow Multi-layer Recurrent Neural Networks (LSTM, RNN) for character-level language models in Python using Tensorflow Python 2440 928   \u2b07\ufe0f16 193 ml5-library Friendly machine learning for the web! \ud83e\udd16 JavaScript 2428 186   \u2b07\ufe0f34 194 tensorflow_poems \u4e2d\u6587\u53e4\u8bd7\u81ea\u52a8\u4f5c\u8bd7\u673a\u5668\u4eba\uff0c\u5c4c\u70b8\u5929\uff0c\u57fa\u4e8etensorflow1.10 api\uff0c\u6b63\u5728\u79ef\u6781\u7ef4\u62a4\u5347\u7ea7\u4e2d\uff0c\u5febstar\uff0c\u4fdd\u6301\u66f4\u65b0\uff01 Python 2420 690   \u2b07\ufe0f61 195 Automatic_Speech_Recognition End-to-end Automatic Speech Recognition for Madarian and English in Tensorflow Python 2407 468   \u2b07\ufe0f48 196 DeepQA My tensorflow implementation of \"A neural conversational model\", a Deep learning based chatbot Python 2389 1052   \u2b07\ufe0f3 197 handong1587.github.io None CSS 2389 880   \u2796 198 carla Open-source simulator for autonomous driving research. C++ 2385 601   \u2b07\ufe0f55 199 fast-rcnn Fast R-CNN Python 2383 1319   \ud83c\udd95 200 Grokking-Deep-Learning this repository accompanies my forthcoming book \"Grokking Deep Learning\" Jupyter Notebook 2371 494    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["http://arxiv.org/abs/1512.03385"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc3f"}, "repo_url": "https://github.com/alonglyn/veg_text_classify", "repo_name": "veg_text_classify", "repo_full_name": "alonglyn/veg_text_classify", "repo_owner": "alonglyn", "repo_desc": "using lstm nerual network to predict the scores of the opinion of sentences related to vegetables.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T10:12:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T10:08:14Z", "homepage": null, "size": 21762, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189391312, "is_fork": false, "readme_text": "2019-5-29\u82b1\u4e86\u4e00\u4e2a\u665a\u4e0a\u91cd\u65b0\u5b9e\u73b0\u4e86\u4e00\u904d\u4ee5\u524d\u7684\u4efb\u52a1 \u4efb\u52a1\u76ee\u7684 \u4e3b\u8981\u76ee\u7684\u662f\u4f7f\u7528\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u6587\u672c\u7684\u60c5\u611f\u503e\u5411\uff0c\u8fd9\u91cc\u662f\u9884\u6d4b\u852c\u83dc\u62a5\u9053\u9488\u5bf9\u852c\u83dc\u4ef7\u683c\u8d70\u52bf\u7684\u5224\u65ad\u3002 \u8ba4\u4e3a\u852c\u83dc\u4ef7\u683c\u8d70\u9ad8\uff0c \u5219\u6807\u8bb0\u4e3a\u6b63\u5411\u6587\u672c\uff0c \u53cd\u4e4b\u5219\u662f\u8d1f\u5411\u3002 \u4f7f\u7528\u7684\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u662f\u6700\u57fa\u672c\u7684bi-lstm\u3002 \u4e3b\u8981\u4efb\u52a1\u662f\u9884\u5904\u7406\u6587\u672c \u6700\u7ec8\u7ed3\u679c \u53ef\u4ee5\u5bf9\u4e00\u7cfb\u5217\u76f8\u5173\u852c\u83dc\u4ef7\u683c\u6587\u672c\u505a\u5173\u4e8e\u4ef7\u683c\u8d70\u52bf\u7684\u9884\u6d4b \u9700\u8981\u7684\u5de5\u5177 \u5206\u8bcd\u5de5\u5177:jieba word2vec:gensim \u6df1\u5ea6\u5b66\u4e60\u6846\u67b6:keras \u5176\u4ed6\uff1apython3\u7684\u57fa\u7840\u5de5\u5177\u5305 \u53c2\u8003\u8d44\u6599\u641c\u7d22\u5173\u952e\u8bcd keras \u6587\u672c\u60c5\u611f\u5206\u7c7b \u6df1\u5ea6\u5b66\u4e60\u60c5\u611f\u5206\u6790 \u6587\u672c\u4e3b\u9898\u5206\u7c7b \u6587\u672c\u591a\u5206\u7c7b\u4efb\u52a1\u3001\u795e\u7ecf\u7f51\u7edc \u76ee\u5f55\u6811\u4ee5\u53ca\u5173\u952e\u7684\u6570\u636e \u251c\u2500\u2500 article \u2502   \u251c\u2500\u2500 all.xlsx \u2502   \u2514\u2500\u2500 sample.csv //\u5f85\u9884\u6d4b\u6587\u7ae0\u91c7\u6837 \u251c\u2500\u2500 corpus \u2502   \u251c\u2500\u2500 corpus.csv             //\u8bed\u6599\u5e93\u3001\u6709\u6807\u6ce8\u8bad\u7ec3\u96c6 \u2502   \u251c\u2500\u2500 stopwords.txt           //\u505c\u7528\u8bcd \u2502   \u251c\u2500\u2500 word2vec             //word2vec\u7684\u8bed\u6599\u5e93\uff0c \u53ef\u4ee5\u5c06all.xlsx\u7684\u6570\u636e\u4e00\u5e76\u62ff\u6765\u8bad\u7ec3 \u2502   \u251c\u2500\u2500 \u8d1f\u5411              //\u62bd\u53d6\u7684\u6b63\u5411\u6587\u672c \u2502   \u2514\u2500\u2500 \u6b63\u5411              //\u62bd\u53d6\u7684\u8d1f\u5411\u6587\u672c \u251c\u2500\u2500 data_help.py \u251c\u2500\u2500 example.csv \u251c\u2500\u2500 model \u2502   \u251c\u2500\u2500 checkpoint \u2502   \u251c\u2500\u2500 checkpoints.data-00000-of-00001 \u2502   \u251c\u2500\u2500 checkpoints.index \u2502   \u251c\u2500\u2500 veg_lstm.data-00000-of-00001    //\u9884\u6d4b\u6a21\u578b \u2502   \u251c\u2500\u2500 veg_lstm.index \u2502   \u2514\u2500\u2500 vocab               //\u8bcd\u8868\u3001\u7b2c\u4e00\u6b21\u4f7f\u7528vegdb\u7684\u65f6\u5019\u9700\u8981\u63d0\u4f9bcorpus\u6784\u5efa\u3002 \u251c\u2500\u2500 predict.py              //\u52a0\u8f7d\u6a21\u578b\u8fdb\u884c\u9884\u6d4b\uff0c \u793a\u4f8b\u7ed3\u679c\u5b58\u653e\u5728example.csv\u91cc \u251c\u2500\u2500 README.md \u251c\u2500\u2500 train_lstm.py             //lstm\u8bad\u7ec3\u6a21\u578b\uff0c \u8bad\u7ec3\u6570\u636e\u8def\u5f84\u3001\u53c2\u6570\u5728\u4ee3\u7801\u91cc\u9762\u6539 \u2514\u2500\u2500 train_wordvec_model.py                               //\u9884\u8bad\u7ec3\u8bcd\u5411\u91cf\uff08\u8fd9\u91cc\u6ca1\u6709\u4f7f\u7528\uff09 \u4e3b\u8981\u7684\u4ee3\u7801   data_help.py \u63d0\u4f9bvegdb\u7c7b\uff0c \u53ef\u4ee5\u5efa\u7acb\u4e00\u4e2a\u8bed\u6599\u5e93\u4ee5\u53ca\u76f8\u5e94\u7684\u8bcd\u8868 \u63d0\u4f9b\u63a5\u53e3\u4ecedataframe\u8868\u4e2d\u62bd\u53d6\u8bad\u7ec3\u6570\u636e\u3001\u9884\u6d4b\u6570\u636e \u4e5f\u5c31\u662f\u5c06\u4e2d\u6587\u6587\u672c\u8f6c\u6362\u6210int\u6570\u7ec4   train_word2vec.py \u53ef\u4ee5\u5f97\u5230word2vec\u9884\u8bad\u7ec3\u7684\u8bcd\u5411\u91cf   train_lstm.py  \u5207\u5206\u8bad\u7ec3\u96c6\u3001\u6d4b\u8bd5\u96c6 \u53e5\u5b50\u8fdb\u884c\u957f\u5ea6\u4e3a500\u7684\u96f6\u586b\u5145 \u6784\u5efalstm\u6a21\u578b\uff0c \u4f7f\u7528\u4e8c\u5206\u7c7b\u8f93\u51fa\u5c42\u3002\u5176\u4ed6\u53c2\u6570\u6ca1\u6709\u8fdb\u884c\u8c03\u8282\u3002    predict.py  \u63d0\u4f9b\u9884\u6d4b\u529f\u80fd \u9884\u6d4b\u7684\u6570\u636e\u6807\u7b7e\u4e8c\u5143\u7ec4\uff0c \u4fdd\u5b58\u518dexample.csv\u91cc    ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc40"}, "repo_url": "https://github.com/avish1993/PosIX-GAN", "repo_name": "PosIX-GAN", "repo_full_name": "avish1993/PosIX-GAN", "repo_owner": "avish1993", "repo_desc": "The official repository for \"PosIX-GAN: Generating multiple poses using GAN for Pose-Invariant Face Recognition\"; GMDL (ECCVW) 2018", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T21:21:20Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-29T13:30:50Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189231549, "is_fork": false, "readme_text": "PosIX-GAN The official repository for \"PosIX-GAN: Generating multiple poses using GAN for Pose-Invariant Face Recognition\"; GMDL (ECCVW) 2018 The code has been written for python 2.7.  Requirements: tensorflow-gpu==1.8.0  keras==2.1.6  graphviz==0.10.1  matplotlib==2.2.4  Pillow==6.0.0  pydot==1.4.1  scipy==1.2.1  Also, make sure to install python-tk: sudo apt-get install python-tk Please note that the data preparation task is vital and the following steps must be followed:   Normalise the image data and save into npy files   Generate the corresponding labels in one-hot vector fashion.   Make sure the separate set of the image data has been prepared to evaluate the PMSE loss. There should be nine separate numpy files (containing images) for each of the nine generator outputs.   Make sure all packages have been installed, of the correct version.   Make changes to the data loader and train functions accordingly.   After above mentioned changes have been made, to start training, run: python main.py If you are using this code for your work, please cite:   @inproceedings{bhattacharjee2018posix, title={PosIX-GAN: Generating multiple poses using GAN for Pose-Invariant Face Recognition}, author={Bhattacharjee, Avishek and Banerjee, Samik and Das, Sukhendu}, booktitle = {The European Conference on Computer Vision (ECCV) Workshops}, month = {September}, year = {2018} } ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc41"}, "repo_url": "https://github.com/stacia/Neural-Network-Manual-Calculation", "repo_name": "Neural-Network-Manual-Calculation", "repo_full_name": "stacia/Neural-Network-Manual-Calculation", "repo_owner": "stacia", "repo_desc": "Try simulate Neural network manually to improve my experience", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T13:41:55Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T10:58:03Z", "homepage": "", "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189207937, "is_fork": false, "readme_text": "Neural Network BackPropagation Manual Beberapa hari yang lalu saya mulai membaca-baca tentang Python. Sebuah bahasa pemrograman yang dulu pernah dipelajari namun vakum selama 2 tahun tidak melanjutkan pembelajarannya. Disini saya menemukan beberapa hal yang menarik tentang Python yaitu kemampuannya dalam Machine Learning yang sangat bagus. Percobaan pertama menghantarkan saya ke tulisan di situs Medium berikut. Part 1 'Artificial Neural Network': https://medium.com/@samuelsena/pengenalan-deep-learning-8fbb7d8028ac Disini dipaparkan secara rinci tentang salah satu metode Machine learning yaitu Neural Network. Percobaan ini merupakan part ketiga dari tutorial ini. Dan part ketiga ini cukup menarik perhatian untuk dipahami. Part 3 'Back Propagation Algorithm': https://medium.com/@samuelsena/pengenalan-deep-learning-part-3-backpropagation-algorithm-720be9a5fbb8 Sebelumnya saya telah melakukan percobaan dengan part ke 4 nya. Menggunakan tensorflow dan Keras. Saya yang masih awam dengan hal ini tertarik ketika melihat perbedaan antara satu program namun hasilnya berbeda. Sehingga saya memutuskan untuk mengolah kembali part sebelumnya menjadi tulisan koding saya sendiri dengan harapan bisa lebih memahami inti dari Neural Network. Tools yang digunakan:  Python 2.7.13 64 bit Visual Studio Code Numpy 1.16.4 Random Math  Silahkan gunakan file ini untuk kebutuhan pembelajaran anda. Semoga bermanfaat. Kritik dan saran sangat saya terima untuk meningkatkan pengetahuan saya dalam materi ini. Regards. M. Razif Rizqullah ", "has_readme": true, "readme_language": "Indonesian", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc42"}, "repo_url": "https://github.com/theredonebastard/D", "repo_name": "D", "repo_full_name": "theredonebastard/D", "repo_owner": "theredonebastard", "repo_desc": "Ryzh staralsya", "description_language": "Norwegian", "repo_ext_links": null, "repo_last_mod": "2019-05-29T14:35:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T13:35:53Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189232419, "is_fork": false, "readme_text": "Development of Software Library for Financial Time Series Forecasting Ryzh staralsya Motivation Chtoby Ryzha hvalily Libraries numpy pandas keras scipy newspaper3k BeautifulSoup nltk sklearn gensim pyLDAvis Features Usage Example btcData = BitcoinData() btcData.load(path='../input/ryzhbtc', filename='Bitcoin Historical Data - Investing.com.csv') btcData.prepare() btcData.resample(period='d') btcData.writeToFile(path='../working', filename='btcDataresample=d.csv') btcPreprocessor = BitcoinPreprocessor() btcPreprocessor.readFromFile(path='../working', filename='btcDataresample=d.csv') btcPreprocessor.calculateDeltas(date_from = '2017-01-01', date_to='2018-12-31', news_period=3, btc_period=2) btcPreprocessor.writeToFile(path='../working', filename='btcPreproc20180101-20180301p=2.csv') newsData = NewsData() newsData.loadFromFile(path='../input/bitcointicker-articles', filename='bitcointicker.csv') newsData.loadFromWeb(linksNumber=1000) newsData.prepare() newsData.writeToFile(path='../working', filename='bitcointickerNewsPrepared10000.csv') newsPreproc = NewsPreprocessor(data = []) newsPreproc.readFromFile(path='../working', filename='bitcointickerNewsPrepared10000.csv') newsPreproc.combine(date_from='2019-04-29', date_to='2019-05-13', period=3) vec = newsPreproc.getKeywords() classifier = NewsBitcoinClassifier(date_from='2017-03-01', date_to='2019-02-14', news_window=6, btc_window=1) classifier.getBTCData() classifier.getNewsData(prepare=False, file=False, path_to='../input/ryzh-prepared-articles/bitcointickernewsprepared10000', filename_to='bitcointickerNewsPrepared10000.csv') classifier.ensemble(classifiers_names=['RandomForest', 'GradientBoosting', 'SVC', 'SGD', 'LogisticRegression']) lstm = LSTMClassifier() lstm.setData(btcData.data) lstm.scale() lstm.split() lstm.classify() ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc43"}, "repo_url": "https://github.com/ffxz/BinWavenet", "repo_name": "BinWavenet", "repo_full_name": "ffxz/BinWavenet", "repo_owner": "ffxz", "repo_desc": "this project is coming from https://github.com/drethage/speech-denoising-wavenet, and i change some details, like speaker information and stack layer and so on. I also rewrite it with tensorflow.keras, it make me more comfortable.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T08:03:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T15:55:32Z", "homepage": null, "size": 218, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189256559, "is_fork": false, "readme_text": "this project is coming from https://github.com/drethage/speech-denoising-wavenet, and i change some details, like speaker information and stack layer and so on. I also rewrite it with tensorflow.keras, it makes me more comfortable. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc44"}, "repo_url": "https://github.com/ricardwl/BCNN", "repo_name": "BCNN", "repo_full_name": "ricardwl/BCNN", "repo_owner": "ricardwl", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T06:09:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T17:20:33Z", "homepage": null, "size": 17, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189632358, "is_fork": false, "readme_text": "BCNN This project makes a simple BCNNN example. In the \"BCNN_VGG16.py\", I construct the BCNN based on VGG16 on keras. Meanwhile,  I add a \"dimension reduction layer\" to the BCNN, which can be seen in \"BCNN_Advanced.py\". By using \"convolutional dimision reduction\", the parameters of BCNN become less than before, so I call the new model \"A-BCNN\". train or test If you want to train the model, you can use the command like this:python BCNN_VGG16.py --train If you want to test the model, you can use the command like this:  python BCNN_VGG16.py --test There are three command parameters: --classes --num --lr  --classes: the num of classes. you can ignore it ,this para is set for my own dataset. You can change this in the main func. --num: the num of freezed layers. If you set  --num -1, then layes[0:-1] will be set notrainable. --lr: learning rate  others \u968f\u4fbf\u505a\u7684\uff0c\u6709\u65f6\u95f4\u518d\u6539\u5427 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc45"}, "repo_url": "https://github.com/YuheiNakasaka/yukanya", "repo_name": "yukanya", "repo_full_name": "YuheiNakasaka/yukanya", "repo_owner": "YuheiNakasaka", "repo_desc": "Juice=Juice\u306e\u30e1\u30f3\u30d0\u30fc\u3092\u753b\u50cf\u304b\u3089\u5224\u5b9a\u3059\u308b\u5206\u985e\u5668", "description_language": "Japanese", "repo_ext_links": null, "repo_last_mod": "2019-06-02T12:39:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T08:50:33Z", "homepage": "https://yuheinakasaka.github.io/yukanya/", "size": 5887, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189561281, "is_fork": false, "readme_text": "yukanya Juice=Juice\u306e\u30e1\u30f3\u30d0\u30fc\u3092\u5224\u5b9a\u3059\u308b\u5206\u985e\u5668 Install pip install -r requirements.txt  mkdir haarcascades && cp <haarcascades\u306ehaarcascade_frontalface_default.xml\u306e\u30d1\u30b9> ./haarcascades  Step 1) \u753b\u50cf\u3092\u53d6\u5f97 python scraping.py  2) \u9854\u8a8d\u8b58\u3092\u3057\u3066\u9854\u753b\u50cf\u30c7\u30fc\u30bf\u3092\u4f5c\u6210 python facecliping.py  3) \u5b66\u7fd2\u30c7\u30fc\u30bf\u3092\u6c34\u5897\u3057 \u4e0b\u8a18\u3092\u751f\u6210\u3059\u308b  \u901a\u5e38\u753b\u50cf/\u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u3092\u4e0a\u3052\u305f\u753b\u50cf/\u30b3\u30f3\u30c8\u30e9\u30b9\u30c8\u3092\u4e0b\u3052\u305f\u753b\u50cf/\u5de6\u53f3\u53cd\u8ee2\u3057\u305f\u753b\u50cf\u306e  \u56de\u8ee2\u753b\u50cf \u30ac\u30a6\u30b7\u30a2\u30f3 \u95be\u5024    python increase_data.py  4) \u5b66\u7fd2\u3059\u308b python train.py  5) \u4e88\u6e2c\u3059\u308b python predict.py <\u9069\u5f53\u306b\u62fe\u3063\u3066\u304d\u305f\u30e1\u30f3\u30d0\u30fc\u306e\u753b\u50cf\u306e\u30d1\u30b9>  Tensorflow.js\u3067\u52d5\u304b\u3059 \u65e2\u5b58\u306e\u30e2\u30c7\u30eb\u3092web\u7528\u306b\u5909\u63db\u3059\u308b tfjs-converter\u306eREADME\u306b\u5247\u3063\u3066\u30a4\u30f3\u30b9\u30c8\u30fc\u30eb\u3059\u308b tensorflowjs_converter --input_format keras YukanyaModel_google_ameba.h5 ./  ", "has_readme": true, "readme_language": "Japanese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc46"}, "repo_url": "https://github.com/aguirrediego/weight-initialization-relu-and-output-layers", "repo_name": "weight-initialization-relu-and-output-layers", "repo_full_name": "aguirrediego/weight-initialization-relu-and-output-layers", "repo_owner": "aguirrediego", "repo_desc": "Paper Title: Improving Weight Initialization of ReLU and Output Layers", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T17:14:05Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T17:07:13Z", "homepage": null, "size": 50, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189452433, "is_fork": false, "readme_text": "Improving Weight Initialization of ReLU and Output Layers Paper Title: Improving Weight Initialization of ReLU and Output Layers We introduce a data-dependent weight initialization scheme for ReLU and output layers commonly found in modern neural network architectures. An initial feedforward pass through the network is performed using an initialization set (a subset of the training data set). Using statistics obtained from this pass, we initialize the weights of the network, so the following properties are met: 1) weight matrices are orthogonal; 2) ReLU layers produce a predetermined fraction of non-zero activations; 3) the outputs produced by internal layers have a predetermined variance; 4) weights in the last layer are chosen to minimize the squared error in the initialization set. We evaluate our method on popular architectures (VGG16, VGG19, and InceptionV3) and faster convergence rates are achieved on the ImageNet data set when compared to state-of-the-art initialization techniques (LSUV, He, and Glorot). In this repository, you will find our initialization code (Keras). If you would like to add our initialization technique to your project, see 'main/simple_example.py' to learn how to do so. It's a fairly simple process. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc47"}, "repo_url": "https://github.com/rickyjames1750/Self-Driving-Car-Simulation", "repo_name": "Self-Driving-Car-Simulation", "repo_full_name": "rickyjames1750/Self-Driving-Car-Simulation", "repo_owner": "rickyjames1750", "repo_desc": "Keras is used to train a convolutional neural network on images from the car's cameras as well as steering angles from human driving. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T04:39:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T03:50:54Z", "homepage": null, "size": 4863, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189341802, "is_fork": false, "readme_text": "Self-Driving-Car-Simulation This code is a work in progress Keras is used to train a convolutional neural network on images from the car's cameras as well as steering angles from human driving. Overview This project is based upon Udacity's self driving car simulator as a testbed for training an autonomous car. Dependencies All the necessary dependencies can be installed with the following commands below. Anaconda is needed to use the environment settings. [//]: # Use Tensorflow without GPU   conda env create -f environments.yml [//]: # Use TensorFlow with GPU conda env create -f environment-gpu.yml  Usage Run the pretrained model Fire up the Udacity self-driving simulator, choose a map and press the Autonomous Mode button. Then, run the model as follows: python drive.py model.h5  Train the model The data folder is needed which contains the training images python model.py  This will generate a file model-.h5 whenever the performance in the epoch is better than the previous best. For example, the first epoch will generate a file called model-000.h5. Credits Udacity Self-Driving Car Simulator: https://github.com/udacity/self-driving-car-sim ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/rickyjames1750/Self-Driving-Car-Simulation/blob/b8976bf1617d8f14e71a596732da74ca39fa29a9/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc48"}, "repo_url": "https://github.com/Kkalais/Music_Genre_Classification", "repo_name": "Music_Genre_Classification", "repo_full_name": "Kkalais/Music_Genre_Classification", "repo_owner": "Kkalais", "repo_desc": "Machine Learning Algorithms for Music Genre Classification", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T05:45:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T13:25:16Z", "homepage": "", "size": 712, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 189230624, "is_fork": false, "readme_text": "Music_Genre_Classification Machine Learning Algorithms for Music Genre Classification Dataset Experiments were carried out on a dataset for music analysis called FMA (Free Music Archive) by Swiss Data Science Center: FMA_dataset. Prerequisites Install the libraries below used by the project by entering in console the following command: pip3 install pandas matplotlib keras scikit-learn numpy more-tertools seaborn Clone the repository locally by entering in console the following command: git clone https://github.com/Kkalais/Music_Genre_Classification.git Run We are using Gradient Boosting, Support-Vectors Machine, Random Forest, and Multilayer Perceptron Neural Network to classify the music samples into 16 different music genres. In order to run the code using the above-mentioned algorithms just enter in console the following commands : python3 main.py Gradient_Boosting python3 main.py svm python3 main.py random_forest python3 main.py mlp respectively. There is also a mode that runs all four algorithms consecutively, and produces a bar plot to compare the algorithms' results. Please enter in console: python3 main.py comparative Authors  Konstantinos Kalais, Developer  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc49"}, "repo_url": "https://github.com/yamady0711/4.-Behavioral-Cloning", "repo_name": "4.-Behavioral-Cloning", "repo_full_name": "yamady0711/4.-Behavioral-Cloning", "repo_owner": "yamady0711", "repo_desc": "C:\\Users\\yamad\\Udacity\\Self-Driving Car Engineer Nanodegree\\Downloaded Projects\\1. Computer Vision, Deep Learning, and Sensor Fusion", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T16:46:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T16:45:44Z", "homepage": null, "size": 37917, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189449443, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/yamady0711/4.-Behavioral-Cloning/blob/88537850dfef19cee0462dd81b53489d39df68ee/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc4a"}, "repo_url": "https://github.com/SivaKaushik/Behavioural-Cloning", "repo_name": "Behavioural-Cloning", "repo_full_name": "SivaKaushik/Behavioural-Cloning", "repo_owner": "SivaKaushik", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T02:25:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T00:34:30Z", "homepage": null, "size": 23975, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189318477, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/SivaKaushik/Behavioural-Cloning/blob/5ad0c495636dff215e1bda0e68677d4f4f376652/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc4b"}, "repo_url": "https://github.com/cblandin/CarND-Behavioral-Cloning", "repo_name": "CarND-Behavioral-Cloning", "repo_full_name": "cblandin/CarND-Behavioral-Cloning", "repo_owner": "cblandin", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T15:10:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T15:09:11Z", "homepage": null, "size": 28, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189614368, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc4c"}, "repo_url": "https://github.com/bhadreshpsavani/KPIT_Udacity_Selfdriving_Car_NenoDegree_Project4", "repo_name": "KPIT_Udacity_Selfdriving_Car_NenoDegree_Project4", "repo_full_name": "bhadreshpsavani/KPIT_Udacity_Selfdriving_Car_NenoDegree_Project4", "repo_owner": "bhadreshpsavani", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T17:28:22Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T08:09:14Z", "homepage": null, "size": 43378, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189180266, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/bhadreshpsavani/KPIT_Udacity_Selfdriving_Car_NenoDegree_Project4/blob/ab68656d1e11f126b22de934102074f85c54642d/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc4d"}, "repo_url": "https://github.com/thomasfurland/project-maverick", "repo_name": "project-maverick", "repo_full_name": "thomasfurland/project-maverick", "repo_owner": "thomasfurland", "repo_desc": "A partner project with @kurtgalvin to explore deep learning and the possibilities of forecasting the Foreign Exchange (FOREX) market", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T03:34:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T02:54:27Z", "homepage": null, "size": 18, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189334815, "is_fork": false, "readme_text": "project-maverick A partner project with @kurtgalvin to explore deep learning and the possibilities of forecasting the Foreign Exchange (FOREX) market. Apologies in advance for lack of documentation. This was not documented at the time of creation and it is quite hard to go back and add relevant comments after the fact. Installation Requires Pandas, Numpy, MatplotLib, Keras, Tensorflow for deep learning models and predictions. Oanda V20 REST API to communicate with oanda brokerage. And Twilio API for live hourly updates to cellphone. Usage python money_machine.py  Was run continuously throughout trade week and closed during weekends to retrain models. Unfortunately not able to do test runs and set up demo for this beauty as my memory on this project is clouded and there are various accounts needed to be created in order for all components to work together. Maverick.py used in order to create LSTM models trained on a set of X-train and y-train data derived from existing candle data provided by Oanda V20 API. Fabricator.py used to combine past candle data together with a prediction made by a model in a chronologically accurate fashion and saves the resulting output as a .txt file. Simulator.py \"plays\" the outputted txt file from fabricator and outputs to console what the resulting wins, losses, pip wins, pip losses, trade count, etc. for the given currency pair in the given timeframe. Fabricator and Simulator were basically used as our backtesting unit. Refinery.py leftout as it contains secret training techniques used to occasionally output a 54% accurate LSTM model. Project Status This project will likely stay as just an experiment. Deep learning practice and experience will be further developed and improved upon in other projects in the future. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc4e"}, "repo_url": "https://github.com/StephanKol/Semantic.KOM", "repo_name": "Semantic.KOM", "repo_full_name": "StephanKol/Semantic.KOM", "repo_owner": "StephanKol", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T09:22:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T09:13:43Z", "homepage": null, "size": 7913, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189191373, "is_fork": false, "readme_text": "Semantic.KOM Python 3 package for relation extraction and classification. pip install . to install the package Requirements Python 3 packages  numpy pandas nltk textblob sklearn keras tqdm  For embedding creation one of the following are required  fasttext (https://github.com/facebookresearch/fastText or https://github.com/salestock/fastText.py on Windows) GloVe binaries (https://github.com/stanfordnlp/GloVe)  Examples Usage examples can be found in the examples folder API Overview preprocessing.py download_preprocessing_prerequisites() text_blob_from_file(file_path) remove_stop_words(text_blob) link_noun_phrases(text_blob)  embeddings.py class WordEmbeddings     load()     embedding_for(word)     words     as_dict()     as_key_values()  class GloVeEmbeddings(path) # For creating GloVe vectors     train(self, corpus_path, epochs=30, embedding_size=300,           context_size=15, min_occurrences=5, glove_path=\".\",           threads=8, keep_csv=False)  class FastTextEmbeddings(path) # For FastText models     train(self, input_file, **kwargs)  class DataFrameEmbeddings(path) # For loading csv files  def create_relation_dataset(embeddings, out_relations_path, out_labels_path,                             relation_paths, out_false_relations_path=None, max_per_class=None,                             unknown_word=None)  relationextraction.py extract_concept_net(file_path, out_path, chunk_size=1000) extract_yago(file_path, out_path, chunk_size=1000) extract_relations_hearst(corpus_path, stop_words_path)  classification.py class RelationClassifier     new(self, input_dim, relation_count, one_hot=False, filters=32, max_filters=128,         subtract_embeddings=False, dropout=False, learn_rate=0.001, optimizer=\"rmsprop\",         kernel_size=3, lr_decay=0)     save(self, path)     load(self, path)     train(self, relations, labels, batch_size=256, validation_split=0.1, epochs=10,           val_data=None, verbose=1)     predict(self, relations)  visualization.py show_embeddings_tsne(embeddings, word_count=1000, size=(100, 100), save_path=None,                      clusters=None, **tsne_args)  clustering.py class EmbeddingClusterer     cluster(self, embeddings, min_clusters=5, max_clusters=100)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc4f"}, "repo_url": "https://github.com/avee1998/Emojinator", "repo_name": "Emojinator", "repo_full_name": "avee1998/Emojinator", "repo_owner": "avee1998", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T19:40:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T19:08:00Z", "homepage": null, "size": 12220, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189645339, "is_fork": false, "readme_text": "Emojinator #Taught very well at Eckovation Machine Learning Course (https://www.eckovation.com/course/machine-learning-value-package ), this code helps you detect and predict emojis from gestures you make in webcam using Convnets.  The techniques used are Deep Neural networks for prediction and OpenCV for pre-processing of images. For pre-processing we read the images in grayscale format for 2-D images rather than 3-D (if we read in RGB). We trained the model on images after resizing it to 50x50x1. The Basic Architecture of Project:    Conv -> MaxPool -> Conv -> MaxPool -> Flatten -> Dense -> Dense ->O/P     Due to some problems with my system i have used Theano Backend and Keras for the implementation. Division of generated dataset:    Train(12000 images)      Test  (1199 images)       Some add-ons:    One can add more layers.     More epochs(4 used) can be used.     Regularisation param(Dropout) can be tweaked to avoid overfitting.     More images can be generated for enhancement of accuracy.     Procedure :    First, you have to create a gesture database. For that, run generate_dataset.py. Enter the gesture name and gesture for that emoji.     Repeat this for all the features you want.      Run gesture_to_csv.py for converting the images to a CSV file.      If you want to train the model, run train_model_emoji.py      Finally, run Predict_emoj.py for testing your model via webcam.       Case:  This project can be useful for getting insight to CNN based Deep Neural networks, also for a decent intuition to Opencv for image processing and optimise the task of training and generating the dataset. Given the rise of digital communication via text, emoji have become key to communicate emotions. In digital communication, emoji serve the purpose of translating emotions to express facial expressions.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/avee1998/Emojinator/blob/4034ac5144b2fd77228e831852f45e85f9ec340c/emojinator.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc50"}, "repo_url": "https://github.com/GuohaoMa/Text-Analytics", "repo_name": "Text-Analytics", "repo_full_name": "GuohaoMa/Text-Analytics", "repo_owner": "GuohaoMa", "repo_desc": "NLP", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T20:56:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T00:51:22Z", "homepage": null, "size": 79036, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189320079, "is_fork": false, "readme_text": "Text-Analytics NLP Student Name:  Guohao Ma Student Number: 20676560                                                      MSCI 641   All assignments will be done using the corpus of Amazon reviews available for download at: https://github.com/fuzhenxin/textstyletransferdata/tree/master/sentiment This dataset contains two classes of consumer product reviews: positive and negative. You must use python 3 for your assignments. It is important that you do all assignments, as each subsequent assignment builds upon the work you have done in all previous assignments. Create a private GitHub repository for all your assignments and share it only with the TA. Upload each of your assignments into a separate folder in your repository by the due date. If an assignment requires a report, place the report in the README file in the main folder for each Assignment. By the due date of Assignment 0, submit the URL of your Github account to \u201cAssignments\u201d dropbox on Learn. Additional instructions on how to upload code for automatic verification will be posted later. All assignments are due by 11:59pm EST on the due day. You may not edit/update your assignment files on Github after the due date. Assignment 0 (no grade). Due date: May 23 Create a development environment for your subsequent assignments:  Create Python 3 virtual environment on your local machine Install the following libraries: keras, NumPy, SciPy and gensim  Assignment 1 (code only, 3%) Due date: May 30 Write a python script to perform the following data preparation activities:  Tokenize the corpus Remove the following special characters: !\"#$%&()*+/:;<=>@[\\]^`{|}~\\t\\n Create two versions of your dataset: (1) with stopwords and (2) without stopwords. Stopword lists are available online. Randomly split your data into training (80%), validation (10%) and test (10%) sets.  Assignment 2 (code + short report, 7%). Due date: June 6 Write a python script using SciPy library to perform the following:  Train Multinomial Na\u00efve Bayes (MNB) classifier to classify the documents in the Amazon corpus into positive and negative classes. Conduct experiments with the following conditions and report classification accuracy in the following table:  For this assignment, you must use your training/validation/test data splits from Assignment 1. Train your models on the training set. You may only tune your models on your validation set. Once the development is complete, run your classifier on your test set. 2. Answer the following two questions: a. Which condition performed better: with or without stopwords? Write a brief paragraph (5-6 sentences) discussing why you think there is a difference in performance. b. Which condition performed better: unigrams, bigrams or unigrams+bigrams? Briefly (in 5-6 sentences) discuss why you think there is a difference? Assignment 3 (code + short report, 6%). Due date: June 13  Write a python script using genism library to train a Word2Vec model on the Amazon corpus. Use genism library to get the most similar words to a given word. Find 20 most similar words to \u201cgood\u201d and \u201cbad\u201d. Are the words most similar to \u201cgood\u201d positive, and words most similar to \u201cbad\u201d negative? Why this is or isn\u2019t the case? Explain your intuition briefly (in 5-6 sentences).  Assignment 4 (code + short report, 9%). Due date: June 27 Write a python script using keras to train a fully-connected feed-forward neural network classifier to classify documents in the Amazon corpus into positive and negative classes. Your network must consist of:  Input layer of the word2vec embeddings you prepared in Assignment 3. One hidden layer. For the hidden layer, try the following activation functions: ReLU, sigmoid and tanh. Final layer with softmax activation function. Use cross-entropy as the loss function. Add L2-norm regularization. Add dropout. Try a few different dropout rates.  For this assignment, you must use your training/validation/test data splits from Assignment 1. Train your models on the training set. You may only tune your models on your validation set. Once the development is complete, run your classifier on your test set. Report your classification accuracy results in a table with three different activation functions in the hidden layer (ReLU, sigmoid and tanh). What effect do activation functions have on your results? What effect does addition of L2-norm regularization have on the results? What effect does dropout have on the results? Explain your intuitions briefly (up to 10 sentences). ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc51"}, "repo_url": "https://github.com/jessefreeman/MarathonTerminalGenerator", "repo_name": "MarathonTerminalGenerator", "repo_full_name": "jessefreeman/MarathonTerminalGenerator", "repo_owner": "jessefreeman", "repo_desc": "Customized version of textgenrnn designed specifically for generating Marathon terminal messages.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T15:31:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T14:58:44Z", "homepage": null, "size": 21453, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 189433297, "is_fork": false, "readme_text": "textgenrnn  Easily train your own text-generating neural network of any size and complexity on any text dataset with a few lines of code, or quickly train on a text using a pretrained model. textgenrnn is a Python 3 module on top of Keras/TensorFlow for creating char-rnns, with many cool features:  A modern neural network architecture which utilizes new techniques as attention-weighting and skip-embedding to accelerate training and improve model quality. Train on and generate text at either the character-level or word-level. Configure RNN size, the number of RNN layers, and whether to use bidirectional RNNs. Train on any generic input text file, including large files. Train models on a GPU and then use them to generate text with a CPU. Utilize a powerful CuDNN implementation of RNNs when trained on the GPU, which massively speeds up training time as opposed to typical LSTM implementations. Train the model using contextual labels, allowing it to learn faster and produce better results in some cases.  You can play with textgenrnn and train any text file with a GPU for free in this Colaboratory Notebook! Read this blog post or watch this video for more information! Examples from textgenrnn import textgenrnn  textgen = textgenrnn() textgen.generate() [Spoiler] Anyone else find this post and their person that was a little more than I really like the Star Wars in the fire or health and posting a personal house of the 2016 Letter for the game in a report of my backyard.  The included model can easily be trained on new texts, and can generate appropriate text even after a single pass of the input data. textgen.train_from_file('hacker-news-2000.txt', num_epochs=1) textgen.generate() Project State Project Firefox  The model weights are relatively small (2 MB on disk), and they can easily be saved and loaded into a new textgenrnn instance. As a result, you can play with models which have been trained on hundreds of passes through the data. (in fact, textgenrnn learns so well that you have to increase the temperature significantly for creative output!) textgen_2 = textgenrnn('/weights/hacker_news.hdf5') textgen_2.generate(3, temperature=1.0) Why we got money \u201cregular alter\u201d  Urburg to Firefox acquires Nelf Multi Shamn  Kubernetes by Google\u2019s Bern  You can also train a new model, with support for word level embeddings and bidirectional RNN layers by adding new_model=True to any train function. Interactive Mode It's also possible to get involved in how the output unfolds, step by step. Interactive mode will suggest you the top N options for the next char/word, and allows you to pick one. When running textgenrnn in the terminal, pass interactive=True and top=N to generate. N defaults to 3. from textgenrnn import textgenrnn  textgen = textgenrnn() textgen.generate(interactive=True, top_n=5)  This can add a human touch to the output; it feels like you're the writer! (reference) Usage textgenrnn can be installed from pypi via pip: pip3 install textgenrnn You will also need to install TensorFlow (pip3 install tensorflow for CPU, pip3 install tensorflow-gpu for GPU). Note that TensorFlow cannot currently be installed this way in Python 3.7; you'll have to use an earlier Python 3 version. You can view a demo of common features and model configuration options in this Jupyter Notebook. /datasets contains example datasets using Hacker News/Reddit data for training textgenrnn. /weights contains further-pretrained models on the aforementioned datasets which can be loaded into textgenrnn. /outputs contains examples of text generated from the above pretrained models. Neural Network Architecture and Implementation textgenrnn is based off of the char-rnn project by Andrej Karpathy with a few modern optimizations, such as the ability to work with very small text sequences.  The included pretrained-model follows a neural network architecture inspired by DeepMoji. For the default model, textgenrnn takes in an input of up to 40 characters, converts each character to a 100-D character embedding vector, and feeds those into a 128-cell long-short-term-memory (LSTM) recurrent layer. Those outputs are then fed into another 128-cell LSTM. All three layers are then fed into an Attention layer to weight the most important temporal features and average them together (and since the embeddings + 1st LSTM are skip-connected into the attention layer, the model updates can backpropagate to them more easily and prevent vanishing gradients). That output is mapped to probabilities for up to 394 different characters that they are the next character in the sequence, including uppercase characters, lowercase, punctuation, and emoji. (if training a new model on a new dataset, all of the numeric parameters above can be configured)  Alternatively, if context labels are provided with each text document, the model can be trained in a contextual mode, where the model learns the text given the context so the recurrent layers learn the decontextualized language. The text-only path can piggy-back off the decontextualized layers; in all, this results in much faster training and better quantitative and qualitative model performance than just training the model gien the text alone. The model weights included with the package are trained on hundreds of thousands of text documents from Reddit submissions (via BigQuery), from a very diverse variety of subreddits. The network was also trained using the decontextual approach noted above in order to both improve training performance and mitigate authorial bias. When fine-tuning the model on a new dataset of texts using textgenrnn, all layers are retrained. However, since the original pretrained network has a much more robust \"knowledge\" initially, the new textgenrnn trains faster and more accurately in the end, and can potentially learn new relationships not present in the original dataset (e.g. the pretrained character embeddings include the context for the character for all possible types of modern internet grammar). Additionally, the retraining is done with a momentum-based optimizer and a linearly decaying learning rate, both of which prevent exploding gradients and makes it much less likely that the model diverges after training for a long time. Notes   You will not get quality generated text 100% of the time, even with a heavily-trained neural network. That's the primary reason viral blog posts/Twitter tweets utilizing NN text generation often generate lots of texts and curate/edit the best ones afterward.   Results will vary greatly between datasets. Because the pretrained neural network is relatively small, it cannot store as much data as RNNs typically flaunted in blog posts. For best results, use a dataset with at least 2,000-5,000 documents. If a dataset is smaller, you'll need to train it for longer by setting num_epochs higher when calling a training method and/or training a new model from scratch. Even then, there is currently no good heuristic for determining a \"good\" model.   A GPU is not required to retrain textgenrnn, but it will take much longer to train on a CPU. If you do use a GPU, I recommend increasing the batch_size parameter for better hardware utilization.   Future Plans for textgenrnn   More formal documentation   A web-based implementation using tensorflow.js (works especially well due to the network's small size)   A way to visualize the attention-layer outputs to see how the network \"learns.\"   A mode to allow the model architecture to be used for chatbot conversations (may be released as a separate project)   More depth toward context (positional context + allowing multiple context labels)   A larger pretrained network which can accommodate longer character sequences and a more indepth understanding of language, creating better generated sentences.   Hierarchical softmax activation for word-level models (once Keras has good support for it).   FP16 for superfast training on Volta/TPUs (once Keras has good support for it).   Articles/Projects using textgenrnn Articles  Lifehacker: How to Train Your Own Neural Network by Beth Skwarecki New York Times: Let Our Algorithm Choose Your Halloween Costume by Janelle Shane CNN Business: This quirky experiment highlights AI's biggest challenges by Rachel Metz  Projects  Tweet Generator \u2014 Train a neural network optimized for generating tweets based off of any number of Twitter users Hacker News Simulator \u2014 Twitter bot trained on 300,000+ Hacker News submissions using textgenrnn. SubredditRNN \u2014 Reddit Subreddit where all submitted content is from textgenrnn bots. Human-AI Collaborated Pizzas \u2014 Pizza recepies generated with textgenrnn and made in real life. Board Game Titles Video Game Discussion Forum Titles A.I Created Cakes AI Created Cookies AI Generated Songs  Tweets  BuzzFeed YouTube Videos AWS Services Recipes + D&D Spells + Heavy Metal Names RPG Adventure Names The Onion + Cosmopolitan Google Conference Room Names Sith Lords  Maintainer/Creator Max Woolf (@minimaxir) Max's open-source projects are supported by his Patreon. If you found this project helpful, any monetary contributions to the Patreon are appreciated and will be put to good creative use. Credits Andrej Karpathy for the original proposal of the char-rnn via the blog post The Unreasonable Effectiveness of Recurrent Neural Networks. Daniel Grijalva for contributing an interactive mode. License MIT Attention-layer code used from DeepMoji (MIT Licensed) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/jessefreeman/MarathonTerminalGenerator/blob/16801de5af4cae315def9f982dbec57411d852c2/textgenrnn/textgenrnn_weights.hdf5", "https://github.com/jessefreeman/MarathonTerminalGenerator/blob/16801de5af4cae315def9f982dbec57411d852c2/weights/marathon_terminal_weights.hdf5"], "see_also_links": ["http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "http://aiweirdness.com/post/170685749687/candy-heart-messages-written-by-a-neural-network", "http://minimaxir.com/2017/04/char-embeddings/", "http://minimaxir.com/2015/10/reddit-bigquery/", "http://aiweirdness.com/post/180892528177/aw-yeah-its-time-for-cookies-with-neural-networks", "http://minimaxir.com", "http://aiweirdness.com/post/180654319147/how-to-begin-a-song", "http://minimaxir.com/2018/05/text-neural-networks/"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc52"}, "repo_url": "https://github.com/ToWindward/DanGAN-A-WGAN-variant-in-keras-", "repo_name": "DanGAN-A-WGAN-variant-in-keras-", "repo_full_name": "ToWindward/DanGAN-A-WGAN-variant-in-keras-", "repo_owner": "ToWindward", "repo_desc": "This is a Wasserstein variant I created in keras, it makes pretty decent images (compared with WGAN) with small networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T22:49:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T21:04:41Z", "homepage": "", "size": 29969, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189297866, "is_fork": false, "readme_text": "Hey all. I created this WGAN variant when I was playing around with classic and gradient penalty Wasserstein GANs and thought I would put it on here in case anyone was interested or felt like coming up with improvements. It's currently called the DanGAN because that's my name and I haven't come up with a fun acronym yet so I'm open to suggestions. It's still a work in progress and I'll update this as I refine it.  The basic premise is that instead of clipping the weights (as in classic WGAN) or penalising the gradients, instead it uses an adaptive learning rate for the discriminator/critic which prevents exploding gradients. For the generator instead of trying to maximise the output, the loss function attempts to match the mean(squared) and variance of the prior real outputs from the discriminator/critic. Matching to the mean allows the generator to be trained for multiple iterations without the generator collapsing, while matching the variance helps to combat mode collapse which was a significant issue in earlier versions (and does sometimes still happen). This GAN is somewhat sensitive to hyperparameters, as well as the priority given to the mean vs the variance. It works well with small networks but has some issues with larger networks. As a methodolgy this doesn't really compare with current state of the art methods and is more comparable with WGAN / WGAN-GP.  Everything is written in python, using GPU keras and the functional API. While you should be able to take the code, run it, and get results, this is a work in progress, so bear in mind it might have quirks. You also need to find some images to train it with, this version is written for 128x128x3 images, but you can modify it pretty easily for other image sizes.  There are quite a lot of hyperparameters and I'm still working on finding the best ones. The ones in the code should work but depending on the images you use they might need some tweaking. If anyone finds better (or more consistent) ones then do let me know. Same goes for network architecture, kernel size etc. I've played around quite a bit with batch norm and regularization layers, these are included where they seem to help but again, if anyone gets decent results with alternatives I'd love to know. With regards to optimizers it currently uses RMSprop and I didn't have good results with Adam but I haven't tested many others.  P.S. There should be 2 files, one for the GAN itself as well as an example execution file. I've also included a folder with some example pictures, these are with fairly low numbers of iterations (~1000) due to limited computing power, I will try and upload some better ones with more training time. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc53"}, "repo_url": "https://github.com/ongov/Archives-of-Ontario", "repo_name": "Archives-of-Ontario", "repo_full_name": "ongov/Archives-of-Ontario", "repo_owner": "ongov", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T02:46:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T02:14:49Z", "homepage": null, "size": 114070, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189329675, "is_fork": false, "readme_text": "AI Archival File Lister Process Page AI Process Page allows users to upload files and obtain their file name, tilte, format and file size using Python's built-in tools, and extract keywords using AI-enable tool-Gensim. This tool has been deployed for the Ministry of Government and Consumer Services. Supported File Format  .png .jpg .doc .docx .pptx .xlsx .pdf .mp4 .wav  How to Run the App on Your Laptop All required libraries and software are downloaded and installed on a Red Hat Enterprise Linux Server (RHEL in short) Create a fileInfoGetter environment If your operating system is RHEL Run the following command in your terminal to clone the project from Gitlab repo: $ git clone https://gitlab.com/FarnazGoortani/gsic--ao.git  If your operating system is NOT RHEL Option 1: Create a virtural machine, and then run the following command in your terminal to clone the project from Gitlab repo: $ git clone https://gitlab.com/FarnazGoortani/gsic--ao.git  Option 2: Download and install all the required libraries and software by yourself. Activate the environment (skip if you are following option 2) Run the following command in your terminal $ source $HOME/fileInfoGetter/fileInfoGetterEnv/bin/activate  Your terminal should now look like this:  Run the fileInfoGetter file Finally, we can run the python file in terminal, or direcly run it on any Python IDE if you are following option 2. $ python3.6 fileInfoGetter.py  Enter the following URL in Google Chrome. http://localhost:5000  What You Should See Home Page  Python Libraries and Software That You Will Need All required libraries and software are already installed in virtual environment - fileInfoGetterEnv. Links are given for references. Python Libraries:  flask (to create the web application) flask_dropzone (to upload files on the website) openpyxl (to generate result spreadsheet) gensim (to extract keywords given plain text content) docx2txt (to extract text content from .docx files) pdfminer.six (to extract text content from .pdf files) speech_recognition(to perform text recognition on .mp4 files) pptx (to extract text content from .pptx files) keras (to perform object recognition for image files)  We are using the pre-trained model - NASNet to perform this task   wand (to convert pdf files to png f(to find root word)iles) hashlib (to generate checksum) nltk (to find root word)  Software/Tools:  antiword (convert .dco to .docx files) ffmpeg (to convert .mp4 files to .wav files) tesseract (to perform character recognition for image files) ImageMagick (to convert bitmap images) TensorFlow (to execute keras)  How to Deploy Application to a WSGI Server The application is currently hosting on a Red Hat Enterprise Linux Server (RHEL in short), user name is cliadmin. RHEL works almost the same as CentOS, and here is the instruction: How To Serve Flask Applications with Gunicorn and Nginx on CentOS 7 How to Test and Refine the Tool The tool consists of three python files: fileInfoGetter.py, infoGetter.py and Summarizer. fileInfoGetter.py is responsible for hosting the flask application and handle all the responses that are happening. infoGetter.py contains all the helper functions, and lastly Summarizer is an object that can be used to summarize content of an article. Whenever you want to make change to any of the three files, make sure you have a backup copy in case something goes wrong. This Gitlab site contains the latest uploaded source code, so download the source codes from here and then start making changes on the server. How to Upload New File to Gitlab If you wrote your source code on your local environment/laptop, you can upload your files here: https://www.file.io/, you will get a link. Then go to the server and use the following command: $ wget https://file.io/example  After that you will see a file called 'example' under current working directory, make sure to assign a meaningful file name or replace with an existing file using: $ mv example newfilename  In most of the cases, after you are done making changes, run the following three commands on the server: $ git add . --ignore-removal  The above command will add/overwrite the files that you have changed and ignore all the files that you have deleted. $ git commit -m \"some meaningful comment\"  The above command will commite all the added files to Gitlab. $ git push origin master  A simple push sends the made changes to the master branch of the remote repository associated with the working directory. It will ask you the account and password that is associated with the master branch. For more git basic commands, here is a cheatsheet for you. Things that You Need to Be Aware of There are something that you may want to know before using this tool:  Title extraction function is not working ideally because there is no universal pattern that can be applied to find the title of the file, we assume the first line of the file content to be the title however that does not apply to all the files. Unable to access the file path from user's end, this is because Flask does not want to expose necessary information about client's system Use the 'Empty Folder' button to get rid of failed files, if a file is unable to be processed, you have to remove it immediately, otherwise, whenever you upload a new file, the tool will start processing from the beginning of the folder and keeps failing. Do not upload more 50 files at once Please sperate video/audio files from common files such as .doc, .pdf as the process time will increase exponentially and cause the tool to crash.  Possible Future Enhancement  Support more file format, so far the tool has covered over 80% of the commonly used file format. For office purposes, there is one more file format that can be added to the tool and it's challenging to do so - outlook data files. You need to configure an email client on the server, if you are interested in this, this article may be helpful to you. Improve title extraction using A.I. with better model. Title extraction is the hardest task among all this tool's funcionalties as there is no universal pattern on every single files. Append information to the existing spreadsheet instead of making a new one. When users upload new files, the information will be added to the end of previous result.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://docs.wand-py.org/en/0.5.1/#user-s-guide", "http://www.winfield.demon.nl/", "http://flask.pocoo.org/"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc54"}, "repo_url": "https://github.com/kevinbache/ritalin", "repo_name": "ritalin", "repo_full_name": "kevinbache/ritalin", "repo_owner": "kevinbache", "repo_desc": "ritalin offers powerful hyperparameter capabilities with the minimal possible amount of typing", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T01:58:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T21:59:31Z", "homepage": "", "size": 34, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189663250, "is_fork": false, "readme_text": "ritalin ritalin offers a surprisingly powerful implementation of (Hyper)ParameterSets which requires the minimal possible amount of work. Installation $ pip install --editable . from within this directory. Examples For complete examples of  Local hyperparameter tuning, see ritalin/examples/local_hp_tuning.py Distributed hyperparameter tuning using Google Cloud Machine Learning Engine, set the variables required in ritalin/examples/cloud_complex_hp_tuning/build_and_submit.sh and then run ritalin/examples/cloud_complex_hp_tuning/run_tuning_job.py.  Usage Define your (hyper)ParameterSet classes by inheriting from params.ParameterSet and defining default values as class members. from ritalin import params  class ModelHyperParams(params.ParameterSet):     filter_size = 3     num_hidden_layers = 2     num_neurons_per_layer = 32     dropout_rate = 0.5     activation = 'relu'     output_dir = '/tmp/output' The params.ParameterSet's __init__ method will copy these class members into any instantiated subclasses so they won't be shared between instantiated objects. ParameterSets can also be instantiated with parameter ranges for conducting (hyper)parameter searches. For example, continuing the code from above: my_param_ranges = ModelHyperParams(     filter_size=params.DiscreteParameter([3, 5, 7]),     num_hidden_layers=params.IntegerParameter(min_value=1, max_value=3),     num_neurons_per_layer=params.DiscreteParameter(np.logspace(2, 8, num=7, base=2)),     dropout_rate=params.DoubleParameter(min_value=-0.1, max_value=0.9),     activation = 'relu', ) Reusing the same class for defining (hyper)Parameter search ranges and default definitions means that you can refactor a parameter name (e.g. filter_size above) one time in your IDE and it changes everywhere you touch it in your code. These parameter ranges can be sampled: hps = my_param_ranges.sample() print(hp) Which yields a fully instantiated version of your hyperparameters. ModelHyperParams(activation: relu, dropout_rate: 0.6031449000058329, filter_size: 3, num_hidden_layers: 2, num_neurons_per_layer: 32, output_dir: /tmp/output) For local tuning of Keras models, check out the KerasHistoryRandomTuner.  For example: from ritalin import params, tuning  def train_fn(params: ModelHyperParams):     ...  # instantiate the same param class you defined above, overriding some parameters with search ranges # the fact that the class is shared my_param_ranges = ModelHyperParams(     filter_size=params.DiscreteParameter([3, 5, 7]),     num_hidden_layers=params.IntegerParameter(min_value=1, max_value=3),     num_neurons_per_layer=params.DiscreteParameter(np.logspace(2, 8, num=7, base=2)),     dropout_rate=params.DoubleParameter(min_value=-0.1, max_value=0.9),     activation = 'relu',     output_dir = '/tmp/output', )  tuner = tuning.KerasHistoryRandomTuner(     param_ranges=my_param_ranges,     num_parameter_sets=10,     metric_name_of_interest='val_acc' )  tuning.run_tuning(tuner, train_fn)  best_acc, best_params = tuner.get_best(do_max=True) See ritalin/examples/local_hp_tuning.py for a complete example of local hyperparameter tuning for Keras models. ParameterSets can also be used in conjunction with search.HyperparamSearchSpec to conduct full hyperparameter searches using Google's Cloud Machine Learning Engine.  For example: from ritalin import search  spec = search.HyperparamSearchSpec(     max_trials=10,     max_parallel_trials=5,     max_failed_trials=2,     hyperparameter_metric_tag='val_acc', )  my_param_ranges = ModelHyperParams(     filter_size=params.DiscreteParameter([3, 5, 7]),     num_hidden_layers=params.IntegerParameter(min_value=1, max_value=3),     num_neurons_per_layer=params.DiscreteParameter(np.logspace(2, 8, num=7, base=2)),     dropout_rate=params.DoubleParameter(min_value=-0.1, max_value=0.9),     activation = 'relu',     output_dir = '/tmp/output', )  spec.add_parameters(my_param_ranges) spec.to_training_input_yaml('hps.yaml') This creates hps.yaml, a file like this: trainingInput:   hyperparameters:     algorithm: ALGORITHM_UNSPECIFIED     enableTrialEarlyStopping: true     goal: MAXIMIZE     hyperparameterMetricTag: val_acc     maxFailedTrials: 2     maxParallelTrials: 5     maxTrials: 10     params:     - {maxValue: 0.9, minValue: -0.1, parameterName: dropout_rate, type: DOUBLE}     - discreteValues: [3, 5, 7]       parameterName: filter_size       type: DISCRETE     - {maxValue: 3, minValue: 1, parameterName: num_hidden_layers, type: INTEGER}     - discreteValues: [4, 8, 16, 32, 64, 128, 256]       parameterName: num_neurons_per_layer       type: DISCRETE     resumePreviousJobId: null which can be used for distributed hyperparameter tuning using Google Cloud Machine Learning Engine.  See ritalin/examples/cloud_complex_hp_tuning/run_tuning_job.py for a complete example. Finally, you can rebuild this (Hyper)ParameterSet object on the cluster using ritalin/simple_argparse.py module and ParameterSet.from_dict().  For example, in a python script invoked with the arguments: python train.py \\     --num_hidden_layers=3 \\     --num_neurons_per_layer=2 \\     --dropout_rate=0.2 \\     --learning_rate=0.4 \\     --activation=relu You could instantiate the (Hyper)ParameterSet class you defined above like so: from ritalin import params from ritalin import simple_argparse params = ModelHyperParams.from_dict(simple_argparse.args_2_dict())  def build_model(params: ModelHyperParams):     pass  model = build_model(params) ...  That's the basic idea of Ritalin.  One (Hyper)ParameterSet class which does everything you need with a minimal amount of typing and refactoring headaches. Alternatives ParameterSet (Hyper)Parameter classes are the best: better than dict-based hyperparameter sets, traditional classes, and dataclasses. Dicts This is better than using a dict for your hyperparameters.  Parameter values can be type checked in your training code (i.e.: you can be sure your parameter object has all the needed parameter values It's easy to rename hyperparameters by refactoring in your IDE (this fails with dicts that map parameter names to values)  Traditional Classes It's also better than defining a custom Hyperparameter class. Take this for example: class ModelHyperParams:     def __init__(             self,             filter_size = 3,             num_hidden_layers = 2,             num_neurons_per_layer = 32,             dropout_rate = 0.5,             activation = 'relu',             output_dir = '/tmp/output'     ):         self.filter_size = filter_size         self.num_hidden_layers = num_hidden_layers         self.num_neurons_per_layer = num_neurons_per_layer         self.dropout_rate = dropout_rate         self.activation = activation         self.output_dir = output_dir This is fine but you had to write out every parameter name three times and if you want to change one you've got to change it in three places. Dataclasses Finally, Python 3.7's dataclasses provide perhaps the most similar analog to our ParameterSets, but they pose two problems which hold them back.  They require Python 3.7. They require type annotations along with default values.  While requiring type annotations is usually a good thing in Python, they're often redundant for ParameterSets on which the types can be easily inferred from default values.  This makes for unnecessary extra typing. They pose problems for subclassing.  For example:  from dataclasses import dataclass from typing import Any  @dataclass class Parent:     pass  class Child(Parent):     name: Any = 'asdf'     value: Any = 42  c = Child(name='ffffff', value=1234) print(c) Yields the following error: TypeError: __init__() got an unexpected keyword argument 'name'.  A (hyper)ParameterSet superclass should be able to be subclassed so that useful methods can be implemented in parents. Summary Overall, PameterSets offer (Hyper)Parameter objects which are powerful but still as simple to use as it is possible to be in python. Authors ritalin was written by Kevin Bache. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc55"}, "repo_url": "https://github.com/vivekanandgoud/Chatbot_flight-booking-with-rasa-framework_-complete-dynamic-no-static-V3", "repo_name": "Chatbot_flight-booking-with-rasa-framework_-complete-dynamic-no-static-V3", "repo_full_name": "vivekanandgoud/Chatbot_flight-booking-with-rasa-framework_-complete-dynamic-no-static-V3", "repo_owner": "vivekanandgoud", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T19:50:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T10:51:54Z", "homepage": null, "size": 778, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189578582, "is_fork": false, "readme_text": "Train Rasa NLU python -m rasa_nlu.train -c nlu_config.yml --data data/nlu.md -o models --fixed_model_name nlu --project current --verbose  2019-06-02 01:03:19 INFO     rasa_nlu.model  - Starting to train component SpacyNLP 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Starting to train component SpacyTokenizer 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Starting to train component SpacyFeaturizer 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Starting to train component RegexFeaturizer 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Starting to train component CRFEntityExtractor 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Starting to train component EntitySynonymMapper 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Starting to train component SklearnIntentClassifier Fitting 2 folds for each of 6 candidates, totalling 12 fits  [Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Finished training component. 2019-06-02 01:03:20 INFO     rasa_nlu.model  - Successfully saved model   Run Rasa NLU python -m rasa_nlu.server --path ./models  Train Rasa Core python -m rasa_core.train -d domain.yml -s data/stories.md -o models/dialogue -c policy.yml   Epoch 94/100 612/612 [==============================] - 0s 111us/sample - loss: 0.2292 - acc: 0.9412 Epoch 95/100 612/612 [==============================] - 0s 111us/sample - loss: 0.1965 - acc: 0.9444 Epoch 96/100 612/612 [==============================] - 0s 111us/sample - loss: 0.1761 - acc: 0.9461 Epoch 97/100 612/612 [==============================] - 0s 111us/sample - loss: 0.1919 - acc: 0.9428 Epoch 98/100 612/612 [==============================] - 0s 111us/sample - loss: 0.1653 - acc: 0.9575 Epoch 99/100 612/612 [==============================] - 0s 111us/sample - loss: 0.2010 - acc: 0.9330 Epoch 100/100 612/612 [==============================] - 0s 98us/sample - loss: 0.2160 - acc: 0.9314 2019-06-02 01:03:34 INFO     rasa_core.policies.keras_policy  - Done fitting keras policy model   To run the custom action python -m rasa_core_sdk.endpoint --actions actions  Run Rasa Core python -m rasa_core.run -d models/dialogue -u models/current/nlu --endpoints endpoints.yml  OutPut_1 Bot loaded. Type a message and press enter (use '/stop' to exit): Your input ->  hey bot Hello, cool how can i help you!.  Your input ->  movie .....>> My appologies my knowlegdge limited to flight booking only.  Your input ->  give me mobiles info .....>> My appologies my knowlegdge limited to flight booking only.  OutPut_2 Bot loaded. Type a message and press enter (use '/stop' to exit): Your input ->  hey chitti Hello, cool how can i help you!.  Your input ->  i just want to book flight tickets from delhi to chennai Sure. Please confirm the  Depature location?.  Your input ->  delhi Sure. Please  confirm the  destination location?.  Your input ->  chennai Sure. Please  confirm the  date of travel?.  Your input ->  25/08/2019 please choose connection flight Yes or No?  Your input ->  yes available flight services: 1) Air Asia 2) Chennai Airlines 3) Delhi Airlines choose the flight number to confrim   Your input ->  2    Booking in process......!!!!  please enter your ID proof No. to continue?  Your input ->  A123456  Congratulations ****  Your Flight2) Chennai Airlines   Booked succesfully  Ref.NO:2) 123678640HLF99980012.   Your input ->  OutPut _3 Your input ->  hello chitti Hello, cool how can i help you!.  Your input ->  flight shedule Sure. Please let me know Depature location?.  Your input ->  bangalore Sure. Please let me know destination location?.  Your input ->  tirupathi Sure. Please let me know travel date?  Your input ->  24/08/2019 please choose connection flight Yes or No?  Your input ->  yes available flight services: 1) Balaji Airlines 2) Indigo Airlines choose the flight number to confrim   Your input ->  1    Booking in process......!!!!  please enter your ID No.?  Your input ->  A123456  Congratulations ****  Your Flight1) Balaji Airlines   Booked succesfully  Ref.NO:1) 12323233245,8686867,86867867,24235345.   Your input ->  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/vivekanandgoud/Chatbot_flight-booking-with-rasa-framework_-complete-dynamic-no-static-V3/blob/d97258d16b4f11ecf8a12da06ef6bb5ab123ddae/models/dialogue/policy_0_KerasPolicy/keras_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc56"}, "repo_url": "https://github.com/soumakm/affects_in_twitter", "repo_name": "affects_in_twitter", "repo_full_name": "soumakm/affects_in_twitter", "repo_owner": "soumakm", "repo_desc": "Sentiment Intensity Affects on Twitter Data", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T13:47:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T01:09:58Z", "homepage": "", "size": 58133, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189126453, "is_fork": false, "readme_text": "Sentiment Intensity Affects on Twitter Data # System Requirements  RAM = 8 GB Minimum required GPU = Not Neccessary but improves performance Programming Platform: Python 3 Libraries Required = Numpy, Pandas, NLTK, Scikit Learn, re, keras with tensorflow backend Processor = Multi core improves performs but not a requirement       # Step 1: Data Preprocessing a) Removal of Stem words     b) Removal of Stop words     c) Removal of Puntuations     d) Removal of Emoticons f) Removal of Stop words -In progress    # Step 2: Feature Selection <--Yet to Start--> 1) Using NRC Hashtag Emotion Association Lexicon (NRC-Hash-Emo)  2 Approach Feature Extraction  a) Score Feature Only  b) Score Feature and ImportantWordCount  2) Bag of N-Grams  3) Using Word2Vec Word Embedding   # Step 3: Sentiment Classification <--Yet to Start--> 1) SVM Classifier 2) LSTM Neural Network   # Step 4: Experiment Results <--Yet to Start-->  File named PerformanceDataset.txt has results   # Final Step: Final Report  # Steps to follow for running this project  1) SVM Classifier  a) Only Score Feature   i) Detecting Intensity of Emotion   Anger:    Run:     python DetectEmotionIntensity.py anger Lexicon-Single svm        Fear:    Run:     python DetectEmotionIntensity.py fear Lexicon-Single svm        Joy:    Run:     python DetectEmotionIntensity.py joy Lexicon-Single svm        Sadness:    Run:     python DetectEmotionIntensity.py sadness Lexicon-Single svm       ii) Detecting Intensity of Sentiment    Run:     python DetectEmotionIntensity.py valence Lexicon-Single svm      b) Score, ImportantWordCount Feature   i) Detecting Intensity of Emotion   Anger:    Run:     python DetectEmotionIntensity.py anger Lexicon-Two svm        Fear:    Run:     python DetectEmotionIntensity.py fear Lexicon-Two svm        Joy:    Run:     python DetectEmotionIntensity.py joy Lexicon-Two svm        Sadness:    Run:     python DetectEmotionIntensity.py sadness Lexicon-Two svm        ii) Detecting Intensity of Sentiment    Run:     python DetectEmotionIntensity.py valence Lexicon-Two svm      c) Word of N-Grams   i) Detecting Intensity of Emotion   Anger:    Run:     python SVM_Ngrams.py anger        Fear:    Run:     python SVM_Ngrams.py fear        Joy:    Run:     python SVM_Ngrams.py joy        Sadness:    Run:     python SVM_Ngrams.py sadness        ii) Detecting Intensity of Sentiment    Run:     python SVM_Ngrams.py valence      d) Word2Vec Word Embedding   i) Detecting Intensity of Emotion   Anger:    Run:     python DetectEmotionIntensity.py anger Word2Vec svm        Fear:    Run:     python DetectEmotionIntensity.py fear Word2Vec svm        Joy:    Run:     python DetectEmotionIntensity.py joy Word2Vec svm        Sadness:    Run:     python DetectEmotionIntensity.py sadness Word2Vec svm        ii) Detecting Intensity of Sentiment    Run:     python DetectEmotionIntensity.py valence Lexicon-Single svm     2) LSTM Classifier  a) Only Score Feature   i) Detecting Intensity of Emotion   Anger:    Run:     python DetectEmotionIntensity.py anger Lexicon-Single lstm        Fear:    Run:     python DetectEmotionIntensity.py fear Lexicon-Single lstm        Joy:    Run:     python DetectEmotionIntensity.py joy Lexicon-Single lstm        Sadness:    Run:     python DetectEmotionIntensity.py sadness Lexicon-Single lstm        ii) Detecting Intensity of Sentiment    Run:     python DetectEmotionIntensity.py valence Lexicon-Single lstm      b) Score, ImportantWordCount Feature   i) Detecting Intensity of Emotion   Anger:    Run:     python DetectEmotionIntensity.py anger Lexicon-Two lstm        Fear:    Run:     python DetectEmotionIntensity.py fear Lexicon-Two lstm        Joy:    Run:     python DetectEmotionIntensity.py joy Lexicon-Two lstm        Sadness:    Run:     python DetectEmotionIntensity.py sadness Lexicon-Two lstm        ii) Detecting Intensity of Sentiment    Run:     python DetectEmotionIntensity.py valence Lexicon-Two lstm      c) Word2Vec Word Embedding   i) Detecting Intensity of Emotion   Anger:    Run:     python DetectEmotionIntensity.py anger Word2Vec lstm        Fear:    Run:     python DetectEmotionIntensity.py fear Word2Vec lstm        Joy:    Run:     python DetectEmotionIntensity.py joy Word2Vec lstm        Sadness:    Run:     python DetectEmotionIntensity.py sadness Word2Vec lstm        ii) Detecting Intensity of Sentiment    Run:     python DetectEmotionIntensity.py valence Word2Vec lstm  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc57"}, "repo_url": "https://github.com/fenghuoxiguozu/Sentiment_analysis", "repo_name": "Sentiment_analysis", "repo_full_name": "fenghuoxiguozu/Sentiment_analysis", "repo_owner": "fenghuoxiguozu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T13:56:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T13:05:52Z", "homepage": null, "size": 4462, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189415416, "is_fork": false, "readme_text": "\u4eac\u4e1c\u8bc4\u8bba\u60c5\u611f\u5206\u6790 \u662f\u5bf9\u5e26\u6709\u60c5\u611f\u8272\u5f69\u7684\u4e3b\u89c2\u6027\u6587\u672c\u8fdb\u884c\u5206\u6790\u3001\u5904\u7406\u3001\u5f52\u7eb3\u548c\u63a8\u7406\u7684\u8fc7\u7a0b\u3002\u4e92\u8054\u7f51(\u5982\u535a\u5ba2\u548c\u8bba\u575b\u4ee5\u53ca\u793e\u4f1a\u670d\u52a1\u7f51\u7edc\u5982\u5927\u4f17\u70b9\u8bc4)\u4e0a\u4ea7\u751f\u4e86\u5927\u91cf\u7684\u7528\u6237\u53c2\u4e0e\u7684\u3001\u5bf9\u4e8e\u8bf8\u5982\u4eba\u7269\u3001\u4e8b\u4ef6\u3001\u4ea7\u54c1\u7b49\u6709\u4ef7\u503c\u7684\u8bc4\u8bba\u4fe1\u606f\u3002\u8fd9\u4e9b\u8bc4\u8bba\u4fe1\u606f\u8868\u8fbe\u4e86\u4eba\u4eec\u7684\u5404\u79cd\u60c5\u611f\u8272\u5f69\u548c\u60c5\u611f\u503e\u5411\u6027,\u5982\u559c\u3001\u6012\u3001\u54c0\u3001\u4e50\u548c\u6279\u8bc4\u3001\u8d5e\u626c\u7b49\u3002\u57fa\u4e8e\u6b64,\u6f5c\u5728\u7684\u7528\u6237\u5c31\u53ef\u4ee5\u901a\u8fc7\u6d4f\u89c8\u8fd9\u4e9b\u4e3b\u89c2\u8272\u5f69\u7684\u8bc4\u8bba\u6765\u4e86\u89e3\u5927\u4f17\u8206\u8bba\u5bf9\u4e8e\u67d0\u4e00\u4e8b\u4ef6\u6216\u4ea7\u54c1\u7684\u770b\u6cd5\u3002 \u672c\u9879\u76ee\u5bf9\u4eac\u4e1c\u624b\u673a\u4e09\u5206\u7c7b\u8fdb\u884c\u60c5\u611f\u5206\u6790\u3002 \u5de5\u5177 \u73af\u5883 Pycharm  python3.6 MySQL Redis \u6240\u9700\u91cd\u8981\u5e93 Scrapy Scrapy_redis Keras jieba pandas matplolib \u6d41\u7a0b  \u722c\u866b\uff1a\u5206\u5e03\u5f0f\u722c\u53d6\u4eac\u4e1c\u624b\u673a\u597d\u8bc4\uff0c\u4e2d\u8bc4\uff0c\u5dee\u8bc4\u4e09\u79cd\u8bc4\u8bba\u3002\u6570\u636e\u4fdd\u5b58\u81f3MySQL \u6570\u636e\u9884\u5904\u7406\uff1aa. \u5bf9\u8bc4\u8bbajieba\u5206\u8bcd\uff0c\u5206\u8bcd\u4e0d\u592a\u51c6\u786e\u7684\u7528\u7528\u6237\u5b57\u5178\u52a0\u8f7d\uff0c\u6d88\u9664\u6b67\u4e49\u3002 b. \u5bf9\u5206\u8bcd\u7ed3\u679c\u4f18\u5316\uff0c\u90e8\u5206\u7a7a\u683c\u65e5\u671f\u7b49\u5783\u573e\u6570\u636e\uff0c\u518d\u6b21\u6e05\u6d17 c. woed_Embedding\u5bf9\u8bcd\u5efa\u7acb\u7d22\u5f15\uff0c\u4fdd\u5b58\u6570\u636e LSTM\u5efa\u6a21\uff1a\u8bad\u7ec350\u6b21\uff0c\u51c6\u786e\u7387\u8fbe\u523085%+  \u91cd\u8981\u8fc7\u7a0b\u5b9e\u73b0  \u4eac\u4e1c\u8bc4\u8bba\u7f51\u5740\uff1a\u6570\u636e\u4fdd\u5b58\u5728json\u91cc\uff0c\u9700\u5bf9\u7f51\u5740\u8fdb\u884c\u53c2\u6570(productId,fetchJSON_comment98vv9547)\u62fc\u63a5 e.g. https://sclub.jd.com/comment/productPageComments.action?callback=fetchJSON_comment98vv9547&productId=100003433872&score=3&sortType=5&page=0&pageSize=10&isShadowSku=0&fold=1   productId: \u5546\u54c1ID,\u5546\u54c1\u8be6\u60c5\u9875URL\u4e0a comment: \u7f51\u9875\u6e90\u7801\u641c\u7d22comment  \u4f18\u5316  \u4e00\u822c\u7f51\u8d2d\u5546\u54c1\u8bc4\u8bba90%+\u90fd\u662f\u597d\u8bc4\uff0c\u722c\u53d6\u6570\u636e\u65f6\u53ef\u4ee5\u5bf9\u597d\u8bc4\u6570\u91cf\u8fdb\u884c\u9650\u5236\uff0c\u5e73\u8861\u6837\u672c\u3002 \u5206\u8bcd\u65f6\u82f1\u6587\u8bcd\u6c47(vivo,ios)\u7b49\u57fa\u672c\u65e0\u8bed\u4e49\uff0c\u6240\u4ee5\u5168\u90e8\u5220\u9664\uff0c\u4f46\u90e8\u5206\u8bcd\u6c47\uff08e.g OK good\uff09\u7b49\u5bf9\u5206\u6790\u6709\u91cd\u8981\u5f71\u54cd\u3002  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc58"}, "repo_url": "https://github.com/guangxush/UAI_CUP_2017", "repo_name": "UAI_CUP_2017", "repo_full_name": "guangxush/UAI_CUP_2017", "repo_owner": "guangxush", "repo_desc": "https://www.biendata.com/competition/UAI/", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T07:28:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T07:14:03Z", "homepage": null, "size": 16, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189364939, "is_fork": false, "readme_text": "UAI-CUP-2017 Requirement Python 2.7 Keras 2.x sklearn pandas Run UAI-CUP-2017\u76ee\u5f55\u4e0b\u7684\u6587\u4ef6\u4e3a\u5404\u79cd\u4e0d\u540c\u7684ML/DL\u6a21\u578b\u4ee3\u7801\uff0c\u8fd0\u884c\u5206\u4e3a\u4e24\u79cd\u65b9\u5f0f\uff1a   \u53ea\u8bad\u7ec3\uff0c\u4e0d\u4ea7\u751f\u63d0\u4ea4\u7ed3\u679c\uff08\u7528\u4e8e\u6d4b\u8bd5\u4ee3\u7801\u7684\u6b63\u786e\u6027\u548c\u4e86\u89e3\u6a21\u578b\u6548\u679c\uff09,\u4f8b\u5982\uff1a   python embedding_mlp.py train    \u8bad\u7ec3\u4e14\u4ea7\u751f\u6d4b\u8bd5\u96c6\u7ed3\u679c\u7528\u4e8e\u63d0\u4ea4\uff0c\u4f8b\u5982\uff1a   python embedding_mlp.py submit    \u6d4b\u8bd5\u7ed3\u679c \u4e0d\u4f7f\u7528POI\u7684ID    \u6a21\u578b/\u65b9\u6cd5 Train MAE Dev MAE Pubulic Test MAE \u5907\u6ce8\u8bf4\u660e     MLP 0.4543 0.4189 2.2674 \u4e0d\u4f7f\u7528One-Hot   MLP 0.4530 0.4184 2.2315 \u5bf9\u5468\u51e0\u548c\u5c0f\u65f6\u4f7f\u7528One-Hot\u5904\u7406   ETR 0.1448 0.5916 2.1252 \u540c\u4e0a    \u4f7f\u7528POI\u7684ID    \u6a21\u578b/\u65b9\u6cd5 Train MAE Dev MAE Pubulic Test MAE \u5907\u6ce8\u8bf4\u660e     MLP 0.4235 0.3992 2.1209 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cOne-Hot\u5904\u7406   Emb_MLP 0.4225 0.3966 2.3590 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cEmbeddding\u5904\u7406   ETR 0.1448 0.5871 2.1316 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cOne-Hot\u5904\u7406    \u5c06\u6240\u6709\u8ba2\u5355\u72b6\u6001\u90fd\u8ba4\u4e3a\u662f\u9700\u6c42\u91cf    \u6a21\u578b/\u65b9\u6cd5 Train MAE Dev MAE Pubulic Test MAE \u5907\u6ce8\u8bf4\u660e     MLP 0.6760 0.6795 2.0317 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cOne-Hot\u5904\u7406   Emb_MLP 0.6847 0.6811 2.2911 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cEmbeddding\u5904\u7406   ETR 0.2232 0.9417 2.1140 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cOne-Hot\u5904\u7406   MLP 0.6874 0.6925 2.0148 \u5bf9\u7ed3\u679c\u56db\u820d\u4e94\u5165   MLP 0.7056 0.6856 2.1154 \u53d6\u4e0b\u6574   MLP 0.6473 0.6827(0.6800) 2.0266 \u4e24\u6bb5one-hot   MLP 0.6769 0.6879(0.6839) xxxx \u4e24\u6bb5one-hot   MLP 1.6831 1.4100(1.3947) 2.0336 \u53bb\u6389\u9700\u6c42\u5927\u4e8e9\u7684\u6570\u636e\uff0c\u6bcf\u4e2a\u533a\u95f4\u968f\u673a\u90099000\u6761   MLP 0.6613 0.6859(0.680244) 2.020768816 POI\u805a\u7c7b\u75280,1\u8868\u793a   MLP 1.6831 0.6859(0.681119) 2.0455278924. \u4e0d\u4f7f\u7528POI\u6570\u636e   MLP 0.7401 0.7091(0.70651) 2.201941466. \u52a0\u5165\u5747\u503c\u6570\u636e\u4e3a53\u7ef4    POI\u6570\u503c\u7279\u5f81\u79bb\u6563\u5316    \u6a21\u578b/\u65b9\u6cd5 Train MAE Dev MAE Pubulic Test MAE \u5907\u6ce8\u8bf4\u660e     ETR 0.0012 0.9691 2.1448 \u5bf9\u79bb\u6563\u578b\u7279\u5f81\u4f5cOne-Hot\u5904\u7406   MLP 0.6640 0.6853 2.058 \u5bf9\u7ed3\u679c\u56db\u820d\u4e94\u5165    \u5c06\u72b6\u6001\u4e3a0\u62162\u7684\u89c6\u4e3a\u9700\u6c42\u91cf\u52a01\uff0c1\u89c6\u4e3a\u9700\u6c42\u91cf\u52a00    \u6a21\u578b/\u65b9\u6cd5 Train MAE Dev MAE Pubulic Test MAE \u5907\u6ce8\u8bf4\u660e     MLP 0.5919 0.6121 2.1574 \u4e0d\u4f7f\u7528ID   MLP 0.5845 0.6049 2.1474 \u4f7f\u7528ID    Copyright The Owner of this project is Dr.E(Tongji University 436Lab), My contribute is the data process. ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc59"}, "repo_url": "https://github.com/Guriido/mnist-tensorflow", "repo_name": "mnist-tensorflow", "repo_full_name": "Guriido/mnist-tensorflow", "repo_owner": "Guriido", "repo_desc": "Versatile mnist example in tensorflow", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T07:52:00Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T06:17:47Z", "homepage": null, "size": 13, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189162260, "is_fork": false, "readme_text": "Mnist example in tensorflow Foreword: If there are no specific framework constraints, please use chainer or pytorch!! The purpose of this repository is to introduce tensorflow notions through a Mnist example. The sample codes on official repositories and such lack of clarity, or versatility (or both). In mnist example version v1~, I tried to produce a parameterizable and yet practical code. Versions V0 This is a simple example written by a coworker that I based my examples on. V1 adds  device selection model as an tf.keras.models.Model object  V2 adds  simple logging tool (inspired by Chainer) parameterizable lr descent  V3 adds  clean use of tf.data.Dataset and iterator evaluation on validation dataset each epoch pretty print of epoch progress with tqdm  Environment This was tested on tensorflow 1.9 with python 3.6 on this docker image: https://cloud.docker.com/u/guriido/repository/docker/guriido/tf Useful references: On dataset creation and options https://www.tensorflow.org/guide/performance/datasets On datasets use with sessions https://medium.com/ymedialabs-innovation/how-to-use-dataset-and-iterators-in-tensorflow-with-code-samples-3bb98b6b74ab advanced tutorial https://www.tensorflow.org/alpha/tutorials/eager ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc5a"}, "repo_url": "https://github.com/rev997/gameBehaviouralCloning", "repo_name": "gameBehaviouralCloning", "repo_full_name": "rev997/gameBehaviouralCloning", "repo_owner": "rev997", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T07:04:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T06:26:18Z", "homepage": null, "size": 32772, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189163528, "is_fork": false, "readme_text": "Steering Prediction  Note: Only steering values are predicted keeping speed constant I have collected the data and trained it on FH3(forza horizon 3).In the game difficulty setting,set the gear shifting to manual(this acts as car speed control).The gear was set to 1 and throttle was set to full through out the process.A Xbox controller is used to control the car and these inputs are recorded for training. Dataset Collection To collect the data run datasetgenerator.py and don't forget to change gear shifting mode to manual.And set the screen monitor values as per your requirement.  While collecting data use:         'P' key to pause                                     'S' key to save collected data Before training the model run remove_zeros.py Training To train model run train.py. Testing To test the model run test.py Note: While testing, first start test.py, then start the game else you might encounter CUBLAS_HANDLING error. Environment My rig:Ryzen 3 1200 (non OC) GTX 1050Ti (non OC) 2x8gb Ram @ 3100Mhz With my rig I am able to predict around 45 fps for the used model.If you have a better card  use a complex model(or YUV channels instead of grayscale).  Game Settings: Dynamic Render Quality: Very low Resolution : 1024 x 768 Pre-trained model A pre-trained model file 'models' is included in the repository. Load it using keras.models.load_model.This is just a sample model  trained on just 10000 images,use large training set for better results. ", "has_readme": true, "readme_language": "English", "repo_tags": ["cnn", "behavioral-cloning", "forza-horizon-3"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc5b"}, "repo_url": "https://github.com/agohr/deep_speck", "repo_name": "deep_speck", "repo_full_name": "agohr/deep_speck", "repo_owner": "agohr", "repo_desc": "Supplementary code and data to \"Improving Attacks on Round-Reduced Speck32/64 Using Deep Learning\"", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T03:43:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T03:11:04Z", "homepage": "", "size": 2004, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189336967, "is_fork": false, "readme_text": "A) Summary  This repository holds supplementary code and data to the paper \"Improving Attacks on Round-Reduced Speck32/64 Using Deep Learning\". In particular, it contains:  - a proof of concept implementation of the 11-round attack on Speck32/64 (test_key_recovery.py), with parameters for a practical 12-round attack at the very end of the code, - a basic training script that will train a 10-block deep residual network to distinguish 5-round Speck from random data using the basic training pipeline described in the paper (train_5_rounds.py), - pre-trained neural distinguishers for 5,6,7 and 8 rounds of Speck, - a proof of concept implementation of neural input difference search using few-shot-learning (neural_difference_search.py), - a script that evaluates the pre-trained neural distinguishers both in the real-vs-random setting and in the real differences setting of section 5 of the paper (eval.py), - a script (key_rank.py) that calculates key rank statistics for the 9-round attack of the paper with a neural distinguisher when between 32 and 128 ciphertext pairs of data are available, - another script (key_rank_ddt.py) that implements the same experiment using DDT-based distinguishers, - a python program that looks at whether the performance of the 7-round neural distinguisher supplied in this repository is key-dependent (key_nonuniformity.py), - a python script that implements the KeyAveraging algorithm from the paper, - a program written in C++ (speck.cpp in the cpp subfolder) that allows to check with a modest computational budget (less than a day on a standard PC) various claims of the paper about the difference distribution of five-round Speck, the power of key-search based distinguishers, the probability of the most likely differential transition for 8-round Speck, and properties of specific output pairs found in the paper, - a program written in C++ (speck_ddt.cpp in the cpp subfolder) that calculates the difference distribution for up to 8 rounds of Speck32/64 given the input difference 0x0040/0000 and writes the results to disk. This requires a machine with about 70 GB of unused RAM and about 300 CPU days calculation time. It will compute the predicted output distributions for the output of each round consecutively and write the result to disk; these files are about 35 GB each in size. The resulting files can be loaded into a Python environment by calling numpy.fromfile(filename) and the arrays so generated can be used by key_rank_ddt.py to run the 9-round attack using the difference distribution table.  This archive also contains precomputed key-rank data for the 9-round attack (subdirectory data_9r_attack), data on the key dependence of the 7-round distinguisher, the complete learning history of our best 5-round neural distinguisher (as referenced in the paper) and the wrong key response profiles for all of the supplied pre-trained neural networks except the 5-round one. The learning history of the 5-round network described in the paper is stored as a pickled dictionary in the supplementary data subdirectory.  B) Dependencies and system requirements  Requirements for running the main attack: python3, current keras installation, h5py. Tested with the tensorflow backend, but the code should be backend-agnostic. Training and evaluation code have the same requirements. Tested configuration: keras 2.1.5, tensorflow 1.6.0, h5py 2.7.1.  The neural difference search demonstrator additionally needs a current version of scikit-learn to be installed. Tested version: scikit-learn 0.19.1.  The C++ programs should work with any version of g++ that supports C++2014. g++7.3 has been tested. The code uses gcc builtin functions, so other compilers will not work without changes to the code.  C) Compiling the cpp-files  To build the C++ programs, run make in the cpp subdirectory. This will produce two executable files:  - speck_analysis: runs a number of experiments that will produce evidence of correctness of various claims in the paper (computational budget around one day on a standard PC).  - speck_ddt: calculates the difference distribution of Speck with the input difference used in the paper. This takes significant memory, hard drive space and computing power.  All programs in this code repository (with the possible exception of the key rank and DDT calculation scripts) should work reasonably well on a standard PC. In particular the 11-round attack code should be quite fast without GPU support.  D) Running the experiments  Each of the python files mentioned contains a script that will run a particular set of experiments when run from the terminal using python3.  Instructions for each of the experiments:  1. test_key_recovery.py: run from terminal using the command python3 test_key_recovery.py .  Note that this will run the 11-round attack 100 times and then run a 12-round attack (not described in the paper but using exactly the same code) 20 times. The latter will take some time unless a fast GPU is available (one run of the 12-round attack should take roughly 12 hours on a quad-core PC without GPU). Note that the 12-round attack is with these settings successful only in about 40 percent of cases, so one run is likely not going to be sufficient to demonstrate that it works. Remove the last few lines of the script to turn the 12-round attack off.  The 11-round attack will produce three files that contain numpy arrays with the following information:  - run_sols1.npy contains the bit difference between real subkey and best guess for the last subkey for each run. - run_sols2.npy contains the bit difference between real subkey and best guess for the penultimate subkey for each run. - run_good.npy records the maximal number of good transitions of the initial differential for all ciphertext structures used in each run. This information is given to show which runs were solvable in principle using the techniques described in the paper.  During the run, the script will show how many test cases have been completed. For each completed test, also the bit difference between the final guesses of the last two subkeys and the actual last subkeys is shown.  At the end of the run, information on average attack runtime and data usage is printed.  2. eval.py: run from terminal using python3 eval.py. The results should be self-explanatory.  3. train_5_rounds.py: run from terminal using python3 train_5_rounds.py. Data will be written to the ./freshly_trained_networks/ subdirectory.  4. speck.cpp: compile as directed above and run the resulting speck_analysis binary from terminal. Output should be self-explanatory. The csv file produced can be read by the readcsv function of the python module implementing speck. It is then possible to compare the predictions made by the Markov model to predictions produced for instance by the pre-trained five-round neural network here included.  5. neural_difference_search.py: run from terminal using python3 neural_difference_search.py. This will first briefly train a fresh 3-round distinguisher for Speck with a random input difference and then use this distinguisher with few-shot-learning and the generic optimization algorithm described in the paper to search for input differences that can be more efficiently distinguished. Progress (few-shot distinguisher efficiency for three rounds, extensions to more rounds, input difference) is shown each time an improvement is found. The search is restarted ten times to give a sample of possible results of the algorithm.  6. key_rank.py: generates key-rank statistics for a simple 9-round attack on Speck32/64 when the attack is using a neural distinguisher.  7. key_rank_ddt.py: use the stats_key_rank function to do the same as the previous script, but using a difference distribution table. The DDT needs to be separately loaded.   8. key_averaging.py: implements KeyAveraging and the creation of high-quality training data using the output of KeyAveraging.  E) Final remarks  The pre-trained networks included in this directory are all small networks with just one residual block. In the five and six round cases, their predictive performance is therefore slightly lower than claimed in the paper. Run the five-round training script to obtain a five-round distinguisher with the performance given in the paper.  F) Citing  If you use the code in this repository for your own research and publish about it, please cite the paper:  Aron Gohr, Improving Attacks on Round-Reduced Speck32/64 Using Deep Learning, Advances in Cryptology - CRYPTO 2019 (to appear)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/agohr/deep_speck/blob/529101ce7e35590b4c6fee4e257510d8a001e3c1/net5_small.h5", "https://github.com/agohr/deep_speck/blob/529101ce7e35590b4c6fee4e257510d8a001e3c1/net6_small.h5", "https://github.com/agohr/deep_speck/blob/529101ce7e35590b4c6fee4e257510d8a001e3c1/net7_small.h5", "https://github.com/agohr/deep_speck/blob/529101ce7e35590b4c6fee4e257510d8a001e3c1/net8_small.h5", "https://github.com/agohr/deep_speck/blob/529101ce7e35590b4c6fee4e257510d8a001e3c1/simple_net/simple_7r_model_preproc_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc5c"}, "repo_url": "https://github.com/mohammadjafri1992/CustomerBehaviorAnalysis", "repo_name": "CustomerBehaviorAnalysis", "repo_full_name": "mohammadjafri1992/CustomerBehaviorAnalysis", "repo_owner": "mohammadjafri1992", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T04:38:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T03:05:19Z", "homepage": null, "size": 2751, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189336233, "is_fork": false, "readme_text": "Customer Behavior Analysis - Converting \"Freemium\" users to \"Premium\" users. In this project, we analyze the behavior of customers of a mobile app. Their behavior is analyzed through their interaction with the app. Objective The primary objective of the project is to convert free customers to paying subscribers. Possible Applications After all is said and done, the bottom line is whether we can use this model anywhere. So following are our possible use cases for the model we created.  Amazon Prime - customer analysis when they are simply browsing the website while they are logged in. Spotify Premium - Learning how free customers interact with the app vs. how premium customers use the app.  A funny story: Interestingly enough, I noticed Spotify using almost the same model that we creted here (extremely advanced version of this model) on my free Spotify account. I rarely use Spotify app, but when I do, I listen to my favouirite podcasts. Throughout the winter holidays, they kept offering me 3 months for $4.99, then $9.99. That's when I listend to some music and they must have gauged my lifetime customer value to be \"X\" and therefore gave me that offer. But when I started to listen to Podcasts more, Spotify must have guessed my lower future impact on their premium content, and thus started offering me $0.99 for 3 months, wich Showtime and Hulu.. ALL INCLUDED! Now that's some live Machine Learning right there!  Similaryly any business with a Freemium business model can use this model to make templting offers to borderline customers and to attract new paid users. The applications are limitless.  Methodology The methodology used here was to analyze customer interaction with the app screens. i.e. look at how many people used premium features, how many people interacted with each specific type of loan (there are several loans available to customers), etc. There are a nubmer of features and their interactions with one another, i.e. positively or negatively correlated, available to us in the dataset. Please note that this is NOT the data from real customers. However, this data was generated using the EXACT same distributions of real data and customers. Python library \"Faker\" was used to generate this data. One-hot encoding - using a for loop in scikitlearn Since we are not using deep learning here, we are relying on scikitlearn to work for us. Which means performing one-hot encoding yourself in a for loop. This was a bit wierd for me as I have used one-hot encoding with Keras before this project. There are 2 python code files. One files prepares the data and the other creates the model. This is a nice break up of code as we can use either file to easily with other source files and we can use the same data processing for similar projects. Dataset The dataset I used is a 50000 instance dataset. I created a 80-20 train test split and then ran further processing on those splits. After preprocessing the main data file, I created a new datafile with the correlated screens merged into one column and then deleting those screen columns from the database subsequently. top_screens.csv The file top_screens.csv contains a list of the most popular screens. One reason why you might see that some screen which is a top screen but is not present in our sample dataset, is that our dataset is a \"sample\" of 50,000 instances of a much larger dataset, possibly containing millions of rows. The most common screens we have, were sampled from that dataset. Consider these screens to be \"absolute\" truth, i.e. we can use the same top screens to perform analytics on the whole dataset with possibly similar results. Check out the codebase Please check out the code to see further explanation of each and every step. If you find anything you don't understand simply by looking at the code, please let me know. I will try my best to help you out. Also if you find an error, please open up an issue. That will be much easier for all of us to learn and solve. Conclusion The project gave us ~77% accurate results. You sould check out the final metrics calculated by the model in order to understand the model performance. We could tune it further and have gotten an even better results. I have some machine learning projects already inside the Deep_learning Specialization from Coursera in one of the repos. There I have explained ML model tuning in detail. Although those solutions are specifically for Deep Learning, we can apply \"similar\" techniques, to this model as well and make it better. Thank you for reading the whole readme file. Hope you enjoyed it! ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc5d"}, "repo_url": "https://github.com/xytjcxy/SSM", "repo_name": "SSM", "repo_full_name": "xytjcxy/SSM", "repo_owner": "xytjcxy", "repo_desc": "single shot mask", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T09:53:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T09:46:14Z", "homepage": null, "size": 6263, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 189197140, "is_fork": false, "readme_text": "SSD: Single Shot MultiBox Object Detector, in PyTorch A PyTorch implementation of Single Shot MultiBox Detector from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found here.  Table of Contents  Installation Datasets Train Evaluate Performance Demos Future Work Reference  \u00a0 \u00a0 \u00a0 \u00a0 Installation  Install PyTorch by selecting your environment on the website and running the appropriate command. Clone this repository.  Note: We currently only support Python 3+.   Then download the dataset by following the instructions below. We now support Visdom for real-time loss visualization during training!  To use Visdom in the browser:  # First install Python server and client pip install visdom # Start the server (probably in a screen or tmux) python -m visdom.server  Then (during training) navigate to http://localhost:8097/ (see the Train section below for training details).   Note: For training, we currently support VOC and COCO, and aim to add ImageNet support soon.  Datasets To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit torch.utils.data.Dataset, making them fully compatible with the torchvision.datasets API. COCO Microsoft COCO: Common Objects in Context Download COCO 2014 # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/COCO2014.sh VOC Dataset PASCAL VOC: Visual Object Classes Download VOC2007 trainval & test # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2007.sh # <directory> Download VOC2012 trainval # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2012.sh # <directory> Training SSD  First download the fc-reduced VGG-16 PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth By default, we assume you have downloaded the file in the ssd.pytorch/weights dir:  mkdir weights cd weights wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth  To train SSD using the train script simply specify the parameters listed in train.py as a flag or manually change them.  python train.py  Note:  For training, an NVIDIA GPU is strongly recommended for speed. For instructions on Visdom usage/installation, see the Installation section. You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see train.py for options)    Evaluation To evaluate a trained network: python eval.py You can specify the parameters listed in the eval.py file by flagging them or manually changing them.  Performance VOC2007 Test mAP    Original Converted weiliu89 weights From scratch w/o data aug From scratch w/ data aug     77.2 % 77.26 % 58.12% 77.43 %    FPS GTX 1060: ~45.45 FPS Demos Use a pre-trained SSD network for detection Download a pre-trained network  We are trying to provide PyTorch state_dicts (dict of weight tensors) of the latest SSD model definitions trained on different datasets. Currently, we provide the following PyTorch models:  SSD300 trained on VOC0712 (newest PyTorch weights)  https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth   SSD300 trained on VOC0712 (original Caffe weights)  https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth     Our goal is to reproduce this table from the original paper    Try the demo notebook  Make sure you have jupyter notebook installed. Two alternatives for installing jupyter notebook:   If you installed PyTorch with conda (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run): jupyter notebook   If using pip:     # make sure pip is upgraded pip3 install --upgrade pip # install jupyter notebook pip install jupyter # Run this inside ssd.pytorch jupyter notebook  Now navigate to demo/demo.ipynb at http://localhost:8888 (by default) and have at it!  Try the webcam demo  Works on CPU (may have to tweak cv2.waitkey for optimal fps) or on an NVIDIA GPU This demo currently requires opencv2+ w/ python bindings and an onboard webcam  You can change the default webcam in demo/live.py   Install the imutils package to leverage multi-threading on CPU:  pip install imutils   Running python -m demo.live opens the webcam and begins detecting!  TODO We have accumulated the following to-do list, which we hope to complete in the near future  Still to come:   Support for the MS COCO dataset  Support for SSD512 training and testing  Support for training on custom datasets    Authors  Max deGroot Ellis Brown  Note: Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible. References  Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" ECCV2016. Original Implementation (CAFFE) A huge thank you to Alex Koltun and his team at Webyclip for their help in finishing the data augmentation portion. A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):  Chainer, Keras, MXNet, Tensorflow    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.image-net.org/", "http://pytorch.org/", "http://host.robots.ox.ac.uk/pascal/VOC/", "http://localhost:8097/", "http://localhost:8888", "http://www.webyclip.com", "http://mscoco.org/", "http://pytorch.org/docs/torchvision/datasets.html", "http://jupyter.readthedocs.io/en/latest/install.html", "http://github.com/ellisbrown"], "reference_list": ["http://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1409.1556", "http://arxiv.org/abs/1512.02325"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc5e"}, "repo_url": "https://github.com/xieydd/nni", "repo_name": "nni", "repo_full_name": "xieydd/nni", "repo_owner": "xieydd", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T08:26:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-29T10:10:47Z", "homepage": null, "size": 22573, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189201172, "is_fork": false, "readme_text": "            \u7b80\u4f53\u4e2d\u6587 NNI (Neural Network Intelligence) is a toolkit to help users run automated machine learning (AutoML) experiments. The tool dispatches and runs trial jobs generated by tuning algorithms to search the best neural architecture and/or hyper-parameters in different environments like local machine, remote servers and cloud. NNI v0.7 has been released!        Supported Frameworks    Tuning Algorithms    Training Services        PyTorch TensorFlow Keras MXNet Caffe2 CNTK (Python language) Chainer Theano    Tuner  TPE Random Search Anneal Naive Evolution SMAC Batch Grid Search Hyperband Network Morphism ENAS Metis Tuner BOHB  Assessor  Median Stop Curve Fitting     Local Machine Remote Servers OpenPAI Kubeflow FrameworkController on K8S (AKS etc.)      Who should consider using NNI  Those who want to try different AutoML algorithms in their training code (model) at their local machine. Those who want to run AutoML trial jobs in different environments to speed up search (e.g. remote servers and cloud). Researchers and data scientists who want to implement their own AutoML algorithms and compare it with other algorithms. ML Platform owners who want to support AutoML in their platform.  Related Projects Targeting at openness and advancing state-of-art technology, Microsoft Research (MSR) had also released few other open source projects.  OpenPAI : an open source platform that provides complete AI model training and resource management capabilities, it is easy to extend and supports on-premise, cloud and hybrid environments in various scale. FrameworkController : an open source general-purpose Kubernetes Pod Controller that orchestrate all kinds of applications on Kubernetes by a single controller. MMdnn : A comprehensive, cross-framework solution to convert, visualize and diagnose deep neural network models. The \"MM\" in MMdnn stands for model management and \"dnn\" is an acronym for deep neural network. We encourage researchers and students leverage these projects to accelerate the AI development and research.  Install & Verify If you choose NNI Windows local mode and you use PowerShell to run script for the first time, you need to run PowerShell as administrator with this command first:     Set-ExecutionPolicy -ExecutionPolicy Unrestricted Install through pip  We support Linux, MacOS and Windows(local mode) in current stage, Ubuntu 16.04 or higher, MacOS 10.14.1 along with Windows 10.1809 are tested and supported. Simply run the following pip install in an environment that has python >= 3.5.  Linux and MacOS python3 -m pip install --upgrade nni Windows python -m pip install --upgrade nni Note:  --user can be added if you want to install NNI in your home directory, which does not require any special privileges. Currently NNI on Windows only support local mode. Anaconda is highly recommended to install NNI on Windows. If there is any error like Segmentation fault, please refer to FAQ  Install through source code  We support Linux (Ubuntu 16.04 or higher), MacOS (10.14.1) and Windows local mode (10.1809) in our current stage.  Linux and MacOS  Run the following commands in an environment that has python >= 3.5, git and wget.      git clone -b v0.7 https://github.com/Microsoft/nni.git     cd nni     source install.sh Windows  Run the following commands in an environment that has python >=3.5, git and PowerShell    git clone -b v0.7 https://github.com/Microsoft/nni.git   cd nni   powershell ./install.ps1 For the system requirements of NNI, please refer to Install NNI For NNI Windows local mode, please refer to NNI Windows local mode Verify install The following example is an experiment built on TensorFlow. Make sure you have TensorFlow installed before running it.  Download the examples via clone the source code.      git clone -b v0.7 https://github.com/Microsoft/nni.git Linux and MacOS  Run the MNIST example.      nnictl create --config nni/examples/trials/mnist/config.yml Windows  Run the MNIST example.      nnictl create --config nni/examples/trials/mnist/config_windows.yml  Wait for the message INFO: Successfully started experiment! in the command line. This message indicates that your experiment has been successfully started. You can explore the experiment using the Web UI url.  INFO: Starting restful server... INFO: Successfully started Restful server! INFO: Setting local config... INFO: Successfully set local config! INFO: Starting experiment... INFO: Successfully started experiment! ----------------------------------------------------------------------- The experiment id is egchD4qy The Web UI urls are: http://223.255.255.1:8080   http://127.0.0.1:8080 -----------------------------------------------------------------------  You can use these commands to get more information about the experiment -----------------------------------------------------------------------          commands                       description 1. nnictl experiment show        show the information of experiments 2. nnictl trial ls               list all of trial jobs 3. nnictl top                    monitor the status of running experiments 4. nnictl log stderr             show stderr log content 5. nnictl log stdout             show stdout log content 6. nnictl stop                   stop an experiment 7. nnictl trial kill             kill a trial job by id 8. nnictl --help                 get help information about nnictl -----------------------------------------------------------------------   Open the Web UI url in your browser, you can view detail information of the experiment and all the submitted trial jobs as shown below. Here are more Web UI pages.      Documentation  NNI overview Quick start  How to  Install NNI Use command line tool nnictl Use NNIBoard How to define search space How to define a trial How to choose tuner/search-algorithm Config an experiment How to use annotation  Tutorials  Run an experiment on local (with multiple GPUs)? Run an experiment on multiple machines? Run an experiment on OpenPAI? Run an experiment on Kubeflow? Try different tuners Try different assessors Implement a customized tuner Implement a customized assessor Use Genetic Algorithm to find good model architectures for Reading Comprehension task  Contribute This project welcomes contributions and suggestions, we use GitHub issues for tracking requests and bugs. Issues with the good first issue label are simple and easy-to-start ones that we recommend new contributors to start with. To set up environment for NNI development, refer to the instruction: Set up NNI developer environment Before start coding, review and get familiar with the NNI Code Contribution Guideline: Contributing We are in construction of the instruction for How to Debug, you are also welcome to contribute questions or suggestions on this area. License The entire codebase is under MIT license ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc5f"}, "repo_url": "https://github.com/herrickli/contraband-detection-ssd", "repo_name": "contraband-detection-ssd", "repo_full_name": "herrickli/contraband-detection-ssd", "repo_owner": "herrickli", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T07:27:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-31T07:16:45Z", "homepage": null, "size": 3932, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189546876, "is_fork": false, "readme_text": "SSD: Single Shot MultiBox Object Detector, in PyTorch A PyTorch implementation of Single Shot MultiBox Detector from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found here.  Table of Contents  Installation Datasets Train Evaluate Performance Demos Future Work Reference  \u00a0 \u00a0 \u00a0 \u00a0 Installation  Install PyTorch by selecting your environment on the website and running the appropriate command. Clone this repository.  Note: We currently only support Python 3+.   Then download the dataset by following the instructions below. We now support Visdom for real-time loss visualization during training!  To use Visdom in the browser:  # First install Python server and client pip install visdom # Start the server (probably in a screen or tmux) python -m visdom.server  Then (during training) navigate to http://localhost:8097/ (see the Train section below for training details).   Note: For training, we currently support VOC and COCO, and aim to add ImageNet support soon.  Datasets To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit torch.utils.data.Dataset, making them fully compatible with the torchvision.datasets API. COCO Microsoft COCO: Common Objects in Context Download COCO 2014 # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/COCO2014.sh VOC Dataset PASCAL VOC: Visual Object Classes Download VOC2007 trainval & test # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2007.sh # <directory> Download VOC2012 trainval # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2012.sh # <directory> Training SSD  First download the fc-reduced VGG-16 PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth By default, we assume you have downloaded the file in the ssd.pytorch/weights dir:  mkdir weights cd weights wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth  To train SSD using the train script simply specify the parameters listed in train.py as a flag or manually change them.  python train.py  Note:  For training, an NVIDIA GPU is strongly recommended for speed. For instructions on Visdom usage/installation, see the Installation section. You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see train.py for options)    Evaluation To evaluate a trained network: python eval.py You can specify the parameters listed in the eval.py file by flagging them or manually changing them.  Performance VOC2007 Test mAP    Original Converted weiliu89 weights From scratch w/o data aug From scratch w/ data aug     77.2 % 77.26 % 58.12% 77.43 %    FPS GTX 1060: ~45.45 FPS Demos Use a pre-trained SSD network for detection Download a pre-trained network  We are trying to provide PyTorch state_dicts (dict of weight tensors) of the latest SSD model definitions trained on different datasets. Currently, we provide the following PyTorch models:  SSD300 trained on VOC0712 (newest PyTorch weights)  https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth   SSD300 trained on VOC0712 (original Caffe weights)  https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth     Our goal is to reproduce this table from the original paper    Try the demo notebook  Make sure you have jupyter notebook installed. Two alternatives for installing jupyter notebook:   If you installed PyTorch with conda (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run): jupyter notebook   If using pip:     # make sure pip is upgraded pip3 install --upgrade pip # install jupyter notebook pip install jupyter # Run this inside ssd.pytorch jupyter notebook  Now navigate to demo/demo.ipynb at http://localhost:8888 (by default) and have at it!  Try the webcam demo  Works on CPU (may have to tweak cv2.waitkey for optimal fps) or on an NVIDIA GPU This demo currently requires opencv2+ w/ python bindings and an onboard webcam  You can change the default webcam in demo/live.py   Install the imutils package to leverage multi-threading on CPU:  pip install imutils   Running python -m demo.live opens the webcam and begins detecting!  TODO We have accumulated the following to-do list, which we hope to complete in the near future  Still to come:   Support for the MS COCO dataset  Support for SSD512 training and testing  Support for training on custom datasets    Authors  Max deGroot Ellis Brown  Note: Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible. References  Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" ECCV2016. Original Implementation (CAFFE) A huge thank you to Alex Koltun and his team at Webyclip for their help in finishing the data augmentation portion. A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):  Chainer, Keras, MXNet, Tensorflow    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.image-net.org/", "http://pytorch.org/", "http://host.robots.ox.ac.uk/pascal/VOC/", "http://localhost:8097/", "http://localhost:8888", "http://www.webyclip.com", "http://mscoco.org/", "http://pytorch.org/docs/torchvision/datasets.html", "http://jupyter.readthedocs.io/en/latest/install.html", "http://github.com/ellisbrown"], "reference_list": ["http://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1409.1556", "http://arxiv.org/abs/1512.02325"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc60"}, "repo_url": "https://github.com/Apollo2Mars/Workspace-of-Machine-Reading-Comprehension", "repo_name": "Workspace-of-Machine-Reading-Comprehension", "repo_full_name": "Apollo2Mars/Workspace-of-Machine-Reading-Comprehension", "repo_owner": "Apollo2Mars", "repo_desc": "Collect some algorithm of Machine Reading Comprehension ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T12:44:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T12:37:14Z", "homepage": null, "size": 294, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 189411058, "is_fork": false, "readme_text": "[TOC] Dataset DuReader Dataset DuReader is a new large-scale real-world and human sourced MRC dataset in Chinese. DuReader focuses on real-world open-domain question answering. The advantages of DuReader over existing datasets are concluded as follows:  Real question Real article Real answer Real application scenario Rich annotation  SQuAD Dataset  links  MARCO V2   MS MARCO (Microsoft Machine Reading Comprehension) is an English dataset focused on machine reading comprehension and question answering. The design of MS MARCO and DuReader is similar. It is worthwhile examining the MRC systems on both Chinese (DuReader) and English (MS MARCO) datasets.   You can download MS MARCO V2 data, and run the following scripts to convert the data from MS MARCO V2 format to DuReader format. Then, you can run and evaluate our DuReader baselines or your DuReader systems on MS MARCO data.   Process Download the Dataset To Download DuReader dataset: cd data && bash download.sh  For more details about DuReader dataset please refer to DuReader Homepage. Download Thirdparty Dependencies We use Bleu and Rouge as evaluation metrics, the calculation of these metrics relies on the scoring scripts under \"https://github.com/tylin/coco-caption\", to download them, run: cd utils && bash download_thirdparty.sh  Preprocess the Data After the dataset is downloaded, there is still some work to do to run the baseline systems. DuReader dataset offers rich amount of documents for every user question, the documents are too long for popular RC models to cope with. In our baseline models, we preprocess the train set and development set data by selecting the paragraph that is most related to the answer string, while for inferring(no available golden answer), we select the paragraph that is most related to the question string. The preprocessing strategy is implemented in utils/preprocess.py. To preprocess the raw data, you should first segment 'question', 'title', 'paragraphs' and then store the segemented result into 'segmented_question', 'segmented_title', 'segmented_paragraphs' like the downloaded preprocessed data, then run: cat data/raw/trainset/search.train.json | python utils/preprocess.py > data/preprocessed/trainset/search.train.json  The preprocessed data can be automatically downloaded by data/download.sh, and is stored in data/preprocessed, the raw data before preprocessing is under data/raw. Framework Run Tensorflow We also implements the BIDAF and Match-LSTM models based on Tensorflow 1.0. You can refer to the official guide for the installation of Tensorflow. The complete options for running our Tensorflow program can be accessed by using python run.py -h. Here we demonstrate a typical workflow as follows: Preparation Before training the model, we have to make sure that the data is ready. For preparation, we will check the data files, make directories and extract a vocabulary for later use. You can run the following command to do this with a specified task name: python run.py --prepare  You can specify the files for train/dev/test by setting the train_files/dev_files/test_files. By default, we use the data in data/demo/ Training To train the reading comprehension model, you can specify the model type by using --algo [BIDAF|MLSTM] and you can also set the hyper-parameters such as the learning rate by using --learning_rate NUM. For example, to train a BIDAF model for 10 epochs, you can run: python run.py --train --algo BIDAF --epochs 10  The training process includes an evaluation on the dev set after each training epoch. By default, the model with the least Bleu-4 score on the dev set will be saved. Evaluation To conduct a single evaluation on the dev set with the the model already trained, you can run the following command: python run.py --evaluate --algo BIDAF  Prediction You can also predict answers for the samples in some files using the following command: python run.py --predict --algo BIDAF --test_files ../data/demo/devset/search.dev.json   By default, the results are saved at ../data/results/ folder. You can change this by specifying --result_dir DIR_PATH. Run PyTorch Run PaddlePaddle Run Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.msmarco.org/dataset.aspx"], "reference_list": []}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc61"}, "repo_url": "https://github.com/jiajunhua/eriklindernoren-PyTorch-GAN", "repo_name": "eriklindernoren-PyTorch-GAN", "repo_full_name": "jiajunhua/eriklindernoren-PyTorch-GAN", "repo_owner": "jiajunhua", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T08:16:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T08:15:10Z", "homepage": null, "size": 62034, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189373727, "is_fork": false, "readme_text": " PyTorch-GAN Collection of PyTorch implementations of Generative Adversarial Network varieties presented in research papers. Model architectures will not always mirror the ones proposed in the papers, but I have chosen to focus on getting the core ideas covered instead of getting every layer configuration right. Contributions and suggestions of GANs to implement are very welcomed. See also: Keras-GAN Table of Contents  Installation Implementations  Auxiliary Classifier GAN Adversarial Autoencoder BEGAN BicycleGAN Boundary-Seeking GAN Conditional GAN Context-Conditional GAN Context Encoder Coupled GAN CycleGAN Deep Convolutional GAN DiscoGAN DRAGAN DualGAN Energy-Based GAN Enhanced Super-Resolution GAN GAN InfoGAN Least Squares GAN MUNIT Pix2Pix PixelDA Relativistic GAN Semi-Supervised GAN Softmax GAN StarGAN Super-Resolution GAN UNIT Wasserstein GAN Wasserstein GAN GP Wasserstein GAN DIV    Installation $ git clone https://github.com/eriklindernoren/PyTorch-GAN $ cd PyTorch-GAN/ $ sudo pip3 install -r requirements.txt  Implementations Auxiliary Classifier GAN Auxiliary Classifier Generative Adversarial Network Authors Augustus Odena, Christopher Olah, Jonathon Shlens Abstract Synthesizing high resolution photorealistic images has been a long-standing challenge in machine learning. In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128x128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128x128 samples are more than twice as discriminable as artificially resized 32x32 samples. In addition, 84.7% of the classes have samples exhibiting diversity comparable to real ImageNet data. [Paper] [Code] Run Example $ cd implementations/acgan/ $ python3 acgan.py     Adversarial Autoencoder Adversarial Autoencoder Authors Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow, Brendan Frey Abstract n this paper, we propose the \"adversarial autoencoder\" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks. [Paper] [Code] Run Example $ cd implementations/aae/ $ python3 aae.py  BEGAN BEGAN: Boundary Equilibrium Generative Adversarial Networks Authors David Berthelot, Thomas Schumm, Luke Metz Abstract We propose a new equilibrium enforcing method paired with a loss derived from the Wasserstein distance for training auto-encoder based Generative Adversarial Networks. This method balances the generator and discriminator during training. Additionally, it provides a new approximate convergence measure, fast and stable training and high visual quality. We also derive a way of controlling the trade-off between image diversity and visual quality. We focus on the image generation task, setting a new milestone in visual quality, even at higher resolutions. This is achieved while using a relatively simple model architecture and a standard training procedure. [Paper] [Code] Run Example $ cd implementations/began/ $ python3 began.py  BicycleGAN Toward Multimodal Image-to-Image Translation Authors Jun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A. Efros, Oliver Wang, Eli Shechtman Abstract Many image-to-image translation problems are ambiguous, as a single input image may correspond to multiple possible outputs. In this work, we aim to model a \\emph{distribution} of possible outputs in a conditional generative modeling setting. The ambiguity of the mapping is distilled in a low-dimensional latent vector, which can be randomly sampled at test time. A generator learns to map the given input, combined with this latent code, to the output. We explicitly encourage the connection between output and the latent code to be invertible. This helps prevent a many-to-one mapping from the latent code to the output during training, also known as the problem of mode collapse, and produces more diverse results. We explore several variants of this approach by employing different training objectives, network architectures, and methods of injecting the latent code. Our proposed method encourages bijective consistency between the latent encoding and output modes. We present a systematic comparison of our method and other variants on both perceptual realism and diversity. [Paper] [Code]    Run Example $ cd data/ $ bash download_pix2pix_dataset.sh edges2shoes $ cd ../implementations/bicyclegan/ $ python3 bicyclegan.py          Various style translations by varying the latent code.  Boundary-Seeking GAN Boundary-Seeking Generative Adversarial Networks Authors R Devon Hjelm, Athul Paul Jacob, Tong Che, Adam Trischler, Kyunghyun Cho, Yoshua Bengio Abstract Generative adversarial networks (GANs) are a learning framework that rely on training a discriminator to estimate a measure of difference between a target and generated distributions. GANs, as normally formulated, rely on the generated samples being completely differentiable w.r.t. the generative parameters, and thus do not work for discrete data. We introduce a method for training GANs with discrete data that uses the estimated difference measure from the discriminator to compute importance weights for generated samples, thus providing a policy gradient for training the generator. The importance weights have a strong connection to the decision boundary of the discriminator, and we call our method boundary-seeking GANs (BGANs). We demonstrate the effectiveness of the proposed algorithm with discrete image and character-based natural language generation. In addition, the boundary-seeking objective extends to continuous data, which can be used to improve stability of training, and we demonstrate this on Celeba, Large-scale Scene Understanding (LSUN) bedrooms, and Imagenet without conditioning. [Paper] [Code] Run Example $ cd implementations/bgan/ $ python3 bgan.py  Conditional GAN Conditional Generative Adversarial Nets Authors Mehdi Mirza, Simon Osindero Abstract Generative Adversarial Nets [8] were recently introduced as a novel way to train generative models. In this work we introduce the conditional version of generative adversarial nets, which can be constructed by simply feeding the data, y, we wish to condition on to both the generator and discriminator. We show that this model can generate MNIST digits conditioned on class labels. We also illustrate how this model could be used to learn a multi-modal model, and provide preliminary examples of an application to image tagging in which we demonstrate how this approach can generate descriptive tags which are not part of training labels. [Paper] [Code] Run Example $ cd implementations/cgan/ $ python3 cgan.py     Context-Conditional GAN Semi-Supervised Learning with Context-Conditional Generative Adversarial Networks Authors Emily Denton, Sam Gross, Rob Fergus Abstract We introduce a simple semi-supervised learning approach for images based on in-painting using an adversarial loss. Images with random patches removed are presented to a generator whose task is to fill in the hole, based on the surrounding pixels. The in-painted images are then presented to a discriminator network that judges if they are real (unaltered training images) or not. This task acts as a regularizer for standard supervised training of the discriminator. Using our approach we are able to directly train large VGG-style networks in a semi-supervised fashion. We evaluate on STL-10 and PASCAL datasets, where our approach obtains performance comparable or superior to existing methods. [Paper] [Code] Run Example $ cd implementations/ccgan/ $ python3 ccgan.py  Context Encoder Context Encoders: Feature Learning by Inpainting Authors Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, Alexei A. Efros Abstract We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods. [Paper] [Code] Run Example $ cd implementations/context_encoder/ <follow steps at the top of context_encoder.py> $ python3 context_encoder.py          Rows: Masked | Inpainted | Original | Masked | Inpainted | Original  Coupled GAN Coupled Generative Adversarial Networks Authors Ming-Yu Liu, Oncel Tuzel Abstract We propose coupled generative adversarial network (CoGAN) for learning a joint distribution of multi-domain images. In contrast to the existing approaches, which require tuples of corresponding images in different domains in the training set, CoGAN can learn a joint distribution without any tuple of corresponding images. It can learn a joint distribution with just samples drawn from the marginal distributions. This is achieved by enforcing a weight-sharing constraint that limits the network capacity and favors a joint distribution solution over a product of marginal distributions one. We apply CoGAN to several joint distribution learning tasks, including learning a joint distribution of color and depth images, and learning a joint distribution of face images with different attributes. For each task it successfully learns the joint distribution without any tuple of corresponding images. We also demonstrate its applications to domain adaptation and image transformation. [Paper] [Code] Run Example $ cd implementations/cogan/ $ python3 cogan.py          Generated MNIST and MNIST-M images  CycleGAN Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks Authors Jun-Yan Zhu, Taesung Park, Phillip Isola, Alexei A. Efros Abstract Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G:X\u2192Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F:Y\u2192X and introduce a cycle consistency loss to push F(G(X))\u2248X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach. [Paper] [Code]    Run Example $ cd data/ $ bash download_cyclegan_dataset.sh monet2photo $ cd ../implementations/cyclegan/ $ python3 cyclegan.py --dataset_name monet2photo          Monet to photo translations.  Deep Convolutional GAN Deep Convolutional Generative Adversarial Network Authors Alec Radford, Luke Metz, Soumith Chintala Abstract In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations. [Paper] [Code] Run Example $ cd implementations/dcgan/ $ python3 dcgan.py     DiscoGAN Learning to Discover Cross-Domain Relations with Generative Adversarial Networks Authors Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, Jiwon Kim Abstract While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. [Paper] [Code]    Run Example $ cd data/ $ bash download_pix2pix_dataset.sh edges2shoes $ cd ../implementations/discogan/ $ python3 discogan.py --dataset_name edges2shoes          Rows from top to bottom: (1) Real image from domain A (2) Translated image from      domain A (3) Reconstructed image from domain A (4) Real image from domain B (5)      Translated image from domain B (6) Reconstructed image from domain B  DRAGAN On Convergence and Stability of GANs Authors Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira Abstract We propose studying GAN training dynamics as regret minimization, which is in contrast to the popular view that there is consistent minimization of a divergence between real and generated distributions. We analyze the convergence of GAN training from this new point of view to understand why mode collapse happens. We hypothesize the existence of undesirable local equilibria in this non-convex game to be responsible for mode collapse. We observe that these local equilibria often exhibit sharp gradients of the discriminator function around some real data points. We demonstrate that these degenerate local equilibria can be avoided with a gradient penalty scheme called DRAGAN. We show that DRAGAN enables faster training, achieves improved stability with fewer mode collapses, and leads to generator networks with better modeling performance across a variety of architectures and objective functions. [Paper] [Code] Run Example $ cd implementations/dragan/ $ python3 dragan.py  DualGAN DualGAN: Unsupervised Dual Learning for Image-to-Image Translation Authors Zili Yi, Hao Zhang, Ping Tan, Minglun Gong Abstract Conditional Generative Adversarial Networks (GANs) for cross-domain image-to-image translation have made much progress recently. Depending on the task complexity, thousands to millions of labeled image pairs are needed to train a conditional GAN. However, human labeling is expensive, even impractical, and large quantities of data may not always be available. Inspired by dual learning from natural language translation, we develop a novel dual-GAN mechanism, which enables image translators to be trained from two sets of unlabeled images from two domains. In our architecture, the primal GAN learns to translate images from domain U to those in domain V, while the dual GAN learns to invert the task. The closed loop made by the primal and dual tasks allows images from either domain to be translated and then reconstructed. Hence a loss function that accounts for the reconstruction error of images can be used to train the translators. Experiments on multiple image translation tasks with unlabeled data show considerable performance gain of DualGAN over a single GAN. For some tasks, DualGAN can even achieve comparable or slightly better results than conditional GAN trained on fully labeled data. [Paper] [Code] Run Example $ cd data/ $ bash download_pix2pix_dataset.sh facades $ cd ../implementations/dualgan/ $ python3 dualgan.py --dataset_name facades  Energy-Based GAN Energy-based Generative Adversarial Network Authors Junbo Zhao, Michael Mathieu, Yann LeCun Abstract We introduce the \"Energy-based Generative Adversarial Network\" model (EBGAN) which views the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. Similar to the probabilistic GANs, a generator is seen as being trained to produce contrastive samples with minimal energies, while the discriminator is trained to assign high energies to these generated samples. Viewing the discriminator as an energy function allows to use a wide variety of architectures and loss functionals in addition to the usual binary classifier with logistic output. Among them, we show one instantiation of EBGAN framework as using an auto-encoder architecture, with the energy being the reconstruction error, in place of the discriminator. We show that this form of EBGAN exhibits more stable behavior than regular GANs during training. We also show that a single-scale architecture can be trained to generate high-resolution images. [Paper] [Code] Run Example $ cd implementations/ebgan/ $ python3 ebgan.py  Enhanced Super-Resolution GAN ESRGAN: Enhanced Super-Resolution Generative Adversarial Networks Authors Xintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Chen Change Loy, Yu Qiao, Xiaoou Tang Abstract The Super-Resolution Generative Adversarial Network (SRGAN) is a seminal work that is capable of generating realistic textures during single image super-resolution. However, the hallucinated details are often accompanied with unpleasant artifacts. To further enhance the visual quality, we thoroughly study three key components of SRGAN - network architecture, adversarial loss and perceptual loss, and improve each of them to derive an Enhanced SRGAN (ESRGAN). In particular, we introduce the Residual-in-Residual Dense Block (RRDB) without batch normalization as the basic network building unit. Moreover, we borrow the idea from relativistic GAN to let the discriminator predict relative realness instead of the absolute value. Finally, we improve the perceptual loss by using the features before activation, which could provide stronger supervision for brightness consistency and texture recovery. Benefiting from these improvements, the proposed ESRGAN achieves consistently better visual quality with more realistic and natural textures than SRGAN and won the first place in the PIRM2018-SR Challenge. The code is available at this https URL. [Paper] [Code] Run Example $ cd implementations/esrgan/ <follow steps at the top of esrgan.py> $ python3 esrgan.py          Nearest Neighbor Upsampling | ESRGAN  GAN Generative Adversarial Network Authors Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, Yoshua Bengio Abstract We propose a new framework for estimating generative models via an adversarial process, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. [Paper] [Code] Run Example $ cd implementations/gan/ $ python3 gan.py     InfoGAN InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets Authors Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever, Pieter Abbeel Abstract This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound to the mutual information objective that can be optimized efficiently, and show that our training procedure can be interpreted as a variation of the Wake-Sleep algorithm. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing fully supervised methods. [Paper] [Code] Run Example $ cd implementations/infogan/ $ python3 infogan.py          Result of varying categorical latent variable by column.          Result of varying continuous latent variable by row.  Least Squares GAN Least Squares Generative Adversarial Networks Authors Xudong Mao, Qing Li, Haoran Xie, Raymond Y.K. Lau, Zhen Wang, Stephen Paul Smolley Abstract Unsupervised learning with generative adversarial networks (GANs) has proven hugely successful. Regular GANs hypothesize the discriminator as a classifier with the sigmoid cross entropy loss function. However, we found that this loss function may lead to the vanishing gradients problem during the learning process. To overcome such a problem, we propose in this paper the Least Squares Generative Adversarial Networks (LSGANs) which adopt the least squares loss function for the discriminator. We show that minimizing the objective function of LSGAN yields minimizing the Pearson \u03c72 divergence. There are two benefits of LSGANs over regular GANs. First, LSGANs are able to generate higher quality images than regular GANs. Second, LSGANs perform more stable during the learning process. We evaluate LSGANs on five scene datasets and the experimental results show that the images generated by LSGANs are of better quality than the ones generated by regular GANs. We also conduct two comparison experiments between LSGANs and regular GANs to illustrate the stability of LSGANs. [Paper] [Code] Run Example $ cd implementations/lsgan/ $ python3 lsgan.py  MUNIT Multimodal Unsupervised Image-to-Image Translation Authors Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz Abstract Unsupervised image-to-image translation is an important and challenging problem in computer vision. Given an image in the source domain, the goal is to learn the conditional distribution of corresponding images in the target domain, without seeing any pairs of corresponding images. While this conditional distribution is inherently multimodal, existing approaches make an overly simplified assumption, modeling it as a deterministic one-to-one mapping. As a result, they fail to generate diverse outputs from a given source domain image. To address this limitation, we propose a Multimodal Unsupervised Image-to-image Translation (MUNIT) framework. We assume that the image representation can be decomposed into a content code that is domain-invariant, and a style code that captures domain-specific properties. To translate an image to another domain, we recombine its content code with a random style code sampled from the style space of the target domain. We analyze the proposed framework and establish several theoretical results. Extensive experiments with comparisons to the state-of-the-art approaches further demonstrates the advantage of the proposed framework. Moreover, our framework allows users to control the style of translation outputs by providing an example style image. Code and pretrained models are available at this https URL [Paper] [Code] Run Example $ cd data/ $ bash download_pix2pix_dataset.sh edges2shoes $ cd ../implementations/munit/ $ python3 munit.py --dataset_name edges2shoes          Results by varying the style code.  Pix2Pix Unpaired Image-to-Image Translation with Conditional Adversarial Networks Authors Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros Abstract We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. Indeed, since the release of the pix2pix software associated with this paper, a large number of internet users (many of them artists) have posted their own experiments with our system, further demonstrating its wide applicability and ease of adoption without the need for parameter tweaking. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either. [Paper] [Code]    Run Example $ cd data/ $ bash download_pix2pix_dataset.sh facades $ cd ../implementations/pix2pix/ $ python3 pix2pix.py --dataset_name facades          Rows from top to bottom: (1) The condition for the generator (2) Generated image      based of condition (3) The true corresponding image to the condition  PixelDA Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks Authors Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, Dilip Krishnan Abstract Collecting well-annotated image datasets to train modern machine learning algorithms is prohibitively expensive for many tasks. One appealing alternative is rendering synthetic data where ground-truth annotations are generated automatically. Unfortunately, models trained purely on rendered images often fail to generalize to real images. To address this shortcoming, prior work introduced unsupervised domain adaptation algorithms that attempt to map representations between the two domains or learn to extract features that are domain-invariant. In this work, we present a new approach that learns, in an unsupervised manner, a transformation in the pixel space from one domain to the other. Our generative adversarial network (GAN)-based method adapts source-domain images to appear as if drawn from the target domain. Our approach not only produces plausible samples, but also outperforms the state-of-the-art on a number of unsupervised domain adaptation scenarios by large margins. Finally, we demonstrate that the adaptation process generalizes to object classes unseen during training. [Paper] [Code] MNIST to MNIST-M Classification Trains a classifier on images that have been translated from the source domain (MNIST) to the target domain (MNIST-M) using the annotations of the source domain images. The classification network is trained jointly with the generator network to optimize the generator for both providing a proper domain translation and also for preserving the semantics of the source domain image. The classification network trained on translated images is compared to the naive solution of training a classifier on MNIST and evaluating it on MNIST-M. The naive model manages a 55% classification accuracy on MNIST-M while the one trained during domain adaptation achieves a 95% classification accuracy. $ cd implementations/pixelda/ $ python3 pixelda.py     Method Accuracy     Naive 55%   PixelDA 95%            Rows from top to bottom: (1) Real images from MNIST (2) Translated images from      MNIST to MNIST-M (3) Examples of images from MNIST-M  Relativistic GAN The relativistic discriminator: a key element missing from standard GAN Authors Alexia Jolicoeur-Martineau Abstract In standard generative adversarial network (SGAN), the discriminator estimates the probability that the input data is real. The generator is trained to increase the probability that fake data is real. We argue that it should also simultaneously decrease the probability that real data is real because 1) this would account for a priori knowledge that half of the data in the mini-batch is fake, 2) this would be observed with divergence minimization, and 3) in optimal settings, SGAN would be equivalent to integral probability metric (IPM) GANs. We show that this property can be induced by using a relativistic discriminator which estimate the probability that the given real data is more realistic than a randomly sampled fake data. We also present a variant in which the discriminator estimate the probability that the given real data is more realistic than fake data, on average. We generalize both approaches to non-standard GAN loss functions and we refer to them respectively as Relativistic GANs (RGANs) and Relativistic average GANs (RaGANs). We show that IPM-based GANs are a subset of RGANs which use the identity function. Empirically, we observe that 1) RGANs and RaGANs are significantly more stable and generate higher quality data samples than their non-relativistic counterparts, 2) Standard RaGAN with gradient penalty generate data of better quality than WGAN-GP while only requiring a single discriminator update per generator update (reducing the time taken for reaching the state-of-the-art by 400%), and 3) RaGANs are able to generate plausible high resolutions images (256x256) from a very small sample (N=2011), while GAN and LSGAN cannot; these images are of significantly better quality than the ones generated by WGAN-GP and SGAN with spectral normalization. [Paper] [Code] Run Example $ cd implementations/relativistic_gan/ $ python3 relativistic_gan.py                 # Relativistic Standard GAN $ python3 relativistic_gan.py --rel_avg_gan   # Relativistic Average GAN  Semi-Supervised GAN Semi-Supervised Generative Adversarial Network Authors Augustus Odena Abstract We extend Generative Adversarial Networks (GANs) to the semi-supervised context by forcing the discriminator network to output class labels. We train a generative model G and a discriminator D on a dataset with inputs belonging to one of N classes. At training time, D is made to predict which of N+1 classes the input belongs to, where an extra class is added to correspond to the outputs of G. We show that this method can be used to create a more data-efficient classifier and that it allows for generating higher quality samples than a regular GAN. [Paper] [Code] Run Example $ cd implementations/sgan/ $ python3 sgan.py  Softmax GAN Softmax GAN Authors Min Lin Abstract Softmax GAN is a novel variant of Generative Adversarial Network (GAN). The key idea of Softmax GAN is to replace the classification loss in the original GAN with a softmax cross-entropy loss in the sample space of one single batch. In the adversarial learning of N real training samples and M generated samples, the target of discriminator training is to distribute all the probability mass to the real samples, each with probability 1M, and distribute zero probability to generated data. In the generator training phase, the target is to assign equal probability to all data points in the batch, each with probability 1M+N. While the original GAN is closely related to Noise Contrastive Estimation (NCE), we show that Softmax GAN is the Importance Sampling version of GAN. We futher demonstrate with experiments that this simple change stabilizes GAN training. [Paper] [Code] Run Example $ cd implementations/softmax_gan/ $ python3 softmax_gan.py  StarGAN StarGAN: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation Authors Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, Jaegul Choo Abstract Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks. [Paper] [Code] Run Example $ cd implementations/stargan/ <follow steps at the top of stargan.py> $ python3 stargan.py          Original | Black Hair | Blonde Hair | Brown Hair | Gender Flip | Aged  Super-Resolution GAN Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network Authors Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, Wenzhe Shi Abstract Despite the breakthroughs in accuracy and speed of single image super-resolution using faster and deeper convolutional neural networks, one central problem remains largely unsolved: how do we recover the finer texture details when we super-resolve at large upscaling factors? The behavior of optimization-based super-resolution methods is principally driven by the choice of the objective function. Recent work has largely focused on minimizing the mean squared reconstruction error. The resulting estimates have high peak signal-to-noise ratios, but they are often lacking high-frequency details and are perceptually unsatisfying in the sense that they fail to match the fidelity expected at the higher resolution. In this paper, we present SRGAN, a generative adversarial network (GAN) for image super-resolution (SR). To our knowledge, it is the first framework capable of inferring photo-realistic natural images for 4x upscaling factors. To achieve this, we propose a perceptual loss function which consists of an adversarial loss and a content loss. The adversarial loss pushes our solution to the natural image manifold using a discriminator network that is trained to differentiate between the super-resolved images and original photo-realistic images. In addition, we use a content loss motivated by perceptual similarity instead of similarity in pixel space. Our deep residual network is able to recover photo-realistic textures from heavily downsampled images on public benchmarks. An extensive mean-opinion-score (MOS) test shows hugely significant gains in perceptual quality using SRGAN. The MOS scores obtained with SRGAN are closer to those of the original high-resolution images than to those obtained with any state-of-the-art method. [Paper] [Code]    Run Example $ cd implementations/srgan/ <follow steps at the top of srgan.py> $ python3 srgan.py          Nearest Neighbor Upsampling | SRGAN  UNIT Unsupervised Image-to-Image Translation Networks Authors Ming-Yu Liu, Thomas Breuel, Jan Kautz Abstract Unsupervised image-to-image translation aims at learning a joint distribution of images in different domains by using images from the marginal distributions in individual domains. Since there exists an infinite set of joint distributions that can arrive the given marginal distributions, one could infer nothing about the joint distribution from the marginal distributions without additional assumptions. To address the problem, we make a shared-latent space assumption and propose an unsupervised image-to-image translation framework based on Coupled GANs. We compare the proposed framework with competing approaches and present high quality image translation results on various challenging unsupervised image translation tasks, including street scene image translation, animal image translation, and face image translation. We also apply the proposed framework to domain adaptation and achieve state-of-the-art performance on benchmark datasets. Code and additional results are available in this https URL. [Paper] [Code] Run Example $ cd data/ $ bash download_cyclegan_dataset.sh apple2orange $ cd implementations/unit/ $ python3 unit.py --dataset_name apple2orange  Wasserstein GAN Wasserstein GAN Authors Martin Arjovsky, Soumith Chintala, L\u00e9on Bottou Abstract We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to other distances between distributions. [Paper] [Code] Run Example $ cd implementations/wgan/ $ python3 wgan.py  Wasserstein GAN GP Improved Training of Wasserstein GANs Authors Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, Aaron Courville Abstract Generative Adversarial Networks (GANs) are powerful generative models, but suffer from training instability. The recently proposed Wasserstein GAN (WGAN) makes progress toward stable training of GANs, but sometimes can still generate only low-quality samples or fail to converge. We find that these problems are often due to the use of weight clipping in WGAN to enforce a Lipschitz constraint on the critic, which can lead to undesired behavior. We propose an alternative to clipping weights: penalize the norm of gradient of the critic with respect to its input. Our proposed method performs better than standard WGAN and enables stable training of a wide variety of GAN architectures with almost no hyperparameter tuning, including 101-layer ResNets and language models over discrete data. We also achieve high quality generations on CIFAR-10 and LSUN bedrooms. [Paper] [Code] Run Example $ cd implementations/wgan_gp/ $ python3 wgan_gp.py     Wasserstein GAN DIV Wasserstein Divergence for GANs Authors Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, Luc Van Gool Abstract In many domains of computer vision, generative adversarial networks (GANs) have achieved great success, among which the fam- ily of Wasserstein GANs (WGANs) is considered to be state-of-the-art due to the theoretical contributions and competitive qualitative performance. However, it is very challenging to approximate the k-Lipschitz constraint required by the Wasserstein-1 metric (W-met). In this paper, we propose a novel Wasserstein divergence (W-div), which is a relaxed version of W-met and does not require the k-Lipschitz constraint.As a concrete application, we introduce a Wasserstein divergence objective for GANs (WGAN-div), which can faithfully approximate W-div through optimization. Under various settings, including progressive growing training, we demonstrate the stability of the proposed WGAN-div owing to its theoretical and practical advantages over WGANs. Also, we study the quantitative and visual performance of WGAN-div on standard image synthesis benchmarks, showing the superior performance of WGAN-div compared to the state-of-the-art methods. [Paper] [Code] Run Example $ cd implementations/wgan_div/ $ python3 wgan_div.py     ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1610.09585", "https://arxiv.org/abs/1511.05644", "https://arxiv.org/abs/1703.10717", "https://arxiv.org/abs/1711.11586", "https://arxiv.org/abs/1702.08431", "https://arxiv.org/abs/1411.1784", "https://arxiv.org/abs/1611.06430", "https://arxiv.org/abs/1604.07379", "https://arxiv.org/abs/1606.07536", "https://arxiv.org/abs/1703.10593", "https://arxiv.org/abs/1511.06434", "https://arxiv.org/abs/1703.05192", "https://arxiv.org/abs/1705.07215", "https://arxiv.org/abs/1704.02510", "https://arxiv.org/abs/1609.03126", "https://arxiv.org/abs/1809.00219", "https://arxiv.org/abs/1406.2661", "https://arxiv.org/abs/1606.03657", "https://arxiv.org/abs/1611.04076", "https://arxiv.org/abs/1804.04732", "https://arxiv.org/abs/1611.07004", "https://arxiv.org/abs/1612.05424", "https://arxiv.org/abs/1807.00734", "https://arxiv.org/abs/1606.01583", "https://arxiv.org/abs/1704.06191", "https://arxiv.org/abs/1711.09020", "https://arxiv.org/abs/1609.04802", "https://arxiv.org/abs/1703.00848", "https://arxiv.org/abs/1701.07875", "https://arxiv.org/abs/1704.00028", "https://arxiv.org/abs/1712.01026"]}, {"_id": {"$oid": "5cf424ba7eb8d60848a3dc62"}, "repo_url": "https://github.com/fzh0627/Mnist", "repo_name": "Mnist", "repo_full_name": "fzh0627/Mnist", "repo_owner": "fzh0627", "repo_desc": "Mnist\u624b\u5199\u6570\u5b57\u8bc6\u522b", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-30T14:49:25Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-30T02:41:34Z", "homepage": null, "size": 13, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189333142, "is_fork": false, "readme_text": "Mnist\u6570\u636e\u96c6\u8bc6\u522b\u62a5\u544a 1. Mnist\u6570\u636e\u96c6 1.1 \u6570\u636e\u96c6\u4ecb\u7ecd \u7531Yann LeCun, Courant Institute, NYU\u63d0\u51fa\u3002 The MNIST database of handwritten digits, available from this page, has a training set of 60,000 examples, and a test set of 10,000 examples. It is a subset of a larger set available from NIST. The digits have been size-normalized and centered in a fixed-size image.  \u8bad\u7ec3\u6837\u672c\u96c660000\uff0c\u6d4b\u8bd5\u6837\u672c\u96c610000 \u6bcf\u4e00\u5f20\u56fe\u7247\u5927\u5c0f\u4e3a[28, 28]\uff0c\u7070\u5ea6\u503c\u8303\u56f4\u4e3a[0, 255] \u6837\u672c\u6807\u7b7e\u8303\u56f4\u4e3a0-9\u7684\u662f\u4e2a\u6574\u6570\uff08scalar\uff09\u96c6\u5408  1.2 TensorFlow2.0\u4e0bMnist\u6570\u636e\u5bfc\u5165 \u672c\u6b21\u5927\u4f5c\u4e1a\u662f\u5728\u6df1\u5ea6\u5b66\u4e60\u6846\u67b6TensorFlow2.0\u4e0b\u8bbe\u8ba1\u5b9e\u73b0\u5b8c\u6210\u7684\uff0c\u6240\u4ee5\u7740\u91cd\u4ee5TensorFlow\u4e3a\u6838\u5fc3\uff0c\u6765\u8be6\u7ec6\u8bb2\u89e3\u5173\u4e8eMnist\u6570\u636e\u96c6\u5408\u7684\u5bfc\u5165\u64cd\u4f5c\u3002 \u5728TensorFlow2.0\u4e2d\u5173\u4e8e\u5e38\u7528\u7684\u6570\u636e\u96c6\uff08\u6bd4\u5982mnist\u3001cifar10\u3001cifar100\u3001fashion_mnist\u7b49\uff09\u6709\u4e00\u4e2a\u5b9e\u7528API\uff0c\u53ef\u4ee5\u5f88\u65b9\u4fbf\u5bfc\u5165\u5728\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u4e2d\u7ecf\u5e38\u4f1a\u7528\u5230\u7684\u6570\u636e\u96c6\u3002 \u6570\u636e\u5bfc\u5165\u7684\u4ee3\u7801\u5982\u4e0b\uff1a (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()  \u5728\u7b2c\u4e00\u6b21\u5bfc\u5165\u6b64\u6570\u636e\u96c6\u65f6\uff0c\u9700\u8981\u4eceGoogle\u7f51\u7ad9\u4e0a\u4e0b\u8f7d\u6570\u636e\u96c6\uff0c\u5728\u4e0b\u8f7d\u4e4b\u540e\u7684\u6570\u636e\u5bfc\u5165\u4e2d\u5c31\u4e0d\u9700\u8981\u518d\u6b21\u4e0b\u8f7d\uff0c\u8c03\u7528\u4ee5\u4e0a\u4ee3\u7801\u76f4\u63a5\u5c31\u53ef\u4ee5\u5bfc\u5165\u5230\u7a0b\u5e8f\u5f53\u4e2d\u3002 1.3 Mnist\u6570\u636e\u9884\u5904\u7406 TensorFlow\u4e2d\u6570\u636e\u662f\u4ee5\u5f20\u91cfTensor\u7684\u5f62\u5f0f\u5728\u7a0b\u5e8f\u5185\u90e8\u8fdb\u884c\u4f20\u64ad\uff0c\u6240\u4ee5\u9700\u8981\u5bf9\u4e0a\u8ff0\u5bfc\u5165\u5230\u7a0b\u5e8f\u4e2d\u7684\u6570\u636e\u96c6\u5408\u8fdb\u884c\u9884\u5904\u7406\u3002\u9884\u5904\u7406\u4e3b\u8981\u5206\u4e3a\u521b\u9020\u6570\u636e\u96c6\uff08database\uff09\u3001\u6570\u636e\u96c6\u6279\u5904\u7406\u5316\uff08batch\uff09\u3001\u6807\u7b7e\u6570\u636e\u9884\u5904\u7406\uff08onehot\uff09\u3001\u6570\u636e\u96c6\u6253\u4e71\u64cd\u4f5c\uff08shuffle\uff09 \u6807\u7b7e\u6570\u636e\u9884\u5904\u7406 Mnist\u4e2d\u7684\u6807\u7b7e\u6570\u636e\u662f\u4e00\u4e2a0-9\u7684\u6574\u6570\u5f62\u5f0f\uff0c\u4e3a\u4e86\u65b9\u4fbf\u8ba1\u7b97\u548c\u7406\u89e3\u6211\u4eec\u9996\u5148\u5c06\u4e00\u4e2a\u6574\u6570\uff08scalar\uff09\u6570\u636e\u8f6c\u6362\u4e3a\u4e00\u4e2a10\u7ef4\u7684\u4e00\u4e2a\u5411\u91cf\uff0c\u6bcf\u4e00\u4e2a\u6807\u7b7e\u5bf9\u5e94\u7684\u4f4d\u7f6e\u6570\u636e\u4e3a1\uff0c\u5176\u4ed6\u4f4d\u7f6e\u4e3a0\uff0c\u8fd9\u6837\u505a\u53ef\u4ee5\u65b9\u4fbf\u7406\u89e3\u8f93\u51fa\u6982\u7387\u4ee5\u53ca\u8ba1\u7b97\u635f\u5931\u51fd\u6570loss\u3002\u4ee3\u7801\u5982\u4e0b\uff1a \u521b\u9020\u6570\u636e\u96c6 \u5bfc\u5165\u7684(x_train, y_train), (x_test, y_test)\u6570\u636e\u96c6\u5e76\u4e0d\u80fd\u76f4\u63a5\u88abTensorFlow\u76f4\u63a5\u5229\u7528\uff0c\u9996\u5148\u9700\u8981\u521b\u9020\u8bad\u7ec3\u548c\u6d4b\u8bd5\u6570\u636e\u96c6\uff1a db = tf.data.Dataset.from_tensor_slices((x_train,y_train)) db_test = tf.data.Dataset.from_tensor_slices((x_test,y_test))  \u6570\u636e\u96c6\u6279\u5904\u7406\u5316 \u5229\u7528TensorFlow\u5e76\u884c\u8ba1\u7b97\u7684\u7279\u6027\uff0c\u6211\u4eec\u9700\u8981\u5bf9Tensor\u6279\u5904\u7406\u5316\uff0c\u4ece\u800c\u63d0\u9ad8\u8bad\u7ec3\u6548\u7387\uff0c\u5728\u6b64\u5b9e\u9a8c\u4e2d\u6211\u4eec\u9009\u53d6 batchsz = 128  \u6570\u636e\u96c6\u6253\u4e71\u64cd\u4f5c \u5728\u591a\u4e2aEpoch\u8bad\u7ec3\u5468\u671f\u4e4b\u540e\uff0c\u4e3a\u4e86\u9632\u6b62\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u5230\u7531\u4e8eMnist\u6570\u636e\u96c6\u6392\u5e8f\u7684\u4e00\u4e9b\u7279\u5f81\uff0c\u6211\u4eec\u9700\u8981\u5c06\u6570\u636e\u96c6\u5408\u8fdb\u884c\u968f\u673a\u6392\u5e8f\u6253\u4e71\u64cd\u4f5c\u3002\u8fd9\u6837\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u5f97\u5230\u7684\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u80fd\u529b\u3002\u65b9\u6cd5\u5982\u4e0b\uff1a  \u5229\u7528\u4ee5\u4e0a\u65b9\u6cd5\u751f\u6210\u7684\u6570\u636e\u96c6\u662f\u4e00\u4e2aiterable\u53ef\u8fed\u4ee3\u578b\u6570\u636e\uff0c\u53ef\u4ee5\u5229\u7528python\u8fed\u4ee3\u5668\u5177\u4f53\u89c2\u5bdf\u5176\u7279\u6027\uff0c\u65b9\u6cd5\u5982\u4e0b\uff1a next(iter(db))[0].shape  next(iter(db))[1].shape  \u7531\u4e0a\u8ff0\u65b9\u6cd5\u53ef\u4ee5\u5f97\u5230\u7684\u6570\u636e\u96c6\u7684\u4e00\u4e2abatch\u7684\u8f93\u5165\u6570\u636e\u5f62\u72b6\u4e3a[128, 28, 28]\uff0c\u6837\u672c\u6807\u7b7e\u4e3a[128, 10],\u5230\u6b64\u6570\u636e\u96c6\u521b\u9020\u6210\u529f\u3002 2. Mnist\u624b\u5199\u6570\u5b57\u6570\u636e\u96c6\u8bc6\u522b\u95ee\u9898 2.1 \u95ee\u9898\u5206\u6790\u4e0e\u5efa\u6a21 Mnist\u624b\u5199\u6570\u5b57\u8bc6\u522b\u662f\u4e00\u4e2a\u795e\u7ecf\u7f51\u7edc\u5b66\u4e60\u7684\u4e00\u4e2a\u57fa\u672c\u7684\u5165\u95e8\u95ee\u9898\uff0c\u76f8\u5f53\u4e8e\u795e\u7ecf\u7f51\u7edc\u7684\u201cHello, world!\u201d\u7a0b\u5e8f\uff0c\u6240\u4ee5\u5176\u95ee\u9898\u5206\u6790\u4e5f\u662f\u6bd4\u8f83\u7b80\u5355\u3002 \u7531Mnist\u6570\u636e\u96c6\u7684\u8f93\u5165\u8f93\u51fa\u683c\u5f0f\u5206\u522b\u4e3a[128, 28, 28]\u548c[128, 10]\uff0c\u6211\u4eec\u53ef\u4ee5\u5c06\u8fd9\u4e2a\u95ee\u9898\u7406\u89e3\u4e3a\u4e00\u4e2a\u5341\u5206\u7c7b\u7684\u95ee\u9898\uff0c\u5c06\u56fe\u7247\u6570\u636e\u8f93\u5165\u5230\u4e00\u4e2a\u7528\u4e8e\u5206\u7c7b\u7684\u795e\u7ecf\u7f51\u7edc\u5f53\u4e2d\u6700\u540e\u901a\u8fc7\u8bbe\u5b9a\u8f93\u51fa\u4e00\u4e2a[128, 10]\u5927\u5c0f\u7684\u6570\u636e\uff0c \u6839\u636e\u54ea\u4e00\u7ef4\u5ea6\u7684\u6570\u636e\u5927\u5c31\u5224\u65ad\u624b\u5199\u6570\u5b57\u56fe\u50cf\u5c5e\u4e8e\u54ea\u4e00\u4e2a\u7c7b\u522b\u7684\u539f\u5219\uff0c\u6240\u4ee5\u624b\u5199\u6570\u5b57\u5206\u7c7b\u95ee\u9898\u7684\u6838\u5fc3\u4e4b\u5904\u5728\u4e8e\u5982\u4f55\u8bbe\u8ba1\u51fa\u4e00\u4e2a\u4f18\u79c0\u7684\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u5feb\u901f\u6709\u6548\u5730\u903c\u8fd1\u6b63\u786e\u89e3\u3002 \u56e0\u4e3a\u8be5\u95ee\u9898\u88ab\u7406\u89e3\u4e3a\u4e00\u4e2a\u5206\u7c7b\u95ee\u9898\uff0c\u6240\u4ee5\u8f93\u51fa\u5c42\u6211\u4eec\u53ef\u4ee5\u7528softmax\u51fd\u6570\uff0c\u5c06\u795e\u7ecf\u7f51\u7edc\u8f93\u51fa\u7406\u89e3\u4e3a\u8be5\u795e\u7ecf\u7f51\u7edc\u7406\u89e3\u4e00\u4e2a\u56fe\u50cf\u7c7b\u522b\u5730\u6982\u7387\uff0closs\u5224\u522b\u5f0f\u53ef\u4ee5\u7528CrossEntropy\u635f\u5931\u51fd\u6570\uff0c\u8fd9\u6837\u53ef\u4ee5\u4e92\u76f8\u5f25\u8865\u4ece\u800c\u51cf\u8f7b\u68af\u5ea6\u8fc7\u5927\u6216\u8005\u8fc7\u5c0f\u7684\u6548\u5e94\u3002 \u5173\u4e8e\u5177\u4f53\u7684\u795e\u7ecf\u7f51\u7edc\u8bbe\u8ba1\uff0c\u6211\u4eec\u5c06\u5728\u540e\u9762\u7ae0\u8282\u8fdb\u884c\u8be6\u7ec6\u4ecb\u7ecd\u3002 2.2 \u5df2\u6709\u7814\u7a76\u8fdb\u5c55 \u5173\u4e8eMnist\u6570\u636e\u96c6\u7684\u7814\u7a76\u4f34\u968f\u795e\u7ecf\u7f51\u7edc\u7814\u7a76\u7684\u53d1\u5c55\u5386\u7a0b\u5df2\u7ecf\u6709\u5f88\u591a\u7c7b\u522b\u7684\u65b9\u6cd5\u4e86\uff0c\u4e0b\u9762\u5c06\u4ee5\u8868\u683c\u7684\u5f62\u5f0f\u5217\u51fa\u4e3b\u8981Mnist\u6570\u636e\u96c6\u8bc6\u522b\u7684\u7814\u7a76\u5386\u7a0b\u3002 \u5206\u7c7b\u5668|\u5224\u522b\u5931\u8bef\u7387\uff08%\uff09|\u53d1\u8868\u5e74\u4efd|\u4f5c\u8005 :---:|:---:|:---:|:---: \u4e00\u5c42\u7f51\u7edc\u7ed3\u6784\u7ebf\u6027\u5206\u7c7b\u5668|12|1998|LeCun| \u975e\u7ebf\u6027\u5f62\u53d8KNN\u7b97\u6cd5|0.54|2007|Keyneth| \u542f\u53d1\u5f0f\u641c\u7d22\u6811\u7b97\u6cd5|0.87|2009|Keyneth| 1000\u4e2aRBF\u6838\u7ebf\u6027\u5206\u7c7b\u5668|3.6|1998|LeCun| SVM\u5206\u7c7b\u5668|0.56|2002|DeCoste and Scholkopf| 6\u5c42\u7684\u7ed3\u6784\u4e3a784-2500-2000-1500-1000-500-10\u7684\u7ebf\u6027\u795e\u7ecf\u7f51\u7edc|0.35|2010|Ciresan| \u5377\u79ef\u795e\u7ecf\u7f51\u7edcLeNet5|0.8|1998|LeCun| \u7531\u4e0a\u8ff0Mnist\u53d1\u5c55\u5386\u7a0b\u53ef\u77e5\uff0c\u5f53\u795e\u7ecf\u7f51\u7edc\u7684\u6df1\u5ea6\u589e\u52a0\u65f6\uff0c\u795e\u7ecf\u7f51\u7edc\u5bf9\u4e8eMnist\u6570\u636e\u96c6\u7684\u8bc6\u522b\u6548\u679c\u660e\u663e\u63d0\u9ad8\uff1b\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u7684\u63d0\u51fa\u63a8\u8fdb\u4e86\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u53d1\u5c55\u6709\u4e86\u8d28\u7684\u63d0\u5347\u3002 3. \u7814\u7a76\u7b97\u6cd5 3.1 Dense+ReLU \u9996\u5148\u7b2c\u4e00\u79cd\u65b9\u6cd5\u65f6\u91c7\u7528\u7684\u662f\u4e94\u5c42\u5168\u8fde\u63a5\u7ebf\u6027Dense\u5c42\uff0c\u6bcf\u4e00\u4e2aDense\u540e\u9762\u63a5\u4e00\u4e2aReLu\u51fd\u6570\u5c42\uff0c\u4fdd\u8bc1\u795e\u7ecf\u7f51\u7edc\u80fd\u591f\u63d0\u53d6\u51faMnist\u7684\u975e\u7ebf\u6027\u7279\u5f81\u3002\u7ecf\u8fc7\u4e94\u6b21\u8fd9\u6837\u7684\u7ed3\u6784\u6211\u4eec\u5f97\u5230\u7684\u6570\u636e\u7ef4\u5ea6\u7684\u53d8\u6362\u8fc7\u7a0b\u4e3a\uff1a[batchsz, 25 * 25] -> [batchsz, 256] -> [batchsz, 128] -> [batchsz, 64] -> [batchsz, 32] -> [batchsz, 10] Dense\u5c42 \u6bcf\u4e00\u4e2aDense\u5c42\u9700\u8981\u7ecf\u8fc7\u521d\u59cb\u5316\uff08init\uff09\u548c\u8c03\u7528\uff08call\uff09\u4e24\u4e2a\u65b9\u6cd5\uff0cDense\u5c42\u7684\u4e24\u4e2a\u5b9e\u73b0\u65b9\u6cd5\u5982\u4e0b\uff1a # \u521d\u59cb\u5316 self.dense = layers.Dense(units=256) #\u8c03\u7528 x = self.dense(x)  ReLU\u5c42 \u7ebf\u6027\u6574\u6d41\u51fd\u6570\uff08Rectified Linear Unit, ReLU\uff09,\u53c8\u79f0\u4fee\u6b63\u7ebf\u6027\u5355\u5143, \u662f\u4e00\u79cd\u4eba\u5de5\u795e\u7ecf\u7f51\u7edc\u4e2d\u5e38\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\uff08activation function\uff09\uff0c\u901a\u5e38\u6307\u4ee3\u4ee5\u659c\u5761\u51fd\u6570\u53ca\u5176\u53d8\u79cd\u4e3a\u4ee3\u8868\u7684\u975e\u7ebf\u6027\u51fd\u6570\u3002 \u6bd4\u8f83\u5e38\u7528\u7684\u7ebf\u6027\u6574\u6d41\u51fd\u6570\u6709\u659c\u5761\u51fd\u6570 \uff0c\u4ee5\u53ca\u5e26\u6cc4\u9732\u6574\u6d41\u51fd\u6570 (Leaky ReLU)\uff0c\u5176\u4e2d  \u4e3a\u795e\u7ecf\u5143(Neuron)\u7684\u8f93\u5165\u3002\u7ebf\u6027\u6574\u6d41\u88ab\u8ba4\u4e3a\u6709\u4e00\u5b9a\u7684\u751f\u7269\u5b66\u539f\u7406\uff0c\u5e76\u4e14\u7531\u4e8e\u5728\u5b9e\u8df5\u4e2d\u901a\u5e38\u6709\u7740\u6bd4\u5176\u4ed6\u5e38\u7528\u6fc0\u6d3b\u51fd\u6570\uff08\u8b6c\u5982\u903b\u8f91\u51fd\u6570\uff09\u66f4\u597d\u7684\u6548\u679c\uff0c\u800c\u88ab\u5982\u4eca\u7684\u6df1\u5ea6\u795e\u7ecf\u7f51\u7edc\u5e7f\u6cdb\u4f7f\u7528\u4e8e\u8bf8\u5982\u56fe\u50cf\u8bc6\u522b\u7b49\u8ba1\u7b97\u673a\u89c6\u89c9\u4eba\u5de5\u667a\u80fd\u9886\u57df\u3002 ReLu\u662f\u5f53\u4ee3\u6df1\u5ea6\u5b66\u4e60\u4f7f\u7528\u6700\u591a\u7684\u4e00\u79cd\u795e\u7ecf\u5143\u6fc0\u6d3b\u51fd\u6570\uff0c\u5176\u610f\u4e49\u5728\u4e8e\u7ed3\u6784\u7b80\u5355\uff0c\u5e76\u4e14\u53ef\u4ee5\u6709\u6548\u63d0\u53d6\u795e\u7ecf\u7f51\u7edc\u7684\u9ad8\u7ef4\u975e\u7ebf\u6027\u7279\u6027\u3002\u4e0b\u56fe\u4e3aReLU\u6fc0\u6d3b\u5c42\u7684\u56fe\u50cf\u3002  \u76f8\u6bd4\u4e8e\u4f20\u7edf\u7684\u795e\u7ecf\u7f51\u7edc\u6fc0\u6d3b\u51fd\u6570\uff0c\u8bf8\u5982\u903b\u8f91\u51fd\u6570\uff08Logistic sigmoid\uff09\u548ctanh\u7b49\u53cc\u66f2\u51fd\u6570\uff0c\u7ebf\u6027\u6574\u6d41\u51fd\u6570\u6709\u7740\u4ee5\u4e0b\u51e0\u65b9\u9762\u7684\u4f18\u52bf\uff1a  \u4eff\u751f\u7269\u5b66\u539f\u7406\uff1a\u76f8\u5173\u5927\u8111\u65b9\u9762\u7684\u7814\u7a76\u8868\u660e\u751f\u7269\u795e\u7ecf\u5143\u7684\u4fe1\u606f\u7f16\u7801\u901a\u5e38\u662f\u6bd4\u8f83\u5206\u6563\u53ca\u7a00\u758f\u7684\u3002\u901a\u5e38\u60c5\u51b5\u4e0b\uff0c\u5927\u8111\u4e2d\u5728\u540c\u4e00\u65f6\u95f4\u5927\u6982\u53ea\u67091%-4%\u7684\u795e\u7ecf\u5143\u5904\u4e8e\u6d3b\u8dc3\u72b6\u6001\u3002\u4f7f\u7528\u7ebf\u6027\u4fee\u6b63\u4ee5\u53ca\u6b63\u5219\u5316\uff08regularization\uff09\u53ef\u4ee5\u5bf9\u673a\u5668\u795e\u7ecf\u7f51\u7edc\u4e2d\u795e\u7ecf\u5143\u7684\u6d3b\u8dc3\u5ea6\uff08\u5373\u8f93\u51fa\u4e3a\u6b63\u503c\uff09\u8fdb\u884c\u8c03\u8bd5\uff1b\u76f8\u6bd4\u4e4b\u4e0b\uff0c\u903b\u8f91\u51fd\u6570\u5728\u8f93\u5165\u4e3a0\u65f6\u8fbe\u5230 \uff0c\u5373\u5df2\u7ecf\u662f\u534a\u9971\u548c\u7684\u7a33\u5b9a\u72b6\u6001\uff0c\u4e0d\u591f\u7b26\u5408\u5b9e\u9645\u751f\u7269\u5b66\u5bf9\u6a21\u62df\u795e\u7ecf\u7f51\u7edc\u7684\u671f\u671b\u3002\u4e0d\u8fc7\u9700\u8981\u6307\u51fa\u7684\u662f\uff0c\u4e00\u822c\u60c5\u51b5\u4e0b\uff0c\u5728\u4e00\u4e2a\u4f7f\u7528\u4fee\u6b63\u7ebf\u6027\u5355\u5143\uff08\u5373\u7ebf\u6027\u6574\u6d41\uff09\u7684\u795e\u7ecf\u7f51\u7edc\u4e2d\u5927\u6982\u670950%\u7684\u795e\u7ecf\u5143\u5904\u4e8e\u6fc0\u6d3b\u6001\u3002 \u66f4\u52a0\u6709\u6548\u7387\u7684\u68af\u5ea6\u4e0b\u964d\u4ee5\u53ca\u53cd\u5411\u4f20\u64ad\uff1a\u907f\u514d\u4e86\u68af\u5ea6\u7206\u70b8\u548c\u68af\u5ea6\u6d88\u5931\u95ee\u9898\u3002 \u7b80\u5316\u8ba1\u7b97\u8fc7\u7a0b\uff1a\u6ca1\u6709\u4e86\u5176\u4ed6\u590d\u6742\u6fc0\u6d3b\u51fd\u6570\u4e2d\u8bf8\u5982\u6307\u6570\u51fd\u6570\u7684\u5f71\u54cd\uff1b\u540c\u65f6\u6d3b\u8dc3\u5ea6\u7684\u5206\u6563\u6027\u4f7f\u5f97\u795e\u7ecf\u7f51\u7edc\u6574\u4f53\u8ba1\u7b97\u6210\u672c\u4e0b\u964d ReLU\u5728TF2.0\u4e2d\u7684\u5b9e\u73b0\u65b9\u6cd5\u4e3a\uff1a  x = tf.nn.relu(x)  3.2 LeNet5  \u624b\u5199\u5b57\u4f53\u8bc6\u522b\u6a21\u578bLeNet5\u8bde\u751f\u4e8e1994\u5e74\uff0c\u662f\u6700\u65e9\u7684\u5377\u79ef\u795e\u7ecf\u7f51\u7edc\u4e4b\u4e00\u3002LeNet5\u901a\u8fc7\u5de7\u5999\u7684\u8bbe\u8ba1\uff0c\u5229\u7528\u5377\u79ef\u3001\u53c2\u6570\u5171\u4eab\u3001\u6c60\u5316\u7b49\u64cd\u4f5c\u63d0\u53d6\u7279\u5f81\uff0c\u907f\u514d\u4e86\u5927\u91cf\u7684\u8ba1\u7b97\u6210\u672c\uff0c\u6700\u540e\u518d\u4f7f\u7528\u5168\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\u8fdb\u884c\u5206\u7c7b\u8bc6\u522b\uff0c\u8fd9\u4e2a\u7f51\u7edc\u4e5f\u662f\u6700\u8fd1\u5927\u91cf\u795e\u7ecf\u7f51\u7edc\u67b6\u6784\u7684\u8d77\u70b9\u3002 LeNet5\u75317\u5c42CNN\uff08\u4e0d\u5305\u542b\u8f93\u5165\u5c42\uff09\u7ec4\u6210\uff0c\u4e0a\u56fe\u4e2d\u8f93\u5165\u7684\u539f\u59cb\u56fe\u50cf\u5927\u5c0f\u662f32\u00d732\u50cf\u7d20\uff0c\u5377\u79ef\u5c42\u7528Ci\u8868\u793a\uff0c\u5b50\u91c7\u6837\u5c42\uff08pooling\uff0c\u6c60\u5316\uff09\u7528Si\u8868\u793a\uff0c\u5168\u8fde\u63a5\u5c42\u7528Fi\u8868\u793a\u3002\u4e0b\u9762\u9010\u5c42\u4ecb\u7ecd\u5176\u4f5c\u7528\u548c\u793a\u610f\u56fe\u4e0a\u65b9\u7684\u6570\u5b57\u542b\u4e49\u3002 Conv2D \u5377\u79ef\uff08convolution\uff09\u662f\u901a\u8fc7\u4e24\u4e2a\u51fd\u6570f\u548cg\u751f\u6210\u7b2c\u4e09\u4e2a\u51fd\u6570\u7684\u4e00\u79cd\u6570\u5b66\u7b97\u5b50\uff0c\u8868\u5f81\u51fd\u6570f\u4e0eg\u7ecf\u8fc7\u7ffb\u8f6c\u548c\u5e73\u79fb\u7684\u91cd\u53e0\u90e8\u5206\u7684\u9762\u79ef\u3002 \u5377\u79ef\u53ef\u5206\u4e3a\u4e00\u7ef4\u5377\u79ef\u548c\u4e8c\u7ef4\u5377\u79ef\uff0c\u4e09\u7ef4\u5377\u79ef\uff0c\u591a\u7ef4\u5377\u79ef\u64cd\u4f5c\u3002\u4e8c\u7ef4\u5377\u79ef\u662f\u6211\u4eec\u6700\u5e38\u7528\u7684\u4e5f\u662f\u6700\u91cd\u8981\u7684\uff0c\u56fe\u50cf\u7684\u8fb9\u7f18\u8ba1\u7b97\u548c\u6a21\u7cca\u7b49\u7b97\u6cd5\u90fd\u662f\u57fa\u4e8e\u5377\u79ef\u64cd\u4f5c\u7684\u53ea\u4e0d\u8fc7\u662f\u5bf9\u5e94\u7684\u4e0d\u540c\u8ba1\u7b97\uff0c\u5377\u79ef\u6ee4\u6ce2\u5668\u4e0d\u540c\u3002  \u8fd9\u91cc\u7684kernel\u5c31\u662f\u5377\u79ef\u6838\uff0ckernel_size\u7684\u5927\u5c0f\u4e00\u822c\u662f\uff083\uff0c 3\uff09\u3001\uff085\uff0c 5\uff09\u3001\uff087\u30017\uff09\u8fd9\u91cc\u662f\u5947\u6570\u7684\u539f\u56e0\u662f\u56e0\u4e3a\u65b9\u4fbf\u8ba1\u7b97\u3002\u5728\u672c\u6b21\u8bd5\u9a8c\u4e2d\u7684kernel_size\u6211\u4eec\u5206\u522b\u7528\u5230\u4e86\uff085\uff0c 5\uff09\u548c\uff083\uff0c 3\uff09\u3002 \u5728TensorFlow\u4e2d\u5377\u79ef\u64cd\u4f5c\u4e5f\u5206\u4e3a\u521d\u59cb\u5316\u548c\u8c03\u7528\u4e24\u4e2a\u65b9\u6cd5\uff0c\u5206\u522b\u7528\u4e0b\u9762\u4ee3\u7801\u5b9e\u73b0\uff1a self.conv1 = layers.Conv2D(filters=6, kernel_size=5, padding='valid') # 6@24*24 x = self.conv1(x)  filters\u8868\u793a\u5377\u79ef\u5c42\u4e2d\u5377\u79ef\u6838\u7684\u4e2a\u6570\uff0c\u4e5f\u8868\u793a\u8f93\u51fa\u5c42\u7684\u7ef4\u5ea6\uff1b padding\u6709'valid'\u548c'same'\u4e24\u4e2a\u53c2\u6570\uff0c\u5176\u4e2d'valid'\u8868\u793a\u4ece\u5377\u79ef\u6838\u4e2d\u5fc3\u4f4d\u4e8e\u56fe\u50cf\u8fb9\u7f18\u65f6\u5f00\u59cb\u8ba1\u7b97\u5377\u79ef\uff0c'same'\u8868\u793a\u5c06\u6240\u6709\u56fe\u50cf\u7684\u5750\u6807\u6210\u4e3a\u5377\u79ef\u64cd\u4f5c\u7684\u4e2d\u5fc3\u70b9\u8fdb\u884c\u5377\u79ef\u64cd\u4f5c\u3002 MaxPool \u6700\u5927\u5b50\u91c7\u6837\u51fd\u6570\u53d6\u533a\u57df\u5185\u6240\u6709\u795e\u7ecf\u5143\u7684\u6700\u5927\u503c\uff08max-pooling\uff09\u3002\u4ee5\u4e0b\u56fe\u4e3a\u4f8b\uff0c\u8f93\u5165\u6570\u636eX\u4e3a4* 4\uff0c\u91c7\u6837\u6838size\u4e3a2\uff0cstride\u4e3a2\uff0cno padding\u3002\u8f93\u5165\u6570\u636e\u5927\u5c0f\u7c7b\u4f3c\u5377\u79ef\u5c42\u7684\u8ba1\u7b97\u65b9\u6cd5\uff0c\uff08input_width+2* pad-pool_size\uff09/stride+1\u3002\u524d\u5411\u4f20\u64ad\u4e2d\u4e0d\u4ec5\u8981\u8ba1\u7b97pool\u533a\u57df\u5185\u7684\u6700\u5927\u503c\uff0c\u8fd8\u8981\u8bb0\u5f55\u8be5\u6700\u5927\u503c\u6240\u5728\u8f93\u5165\u6570\u636e\u4e2d\u7684\u4f4d\u7f6e\uff0c\u76ee\u7684\u662f\u4e3a\u4e86\u5728\u53cd\u5411\u4f20\u64ad\u4e2d\uff0c\u9700\u8981\u628a\u68af\u5ea6\u503c\u4f20\u5230\u5bf9\u5e94\u6700\u5927\u503c\u6240\u5728\u7684\u4f4d\u7f6e\u3002  \u4e0a\u56fe\u4e3aMaxPool\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u6cd5\u3002\u901a\u8fc7MaxPool\u65b9\u6cd5\u6211\u4eec\u53ef\u4ee5\u63d0\u53d6\u7279\u5f81\u503c\u8f83\u5927\u7279\u5f81\u70b9\uff0c\u629b\u5f03\u4e00\u4e9b\u65e0\u7528\u7684\u7279\u5f81\u503c/ \u5728TF2.0\u4e2d\u5177\u4f53\u7684\u5b9e\u73b0\u65b9\u6cd5\u4e3a # \u521d\u59cb\u5316 self.maxpool1 = layers.MaxPool2D() # \u8c03\u7528 x = self.maxpool1(x)  \u5982\u679c\u5728\u521d\u59cb\u5316\u65f6\u6ca1\u6709\u8bbe\u7f6e\u4efb\u4f55\u53c2\u6570\u7684\u60c5\u51b5\u4e0b\uff0c\u5219kernel_size = (2, 2)\uff0cstride = 2\uff0c\u5c31\u76f8\u5f53\u4e8e\u5bf9feature_map\u5b9e\u73b0\u4e86\u7ef4\u5ea6\u7f29\u5c0f\u4e3a\u4e00\u534a\u7684\u64cd\u4f5c\uff0c\u6b64\u6b21\u8bd5\u9a8c\u5c31\u662f\u5982\u6b64\u3002 Flatten \u7531\u4e8e\u6b64\u6b21\u8bd5\u9a8c\u4e2d\u6211\u4eec\u6700\u540e\u9700\u8981\u8f93\u51fa\u7684\u662f\u4e00\u4e2a\u4e00\u7ef4\u7684\u8868\u793a\u6982\u7387\u7684\u5411\u91cf\uff0c\u6240\u4ee5\u6211\u4eec\u9700\u8981\u5c06\u4e00\u4e2a\u4e8c\u7ef4\u7684\u6570\u636e\u505a\u94fa\u5e73\u5316\u64cd\u4f5c\u3002\u8be5\u65b9\u6cd5\u5728TensorFlow2.0\u4e2d\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u6cd5\u4e3a\uff1a # \u521d\u59cb\u5316 self.flatten = layers.Flatten() # \u8c03\u7528 x = self.flatten(x)  \u9ad8\u65af\u5168\u8fde\u63a5GaussianConnection \u9ad8\u65af\u5168\u8fde\u63a5\u4e5f\u5c31\u662f\u91c7\u7528\u4e86\u5f84\u5411\u57fa\u6838\u51fd\u6570\u7684\u7f51\u7edc\u8fde\u63a5\u65b9\u5f0f\uff0c\u5177\u4f53\u8ba1\u7b97\u8fc7\u7a0b\u4e3a\uff1a  \u5728TF2.0\u4e2d\u7684\u5177\u4f53\u5b9e\u73b0\u65b9\u6cd5\u4e3a\uff1a class RBF(layers.Layer):       def __init__(self, input_dim, output_dim):             super(RBF, self).__init__()             self.kernel = self.add_variable('w', [input_dim, output_dim])       def call(self, inputs):             inputs = tf.expand_dims(inputs, axis=-1)             out = tf.reduce_sum(tf.pow(inputs-self.kernel, 2), axis=1)return out  3.3 ResNet  \u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edcResNet\u662f\u7531\u534e\u4eba\u79d1\u5b66\u5bb6\u4f55\u51ef\u660e\u5728\u5fae\u8f6f\u4e9a\u6d32\u7814\u7a76\u9662\u5de5\u4f5c\u671f\u95f4\u63d0\u51fa\u7684\u3002 \u6df1\u5ea6\u7f51\u7edc\u5bb9\u6613\u9020\u6210\u68af\u5ea6\u5728back propagation\u7684\u8fc7\u7a0b\u4e2d\u6d88\u5931\uff0c\u5bfc\u81f4\u8bad\u7ec3\u6548\u679c\u5f88\u5dee\uff0c\u5c31\u662f\u6211\u4eec\u6240\u719f\u77e5\u7684\u68af\u5ea6\u5f25\u6563\u3002\u800c\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u5728\u795e\u7ecf\u7f51\u7edc\u7684\u7ed3\u6784\u5c42\u9762\u89e3\u51b3\u4e86\u8fd9\u4e00\u95ee\u9898\uff0c\u4f7f\u5f97\u5c31\u7b97\u7f51\u7edc\u5f88\u6df1\uff0c\u68af\u5ea6\u4e5f\u4e0d\u4f1a\u6d88\u5931\u3002\u4e0b\u56fe\u4e3a\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u7684\u4e00\u4e2a\u57fa\u672c\u7ed3\u6784\u5355\u5143\u3002  \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u7684\u7ed3\u6784\u53ef\u77e5\uff0c\u6df1\u5ea6\u6b8b\u5dee\u7f51\u7edc\u7684\u6548\u679c\u5373\u4f7f\u9000\u5316\u4e5f\u4f1a\u6bd4\u6d45\u5c42\u6b21\u7684\u795e\u7ecf\u7f51\u7edc\u6548\u679c\u8981\u597d\u3002 4 \u5b9e\u9a8c\u7ed3\u679c Dense+ReLU  batch_accuracy  batch_loss  epoch_accuracy  epoch_loss LeNet  batch_accuracy  batch_loss  epoch_accuracy  epoch_loss  ResNet  batch_accuracy  batch_loss  epoch_accuracy  epoch_loss  \u7ed3\u8bba \u5728\u7f51\u7edc\u6df1\u5ea6\u4e0aResNet > LeNet > Denset+ReLU\uff0c\u5728\u5b9e\u73b0\u6548\u679c\u4e0a\uff0c ResNet > LeNet > Denset+ReLU\uff0c\u6211\u4eec\u53ef\u4ee5\u5f97\u51fa\u7f51\u7edc\u6df1\u5ea6\u8d8a\u6df1\u65f6\uff0c\u7f51\u7edc\u5bf9\u4e8eMnist\u6570\u636e\u96c6\u7684\u8bc6\u522b\u6548\u679c\u8d8a\u597d\u7684\u7ed3\u8bba\u3002\u4e09\u4e2a\u65b9\u6cd5\u5bf9\u4e8eMnist\u6570\u636e\u96c6\u7684\u8bc6\u522b\u6548\u679c\u5728\u521d\u59cb\u8fed\u4ee3\u7684\u65f6\u523b\u5c31\u5df2\u7ecf\u6548\u679c\u8fbe\u5230\u4e86\u8f83\u4f18\u7684\u503c\uff0c\u8bf4\u660eMnist\u6570\u636e\u96c6\u7684\u7279\u5f81\u8f83\u4e3a\u7b80\u5355\uff0c\u6240\u4ee5\u5229\u7528\u7b80\u5355\u7684\u795e\u7ecf\u7f51\u7edc\u7ed3\u6784\u4fbf\u53ef\u4ee5\u5f88\u6709\u6548\u7684\u8fdb\u884c\u8bc6\u522b\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://yann.lecun.com/", "http://yann.lecun.com/exdb/mnist/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dea"}, "repo_url": "https://github.com/bojone/on-lstm", "repo_name": "on-lstm", "repo_full_name": "bojone/on-lstm", "repo_owner": "bojone", "repo_desc": "Keras implement of ON-LSTM (Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T14:29:56Z", "repo_watch": 55, "repo_forks": 3, "private": false, "repo_created_at": "2019-05-27T11:06:11Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188827569, "is_fork": false, "readme_text": "Keras implement of ON-LSTM ON-LSTM is from paper Ordered Neurons: Integrating Tree Structures into Recurrent Neural Networks Requirements Python 2.7 + Keras 2.2.4 + Tensorflow 1.8 Introduction https://kexue.fm/archives/6621 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1810.09536"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258deb"}, "repo_url": "https://github.com/baidu-advbox/perceptron-benchmark", "repo_name": "perceptron-benchmark", "repo_full_name": "baidu-advbox/perceptron-benchmark", "repo_owner": "baidu-advbox", "repo_desc": "Robustness benchmark for DNN models. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T02:24:56Z", "repo_watch": 3, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-28T20:04:20Z", "homepage": null, "size": 41021, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 189095274, "is_fork": false, "readme_text": "Perceptron Robustness Benchmark Perceptron is a robustness benchmark for computer vision DNN models. It supports both image classification and object detection models on PyTorch, Tensorflow, Keras, PaddlePaddle (in progress), as well as cloud APIs. Perceptron inherits the design from foolbox, and is designed to be agnostic to the deep learning frameworks the models are built on. Documentation is available on readthedoc Currently, you can use Perceptron either through its python API or its command line tool. Getting Started Running Examples via Command Lines In the docker shell, run the test through Perceptron command line interface python perceptron/launcher.py \\     --framework keras \\     --model resnet50 \\     --metric carlini_wagner_l2 \\     --image example.png In the example, user specifies framework as keras, the model as resnet50, the metric as carlini_wagner_l2, the input image as example.png. The output would be as follows.  To visualize the adversary, we also provide the plot of the original image, adversary image, and their difference as follows.  You can try different combinations of frameworks, models, criteria, and metrics. To see more options using -h for help message. python perceptron/launcher.py -h  Docker Quick Start Build the docker image and all dependencies will be installed automatically. nvidia-docker build -t perceptron:env .  Keras: ResNet50 - C&W2 Benchmarking The following example serves the same purpose as the command line example. This example benchmarks the robustness of Keras ResNet50 model against C&W2 metric by measuring the minimal required :math:L2 perturbation for a CW2 to success. The minimum Mean Squred Distance (MSE) will be logged. import numpy as np import keras.applications as models from perceptron.models.classification.keras import KerasModel from perceptron.utils.image import imagenet_example from perceptron.benchmarks.carlini_wagner import CarliniWagnerL2Metric from perceptron.utils.criteria.classification import Misclassification  # instantiate the model from keras applications resnet50 = models.ResNet50(weights='imagenet')  # initialize the KerasModel # keras resnet50 has input bound (0, 255) preprocessing = (np.array([104, 116, 123]), 1)  # the mean and stv of the whole dataset kmodel = KerasModel(resnet50, bounds=(0, 255), preprocessing=preprocessing)  # get source image and label # the model expects values in [0, 255], and channles_last image, _ = imagenet_example(data_format='channels_last') label = np.argmax(kmodel.predictions(image))  metric = CarliniWagnerL2Metric(kmodel, criterion=Misclassification())  adversary = metric(image, label, unpack=False) Running the example will give you the minimal MSE required for C&W2 to fool resnet50 model (i.e., changing the predicted label). Acknowledgements Perceptron Robustness Benchmark would not have been possible without the foolbox projects authored by @Jonas and @Wieland. Thanks for the code and inspiration! Contributing You are welcome to send pull requests and report issues on GitHub or iCode. Note that the Perceptron Benchmark project follows the Git flow development model. Authors  Yunhan Jia (jiayunhan@baidu.com) Yantao Lu (yantaolu@baidu.com) Xiaowei Chen (xiaoweichen01@baidu.com)  Steering Committee  Tao Wei (lenx@baidu.com)  License Perceptron Robustness Benchmark is provided under Apache-2.0 license. For a copy, see LICENSE file. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dec"}, "repo_url": "https://github.com/fazeVaib/DigiVision", "repo_name": "DigiVision", "repo_full_name": "fazeVaib/DigiVision", "repo_owner": "fazeVaib", "repo_desc": "A deep learning based application which is entitled to help the visually impaired people. The application automatically generates the textual description of what's happening in front of the camera and conveys it to person through audio. It is capable of recognising faces and tell user whether a known person is standing in front of him or not.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T18:46:27Z", "repo_watch": 4, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-27T18:38:50Z", "homepage": null, "size": 143571, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188892849, "is_fork": false, "readme_text": "DigiVision A deep learning based application which is entitled to help the visually impaired people. The application automatically generates the textual description of what's happening in front of the camera and conveys it to person through audio. It is capable of recognising faces and tell user whether a known person is standing in front of him or not. Requirements  Tensorflow (>1.9) Keras OpenCV Python 3.5+ gTTS  Dataset used MS COCO ", "has_readme": true, "readme_language": "English", "repo_tags": ["computer-vision", "natural-language-processing", "tensorflow", "keras-neural-networks", "face-recognition", "opencv", "mongodb", "mongodb-atlas", "haar-cascade", "oneshotlearning", "vgg16", "gru", "transfer-learning", "live-video", "mtcnn-face-detection", "siamese-neural-network", "facenet-trained-models", "image-processing", "imagenet-classifier", "ms-coco"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258ded"}, "repo_url": "https://github.com/AidenHuen/BERT-BiLSTM-CRF", "repo_name": "BERT-BiLSTM-CRF", "repo_full_name": "AidenHuen/BERT-BiLSTM-CRF", "repo_owner": "AidenHuen", "repo_desc": "BERT-BiLSTM-CRF\u7684Keras\u7248\u5b9e\u73b0", "description_language": "Faroese", "repo_ext_links": null, "repo_last_mod": "2019-05-30T07:26:35Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T12:31:47Z", "homepage": "", "size": 20153, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188839763, "is_fork": false, "readme_text": "BERT-BiLSTM-CRF BERT-BiLSTM-CRF\u7684Keras\u7248\u5b9e\u73b0 BERT\u914d\u7f6e  \u9996\u5148\u9700\u8981\u4e0b\u8f7dPre-trained\u7684BERT\u6a21\u578b\uff0c\u672c\u6587\u7528\u7684\u662fGoogle\u5f00\u6e90\u7684\u4e2d\u6587BERT\u6a21\u578b\uff1a   https://storage.googleapis.com/bert_models/2018_11_03/chinese_L-12_H-768_A-12.zip   \u5b89\u88c5BERT\u5ba2\u6237\u7aef\u548c\u670d\u52a1\u5668 pip install bert-serving-server pip install bert-serving-client\uff0c\u6e90\u9879\u76ee\u5982\u4e0b\uff1a   https://github.com/hanxiao/bert-as-service   \u6253\u5f00\u670d\u52a1\u5668\uff0c\u5728BERT\u6839\u76ee\u5f55\u4e0b\uff0c\u6253\u5f00\u7ec8\u7aef\uff0c\u8f93\u5165\u547d\u4ee4\uff1a   bert-serving-start -pooling_strategy NONE -max_seq_len 144 -mask_cls_sep -model_dir chinese_L-12_H-768_A-12/  -num_worker 1  DEMO\u6570\u636e  2015\u8bcd\u6027\u6807\u6ce8\u6570\u636e\u96c6  \u6587\u4ef6\u63cf\u8ff0  preprocess.py \u6570\u636e\u9884\u5904\u7406\uff0c\u4ea7\u751f\u6a21\u578b\u8f93\u5165\u7684pickle\u6587\u4ef6 train.py \u901a\u8fc7\u8bad\u7ec3\u96c6\uff0c\u8bad\u7ec3\u6a21\u578b test.py \u8ba1\u7b97\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e2d\u7684F1\u503c Modellib.py \u6a21\u578b\u4f4d\u7f6e config.py \u53c2\u6570\u914d\u7f6e  \u6a21\u578b\u8bad\u7ec3 \u914d\u7f6eBERT->>\u6267\u884cpreprocess.py->>\u6267\u884ctrain.py \u914d\u7f6e  python 2.7 tensorflow-gpu 1.10.0 Keras 2.2.4 keras-contrib 2.0.8  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": ["bert", "bilstm-crf", "keras", "sequence-labeling"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dee"}, "repo_url": "https://github.com/manuvn/lpRNNs", "repo_name": "lpRNNs", "repo_full_name": "manuvn/lpRNNs", "repo_owner": "manuvn", "repo_desc": "Companion code for the paper \"A neuromorphic boost for RNNs with low pass filters", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T15:33:01Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T15:12:26Z", "homepage": null, "size": 399, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189052423, "is_fork": false, "readme_text": "A neuromorphic boost for RNNs using low pass filters Implementation code for the low pass RNN models and tasks from the paper - \"A neuromorphic boost for RNNs using low pass filters\". Each folder contains code for the th different tests conducted in the paper. We also provide with a reference implementation of the low pass models here compatible with Keras. Simply import and use ... Enjoy! :) Tasks done in the paper:  Addition task Copying task PennTree Bank model Google speech command detection task.  Common requirements: Python 3.6. Keras. Other usual suspects such as matplotlib, etc. Different tasks has different dependencies that we have attempted to list within the task folders. However, it is likely to be incomplete. Apologies if you have trouble running the code. List of all packages in our enviroment Note that all will not be required for all the tests:  absl-py                0.6.1 astor                  0.7.1 astroid                2.0.3 attrs                  18.2.0 audioread              2.1.7 backcall               0.1.0 bleach                 3.1.0 certifi                2019.3.9 chardet                3.0.4 colorama               0.4.0 cycler                 0.10.0 decorator              4.3.0 entrypoints            0.3 flake8                 3.7.7 future                 0.17.1 gast                   0.2.0 grpcio                 1.16.1 h5py                   2.8.0 idna                   2.8 ipykernel              5.1.0 ipython                7.2.0 ipython-genutils       0.2.0 isort                  4.3.4 jedi                   0.13.2 Jinja2                 2.10 joblib                 0.13.2 jsonschema             3.0.0a3 jupyter-client         5.2.4 jupyter-core           4.4.0 jupyterlab             0.35.4 jupyterlab-server      0.2.0 kapre                  0.1.4 Keras                  2.2.4 Keras-Applications     1.0.6 Keras-Preprocessing    1.0.5 kiwisolver             1.0.1 lazy-object-proxy      1.3.1 librosa                0.6.3 llvmlite               0.28.0 Markdown               3.0.1 MarkupSafe             1.1.0 matplotlib             3.0.1 mccabe                 0.6.1 mistune                0.8.4 mkl-fft                1.0.6 mkl-random             1.0.2 nbconvert              5.3.1 nbformat               4.4.0 networkx               2.3 notebook               5.7.1 numba                  0.43.1 numpy                  1.15.4 pandas                 0.23.4 pandocfilters          1.4.2 parso                  0.3.1 patsy                  0.5.1 pep8                   1.7.1 pickleshare            0.7.5 pip                    18.1 prometheus-client      0.5.0 prompt-toolkit         2.0.7 protobuf               3.6.1 pycodestyle            2.5.0 pyflakes               2.1.1 Pygments               2.3.1 pylint                 2.1.1 pyparsing              2.3.0 pyrsistent             0.14.9 python-dateutil        2.7.5 python-speech-features 0.6 pytz                   2018.7 pywinpty               0.5.5 PyYAML                 3.13 pyzmq                  17.1.2 requests               2.22.0 resampy                0.2.1 scikit-learn           0.20.0 scipy                  1.1.0 seaborn                0.9.0 Send2Trash             1.5.0 setuptools             40.6.2 six                    1.11.0 statsmodels            0.9.0 tensorboard            1.12.0 tensorflow-gpu         1.12.0 termcolor              1.1.0 terminado              0.8.1 testpath               0.4.2 tornado                5.1.1 tqdm                   4.28.1 traitlets              4.3.2 typed-ast              1.1.0 urllib3                1.25.2 wcwidth                0.1.7 webencodings           0.5.1 Werkzeug               0.14.1 wheel                  0.32.2 wincertstore           0.2 wrapt                  1.10.11 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258def"}, "repo_url": "https://github.com/buiduchanh/keras_classification", "repo_name": "keras_classification", "repo_full_name": "buiduchanh/keras_classification", "repo_owner": "buiduchanh", "repo_desc": "Classification keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T07:31:05Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T07:28:48Z", "homepage": "", "size": 21, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188793354, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df0"}, "repo_url": "https://github.com/doandongnguyen/FuzzyDQN", "repo_name": "FuzzyDQN", "repo_full_name": "doandongnguyen/FuzzyDQN", "repo_owner": "doandongnguyen", "repo_desc": "Fuzzy DQN in Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T14:52:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T11:40:23Z", "homepage": "", "size": 526, "language": "Python", "has_wiki": true, "license": {"key": "unlicense", "name": "The Unlicense", "spdx_id": "Unlicense", "url": "https://api.github.com/licenses/unlicense", "node_id": "MDc6TGljZW5zZTE1"}, "open_issues_count": 0, "github_id": 189016889, "is_fork": false, "readme_text": "Fuzzy DQN Fuzzy Logic is used to generalize the state space for RL. In the fact that Fuzzy DQN's convergence is faster. For the use of fuzzy Q-Learning, please see FQL and autoscaling ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df1"}, "repo_url": "https://github.com/enginning/poetry_generator", "repo_name": "poetry_generator", "repo_full_name": "enginning/poetry_generator", "repo_owner": "enginning", "repo_desc": "lstm poetry_generator keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T14:52:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T03:21:11Z", "homepage": null, "size": 4941, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188764051, "is_fork": false, "readme_text": "poetry_generator_Keras use RNN + LSTM generate poetry by Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df2"}, "repo_url": "https://github.com/nric/VAE_CNN_Keras_TF2.0", "repo_name": "VAE_CNN_Keras_TF2.0", "repo_full_name": "nric/VAE_CNN_Keras_TF2.0", "repo_owner": "nric", "repo_desc": "Two different implementations of a Variational Autoencoder VAE with convolutional  Neural networks via Tesorflow 2.0/Keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T21:42:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T09:37:05Z", "homepage": null, "size": 522, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188814762, "is_fork": false, "readme_text": "VAE_CNN_Keras_TF2.0 Two different implementations of a Variational Autoencoder VAE with convolutional neural networks via Tesorflow 2.0/Keras. Both are encapulated into an object wich a train() (or alternativle load_weights()) and carries accesible enoder, decoder, and vae models inside them. Thier implementation hover differ. I did this for learning better how to implement statistical nodes and predictive models via tf2.0/keras. Whilst I don't claim any stability for implementation, I think it is very educational for understanding both the concepts of VAE and the options Keras have to implement statistical nodes and complex loss functions and divergeses. It is tested here on MNIST dataset. Performence wise, both are pretty identical. Class VAE_CNN1 is smilar to the Keras reference example: https://keras.io/examples/variational_autoencoder_deconv/ Class VAE_CNN2 is in accordance with the nice description of Tiao: http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://louistiao.me/posts/implementing-variational-autoencoders-in-keras-beyond-the-quickstart-tutorial/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df3"}, "repo_url": "https://github.com/Piotrek98/Object-detection", "repo_name": "Object-detection", "repo_full_name": "Piotrek98/Object-detection", "repo_owner": "Piotrek98", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T22:36:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T19:48:22Z", "homepage": null, "size": 2217, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188900898, "is_fork": false, "readme_text": "Object recogniton First commit: 14.05.2019 Language: Python 3.7.1 Python installation   Click here and download   Full Python documentation   Python Library:   OpenCV  pip install opencv-python  OpenCV documentation   TensorFlow  pip install tensorflow   Note: install possible only on 64bit system  TensorFlow documentation   Keras  pip install keras  Keras documentation  Note: before installation keras, you should install one of the backend engines like:   TensorFlow Theano CNTK    Numpy  pip install numpy  Numpy documentation   Argparse Parser for command line.  pip install argparse  Argparse documentation   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://deeplearning.net/software/theano/install.html#install"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df4"}, "repo_url": "https://github.com/SilverQ/Sentence-CNN-keras", "repo_name": "Sentence-CNN-keras", "repo_full_name": "SilverQ/Sentence-CNN-keras", "repo_owner": "SilverQ", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T03:12:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T09:26:34Z", "homepage": null, "size": 492, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188997102, "is_fork": false, "readme_text": "CNN-text-classification-keras It is simplified implementation of Implementing a CNN for Text Classification in TensorFlow in Keras as functional api The original code is from Bhavesh Vinod Oswal's github (https://github.com/bhaveshoswal/CNN-text-classification-keras) Requirements  Python 3.6.7 Tensorflow 1.13.1  Modifications  Adjusted the codes to Tensorflow 1.13.1 Add layers (see the file \"Modified Model.txt\") Reduced the epochs to 10 Change the checkpoint folder  Traning Command python model.py Result Overfitted so much. But I can't fix it.  loss: 0.0052 acc: 0.9995 val_loss: 1.5170 val_acc: 0.7459  For new data You have to rebuild the vocabulary and then train. For Citation @misc{bhaveshoswal,   author = {Bhavesh Vinod Oswal},   title = {CNN-text-classification-keras},   year = {2016},   publisher = {GitHub},   journal = {GitHub repository},   howpublished = {\\url{https://github.com/bhaveshoswal/CNN-text-classification-keras}}, }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df5"}, "repo_url": "https://github.com/nityansuman/tensormine", "repo_name": "tensormine", "repo_full_name": "nityansuman/tensormine", "repo_owner": "nityansuman", "repo_desc": "Hub repository for models from NLP and Computer Vision Domain in tf2.0", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T19:16:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T06:05:12Z", "homepage": "", "size": 37, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188780814, "is_fork": false, "readme_text": "TensorMine Standard and State-Of-The-Art Models Implemented in TensorFlow 2.0-alpha  Sequential/Functional Interface The best place to start is with the user-friendly Sequential API. You can create models by plugging together building blocks. Run the \u201cHello World\u201d example below, then visit the tutorials to learn more. my_model = tf.keras.models.Sequential([     tf.keras.layers.Flatten(input_shape=(28, 28)),     tf.keras.layers.Dense(128, activation='relu'),     tf.keras.layers.Dropout(0.2),     tf.keras.layers.Dense(10, activation='softmax') ])  OR  inputs = tf.keras.layers.Input(shape=(784,)) x = tf.keras.layers.Dense(64, activation='relu')(inputs) x = tf.keras.layers.Dense(64, activation='relu')(x) predictions = Dense(10, activation='softmax')(x) my_model = tf.keras.models.Model(inputs=inputs, outputs=predictions)  Subclassing Interface The Subclassing API provides a define-by-run interface for advanced research. Create a class for your model, then write the forward pass imperatively. Easily author custom layers, activations, training loop and much more. We will be using Subclassing to implement all models since it gives more control for advanced research. class MyModel(tf.keras.Model):     def __init__(self):         super(MyModel, self).__init__()         self.conv1 = Conv2D(32, 3, activation='relu')         self.flatten = Flatten()         self.d1 = Dense(128, activation='relu')         self.d2 = Dense(10, activation='softmax')      def call(self, x):         x = self.conv1(x)         x = self.flatten(x)         x = self.d1(x)         return self.d2(x)  # Call your model model = MyModel()  ", "has_readme": true, "readme_language": "English", "repo_tags": ["tensorflow", "model", "natural-language-processing", "computer-vision", "classification", "named-entity-recognition", "segmentation", "object-detection", "ml-pipeline"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df6"}, "repo_url": "https://github.com/1334390445/LeNet-keras", "repo_name": "LeNet-keras", "repo_full_name": "1334390445/LeNet-keras", "repo_owner": "1334390445", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T14:24:42Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T14:17:48Z", "homepage": null, "size": 70832, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188692059, "is_fork": false, "readme_text": "LeNet-keras \u4f7f\u7528keras\u6784\u5efaLeNet\u6a21\u578b ", "has_readme": true, "readme_language": "Samoan", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-01.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-02.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-03.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-04.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-05.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-06.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-07.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-08.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-09.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-10.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-11.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-12.h5", "https://github.com/1334390445/LeNet-keras/blob/fce1a80f05990c6d4806e7cbd2db907b688cbe65/checkpoint/model-13.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df7"}, "repo_url": "https://github.com/NasimSulaiman/keras-retinanet", "repo_name": "keras-retinanet", "repo_full_name": "NasimSulaiman/keras-retinanet", "repo_owner": "NasimSulaiman", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T08:22:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T07:08:15Z", "homepage": null, "size": 2166, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188789974, "is_fork": false, "readme_text": "Keras RetinaNet   Keras implementation of RetinaNet object detection as described in Focal Loss for Dense Object Detection by Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He and Piotr Doll\u00e1r. Installation  Clone this repository. Ensure numpy is installed using pip install numpy --user In the repository, execute pip install . --user. Note that due to inconsistencies with how tensorflow should be installed, this package does not define a dependency on tensorflow as it will try to install that (which at least on Arch Linux results in an incorrect installation). Please make sure tensorflow is installed as per your systems requirements. Alternatively, you can run the code directly from the cloned  repository, however you need to run python setup.py build_ext --inplace to compile Cython code first. Optionally, install pycocotools if you want to train / test on the MS COCO dataset by running pip install --user git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI.  Testing An example of testing the network can be seen in this Notebook. In general, inference of the network works as follows: boxes, scores, labels = model.predict_on_batch(inputs) Where boxes are shaped (None, None, 4) (for (x1, y1, x2, y2)), scores is shaped (None, None) (classification score) and labels is shaped (None, None) (label corresponding to the score). In all three outputs, the first dimension represents the shape and the second dimension indexes the list of detections. Loading models can be done in the following manner: from keras_retinanet.models import load_model model = load_model('/path/to/model.h5', backbone_name='resnet50') Execution time on NVIDIA Pascal Titan X is roughly 75msec for an image of shape 1000x800x3. Converting a training model to inference model The training procedure of keras-retinanet works with training models. These are stripped down versions compared to the inference model and only contains the layers necessary for training (regression and classification values). If you wish to do inference on a model (perform object detection on an image), you need to convert the trained model to an inference model. This is done as follows: # Running directly from the repository: keras_retinanet/bin/convert_model.py /path/to/training/model.h5 /path/to/save/inference/model.h5  # Using the installed script: retinanet-convert-model /path/to/training/model.h5 /path/to/save/inference/model.h5 Most scripts (like retinanet-evaluate) also support converting on the fly, using the --convert-model argument. Training keras-retinanet can be trained using this script. Note that the train script uses relative imports since it is inside the keras_retinanet package. If you want to adjust the script for your own use outside of this repository, you will need to switch it to use absolute imports. If you installed keras-retinanet correctly, the train script will be installed as retinanet-train. However, if you make local modifications to the keras-retinanet repository, you should run the script directly from the repository. That will ensure that your local changes will be used by the train script. The default backbone is resnet50. You can change this using the --backbone=xxx argument in the running script. xxx can be one of the backbones in resnet models (resnet50, resnet101, resnet152), mobilenet models (mobilenet128_1.0, mobilenet128_0.75, mobilenet160_1.0, etc), densenet models or vgg models. The different options are defined by each model in their corresponding python scripts (resnet.py, mobilenet.py, etc). Trained models can't be used directly for inference. To convert a trained model to an inference model, check here. Usage For training on Pascal VOC, run: # Running directly from the repository: keras_retinanet/bin/train.py pascal /path/to/VOCdevkit/VOC2007  # Using the installed script: retinanet-train pascal /path/to/VOCdevkit/VOC2007 For training on MS COCO, run: # Running directly from the repository: keras_retinanet/bin/train.py coco /path/to/MS/COCO  # Using the installed script: retinanet-train coco /path/to/MS/COCO The pretrained MS COCO model can be downloaded here. Results using the cocoapi are shown below (note: according to the paper, this configuration should achieve a mAP of 0.357).  Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.350  Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.537  Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.374  Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.191  Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.383  Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.472  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.306  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.491  Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.533  Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.345  Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.577  Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.681  For training on Open Images Dataset OID or taking place to the OID challenges, run: # Running directly from the repository: keras_retinanet/bin/train.py oid /path/to/OID  # Using the installed script: retinanet-train oid /path/to/OID  # You can also specify a list of labels if you want to train on a subset # by adding the argument 'labels_filter': keras_retinanet/bin/train.py oid /path/to/OID --labels-filter=Helmet,Tree  # You can also specify a parent label if you want to train on a branch # from the semantic hierarchical tree (i.e a parent and all children) (https://storage.googleapis.com/openimages/challenge_2018/bbox_labels_500_hierarchy_visualizer/circle.html) # by adding the argument 'parent-label': keras_retinanet/bin/train.py oid /path/to/OID --parent-label=Boat For training on KITTI, run: # Running directly from the repository: keras_retinanet/bin/train.py kitti /path/to/KITTI  # Using the installed script: retinanet-train kitti /path/to/KITTI  If you want to prepare the dataset you can use the following script: https://github.com/NVIDIA/DIGITS/blob/master/examples/object-detection/prepare_kitti_data.py For training on a [custom dataset], a CSV file can be used as a way to pass the data. See below for more details on the format of these CSV files. To train using your CSV, run: # Running directly from the repository: keras_retinanet/bin/train.py csv /path/to/csv/file/containing/annotations /path/to/csv/file/containing/classes  # Using the installed script: retinanet-train csv /path/to/csv/file/containing/annotations /path/to/csv/file/containing/classes In general, the steps to train on your own datasets are:  Create a model by calling for instance keras_retinanet.models.backbone('resnet50').retinanet(num_classes=80) and compile it. Empirically, the following compile arguments have been found to work well:  model.compile(     loss={         'regression'    : keras_retinanet.losses.smooth_l1(),         'classification': keras_retinanet.losses.focal()     },     optimizer=keras.optimizers.adam(lr=1e-5, clipnorm=0.001) )  Create generators for training and testing data (an example is show in keras_retinanet.preprocessing.pascal_voc.PascalVocGenerator). Use model.fit_generator to start training.  CSV datasets The CSVGenerator provides an easy way to define your own datasets. It uses two CSV files: one file containing annotations and one file containing a class name to ID mapping. Annotations format The CSV file with annotations should contain one annotation per line. Images with multiple bounding boxes should use one row per bounding box. Note that indexing for pixel values starts at 0. The expected format of each line is: path/to/image.jpg,x1,y1,x2,y2,class_name  Some images may not contain any labeled objects. To add these images to the dataset as negative examples, add an annotation where x1, y1, x2, y2 and class_name are all empty: path/to/image.jpg,,,,,  A full example: /data/imgs/img_001.jpg,837,346,981,456,cow /data/imgs/img_002.jpg,215,312,279,391,cat /data/imgs/img_002.jpg,22,5,89,84,bird /data/imgs/img_003.jpg,,,,,  This defines a dataset with 3 images. img_001.jpg contains a cow. img_002.jpg contains a cat and a bird. img_003.jpg contains no interesting objects/animals. Class mapping format The class name to ID mapping file should contain one mapping per line. Each line should use the following format: class_name,id  Indexing for classes starts at 0. Do not include a background class as it is implicit. For example: cow,0 cat,1 bird,2  Debugging Creating your own dataset does not always work out of the box. There is a debug.py tool to help find the most common mistakes. Particularly helpful is the --annotations flag which displays your annotations on the images from your dataset. Annotations are colored in green when there are anchors available and colored in red when there are no anchors available. If an annotation doesn't have anchors available, it means it won't contribute to training. It is normal for a small amount of annotations to show up in red, but if most or all annotations are red there is cause for concern. The most common issues are that the annotations are too small or too oddly shaped (stretched out). Results MS COCO Status Example output images using keras-retinanet are shown below.      Projects using keras-retinanet  NudeNet. Project that focuses on detecting and censoring of nudity. Individual tree-crown detection in RGB imagery using self-supervised deep learning neural networks. Research project focused on improving the performance of remotely sensed tree surveys. ESRI Object Detection Challenge 2019. Winning implementation of the ESRI Object Detection Challenge 2019. Lunar Rockfall Detector Project. The aim of this project is to map lunar rockfalls on a global scale using the available > 1.6 million satellite images. NATO Innovation Challenge. The winning team of the NATO Innovation Challenge used keras-retinanet to detect cars in aerial images (COWC dataset). Microsoft Research for Horovod on Azure. A research project by Microsoft, using keras-retinanet to distribute training over multiple GPUs using Horovod on Azure. Anno-Mage. A tool that helps you annotate images, using input from the keras-retinanet COCO model as suggestions. Telenav.AI. For the detection of traffic signs using keras-retinanet. Towards Deep Placental Histology Phenotyping. This research project uses keras-retinanet for analysing the placenta at a cellular level. 4k video example. This demo shows the use of keras-retinanet on a 4k input video. boring-detector. I suppose not all projects need to solve life's biggest questions. This project detects the \"The Boring Company\" hats in videos. comet.ml. Using keras-retinanet in combination with comet.ml to interactively inspect and compare experiments.  If you have a project based on keras-retinanet and would like to have it published here, shoot me a message on Slack. Notes  This repository requires Keras 2.2.4 or higher. This repository is tested using OpenCV 3.4. This repository is tested using Python 2.7 and 3.6.  Contributions to this project are welcome. Discussions Feel free to join the #keras-retinanet Keras Slack channel for discussions and questions. FAQ  I get the warning UserWarning: No training configuration found in save file: the model was not compiled. Compile it manually., should I be worried? This warning can safely be ignored during inference. I get the error ValueError: not enough values to unpack (expected 3, got 2) during inference, what to do?. This is because you are using a train model to do inference. See https://github.com/fizyr/keras-retinanet#converting-a-training-model-to-inference-model for more information. How do I do transfer learning? The easiest solution is to use the --weights argument when training. Keras will load models, even if the number of classes don't match (it will simply skip loading of weights when there is a mismatch). Run for example retinanet-train --weights snapshots/some_coco_model.h5 pascal /path/to/pascal to transfer weights from a COCO model to a PascalVOC training session. If your dataset is small, you can also use the --freeze-backbone argument to freeze the backbone layers. How do I change the number / shape of the anchors? The train tool allows to pass a configuration file, where the anchor parameters can be adjusted. Check here for an example config file. I get a loss of 0, what is going on? This mostly happens when none of the anchors \"fit\" on your objects, because they are most likely too small or elongated. You can verify this using the debug tool. I have an older model, can I use it after an update of keras-retinanet? This depends on what has changed. If it is a change that doesn't affect the weights then you can \"update\" models by creating a new retinanet model, loading your old weights using model.load_weights(weights_path, by_name=True) and saving this model. If the change has been too significant, you should retrain your model (you can try to load in the weights from your old model when starting training, this might be a better starting position than ImageNet). I get the error ModuleNotFoundError: No module named 'keras_retinanet.utils.compute_overlap', how do I fix this? Most likely you are running the code from the cloned repository. This is fine, but you need to compile some extensions for this to work (python setup.py build_ext --inplace). How do I train on my own dataset? The steps to train on your dataset are roughly as follows:   Prepare your dataset in the CSV format (a training and validation split is advised).     Check that your dataset is correct using retinanet-debug.     Train retinanet, preferably using the pretrained COCO weights (this gives a far better starting point, making training much quicker and accurate). You can optionally perform evaluation of your validation set during training to keep track of how well it performs (advised).     Convert your training model to an inference model.     Evaluate your inference model on your test or validation set.     Profit!    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.cvlibs.net/datasets/kitti/eval_object.php", "http://host.robots.ox.ac.uk/pascal/VOC/", "http://cocodataset.org/#home"], "reference_list": ["https://arxiv.org/abs/1708.02002", "https://ieeexplore.ieee.org/document/8587120"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258df8"}, "repo_url": "https://github.com/vinojhosan/vinoj_yolo_object_dection", "repo_name": "vinoj_yolo_object_dection", "repo_full_name": "vinojhosan/vinoj_yolo_object_dection", "repo_owner": "vinojhosan", "repo_desc": "Yolo Object detection using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T09:09:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T09:34:16Z", "homepage": null, "size": 110, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188814297, "is_fork": false, "readme_text": "vinoj_yolo_object_dection Yolo Object detection using Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258df9"}, "repo_url": "https://github.com/Osdel/Char-Level-Sequence-Generator-Keras-", "repo_name": "Char-Level-Sequence-Generator-Keras-", "repo_full_name": "Osdel/Char-Level-Sequence-Generator-Keras-", "repo_owner": "Osdel", "repo_desc": "In this project a Keras model is used for generate sequences using a char-level approach. The code uses Tensorflow 1.13 as backed and Keras 2.2.4", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T17:51:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T15:00:15Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188863949, "is_fork": false, "readme_text": "Char-Level-Sequence-Generator-Keras- In this project a Keras model is used for generate sequences using a char-level approach. The code uses Tensorflow 1.13 as backed and Keras 2.2.4, because all previous public repositories I found where deprecated. The model was trained and tested first with a dinosaur name dataset. This was a DeepLearning.ai specialization project I took, but in that moment I did it from scratch using numpy. So, I decided to develop a general model with Keras which allows to generated sequences char-by-char using one-hot encoding representation of chars, in this case dinosaurs names. Dependencies  numpy tensorflow keras  Training on Dino Names Dataset The script dino_names_train.py allows to train the model on the dino dataset. The dataset wasn't uploaded to let the repo clean, in case you needed I will send you. To run the training type into the command line: python dino_names_train.py dataset_path output_json_configuration_path output_weights_h5file_path \\ Tx n_a epochs  The parameter Tx will be removed soon, by now it defines the max lenght of the sequences and all names are padded to size Tx. For more information about the command line arguments, type: python dino_names_train.py -h  Generating Names Sequences For sampling new sequences, run the script generator.py as follow: python names_generator.py json_configuration_file_path weights_h5file_path number_of_samples  Note that the json and h5 files requeried are the same we stored in the training step. For more information about the arguments run the -h command as mentioned above in the training section. Some examples of generated names  injiangovenator unowasia roaesaurus nuanoddes aganocacer ndosaurus anatasaura ianghangosaurus aacorenator aoeyia oteuarlausaurn ingxiusaurus uoasaurus olindapteryx unchisaurus acraasaurus iniaosaurus  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dfa"}, "repo_url": "https://github.com/pypancho/keras-yolov3", "repo_name": "keras-yolov3", "repo_full_name": "pypancho/keras-yolov3", "repo_owner": "pypancho", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T00:17:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T00:13:27Z", "homepage": null, "size": 10373, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188744389, "is_fork": false, "readme_text": "keras-yolov3 for reference, weights, etc: https://github.com/bing0037/keras-yolo3 tiny_KAIST_dataset link. https://drive.google.com/drive/folders/1YEyWAQzrUUorao-Eh89aZO7UfCc_NM6s?usp=sharing ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dfb"}, "repo_url": "https://github.com/harrytsz/DeepLearning_On_Music_mid", "repo_name": "DeepLearning_On_Music_mid", "repo_full_name": "harrytsz/DeepLearning_On_Music_mid", "repo_owner": "harrytsz", "repo_desc": "Deeplearning on music of mid data type.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T08:45:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T07:48:59Z", "homepage": null, "size": 22, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188796605, "is_fork": false, "readme_text": "   Note: deepjazz has been succeeded by songbird.ai and is no longer being actively developed. deepjazz Using Keras & Theano for deep learning driven jazz generation I built deepjazz in 36 hours at a hackathon. It uses Keras & Theano, two deep learning libraries, to generate jazz music. Specifically, it builds a two-layer LSTM, learning from the given MIDI file. It uses deep learning, the AI tech that powers Google's AlphaGo and IBM's Watson, to make music -- something that's considered as deeply human. SoundCloud Check out deepjazz's music on SoundCloud! Dependencies  Keras Theano (\"bleeding-edge\" version on GitHub) music21  Run on CPU with command: python generator.py [# of epochs]  Run on GPU with command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python generator.py [# of epochs] Note: running Keras/Theano on GPU is formally supported for only NVIDIA cards (CUDA backend).  Note: preprocess.py must be modified to work with other MIDI files (the relevant \"melody\" MIDI part needs to be selected). The ability to handle this natively is a planned feature. If you want to get more detail about music21, you can visit the webpage below: Music21 Documentation: http://web.mit.edu/music21/doc/ Installation: 1.music21 pip install music21  2.pygame You may should also install the pygame package: pip install pygame  You can download mid music from the webpage : http://midi.midicn.com/  Created By Harrytsz  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://web.mit.edu/music21/doc/", "http://midi.midicn.com/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dfc"}, "repo_url": "https://github.com/h4rr9/BERT-CNN", "repo_name": "BERT-CNN", "repo_full_name": "h4rr9/BERT-CNN", "repo_owner": "h4rr9", "repo_desc": "testing the performance of CNN and BERT embeddings  on GLUE tasks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T16:43:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T08:24:15Z", "homepage": null, "size": 36, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188802411, "is_fork": false, "readme_text": "CNN on BERT Embeddings Testing the performance of CNN and pretrained BERT embeddings on the GLUE Tasks BERT Model The BERT model used is the BERT-Base, Uncased: 12-layer, 768-hidden, 12-heads, 110M parameters it is available at https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1 replace the https://tfhub.dev with https://storage.googleapis.com/tfhub-modules and append a .tar.gz and place it in ./data/bert_module the tokenization.py file is from the google-research/bert repo https://github.com/google-research/bert GLUE tasks execute python ./utils/download_glue_data.py --data_dir ./data/glue_data to download the tasks. execute python ./utils/preprocess_tasks.py --data_dir ./data/glue_data to process the tsv files so pandas can read them. Keras template the keras template is based on https://github.com/Ahmkel/Keras-Project-Template ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dfd"}, "repo_url": "https://github.com/cyrushu/tfkeras_project_template", "repo_name": "tfkeras_project_template", "repo_full_name": "cyrushu/tfkeras_project_template", "repo_owner": "cyrushu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T06:44:17Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T06:38:19Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188969169, "is_fork": false, "readme_text": "tensorflow keras \u9879\u76ee\u6a21\u677f   \u5c06project\u91cd\u547d\u540d rename project into project name   \u4fee\u6539setup.py\u91cc\u9762\u9879\u76ee\u540d\u5b57\uff0c\u6a21\u677f\u5373\u53ef\u4f7f\u7528 modify setup.py project name and template will works fine   \u6b64\u6a21\u677f\u7531Keras-Project-Template\u4fee\u6539\u800c\u6765 Template modified with project: Keras-Project-Template   setup.py\u914d\u7f6e\u9879\u76ee\u4f9d\u8d56 Use setup.py to configure project required packages   \u65b0\u589edata_generator\u4ee5\u66f4\u65b0\u6570\u636e\u8f93\u5165 Add data_generator class for the Inputs   \u65b0\u589emodel\u4ee5\u8f93\u5165\u7f51\u7edc\u8bbe\u8ba1 Add model class for Network design   \u590d\u5236config/run_train.yaml\u4ee5\u521b\u5efa\u914d\u7f6e make a version of project/config/run_train.yaml and configure the yaml file for training/other usage   python train.py -c my_run_train.yaml   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258dfe"}, "repo_url": "https://github.com/adesgautam/Fedlearn", "repo_name": "Fedlearn", "repo_full_name": "adesgautam/Fedlearn", "repo_owner": "adesgautam", "repo_desc": "An implementation for a federated learning environment in python using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T10:50:07Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T08:39:45Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188659093, "is_fork": false, "readme_text": "Fedlearn An implementation to simulate a federated learning environment. It is an attempt to mimic the scenario described in the paper Towards Federated Learning at Scale: System Design Tech stack  Python 3.6.1 Keras Flask  Run the system using the steps below: Booting up  Run \"Device 1\" using python app.py Run \"Device 2\" using python app.py Run \"Main Server\" using python main_server.py  This will start the Flask servers of the two devices and the main server. Servers -  Main server - http://localhost:8000/ Device1 - http://localhost:8001/ Device2 - http://localhost:8002/  Everything will work using the REST APIs. System working  First a model will be trained locally on the device. On 'Device1' and 'Device2' server navigate to: http://localhost:8001/modeltrain and http://localhost:8002/modeltrain respectively.  The models will be trained on MNIST data.  Once the devices are ready send a status signal to the server that they are ready using, http://localhost:8001/sendstatus and http://localhost:8002/sendstatus.  There will be a response from the main server.   Now, the trained models will be sent to the main server using http://localhost:8001/sendmodel and http://localhost:8002/sendmodel   The main server will aggregate the model and build a global model.  http://localhost:80010/aggregation   The main server will send this aggregated model to the devices. http://localhost:8000/sendagg   Goto step 1. The whole process is repeated again and the aggregated global model is improved at every round.   I tested this on the MNIST data and after 2 rounds got an accuracy of about 95%. The model will eventually update when more devices will be used. The models on devices are being trained on all the train data and also because of this the accuracy is consistent, if the data is partitioned among the devices it would reveal the real performance. If you would improve the current system please feel free to experiment and submit a PR. Future improvements  Better model aggregation methods. Main server to send parameter configuration to devices based upon their performance. A better dataset to simulate the real world usage. Android app for simulating devices. Main server migration to Heroku or AWS.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1902.01046"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258dff"}, "repo_url": "https://github.com/popcornell/keras-triplet-center-loss", "repo_name": "keras-triplet-center-loss", "repo_full_name": "popcornell/keras-triplet-center-loss", "repo_owner": "popcornell", "repo_desc": "Simple Keras implementation of Triplet-Center Loss on the MNIST dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T11:15:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T08:51:31Z", "homepage": "", "size": 4546, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188991143, "is_fork": false, "readme_text": "keras-triplet-center-loss A simple Keras implementation of Triplet-Center Loss on the MNIST dataset. As a reference in this repository also implementations of other two similar losses, Center-Loss and Triplet-Loss are included. The Center-Loss implementation is from shamangary: https://github.com/shamangary/Keras-MNIST-center-loss-with-visualization The Triplet-Loss implementation is from  KinWaiCheuk: https://github.com/KinWaiCheuk/Triplet-net-keras  Triplet-Center Loss Triplet-Center Loss has been introduced by He et al. in https://arxiv.org/abs/1803.06189. It is an \"hybrid\" loss between Center Loss and Triplet Loss that allows to maximise inter-class distance and minimize intra-class distance. Details In this repository a simple implementation on the MNSIT or alternatively Fashion MNIST is shown. Running main.py will start sequentially 4 training routines with 4 different losses:  Categorical Crossentropy only Center-loss + Categorical Crossentropy Triplet-loss + Categorical Crossentropy Triplet-Center loss + Categorical Crossentropy  In Folder runs there will be the results of those models, including Tensorboard summaries. Also T-SNE is run on the embeddings to visualize how the network internal representation changes as the loss is changed.  triplet-center loss, T-SNE on internal representation (Train Data):   Center loss, T-SNE on internal representation (Train Data):   Center loss, T-SNE on internal representation (Train Data):  As it can be seen the triplet-center loss maximises the inter-class distance as the Triplet Loss while keeping the Center-loss characteristic of minimizing intra-class distance. Another advantage of Triplet-Center loss is that it does not need advanced batching and triplet selection mining techniques as the Triplet-Loss does. ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "tensorflow", "triplet-loss", "center-loss", "mnist", "machine-learning"], "has_h5": true, "h5_files_links": ["https://github.com/popcornell/keras-triplet-center-loss/blob/3a357f2d03e907a05512e187b5b2d31c57d87584/runs/center_loss/center_loss_model.h5", "https://github.com/popcornell/keras-triplet-center-loss/blob/3a357f2d03e907a05512e187b5b2d31c57d87584/runs/triplet_center_loss/triplet_center_loss_model.h5", "https://github.com/popcornell/keras-triplet-center-loss/blob/3a357f2d03e907a05512e187b5b2d31c57d87584/runs/triplet_loss/triplet_loss_model.h5", "https://github.com/popcornell/keras-triplet-center-loss/blob/3a357f2d03e907a05512e187b5b2d31c57d87584/runs/xentropy_only_loss/xentropy_loss_model.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1803.06189"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e00"}, "repo_url": "https://github.com/jeju-sw-hackathon/django-web", "repo_name": "django-web", "repo_full_name": "jeju-sw-hackathon/django-web", "repo_owner": "jeju-sw-hackathon", "repo_desc": "Django web", "description_language": "Lingala", "repo_ext_links": null, "repo_last_mod": "2019-05-31T22:53:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T05:33:06Z", "homepage": null, "size": 82664, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188960355, "is_fork": false, "readme_text": "Dango web application using emotion classifier This project was inspired by this projects. We used Multi-Bilstm-layer with attention layer using Keras Dependency  python 3.7.3 django 2.2.1 tensorflow 1.13.1 keras 2.2.4 text-unidecode 1.2 emoji 0.5.2 django-crispy-form 1.7.2 django-crispy-bulma 0.1.2  Future Works We will add wysiwyg editor to our board. To do this, We will use this repository. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/jeju-sw-hackathon/django-web/blob/a692a73759f16c106df9c59ea274c14f501e81ba/boards/jejuDLcamp_emotion/models/deepmoji_weights.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e01"}, "repo_url": "https://github.com/clauden/ml-fun", "repo_name": "ml-fun", "repo_full_name": "clauden/ml-fun", "repo_owner": "clauden", "repo_desc": "Experimenting with ML tools", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T06:43:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T05:48:30Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188962146, "is_fork": false, "readme_text": "Playing around with Keras and stuff  test-keras.py Based on the Basic classification example from Tensorflow 5/27/2019  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e02"}, "repo_url": "https://github.com/Dolphinn123/mix-fix-point", "repo_name": "mix-fix-point", "repo_full_name": "Dolphinn123/mix-fix-point", "repo_owner": "Dolphinn123", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T23:05:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T22:11:13Z", "homepage": null, "size": 2415, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188736487, "is_fork": false, "readme_text": "mix-fix-point This is a fix-point version of tensorpack (backend is tensorflow). train  install tensorflow-gpu v1.9.0 and tensorpack.  pip install tensorflow-gpu==1.9.0 pip install tensorpack   use the folds and files in this repository to replace the origin ones.  cp tensorflow/python/keras/layers/* YOUR_PATH_TO_TENSORFLOW/python/keras/layers/ cp tensorflow/python/ops/* YOUR_PATH_TO_TENSORFLOW/python/ops/ cp tensorflow/python/training/optimizer.py YOUR_PATH_TO_TENSORFLOW/python/training/ cp tensorpack/train/tower.py YOUR_PATH_TO_TENSORPACK/train/   change the parameters.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e03"}, "repo_url": "https://github.com/ttvand/Basic-Generative-Models", "repo_name": "Basic-Generative-Models", "repo_full_name": "ttvand/Basic-Generative-Models", "repo_owner": "ttvand", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T17:23:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T13:00:21Z", "homepage": null, "size": 22139, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188844139, "is_fork": false, "readme_text": "Basic Generative Models using Keras on MNIST This repository contains some starter code to train and inspect basic generative models. Keras is used to model the classic MNIST data set. There is a separate folder for Variational AutoEncoders (VAE) and Generative Adversarial Networks (GAN). Both generative model folders consist of a main file and a utilities file. All hyperparameters, training and inspection logic is located in a concise main file with all additional logic in the utilities file. Both models also allow basic conditional encoding/decoding and generation/discrimination of MNIST digits. DISCLAIMER: these models are not tuned for performance. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e04"}, "repo_url": "https://github.com/ayakut16/alpha-zero-connect4", "repo_name": "alpha-zero-connect4", "repo_full_name": "ayakut16/alpha-zero-connect4", "repo_owner": "ayakut16", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T00:18:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T22:23:36Z", "homepage": null, "size": 235539, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189111288, "is_fork": false, "readme_text": "Alpha Zero General (any game, any framework!) A simplified, highly flexible, commented and (hopefully) easy to understand implementation of self-play based reinforcement learning based on the AlphaGo Zero paper (Silver et al). It is designed to be easy to adopt for any two-player turn-based adversarial game and any deep learning framework of your choice. A sample implementation has been provided for the game of Othello in PyTorch, Keras, TensorFlow and Chainer. An accompanying tutorial can be found here. We also have implementations for GoBang and TicTacToe. To use a game of your choice, subclass the classes in Game.py and NeuralNet.py and implement their functions. Example implementations for Othello can be found in othello/OthelloGame.py and othello/{pytorch,keras,tensorflow,chainer}/NNet.py. Coach.py contains the core training loop and MCTS.py performs the Monte Carlo Tree Search. The parameters for the self-play can be specified in main.py. Additional neural network parameters are in othello/{pytorch,keras,tensorflow,chainer}/NNet.py (cuda flag, batch size, epochs, learning rate etc.). To start training a model for Othello: python main.py Choose your framework and game in main.py. Docker Installation For easy environment setup, we can use nvidia-docker. Once you have nvidia-docker set up, we can then simply run: ./setup_env.sh  to set up a (default: pyTorch) Jupyter docker container. We can now open a new terminal and enter: docker exec -ti pytorch_notebook python main.py  Experiments We trained a PyTorch model for 6x6 Othello (~80 iterations, 100 episodes per iteration and 25 MCTS simulations per turn). This took about 3 days on an NVIDIA Tesla K80. The pretrained model (PyTorch) can be found in pretrained_models/othello/pytorch/. You can play a game against it using pit.py. Below is the performance of the model against a random and a greedy baseline with the number of iterations.  A concise description of our algorithm can be found here. Contributing While the current code is fairly functional, we could benefit from the following contributions:  Game logic files for more games that follow the specifications in Game.py, along with their neural networks Neural networks in other frameworks Pre-trained models for different game configurations An asynchronous version of the code- parallel processes for self-play, neural net training and model comparison. Asynchronous MCTS as described in the paper  Contributors and Credits  Shantanu Thakoor and Megha Jhunjhunwala helped with core design and implementation. Shantanu Kumar contributed TensorFlow and Keras models for Othello. Evgeny Tyurin contributed rules and a trained model for TicTacToe. MBoss contributed rules and a model for GoBang. Jernej Habjan contributed RTS game.  Thanks to pytorch-classification and progress. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://web.stanford.edu/~surag/posts/alphazero.html"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e05"}, "repo_url": "https://github.com/Happiyin/keras-yolo3-master", "repo_name": "keras-yolo3-master", "repo_full_name": "Happiyin/keras-yolo3-master", "repo_owner": "Happiyin", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T13:05:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T12:51:21Z", "homepage": null, "size": 22328, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188682481, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e06"}, "repo_url": "https://github.com/luozhouyang/matchpyramid", "repo_name": "matchpyramid", "repo_full_name": "luozhouyang/matchpyramid", "repo_owner": "luozhouyang", "repo_desc": "MatchPyramid implementation in tensorflow 2.x (Keras).", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T10:16:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T09:00:44Z", "homepage": null, "size": 13, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188992748, "is_fork": false, "readme_text": "matchpyramid MatchPyramid implementation in tensorflow 2.x (Keras). Paper: A Study of MatchPyramid Models on Ad-hoc Retrieval Implemented matching matrix methods:  Indicator function Cosine Dot-product  Not implemented methods:  Gaussian kernel  Requirements tensorflow==2.0.0a easylib==0.0.5  License Copyright 2019 luozhouyang  Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at      http://www.apache.org/licenses/LICENSE-2.0  Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1606.04648"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e07"}, "repo_url": "https://github.com/brandontrabucco/big_gan", "repo_name": "big_gan", "repo_full_name": "brandontrabucco/big_gan", "repo_owner": "brandontrabucco", "repo_desc": "Implements big gan in tensorflow using keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T14:21:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T14:21:15Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189043113, "is_fork": false, "readme_text": "big_gan Implements big gan in tensorflow using keras. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e08"}, "repo_url": "https://github.com/rabahFr/AreYouOld", "repo_name": "AreYouOld", "repo_full_name": "rabahFr/AreYouOld", "repo_owner": "rabahFr", "repo_desc": "CNN model for \"age\" recognition with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:32:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T10:30:50Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189007132, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e09"}, "repo_url": "https://github.com/yanjintao25/facial_orientation_recognition", "repo_name": "facial_orientation_recognition", "repo_full_name": "yanjintao25/facial_orientation_recognition", "repo_owner": "yanjintao25", "repo_desc": "facial orientation recognition besed on keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T11:11:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T08:48:02Z", "homepage": "", "size": 2633, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188990511, "is_fork": false, "readme_text": "facial_orientation_recognition facial orientation recognition besed on keras weights download: https://pan.baidu.com/s/1UvPz0xLRhnq7a576SmfaqA Extraction code\uff1au2pe ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e0a"}, "repo_url": "https://github.com/frankzielen/xorneuralnet", "repo_name": "xorneuralnet", "repo_full_name": "frankzielen/xorneuralnet", "repo_owner": "frankzielen", "repo_desc": "Neural net for XOR with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:53:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T12:48:27Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189027149, "is_fork": false, "readme_text": "xorneuralnet Neural net for XOR with Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e0b"}, "repo_url": "https://github.com/hamza1331/Simple-HTR", "repo_name": "Simple-HTR", "repo_full_name": "hamza1331/Simple-HTR", "repo_owner": "hamza1331", "repo_desc": "Handwritten Text Recognition using TensorFlow and Keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T05:37:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T05:36:44Z", "homepage": null, "size": 36713, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188960738, "is_fork": false, "readme_text": "Handwritten Text Recognition with TensorFlow Handwritten Text Recognition (HTR) system implemented with TensorFlow (TF) and trained on the IAM off-line HTR dataset. This Neural Network (NN) model recognizes the text contained in the images of segmented words as shown in the illustration below. As these word-images are smaller than images of complete text-lines, the NN can be kept small and training on the CPU is feasible. 3/4 of the words from the validation-set are correctly recognized and the character error rate is around 10%. I will give some hints how to extend the model in case you need larger input-images (e.g. to recognize text-lines) or want better recognition accuracy.  Run demo Go to the model/ directory and unzip the file model.zip (pre-trained on the IAM dataset). Take care that the unzipped files are placed directly into the model/ directory and not some subdirectory created by the unzip-program. Afterwards, go to the src/ directory and run python main.py. The input image and the expected output is shown below.  > python main.py Validation character error rate of saved model: 10.624916% Init with stored values from ../model/snapshot-38 Recognized: \"little\" Probability: 0.96625507  Tested with:  Python 2 and Python 3 TF 1.3, 1.10 and 1.12 MacOS High Sierra (10.13.6).  Command line arguments  --train: train the NN, details see below. --validate: validate the NN, details see below. --beamsearch: use vanilla beam search decoding (better, but slower) instead of best path decoding. --wordbeamsearch: use word beam search decoding (only outputs words contained in a dictionary) instead of best path decoding. This is a custom TF operation and must be compiled from source, more information see corresponding section below. It should not be used when training the NN. --dump: dumps the output of the NN to CSV file(s) saved in the dump/ folder. Can be used as input for the CTCDecoder.  If neither --train nor --validate is specified, the NN infers the text from the test image (data/test.png). Two examples: if you want to infer using beam search, execute python main.py --beamsearch, while you have to execute python main.py --train --beamsearch if you want to train the NN and do the validation using beam search. Integrate word beam search decoding Besides the two decoders shipped with TF, it is possible to use word beam search decoding [4]. Using this decoder, words are constrained to those contained in a dictionary, but arbitrary non-word character strings (numbers, punctuation marks) can still be recognized. The following illustration shows a sample for which word beam search is able to recognize the correct text, while the other decoders fail.  Follow these instructions to integrate word beam search decoding:  Clone repository CTCWordBeamSearch. Compile custom TF operation (follow instructions given in README). Copy binary TFWordBeamSearch.so from the CTCWordBeamSearch repository to the src/ directory of the SimpleHTR repository.  Word beam search can now be enabled by setting the corresponding command line argument. The dictionary is created (in training and validation mode) by using all words contained in the IAM dataset (i.e. also including words from validation set) and is saved into the file data/corpus.txt. Further, the (manually created) list of word-characters can be found in the file model/wordCharList.txt. Beam width is set to 50 to conform with the beam width of vanilla beam search decoding. Using this configuration, a character error rate of 8% and a word accuracy of 84% is achieved. Train model IAM dataset The data-loader expects the IAM dataset [5] (or any other dataset that is compatible with it) in the data/ directory. Follow these instructions to get the dataset:  Register for free at this website. Download words/words.tgz. Download ascii/words.txt. Put words.txt into the data/ directory. Create the directory data/words/. Put the content (directories a01, a02, ...) of words.tgz into data/words/. Go to data/ and run python checkDirs.py for a rough check if everything is ok.  If you want to train the model from scratch, delete the files contained in the model/ directory. Otherwise, the parameters are loaded from the last model-snapshot before training begins. Then, go to the src/ directory and execute python main.py --train. After each epoch of training, validation is done on a validation set (the dataset is split into 95% of the samples used for training and 5% for validation as defined in the class DataLoader). If you only want to do validation given a trained NN, execute python main.py --validate. Training on the CPU takes 18 hours on my system (VM, Ubuntu 16.04, 8GB of RAM and 4 cores running at 3.9GHz). The expected output is shown below. > python main.py --train Init with new values Epoch: 1 Train NN Batch: 1 / 500 Loss: 130.354 Batch: 2 / 500 Loss: 66.6619 Batch: 3 / 500 Loss: 36.0154 Batch: 4 / 500 Loss: 24.5898 Batch: 5 / 500 Loss: 20.1845 Batch: 6 / 500 Loss: 19.2857 Batch: 7 / 500 Loss: 18.3493 ...  Validate NN Batch: 1 / 115 Ground truth -> Recognized [OK] \",\" -> \",\" [ERR:1] \"Di\" -> \"D\" [OK] \",\" -> \",\" [OK] \"\"\" -> \"\"\" [OK] \"he\" -> \"he\" [OK] \"told\" -> \"told\" [ERR:2] \"her\" -> \"nor\" ... Character error rate: 13.956289%. Word accuracy: 67.721739%.  Other datasets Either convert your dataset to the IAM format (look at words.txt and the corresponding directory structure) or change the class DataLoader according to your dataset format. More information can be found in this article. Information about model Overview The model [1] is a stripped-down version of the HTR system I implemented for my thesis [2][3]. What remains is what I think is the bare minimum to recognize text with an acceptable accuracy. The implementation only depends on numpy, cv2 and tensorflow imports. It consists of 5 CNN layers, 2 RNN (LSTM) layers and the CTC loss and decoding layer. The illustration below gives an overview of the NN (green: operations, pink: data flowing through NN) and here follows a short description:  The input image is a gray-value image and has a size of 128x32 5 CNN layers map the input image to a feature sequence of size 32x256 2 LSTM layers with 256 units propagate information through the sequence and map the sequence to a matrix of size 32x80. Each matrix-element represents a score for one of the 80 characters at one of the 32 time-steps The CTC layer either calculates the loss value given the matrix and the ground-truth text (when training), or it decodes the matrix to the final text with best path decoding or beam search decoding (when inferring) Batch size is set to 50   Improve accuracy 74% of the words from the IAM dataset are correctly recognized by the NN when using vanilla beam search decoding. If you need a better accuracy, here are some ideas how to improve it [2]:  Data augmentation: increase dataset-size by applying further (random) transformations to the input images. At the moment, only random distortions are performed. Remove cursive writing style in the input images (see DeslantImg). Increase input size (if input of NN is large enough, complete text-lines can be used, see lamhoangtung/LineHTR). Add more CNN layers (see discussion). Replace LSTM by 2D-LSTM. Replace optimizer: Adam improves the accuracy, however, the number of training epochs increases (see discussion). Decoder: use token passing or word beam search decoding [4] (see CTCWordBeamSearch) to constrain the output to dictionary words. Text correction: if the recognized word is not contained in a dictionary, search for the most similar one.  Analyze model Run python analyze.py with the following arguments to analyze the image file data/analyze.png with the ground-truth text \"are\":  --relevance: compute the pixel relevance for the correct prediction. --invariance: check if the model is invariant to horizontal translations of the text. No argument provided: show the results.  Results are shown in the plots below. The pixel relevance (left) shows how a pixel influences the score for the correct class. Red pixels vote for the correct class, while blue pixels vote against the correct class. It can be seen that the white space above vertical lines in images is important for the classifier to decide against the \"i\" character with its superscript dot. Draw a dot above the \"a\" (red region in plot) and you will get \"aive\" instead of \"are\". The second plot (right) shows how the probability of the ground-truth text changes when the text is shifted to the right. As can be seen, the model is not translation invariant, as all training images from IAM are left-aligned. Adding data augmentation which uses random text-alignments can improve the translation invariance of the model. More information can be found in this article.  FAQ  I get the error message \"Exception: No saved model found in: ... \": unzip the file model/model.zip. All files contained must be placed directly into the model/ directory and not in some subdirectory created by the unzip-program. I get the error message \"... TFWordBeamSearch.so: cannot open shared object file: No such file or directory\": if you want to use word beam search decoding, you have to compile the custom TF operation from source. I get the error message \"... ModuleNotFoundError: No module named 'editdistance'\": you have to install the mentioned module by executing pip install editdistance. Where can I find the file words.txt of the IAM dataset: it is located in the subfolder ascii of the IAM website. I want to recognize text of line (or sentence) images: this is not possible with the provided model. The size of the input image is too small. For more information read this article or have a look at the lamhoangtung/LineHTR repository. I need a confidence score for the recognized text: after recognizing the text, you can calculate the loss value for the NN output and the recognized text. The loss simply is the negative logarithm of the score. See this article. I use a custom image of handwritten text, but the NN outputs a wrong result: the NN is trained on the IAM dataset. The NN not only learns to recognize text, but it also learns properties of the dataset-images. Some obvious properties of the IAM dataset are: text is tightly cropped, contrast is very high, most of the characters are lower-case. Either you preprocess your image to look like an IAM image, or you train the NN on your own dataset. See this article. I get an error when running the script more than once from an interactive Python session: do not call function main() in file main.py from an interactive session, as the TF computation graph is created multiple times when calling main() multiple times. Run the script by executing python main.py instead.  References [1] Build a Handwritten Text Recognition System using TensorFlow [2] Scheidl - Handwritten Text Recognition in Historical Documents [3] Shi - An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition [4] Scheidl - Word Beam Search: A Connectionist Temporal Classification Decoding Algorithm [5] Marti - The IAM-database: an English sentence database for offline handwriting recognition ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.fki.inf.unibe.ch/databases/iam-handwriting-database"], "reference_list": ["https://arxiv.org/pdf/1507.05717.pdf"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e0c"}, "repo_url": "https://github.com/phunc20/DCGAN-keras", "repo_name": "DCGAN-keras", "repo_full_name": "phunc20/DCGAN-keras", "repo_owner": "phunc20", "repo_desc": "A keras implementation for carpedm20's DCGAN-tensorflow", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T09:48:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T09:03:43Z", "homepage": "https://github.com/carpedm20/DCGAN-tensorflow", "size": 1767, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188993255, "is_fork": false, "readme_text": "DCGAN-keras One can download the celebA dataset from, for example, kaggle. After unzip the dataset file, there should be a folder called img_align_celeba, point the path celebA_path in the python code to the path of your downloaded img_align_celeba. After this, I think one can proceed to run the kh5_carpedm20.py code. Rmk.   I did not find the time to include a requirements.txt file for the python packages, but they are just some standard packages described in the import part of the code like keras, pillow, etc.   Windows users might find the printed stdout annoying; if so, simply delete all print involving bcolor. I think that only works under linux.   The one I search to fix  What simpler model can achieve  What carpedm20's model should achieve  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e0d"}, "repo_url": "https://github.com/DmitrievEgor94/StyleGAN-Keras-implementation", "repo_name": "StyleGAN-Keras-implementation", "repo_full_name": "DmitrievEgor94/StyleGAN-Keras-implementation", "repo_owner": "DmitrievEgor94", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T08:27:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T08:13:46Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188656853, "is_fork": false, "readme_text": "StyleGAN(Keras implementation) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e0e"}, "repo_url": "https://github.com/luciaL/paper_code_keras", "repo_name": "paper_code_keras", "repo_full_name": "luciaL/paper_code_keras", "repo_owner": "luciaL", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T13:05:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T12:49:33Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188842531, "is_fork": false, "readme_text": "Networks in papers with Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e0f"}, "repo_url": "https://github.com/nmalcev/cifar10-demo-with-keras", "repo_name": "cifar10-demo-with-keras", "repo_full_name": "nmalcev/cifar10-demo-with-keras", "repo_owner": "nmalcev", "repo_desc": "Realization for Jetson TK1", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T14:15:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T13:53:06Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188852594, "is_fork": false, "readme_text": "cifar10-demo-with-keras Realization for Jetson TK1 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e10"}, "repo_url": "https://github.com/go1217jo/face_and_plate-blurring-using-keras", "repo_name": "face_and_plate-blurring-using-keras", "repo_full_name": "go1217jo/face_and_plate-blurring-using-keras", "repo_owner": "go1217jo", "repo_desc": "Blurring face and car registration plate detected by Yolo3 model except to recognized faces", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T07:54:05Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T07:52:15Z", "homepage": null, "size": 24, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188655002, "is_fork": false, "readme_text": "yolo3 library : https://github.com/qqwweee/keras-yolo3.git ", "has_readme": true, "readme_language": "Danish", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e11"}, "repo_url": "https://github.com/MaDDDDDoG/DL", "repo_name": "DL", "repo_full_name": "MaDDDDDoG/DL", "repo_owner": "MaDDDDDoG", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T14:59:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T14:45:40Z", "homepage": null, "size": 11422, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188861447, "is_fork": false, "readme_text": "Deeplearning Book example use Keras TensorFlow MLP CNN RNN ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e12"}, "repo_url": "https://github.com/karthikkamathk/deep_Learning", "repo_name": "deep_Learning", "repo_full_name": "karthikkamathk/deep_Learning", "repo_owner": "karthikkamathk", "repo_desc": "Project in deep learning using Keras and Pytorch in Python", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T14:58:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T14:49:46Z", "homepage": null, "size": 1639, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188695844, "is_fork": false, "readme_text": "deep_Learning Project in deep learning using Keras and Pytorch in Python ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e13"}, "repo_url": "https://github.com/piyushchaudhari04/DeepLearningWithKeras", "repo_name": "DeepLearningWithKeras", "repo_full_name": "piyushchaudhari04/DeepLearningWithKeras", "repo_owner": "piyushchaudhari04", "repo_desc": "This repository contains all the code examples related to deep learning using scikit learn and keras using the Tensorflow backend", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T09:13:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T11:13:49Z", "homepage": null, "size": 263, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189013085, "is_fork": false, "readme_text": "Deep learning basic implementation using scikit learn and Keras with Tensorflow backend ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e14"}, "repo_url": "https://github.com/mervebds/keras-restApi", "repo_name": "keras-restApi", "repo_full_name": "mervebds/keras-restApi", "repo_owner": "mervebds", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T23:25:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T23:13:52Z", "homepage": null, "size": 1979, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188740628, "is_fork": false, "readme_text": "ai-image-recognition-web Derin \u00d6\u011frenme K\u00fct\u00fcphanesi Keras ile Python Flask Web Framework \u00dczerinde Nesne Tan\u0131ma Uygulamas\u0131 ", "has_readme": true, "readme_language": "Turkish", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e15"}, "repo_url": "https://github.com/shuntos/Deep_Learning", "repo_name": "Deep_Learning", "repo_full_name": "shuntos/Deep_Learning", "repo_owner": "shuntos", "repo_desc": "Experiment on Different deep learning models", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:12:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T11:55:40Z", "homepage": null, "size": 86150, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189019100, "is_fork": false, "readme_text": "Implementation of Resnet Models ( Resnet50 , Resnet-101, Resnet-152 ) On Keras. Experiment on Different deep learning models ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shuntos/Deep_Learning/blob/9816eb2e59dd56864fd11a5566401de78b2c30d0/RESNET_Implementation/src/final.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e16"}, "repo_url": "https://github.com/erickanski0402/DogsVsCats", "repo_name": "DogsVsCats", "repo_full_name": "erickanski0402/DogsVsCats", "repo_owner": "erickanski0402", "repo_desc": "Script to create a Convolutional Neural Network using Keras to classify photos of cats and dogs", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T17:59:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T17:59:11Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189077786, "is_fork": false, "readme_text": "DogsVsCats Script to create a Convolutional Neural Network using Keras to classify photos of cats and dogs ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e17"}, "repo_url": "https://github.com/matthew-ritch/max-subarray-tensorflow", "repo_name": "max-subarray-tensorflow", "repo_full_name": "matthew-ritch/max-subarray-tensorflow", "repo_owner": "matthew-ritch", "repo_desc": "implementations of kadane's algorithm in 1D and 2D for tensor inputs", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T16:27:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T14:47:39Z", "homepage": "", "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189048072, "is_fork": false, "readme_text": "max-subarray-tensorflow Implementations of kadane's algorithm in 1D and 2D for tensor inputs- useful for maxsubarray procedures in tensorflow and keras. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e18"}, "repo_url": "https://github.com/Ch4uwa/python_training", "repo_name": "python_training", "repo_full_name": "Ch4uwa/python_training", "repo_owner": "Ch4uwa", "repo_desc": "Tensorflow training", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T17:08:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T16:59:51Z", "homepage": "", "size": 4, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189069245, "is_fork": false, "readme_text": "Trying some Tensorflow machine learning Following Tensorflows tutorial.  Loading images from Keras fashion mnist Making the values 0-1 Setting a name on the titles  LICENSE MIT ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e19"}, "repo_url": "https://github.com/aryaman4/sentiment-classifier", "repo_name": "sentiment-classifier", "repo_full_name": "aryaman4/sentiment-classifier", "repo_owner": "aryaman4", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T13:10:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T11:42:38Z", "homepage": null, "size": 18748, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188675695, "is_fork": false, "readme_text": "Sentiment Classifier Used the IMDB movie review dataset from Kaggle to create a trained Word2Vec model and used it with a Tensorflow/Keras model as an embedded layer along with a GRU and Dense layer. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e1a"}, "repo_url": "https://github.com/jiegenghua/faster-rcnn", "repo_name": "faster-rcnn", "repo_full_name": "jiegenghua/faster-rcnn", "repo_owner": "jiegenghua", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T20:00:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T12:59:06Z", "homepage": null, "size": 832, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188683346, "is_fork": false, "readme_text": "keras-frcnn  train  python train.frcnn.py -o simple -p train.txt   test  python test_frcnn.py -p ../data/test/   evaluate  python evaluate.py -o simple -p test.txt  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e1b"}, "repo_url": "https://github.com/Preston5789/CNN-Whale-Tail-Identification", "repo_name": "CNN-Whale-Tail-Identification", "repo_full_name": "Preston5789/CNN-Whale-Tail-Identification", "repo_owner": "Preston5789", "repo_desc": "A CNN to identify individual whales from over 25,000 images of tails.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T06:59:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T17:27:29Z", "homepage": "", "size": 1259, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188884039, "is_fork": false, "readme_text": "Machine Learning Kaggle Project kaggle-humpback-submission Code for Kaggle Humpback Whale Identification Challange. Prerequisites   tensorflow-gpu 1.6.0   keras 2.0.8   Reference  https://www.kaggle.com/c/whale-categorization-playground  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e1c"}, "repo_url": "https://github.com/Sotaneum/VideoToSMI", "repo_name": "VideoToSMI", "repo_full_name": "Sotaneum/VideoToSMI", "repo_owner": "Sotaneum", "repo_desc": "Create a smi file based on the video.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:21:04Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-26T15:02:17Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188697265, "is_fork": false, "readme_text": "VideoToSMI Create a smi file based on the video.  Copyright (c) 2019 InfoLab (Donggun LEE) How to install  requirement # python 3.6 -- tensorflow pip install tensorflow==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo  # python 3.6 -- tensorflow-gpu pip install tensorflow-gpu==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo  install pip install VideoToSMI  other version # 0.0.1 pip install VideoToSMI==0.0.1    Functions video = Video()  # Goto https://github.com/Sotaneum/deepgeo def add_model(model_name, lib_name, config_data): pass  # TO SMI def detect(file_url, model_name, frame_set, rotation, filter) return FILE  How to use  Import VideoToSMI from videotosmi import Video  Video To SMI video = Video() video.add_model('mscoco','maskrcnn','d:/config.json') filter_ = ['car','bus','truck'] video.detect('d:/test.mov','mscoco',ftr=filter_)  # rotation 180 video.detect('d:/test.mov','mscoco',rotation=180, ftr=filter_)  #Frame 60 video.detect('d:/test.mov','mscoco',frame_set=60,rotation=180, ftr=filter_)     ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://duration.digimoon.net", "http://infolab.kunsan.ac.kr"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e1d"}, "repo_url": "https://github.com/ltotsas/gui-workbench-for-yolov2", "repo_name": "gui-workbench-for-yolov2", "repo_full_name": "ltotsas/gui-workbench-for-yolov2", "repo_owner": "ltotsas", "repo_desc": "Graphical user interface (GUI) workbench for utilising Yolov2 algorithm", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T14:22:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T05:45:14Z", "homepage": null, "size": 4369, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188778356, "is_fork": false, "readme_text": "Graphical user interface (GUI) for Yolov2 As a master thesis, this project tries to minimize/eliminate the necessary knowledge that is required to develop (train, test) an Object Detection model Yolov2 using a GUI. The project structure follows a 3-Tier architecture, where the presentation layer sends rest api commands to the application layer to control and utilize the ML algorithm. The data layer includes the database and the algorithm itself.  Python/Keras/Tensorflow Typescript/Angular7/NodeJS/ExpressJS/InversifyJS NoSQL/LokiJS  Installation  detector-api and detector-app must be build. On each folder inside execute and access it throught http://localhost:4200  $ npm install $ nnpm build $ npm run start  For using yolov2 the current implementation is using conda as virtual environment, thus it is a must. Therefore, anaconda installation must be present. On the root of the folder where the \"environment.yml\" file is, execute \"conda env create -f environment.yml\" Lastly, the weights of the backend Yolo must be downloaded and placed inside the folder Yolov2 https://drive.google.com/file/d/1Vk4wXj4EfNOZxvQJNR5WhKgiZlhqrhRy/view?usp=sharing  Special thanks @rodrigo2019 for his help and ML scriptings and extending @experiencor project @experiencor for starting the Keras-Yolo2 project ", "has_readme": true, "readme_language": "English", "repo_tags": ["yolov2", "angular7", "python", "nodejs", "inversify", "expressjs", "thesis"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://lokijs.org/", "http://localhost:4200", "http://inversify.io/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e1e"}, "repo_url": "https://github.com/YashNita/Audio-classification", "repo_name": "Audio-classification", "repo_full_name": "YashNita/Audio-classification", "repo_owner": "YashNita", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T12:25:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T12:22:44Z", "homepage": null, "size": 16350, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188838437, "is_fork": false, "readme_text": "Environmental Sound Classification using Deep Learning  A project from Digital Signal Processing course  Dependencies  Python 3.6 numpy librosa pysoundfile matplotlib scikit-learn tensorflow keras  Dataset Dataset could be downloaded at Dataverse or Github. I'd recommend use ESC-10 for the sake of convenience. Example: \u251c\u2500\u2500 001 - Cat \u2502  \u251c\u2500\u2500 cat_1.ogg \u2502  \u251c\u2500\u2500 cat_2.ogg \u2502  \u251c\u2500\u2500 cat_3.ogg \u2502  ... ... \u2514\u2500\u2500 002 - Dog    \u251c\u2500\u2500 dog_barking_0.ogg    \u251c\u2500\u2500 dog_barking_1.ogg    \u251c\u2500\u2500 dog_barking_2.ogg    ...  Feature Extraction Put audio files (.wav untested) under data directory and run the following command: python feat_extract.py Features and labels will be generated and saved in the directory. Classify with SVM Make sure you have scikit-learn installed and feat.npy and label.npy under the same directory. Run svm.py and you could see the result. Classify with Multilayer Perceptron Install tensorflow and keras at first. Run nn.py to train and test the network. Classify with Convolutional Neural Network  Run cnn.py -t to train and test a CNN. Optionally set how many epochs to train on. Predict files by either:  Putting target files under predict/ directory and running cnn.py -p Recording on the fly with cnn.py -P    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e1f"}, "repo_url": "https://github.com/Flyfoxs/ty_m", "repo_name": "ty_m", "repo_full_name": "Flyfoxs/ty_m", "repo_owner": "Flyfoxs", "repo_desc": "TinyMind\u4eba\u6c11\u5e01\u9762\u503c&\u51a0\u5b57\u53f7\u7f16\u7801\u8bc6\u522b\u6311\u6218\u8d5b(\u6ee1\u5206)", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-06-02T07:32:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T07:52:33Z", "homepage": "https://www.tinymind.cn/competitions/47", "size": 8061, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188797145, "is_fork": false, "readme_text": "import sys import os import pandas as pd import numpy as np import warnings warnings.filterwarnings(\"ignore\")   from bokeh.palettes import Category10   from tqdm import tqdm, tqdm_notebook  file_folder = globals()['_dh'][0] wk_dir = os.path.dirname(file_folder) os.chdir(wk_dir)  %matplotlib inline    from glob import glob import json  import matplotlib.pyplot as plt import seaborn as sns   !hostname ai-prd-01  from __future__ import division import numpy as np import os import glob  from random import * from PIL import Image from keras.utils import to_categorical from sklearn.preprocessing import LabelEncoder from sklearn.model_selection import train_test_split import matplotlib.pyplot as plt import pandas as pd import matplotlib.image as mpimg %matplotlib inline  from keras.models import Sequential from keras.layers import Dense, Dropout, Flatten, Lambda, ELU, Activation, BatchNormalization from keras.layers.convolutional import Convolution2D, Cropping2D, ZeroPadding2D, MaxPooling2D from keras.preprocessing.image import ImageDataGenerator from keras.optimizers import SGD, Adam, RMSprop Using TensorFlow backend.  \u67e5\u770b\u6bcf\u79cd\u8d27\u5e01\u7684\u6837\u672c\u6570\u91cf\u662f\u5426\u503e\u659c pd.read_csv('./input/train_face_value_label.csv', names=['name','label'], skiprows=1).label.value_counts()  sample = pd.read_csv('./input/train_face_value_label.csv', names=['name','label'], skiprows=1)#.head() sample['label_cat'] = sample.label.astype('category').cat.codes.astype(int) sample.label = sample.label.astype('str') sample.head() sample.label_cat.value_counts().sort_index() 0    4233 1    4373 2    4407 3    4424 4    4411 5    4413 6    4283 7    4408 8    4668 Name: label_cat, dtype: int64  \u6bcf\u79cd\u6837\u672c\u663e\u793a4\u5f20\u56fe\u7247,\u770b\u770b\u56fe\u7247\u8d28\u91cf import cv2 for label in range(9):     row_num = 2     fig,ax = plt.subplots(row_num,2, figsize=(18,4*row_num))     sub_sample = sample.loc[sample.label_cat==label].sample(row_num * 2).reset_index()     for i, file in sub_sample.name.iteritems():                  with open(f'./input/train_data/{file}' ,'rb') as f:             img = Image.open(f)             ax[i%row_num][i//row_num].title.set_text(file)             ax[i%row_num][i//row_num].imshow(img)                           img = cv2.imread(f'./input/train_data/{file}')             R, B, G = img[:,:,0].sum().sum(), img[:,:,1].sum().sum(), img[:,:,2].sum().sum()             total = img.sum().sum().sum()             R, B, G = round(R/total,4), round(B/total,4), round(G/total,4)             print(file, img.shape, i%row_num, R, B, G)     #fig.show()     plt.show()           EDA \u7ed3\u679c  \u53ef\u4ee5\u770b\u51fa\u6570\u636e\u5206\u5e03\u5341\u5206\u5747\u5300, \u56fe\u7247\u6570\u636e\u4e5f\u5341\u5206\u5e72\u51c0 \u5e94\u8be5\u4e0d\u9700\u8981\u6240\u6709\u7684\u6570\u636e\u6295\u5165\u8bad\u7ec3, \u53ef\u4ee5\u6210\u500d\u52a0\u5feb\u8bad\u7ec3\u901f\u5ea6 \u4e3a\u4e86\u589e\u52a0\u6570\u636e\u7684\u9c81\u68d2\u6027,\u589e\u52a0\u4e86\u6570\u636e\u7684\u5de6\u53f3\u5207\u5272,\u4ee5\u53ca\u6570\u636e\u5c0f\u89d2\u5ea6\u65cb\u8f6c  from __future__ import division  import os  import pandas as pd  pd.read_csv('./input/train_face_value_label.csv', names=['name','label'], skiprows=1).label.value_counts()  sample = pd.read_csv('./input/train_face_value_label.csv', names=['name','label'], skiprows=1)#.head() sample['label_cat'] = sample.label.astype('category').cat.codes.astype(int) sample.label = sample.label.astype('str') sample.head()  from sklearn.model_selection import StratifiedKFold #\u6309\u7167\u539f\u59cb\u6bd4\u4f8b\u968f\u673a5\u6298,20%\u4fdd\u7559\u9a8c\u8bc1 (\u5176\u5b9etraning\u548c\u9a8c\u8bc1\u6570\u636e\u90fd\u4e0d\u9700\u8981\u8fd9\u4e48\u591a,\u53ef\u4ee5\u6210\u500d\u63d0\u9ad8\u6027\u80fd) folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019) for train_index, valid_index in folds.split(sample, sample.label_cat.values):     train_df = sample.iloc[train_index]     valid_df = sample.iloc[valid_index]     break  train_df.dtypes  \"\"\" This script goes along my blog post: Keras InceptionResetV2 (https://jkjung-avt.github.io/keras-inceptionresnetv2/) https://medium.com/@vijayabhaskar96/tutorial-on-keras-flow-from-dataframe-1fd4493d237c \"\"\"  from tensorflow.python.keras.models import Model from tensorflow.python.keras.layers import Flatten, Dense, Dropout from tensorflow.python.keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input from tensorflow.python.keras.optimizers import Adam from tensorflow.python.keras.preprocessing.image import ImageDataGenerator   DATASET_PATH  = './input/train_data' IMAGE_SIZE    = (299, 299) NUM_CLASSES   = 9 BATCH_SIZE    = 8  # try reducing batch size or freeze more layers if your GPU runs out of memory FREEZE_LAYERS = 2  # freeze the first this many layers for training NUM_EPOCHS    = 20   #Traning \u6570\u636e, \u6700\u591a\u9009\u62e92\u5ea6, \u56e0\u4e3a\u8d27\u5e01\u90fd\u662f\u6bd4\u8f83\u5bbd\u7684\u957f\u65b9\u5f62,\u6240\u4ee5\u5bf9\u56fe\u7247\u5bbd\u5ea6\u968f\u673a\u88c1\u526a70%, \u9ad8\u5ea6\u968f\u673a\u9ad8\u5ea6\u88c1\u526a10%,  train_datagen = ImageDataGenerator(preprocessing_function=preprocess_input,                                    rotation_range=2,                                    width_shift_range=0.7,                                    height_shift_range=0.1,                                    zoom_range=[0.9, 1.2],                                    fill_mode='nearest'                                    )    train_batches = train_datagen.flow_from_dataframe(train_df, directory=DATASET_PATH, x_col='name',                     y_col='label', target_size=IMAGE_SIZE,                     color_mode='rgb', classes=None,                     class_mode='categorical', batch_size=32,                     shuffle=True, seed=None, save_to_dir=None,                     save_prefix='', save_format='png', subset=None,                     interpolation='nearest', drop_duplicates=True)  valid_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)  valid_batches = valid_datagen.flow_from_dataframe(valid_df, directory=DATASET_PATH, x_col='name',                     y_col='label', target_size=IMAGE_SIZE,                     color_mode='rgb', classes=None,                     class_mode='categorical', batch_size=32,                     shuffle=True, seed=None, save_to_dir=None,                     save_prefix='', save_format='png', subset=None,                     interpolation='nearest', drop_duplicates=True)   test_dir = \"./input/public_test_data/\" testdf = pd.DataFrame({'name':os.listdir(test_dir)})  test_datagen=ImageDataGenerator(preprocessing_function=preprocess_input) test_generator=test_datagen.flow_from_dataframe(     dataframe=testdf,     directory=test_dir,     x_col=\"name\",     y_col=None,     batch_size=32,     seed=42,     shuffle=False,     class_mode=None,     target_size=IMAGE_SIZE )   def gen_sub(model, testdf, sn=0):     \"\"\"     \u8f93\u5165\u6a21\u578b,\u5f97\u5230\u63d0\u4ea4\u6587\u4ef6     \"\"\"     STEP_SIZE_TEST = test_generator.n // test_generator.batch_size     print(f'{test_generator.n}, {test_generator.batch_size}, {STEP_SIZE_TEST}')     pred = model.predict_generator(test_generator,                                    steps=STEP_SIZE_TEST,                                    verbose=1)     df = pd.read_csv('./input/train_face_value_label.csv')     print(df.columns)     label_list = ['0.1', '0.2', '0.5', '1', '10', '100', '2', '5', '50']      res = round(pd.DataFrame(pred, columns=label_list, index=testdf.name), 2)     res['label'] = res.idxmax(axis=1)     res[['label']].to_csv(f'./output/res_inception_res_{sn}.csv')  Found 31692 images belonging to 9 classes. Found 7928 images belonging to 9 classes. Found 20000 images.  def train_raw():     # show class indices     print('****************')     for cls, idx in train_batches.class_indices.items():         print('Class #{} = {}'.format(idx, cls))     print('****************')      # build our classifier model based on pre-trained InceptionResNetV2:     # 1. we don't include the top (fully connected) layers of InceptionResNetV2     # 2. we add a DropOut layer followed by a Dense (fully connected)     #    layer which generates softmax class score for each class     # 3. we compile the final model using an Adam optimizer, with a     #    low learning rate (since we are 'fine-tuning')     net = InceptionResNetV2(include_top=False,                             weights='imagenet',                             input_tensor=None,                             input_shape=(IMAGE_SIZE[0],IMAGE_SIZE[1],3))     x = net.output     x = Flatten()(x)     x = Dropout(0.5)(x)     output_layer = Dense(NUM_CLASSES, activation='softmax', name='softmax')(x)     net_final = Model(inputs=net.input, outputs=output_layer)     for layer in net_final.layers[:FREEZE_LAYERS]:         layer.trainable = False     for layer in net_final.layers[FREEZE_LAYERS:]:         layer.trainable = True     net_final.compile(optimizer=Adam(lr=1e-5),                       loss='categorical_crossentropy', metrics=['accuracy'])     #print(net_final.summary())      # train the model     # \u6bcf\u6b21\u8fed\u4ee3,\u90fd\u751f\u6210\u4e00\u4e2a\u6587\u4ef6,\u53ef\u4ee5\u4e0a\u7ebf\u6d4b\u8bd5,\u4e0d\u7528\u7b49\u592a\u4e45\u5c31\u53ef\u4ee5\u5f97\u5230\u63d0\u4ea4\u6587\u4ef6.\u5982\u679c\u6027\u80fd\u7a33\u5b9a,\u5230\u540e\u9762\u6bcf\u6b21\u751f\u6210\u7684\u6587\u4ef6\u4f1a\u662f\u4e00\u6a21\u4e00\u6837.     for i in range(2):         net_final.fit_generator(train_batches,                                 steps_per_epoch = train_batches.samples // BATCH_SIZE//10,                                 validation_data = valid_batches,                                 validation_steps = valid_batches.samples // BATCH_SIZE//10,                                 epochs = 1)          gen_sub(net_final, testdf, sn=i)          WEIGHTS_FINAL = f'./output/model-inception_resnet_v{i}-27.h5'          # save trained weights         net_final.save(WEIGHTS_FINAL)         print(f'weight save to {WEIGHTS_FINAL}')  \u5f00\u59cb\u8bad\u7ec3\u6a21\u578b,\u5e76\u751f\u6210\u63d0\u4ea4\u6587\u4ef6 #\u751f\u6210\u4e862\u4e2a\u63d0\u4ea4\u6587\u4ef6,\u53ef\u4ee5\u63d0\u4ea4\u5728\u7ebf\u6d4b\u8bd5(\u5728\u6a21\u578b\u6536\u655b\u7684\u60c5\u51b5\u4e0b,2\u4e2a\u6587\u4ef6\u5927\u6982\u7387\u662f\u4e00\u6a21\u4e00\u6837) train_raw() **************** Class #0 = 0.1 Class #1 = 0.2 Class #2 = 0.5 Class #3 = 1.0 Class #4 = 10.0 Class #5 = 100.0 Class #6 = 2.0 Class #7 = 5.0 Class #8 = 50.0 **************** Epoch 1/1 396/396 [==============================] - 953s 2s/step - loss: 0.5867 - acc: 0.8205 - val_loss: 0.0167 - val_acc: 0.9991 20000, 32, 625 625/625 [==============================] - 1129s 2s/step Index(['name', ' label'], dtype='object') weight save to ./output/model-inception_resnet_v0-27.h5 Epoch 1/1 396/396 [==============================] - 1027s 3s/step - loss: 0.0288 - acc: 0.9957 - val_loss: 0.0118 - val_acc: 0.9991 20000, 32, 625 625/625 [==============================] - 1055s 2s/step Index(['name', ' label'], dtype='object') weight save to ./output/model-inception_resnet_v1-27.h5  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e20"}, "repo_url": "https://github.com/Hollyqui/JokeRecognition", "repo_name": "JokeRecognition", "repo_full_name": "Hollyqui/JokeRecognition", "repo_owner": "Hollyqui", "repo_desc": "This project is dedicated to identify humour using a convolutional neural network. It is programmed in Python and uses Tensorflow/Keras for the creation of the network", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:24:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T14:04:32Z", "homepage": null, "size": 265, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188690481, "is_fork": false, "readme_text": "Goal and Initial Idea Upon reading previous papers it came to our notice that previous humour recognition papers are based on a comparison between jokes and news headlines. The results attained with those papers are questionable since news headlines do not represent spoken everyday language (such as used in most jokes). We propose that the major differences in the domain of the datasets may cause a systematic error in the experiment. In order to investigate this, we will run an experiment using a convolutional neural network using a negative dataset of news headlines and a different dataset of quotes or rather proverbs/sayings, which usually contain more common language. Dataset information In order to test whether the network can distinguish between humorous and non-humorous sentences we need a positive (contains humour) and a negative dataset (does not contain humour). In our experiment we used 3 different datasets, for our positive data we used the short jokes dataset available via Kaggle which was also used previously by Chen and Soo. As for the negative datasets, we used the 'a million headlines' dataset and the 'quotes' dataset. Both negative datasets were compared to the jokes dataset separately in order to find whether there is a difference in how challenging the classification task is for the network. This can be done because the 'a million headlines' dataset consists of only news headlines. Network A convolutional network was created using Keras and Tensorflow. It consists of 7 layers as shown in the source code below. # Network Architecture:  model = keras.Sequential([    keras.layers.Conv1D(filters,                        kernel_size=(3),                        activation='relu'),    keras.layers.MaxPool1D((2)),    keras.layers.Dropout(0.25),    keras.layers.Flatten(),    keras.layers.Dense(128, activation='relu'),    keras.layers.Dropout(0.5),    keras.layers.Dense(2, activation='softmax') ])  The network is trained using the Adam optimizer and the Categorical Cross Entropy loss function Results In this section, we will showcase the outputs of our network and its performance compared to the paper by Chen and Soo. In table 1 we can clearly see the difference in performances of the 2 different datasets and also compared to the previous research. One should note that our better results using the headline dataset might be due to a less rigorous headline selection so this does not mean our network is necessarily better. But the goal of this experiment was not to improve on the network as much as it was to investigate the effect of the dataset on the network performance.  One should also note that all the graphs and results were computed using test data and should therefore not show inflated accuracy because of over-training.  Furthermore, Figure 1 shows how the performance of the network differs depending on what dataset is used for the negatives, confirming our hypothesis. It clearly shows that the classification between jokes and headlines is a much easier task, featuring an AUC (Area Under the Curve) of 0.991, whereas the classification between jokes and quotes only yields an AUC of 0.764. This proves the initial assumption that the experiments in previous papers contain a systematic error through the choice of datasets. The high accuracies found by previous papers (ca. 83%) is probably achieved because news headlines have very obvious identifying features (rare words, more condensed language, less colloquial terms etc.). We assume that our accuracy of 93.8% is due to a smaller size of the training and test datasets as well as a more complex network.  The findings of Figure 1 and Table 1 are also supported by Figure 2 and Figure 3. Figure 2 (jokes vs headlines) shows a nearly ideal histogram with barely any overlap in between the classes. Figure 3 (jokes vs quotes) shows a significant overlap of the classes, indicating more false predictions. Additionally, the typical peaks at 0 and 1 are far less pronounced, implying a more difficult classification task. Both histograms indicate that the respective networks hold predictive power.  Discussion Our results showcase that there is a clear difference in performances depending on which dataset you use and from which domain it is. Chen and Soo obtained extremely good results using CNNs on their datasets because of the domains between positive and negative set were very different. This, of course, doesn\u2019t change the fact that their model is an improvement on the state of the art, as most previous studies used the same or similar datasets. Our results suggest a change of focus of negative datasets used in creating humour detection networks, towards a more diverse range of negative examples. Such that multiple non-humorous domains can be taken into consideration, making future models more diverse. Contact This project is developed by Szymon Fonau @Sarkosos and Felix Quinque @Hollyqui as a project for Maastricht University(supervised by Jerry Spanakis). If you have any questions about the project you can contact us via: s.fonau@student.maastrichtuniversity.nl f.quinque@student.maastrichtuniversity.nl For further information you can read our paper ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e21"}, "repo_url": "https://github.com/humblefool96/Digit-Recognizer", "repo_name": "Digit-Recognizer", "repo_full_name": "humblefool96/Digit-Recognizer", "repo_owner": "humblefool96", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T18:38:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T17:52:11Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188714649, "is_fork": false, "readme_text": "Digit-Recognizer Digit Recognizer is a ongoing competition on kaggle i am putting the link of the competition and datset used in the above problem below. https://www.kaggle.com/c/digit-recognizer https://www.kaggle.com/c/digit-recognizer/data Digit Recognizer is a CNN based model which recognizes images of handwritten digits using keras. ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "neural-networks", "keras", "mnist-dataset", "kaggle-competition"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e22"}, "repo_url": "https://github.com/ComputerCoding12/GenderRecognizer", "repo_name": "GenderRecognizer", "repo_full_name": "ComputerCoding12/GenderRecognizer", "repo_owner": "ComputerCoding12", "repo_desc": "The project is about identifying the gender of a certain typed name by training a model the do so", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T17:45:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T17:25:26Z", "homepage": null, "size": 160, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188883820, "is_fork": false, "readme_text": "GenderRecognizer The project is about identifying the gender of a name by training a model the do so This project uses python 3.6 and above The modules used are listed below:  numpy pandas keras tensorflow as backend If this modules are not installed they are required for the project to run.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e23"}, "repo_url": "https://github.com/tushar445/Hopfield-Neural-Network", "repo_name": "Hopfield-Neural-Network", "repo_full_name": "tushar445/Hopfield-Neural-Network", "repo_owner": "tushar445", "repo_desc": "Perspective from Memory", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T10:06:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T10:16:53Z", "homepage": null, "size": 179, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189005192, "is_fork": false, "readme_text": "Hopfield-Neural-Network Perspective from Memory Hopfield network (Amari-Hopfield network) implemented with Python. Two update rules are implemented: Asynchronous & Synchronous. REQUIREMENT Python >= 3.5 numpy matplotlib skimage tqdm keras (to load MNIST dataset) USAGE RUN   For Synchronous: train.py   For Asynchronous: train_mnist.py.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e24"}, "repo_url": "https://github.com/hasibzunair/wandb-mnist", "repo_name": "wandb-mnist", "repo_full_name": "hasibzunair/wandb-mnist", "repo_owner": "hasibzunair", "repo_desc": "A simple template for tracking and logging machine learning experiments using W&B.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T10:28:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T20:23:40Z", "homepage": "https://app.wandb.ai/hasibzunair/mnist", "size": 181, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188904824, "is_fork": false, "readme_text": "   A template for tracking and logging machine learning experiments using W&B. Rather than using Tensorboard or a homebrew version of your own visualization tool for tracking and logging your experiments, you can use Weights and Biases(W&B). It is framework agnostic and hence you can use it with continuous integration. Also go back and compare against all experiments or download a model you've trained a couple of weeks back and deploy it on some new dataset. It's kind of a shared log book for the team. Follow along for a not-well-documented ride or guide to start tracking your experiments. Requirements  Pipenv Keras W&B  Setup project  Clone this repository cd to repo Run pipenv install  Now you have your environment setup, hopefully. Lets start tracking our experiments because those who don't track training are doomed to repeat it! Features  Integration with Keras (W&B is framework-agnostic) Saving configuration setting and hyperparameters Logging metrics Saving weights  Sign up Open an account in their page or type  wandb login in your terminal. Logging code # Get the modules and callbacks import wandb from wandb.keras import WandbCallback # Initialize wandb project wandb.init(name=\"your experiment name\", project=\"your project name\") # Initializes wandb  # Setup config, initialize and save hyperparamters for training config = wandb.config   # Hyperparamters config.learn_rate = 0.01 config.decay = 1e-6 config.momentum = 0.9 config.epochs = 20 # list goes on..... # Initiate training model.fit(x_train, y_train, validation_data=(x_test, y_test),         batch_size=batch_size, verbose=1, epochs=config.epochs, callbacks=[WandbCallback()])  # Save weight file associated with the experiment to wandb repository model.save(os.path.join(wandb.run.dir, path_model)) The above code snippets are essential for the integration of logging and saving functionalities. After you have these setup. Run the following in your terminal. # Login to wandb wandb login # Run the training script wandb run train.py OR pipenv run python train.py  After completion you should see something like this.   Now, go to your W&B web interface. You would see your project has been created magically(after you ran the script).  Select the experiment you ran. You would be able to see all the log files associated with that particular experiment.  Reference  Weights and Biases  License MIT License ", "has_readme": true, "readme_language": "English", "repo_tags": ["visualization", "tracking", "deeplearning"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://keras.io/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e25"}, "repo_url": "https://github.com/windowsofgreen/pyApp-droid", "repo_name": "pyApp-droid", "repo_full_name": "windowsofgreen/pyApp-droid", "repo_owner": "windowsofgreen", "repo_desc": "Run python codes on the android ...", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T05:48:30Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T05:39:22Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188644563, "is_fork": false, "readme_text": "pyApp-droid Run python codes on the android ... Aid Learning FrameWork is a Linux system with GUI running on Android phone for AI programming without root. Aidlearning is also a Python programming framwork for mobile devices. In addition to some of the features available in the Linux environment, Aidlearning has supported GUI and neural network environments. For example, Caffe, Tensorflow, Mxnet, ncnn, Keras are perfectly supported. we using it to create the app of python.... ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e26"}, "repo_url": "https://github.com/Hu-WF/TemperatureField-Reconstruction", "repo_name": "TemperatureField-Reconstruction", "repo_full_name": "Hu-WF/TemperatureField-Reconstruction", "repo_owner": "Hu-WF", "repo_desc": "Sparse reconstruction of temperature field in lithium battery pack\uff082019-05-27\uff09", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T12:09:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T11:48:40Z", "homepage": "", "size": 1157, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188833411, "is_fork": false, "readme_text": "2019-05-27:TemperatureField-Reconstruction\uff08Prediction\uff09 1.Requirements  python >= 3.6 keras >= 2.2.2 scikit-learn >= 0.19.1 pandas numpy matplotlib  2.Modules data_visualization.py :for data analysis. temperature_prediction.py :main. 3.How-to-run Make sure that the file is shown below, and then run temperature_prediction.py directly: \u3000\u3000|-TemperatureField-Reconstruction \u3000\u3000\u3000\u3000|-data \u3000\u3000\u3000\u3000\u3000\u3000|-AT4532 \u3000\u3000\u3000\u3000\u3000\u3000    |-AUTO_0001_2018-07-09.csv \u3000\u3000\u3000\u3000\u3000\u3000    |-AUTO_0002_2018-07-13.csv \u3000\u3000\u3000\u3000|-output \u3000\u3000\u3000\u3000\u3000\u3000|-picture \u3000\u3000\u3000\u3000\u3000\u3000|-video \u3000\u3000\u3000\u3000|-src \u3000\u3000\u3000\u3000\u3000\u3000|-data_visualization.py \u3000\u3000\u3000\u3000\u3000\u3000|-temperature_prediction.py \u3000\u3000\u3000\u3000|-readme.md ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e27"}, "repo_url": "https://github.com/pohjie/rare-event-classification", "repo_name": "rare-event-classification", "repo_full_name": "pohjie/rare-event-classification", "repo_owner": "pohjie", "repo_desc": "Following of tutorials for rare event classification from Chitta Ranjan on Towards Data Science platform", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T03:16:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T04:15:48Z", "homepage": null, "size": 13, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188769588, "is_fork": false, "readme_text": "rare-event-classification Following of tutorials for rare event classification from Chitta Ranjan on Towards Data Science platform Bulk of code obtained from https://towardsdatascience.com/extreme-rare-event-classification-using-autoencoders-in-keras-a565b386f098 Minor modifications from myself may be included Purpose This tutorial is followed as the goal is to predict when to long/short certain stocks, whereby their labelled data of 'trade' is a lot more rare than 'not trade'. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e28"}, "repo_url": "https://github.com/zzwwdr/tensorflow2.0_ner", "repo_name": "tensorflow2.0_ner", "repo_full_name": "zzwwdr/tensorflow2.0_ner", "repo_owner": "zzwwdr", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T04:02:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T03:53:35Z", "homepage": null, "size": 3095, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188950358, "is_fork": false, "readme_text": "\u4f7f\u7528Tensorflow2.0\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8 Description \u4f7f\u7528Tensorflow2.0\u4e2d\u7684keras api\u548c\u4eba\u6c11\u65e5\u62a51998\u8bed\u6599\uff0c\u5b9e\u73b0LSTM\u7684\u5e8f\u5217\u6807\u6ce8 Requirments numpy tensorflow==2.0.0a0 Todo \u5b8c\u6210\u6a21\u578b\u7684\u5b58\u50a8\u548c\u8c03\u7528\u4ee3\u7801 \u4f7f\u7528\u6a21\u578b\u5bf9\u4efb\u610f\u7684\u8bed\u53e5\u8fdb\u884c\u5e8f\u5217\u6807\u6ce8\uff0c\u5e76\u8fd4\u56de\u7ed3\u679c ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e29"}, "repo_url": "https://github.com/mingliangbai/neural-network", "repo_name": "neural-network", "repo_full_name": "mingliangbai/neural-network", "repo_owner": "mingliangbai", "repo_desc": "fine-tuning of GRU netwrok", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T05:45:01Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T03:54:57Z", "homepage": "", "size": 42, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188950518, "is_fork": false, "readme_text": "neural-network This code uses GRU network for minist digit classification through Keras. First, the network is trained and saved in .h5 files. Then the .h5 files is loaded to reproduce the results of the trained netowrk. Next, the weights and bias of the netwrok is tuned to make better classification. Note that this code is used for fine-tuning of the netwrok and the parameter of the original network is not tuned carefully. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/mingliangbai/neural-network/blob/ad27dcbf2c4a381811e46ee1be9bb8e868864d36/model.h5", "https://github.com/mingliangbai/neural-network/blob/ad27dcbf2c4a381811e46ee1be9bb8e868864d36/my_model_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e2a"}, "repo_url": "https://github.com/shrishtimaithani/Object-Detection-Using-CNN", "repo_name": "Object-Detection-Using-CNN", "repo_full_name": "shrishtimaithani/Object-Detection-Using-CNN", "repo_owner": "shrishtimaithani", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T13:08:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T09:08:50Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188994144, "is_fork": false, "readme_text": "Object-Detection-Using-CNN Steps to run this project   Create a virtual environment use this ------>    $ python3 -m venv myvenv   Save the RecognitionObject.py file to the virtual environemnt folder.   Install the neccessary python libraries using pip installer pip install keras pip install pillow pip install tensorflow . . . .   to run the project i. Navingate to the virtual environment and actiavte the virtual environment ii. Run the file using the following command python RecognitionObject.py   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e2b"}, "repo_url": "https://github.com/Abdulhadi77/AI-MachineLearningForStockTrading", "repo_name": "AI-MachineLearningForStockTrading", "repo_full_name": "Abdulhadi77/AI-MachineLearningForStockTrading", "repo_owner": "Abdulhadi77", "repo_desc": "AI-MachineLearningForStockTrading", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T10:53:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T10:52:00Z", "homepage": null, "size": 166, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 189010039, "is_fork": false, "readme_text": "#install pip3 install -r requirements.txt  Adjust CronJob Intervals in trainer, trader and data services Create a oandaapi.py file with a variable with you API-Key from oanda.com Broker  #run services nameko run data nameko run trainer nameko run trader Trader Service only prints out the prediction. You can chose what you do with the prediction (e.g. place order, visualize, etc.) Recommended Extensions:  Change Keras Input Shape to a LSTM Layer Let Trader Service chose the most recent saved model-weights.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e2c"}, "repo_url": "https://github.com/shanlau/DAT264x_Identifying_Malaria", "repo_name": "DAT264x_Identifying_Malaria", "repo_full_name": "shanlau/DAT264x_Identifying_Malaria", "repo_owner": "shanlau", "repo_desc": "Build a CNN model to detect malaria in blood smear slide images", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:10:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T07:04:25Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188650828, "is_fork": false, "readme_text": "DAT264x_Identifying_Malaria Build a CNN model to detect malaria in blood smear slide images for competition https://www.datasciencecapstone.org/competitions/13/identifying-malaria-in-blood/ Download Data source from: https://www.datasciencecapstone.org/competitions/13/identifying-malaria-in-blood/data/ 0_increasecontrast.py: After comparison, increaseing image contrast before model training can improve the result score around 40% 1_movefile.py: Arrange the downloaded source files into Keras data directory structure 2_cnn.py: Build a CNN model for data training 3_predict.py: Predict the result of the testing data in competition by created training model Prediction result of new image by created model:  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e2d"}, "repo_url": "https://github.com/kwonsye/MIYU_APIServer", "repo_name": "MIYU_APIServer", "repo_full_name": "kwonsye/MIYU_APIServer", "repo_owner": "kwonsye", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T14:59:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T05:04:41Z", "homepage": null, "size": 3377, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188642244, "is_fork": false, "readme_text": "MIYU API  MIYU Project's Server side framework : Django 2.2.1 language : python 3.6.7 IDE : Pycharm community ver. packages :  \uac00\uc0c1\ud658\uacbd : virtualvenv rest api \uc791\uc131\uc744 \uc704\ud55c library : djangorestframework 3.9.4 swagger-ui/redoc custom\uc744 \uc704\ud55c library : drf-yasg 1.15.0 For machine-learning  Keras==2.2.4 tensorflow==1.13.1   For data handling  librosa==0.6.3 numpy==1.16.3 pandas==0.24.2      API Doc  swagger-ui : http://13.125.247.188/swagger/ redoc : http://13.125.247.188/redoc/  Deploy  AWS EC2 Linux t2.micro instance Server : Nginx WSGI : uWSGI  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/kwonsye/MIYU_APIServer/blob/e43c90882af07b604344681d4c148328aaf147eb/miyu_app/saved_models/Emotion_Voice_Detection_Model.h5"], "see_also_links": ["http://13.125.247.188/swagger/", "http://13.125.247.188/redoc/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e2e"}, "repo_url": "https://github.com/drewszurko/tensorflow-WGAN-GP", "repo_name": "tensorflow-WGAN-GP", "repo_full_name": "drewszurko/tensorflow-WGAN-GP", "repo_owner": "drewszurko", "repo_desc": "TensorFlow 2.0 implementation of Improved Training of Wasserstein GANs", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T23:45:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T02:43:49Z", "homepage": "https://arxiv.org/abs/1704.00028", "size": 12, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188941538, "is_fork": false, "readme_text": "TensorFlow 2.0 WGAN-GP TensorFlow 2.0 implementation of Improved Training of Wasserstein GANs [1]. New/existing TensorFlow features found in this repository include eager execution, AutoGraph, Keras high-level API, and TensorFlow Datasets. Requirements  Python 3 Abseil NumPy TensorFlow >= 2.0 TensorFlow Datasets tqdm  Datasets  CIFAR-10 CelebA TF Flowers Oxford Flowers 102 Oxford-IIIT pet  Usage Install requirements $ pip install -r requirements.txt  Train model $ python main.py -dataset celeb_a -batch_size 64 -image_size 64  References [1] Improved Training of Wasserstein GANs ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html", "http://download.tensorflow.org/example_images/flower_photos.tgz", "http://www.robots.ox.ac.uk/~vgg/data/pets/", "http://www.numpy.org/"], "reference_list": ["https://arxiv.org/abs/1704.00028", "https://arxiv.org/abs/1704.00028"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e2f"}, "repo_url": "https://github.com/HiroshiARAKI/handwritten_recog_app", "repo_name": "handwritten_recog_app", "repo_full_name": "HiroshiARAKI/handwritten_recog_app", "repo_owner": "HiroshiARAKI", "repo_desc": "This is a handwritten digits recognition application with Keras and Kivy.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T03:36:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T03:49:41Z", "homepage": "", "size": 1004, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188767238, "is_fork": false, "readme_text": "A Simple digits recognition GUI app. Keras\u3068Kivy\u3092\u4f7f\u3063\u305f\u5358\u7d14\u306a\u624b\u66f8\u304d\u6570\u5b57\u30a2\u30d7\u30ea\u3067\u3059\u3002 Kivy\u3092\u7528\u3044\u305fGUI\u90e8\u5206\u306fPython Kivy\u306e\u4f7f\u3044\u65b9\u2464\u3000\uff5e\u516c\u5f0f\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb\u306e\u62e1\u5f35\uff5e \u3092\u53c2\u8003\u306b\u3057\u3066\u3044\u307e\u3059\u3002 \u5b9f\u884c(\u5b9f\u88c5)\u74b0\u5883      \uff08IDE\uff1aPycharm2019.1.1\uff09 \u5b9f\u884c\u4f8b $ python main.py    LICENSE Copyright (c) 2019 Hiroshi ARAKI Released under the MIT LICENSE https://github.com/HiroshiARAKI/handwritten_recog_app/LICENSE.txt ", "has_readme": true, "readme_language": "Japanese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e30"}, "repo_url": "https://github.com/DavideMerlin/Stock_Price_Predictor", "repo_name": "Stock_Price_Predictor", "repo_full_name": "DavideMerlin/Stock_Price_Predictor", "repo_owner": "DavideMerlin", "repo_desc": "This projects allows users to predict stock prices through the use of scikit-learn to train a support vector regression on a Google Finance dataset (apple in this case). The code produces a graph showing the 3 model used: RBF, Linear, and Polynomial (RBF turned out to be the best one). The Machine Learning model can be adjusted to Keras, as well, to adapt it to Neural Networks. A further upgrade might be prediction of stock prices by using sentiment analysis and price history. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T13:16:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T13:09:45Z", "homepage": null, "size": 41, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188845636, "is_fork": false, "readme_text": "Stock_Price_Predictor This projects allows users to predict stock prices through the use of scikit-learn to train a support vector regression on a Google Finance dataset (apple in this case). The code produces a graph showing the 3 model used: RBF, Linear, and Polynomial (RBF turned out to be the best one). The Machine Learning model can be adjusted to Keras, as well, to adapt it to Neural Networks. A further upgrade might be prediction of stock prices by using sentiment analysis and price history. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e31"}, "repo_url": "https://github.com/hghimanshu/Cactus-Identification-Kaggle", "repo_name": "Cactus-Identification-Kaggle", "repo_full_name": "hghimanshu/Cactus-Identification-Kaggle", "repo_owner": "hghimanshu", "repo_desc": "Aerial Cactus Identification is a Kaggle challenge and this model solves the challenge over the accuracy of 99%", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T09:04:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T07:37:24Z", "homepage": "", "size": 1423, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188794781, "is_fork": false, "readme_text": "Cactus-Identification-Kaggle This repo contains the code for the Kaggle Challenge Aerial Cactus Idetification. ----------------------------------- Dependencies  Keras Matplotlib Numpy Opencv Tensorflow Pandas  Running the code  Clone the repository using   git clone https://github.com/hghimanshu/Cactus-Identification-Kaggle.git   After cloning type   cd Cactus-Identification-Kaggle   Run the training file as   python cactus_identification.py   Run the testing file as   python testing_network.py  Note : Some values used in the model are --  epochs = 100 , Optimizer = RMSProp with learning rate = 1e-3 and decay of 1e-6  The training and testing images can be downloaded from this link. ", "has_readme": true, "readme_language": "English", "repo_tags": ["kaggle", "kaggle-competition", "cnn-keras", "cnn-classification", "cnn-model", "keras", "keras-neural-networks"], "has_h5": true, "h5_files_links": ["https://github.com/hghimanshu/Cactus-Identification-Kaggle/blob/8b7165f52bb9224dc6c350470f1d7bf4162fb8eb/Weights/weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e32"}, "repo_url": "https://github.com/sgttwld/CNNs-with-TensorFlow", "repo_name": "CNNs-with-TensorFlow", "repo_full_name": "sgttwld/CNNs-with-TensorFlow", "repo_owner": "sgttwld", "repo_desc": "Different levels of abstraction in convolutional neural network implementations with TensorFlow", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T16:16:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T22:51:21Z", "homepage": "", "size": 13, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188739191, "is_fork": false, "readme_text": "CNNs-with-TensorFlow Differently abstract implementations of an exemplary convolutional neural network with TensorFlow.   Convolutional layer from scratch: Convolutional neural network implementation with convolutional and pooling layers built from scratch with core TensorFlow.   Low-level TensorFlow: Convolutional neural network using tf.nn.conv2d and tf.nn.avg_pool with explicit definitions of weights, biases, and placeholders.   Mid-level TensorFlow: Convolutional neural network using tf.keras.layers managing weights and biases for us, whereas placeholders and the session are still explicit.   High-level TensorFlow: Convolutional neural network using tf.keras.model.Sequential (everything is managed).   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e33"}, "repo_url": "https://github.com/christopher5106/convert_pytorch_pth_file_to_h5_keras_file", "repo_name": "convert_pytorch_pth_file_to_h5_keras_file", "repo_full_name": "christopher5106/convert_pytorch_pth_file_to_h5_keras_file", "repo_owner": "christopher5106", "repo_desc": "convert pytorch pth file to h5 keras file", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T12:59:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T12:23:48Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188838598, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e34"}, "repo_url": "https://github.com/SharifElfouly/siamese-network-with-keras", "repo_name": "siamese-network-with-keras", "repo_full_name": "SharifElfouly/siamese-network-with-keras", "repo_owner": "SharifElfouly", "repo_desc": "An example on how siamese networks work with keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T18:19:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T18:18:00Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188716994, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e35"}, "repo_url": "https://github.com/juntawu/AMDDPG-TORCS", "repo_name": "AMDDPG-TORCS", "repo_full_name": "juntawu/AMDDPG-TORCS", "repo_owner": "juntawu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T13:13:41Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T13:01:15Z", "homepage": null, "size": 132522, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188683569, "is_fork": false, "readme_text": "RL-TORCS Paper Junta Wu, Huiyun Li. Aggregated Multi-deep Deterministic Policy Gradient for Self-driving Policy, International Conference on Internet of Vehicles. Springer, Cham, 2018: 179-192. Introduction AMDDPG is a deep reinforcement learning algorithm designed based on multi-DDPG structure. This repository tested AMDDPG on Gym-TORCS and compared it with DDPG. Requirements  Ubuntu 16.04 Python 2.7 Numpy, Matplotlib, OpenCV Gym Keras 1.1.0 Tensorflow 0.12.0 (CPU version or GPU version) Gym-TORCS CUDA (unnecessary if cpu-version tensorflow is installed, no more than CUDA 8.0)  Environment Configuration For convenience, environment configuration is done on Anaconda. Terminal commands are shown below.   Follow installation instructions to install Anaconda.   Create Python 2.7 virtual environment on Anaconda conda create --name python2.7 python=2.7 source activate python2.7   Install Numpy, Matplotlib, OpenCV, h5py in \"python2.7\" environment pip install --upgrade setuptools pip install -U --pre numpy Matplotlib pip install -U --pre opencv-python pip install h5py   Install Gym pip install -U --pre gym   Install Kears pip install -U --pre keras==1.1.0   Install Tensorflow (CPU version) pip install -U --pre tensorflow==0.12.0   Install Gym-TORCS Note: We'll call the directory that you cloned Gym-TORCS as $GYM-TORCS-ROOT git clone https://github.com/ugo-nama-kun/gym_torcs sudo apt-get install xautomation libglib2.0-dev  libgl1-mesa-dev libglu1-mesa-dev  freeglut3-dev  libplib-dev  libopenal-dev libalut-dev libxi-dev libxmu-dev libxrender-dev  libxrandr-dev libpng12-dev  cd $Gym-TORCS-ROOT/vtorcs-RL-colors/ ./configure sudo make  sudo make install sudo make datainstall   Clone AMDDPG-TORCS repository git clone https://github.com/juntawu/AMDDPG-TORCS Note: We'll call the directory that you cloned AMDDPG-TORCS as $AMDDPG-TORCS-ROOT   Testing DDPG ```Shell cd $AMDDPG-TORCS-ROOT/DDPG/ddpg_2018_03_24 python test_policy.py ```  Testing AMDDPG ```Shell cd $AMDDPG-TORCS-ROOT/AMDDPG/multi_ddpg_2018_05_10 python test_multi_ddpg.py ```  Training DDPG ```Shell cd $AMDDPG-TORCS-ROOT/DDPG/ddpg_2018_03_24 python train_ddpg.py ```  Training AMDDPG ```Shell cd $AMDDPG-TORCS-ROOT/AMDDPG/multi_ddpg_2018_05_10 python train_multi_ddpg.py ```  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/juntawu/AMDDPG-TORCS/blob/a4e7ee8543cfaa7ce99cc5f12a5aa7facc84573a/DDPG/ddpg_2018_03_24/actormodel.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a4e7ee8543cfaa7ce99cc5f12a5aa7facc84573a/DDPG/ddpg_2018_03_24/criticmodel.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode1199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode1399.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode1599.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode1799.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode1999.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode2199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode2399.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode2599.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode2799.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode2999.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode3199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode399.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode599.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode799.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/actormodel-2017-12-29-episode999.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode1199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode1399.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode1599.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode1799.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode1999.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode2199.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode2399.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode2599.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode2799.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode2999.h5", "https://github.com/juntawu/AMDDPG-TORCS/blob/a69976b5c9698a50ebf1adbdbd28839bb9e6ea55/DDPG/trained_model_2017_12_29/criticmodel-2017-12-29-episode3199.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e36"}, "repo_url": "https://github.com/bryzn/Deep_Learning", "repo_name": "Deep_Learning", "repo_full_name": "bryzn/Deep_Learning", "repo_owner": "bryzn", "repo_desc": "Here I will accumulate my deep learning projects and notes", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T22:41:35Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T22:26:18Z", "homepage": null, "size": 1212, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188916097, "is_fork": false, "readme_text": "Learn_Deep_Learning_in_6_Weeks This is the Curriculum for \"Learn Deep Learning in 6 Weeks\" by Siraj Raval on Youtube Overview This is the curriculum for this video on Youtube by Siraj Raval Week 1 - Feedforward Neural Networks and Backpropagation   Code a Deep Learning Network with Python, TensorFlow, and Keras tutorial https://www.youtube.com/watch?v=wQ8BIBpya2k  Begin watching Linear Algebra by Gilbert Strang https://ocw.mit.edu/courses/mathematics/18-06sc-linear-algebra-fall-2011/syllabus/  Read Part I of the Deep Learning Book found here  Use this cheat sheet to help understand any math notation, found here  Watch Build a Neural Net in 4 Minutes  Read Neural Net in 11 lines  Type out the neural network code yourself in a text editor, compile, and run it locally (using no ML libraries)  Watch Backpropagation in 5 minutes  Week 2 - Convolutional Networks   Watch the Convolutional Networks Specialization on Coursera, found here.  Read all 3 lecture notes under Module 2 for Karpathy CNN course found here  Watch my video on CNNs here and here  Write out a simple CNN yourself (using no ML libraries)  Week 3 - Recurrent Networks   Watch the Sequence Models Specialization on Coursera, found here  Watch my videos on recurrent networks, here, here, and here  Read Trask's blogpost on LSTM RNNs found here  Write out a simple RNN yourself (using no ML libraries)  Week 4 - Tooling   Watch CS20 (Tensorflow for DL research). Slides are here. Playlist is here  Watch my intro to tensorflow playlist here  Read Keras Example code to quickly understand its structure here  Learn which GPU provider is best for you here  Write out a simple image classifier using Tensorflow  Week 5 - Generative Adversarial Network   Watch the first 7 videos you see here  Build a GAN using no ML libraries  Build a GAN using tensorflow  Read this to understand the math of GANs, but don't worry if you dont understand it all. This is the bleeding edge here  Week 6 - Deep Reinforcement Learning   Watch CS 294 here  Build a Deep Q Network using Tensorflow  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.deeplearningbook.org/", "http://web.stanford.edu/class/cs20si/syllabus.html", "http://cs231n.github.io/", "http://rail.eecs.berkeley.edu/deeprlcourse/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e37"}, "repo_url": "https://github.com/gilbert-sun/rms_tf_node", "repo_name": "rms_tf_node", "repo_full_name": "gilbert-sun/rms_tf_node", "repo_owner": "gilbert-sun", "repo_desc": "ros model running tensorflow at nvidia tx2", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T04:05:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T07:52:56Z", "homepage": null, "size": 828, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188655061, "is_fork": false, "readme_text": "Tensorflow Objtect recognization by image_net at ROS Nvidia Tx2 This is the tensorflow slim model for Object detection Ret1):  Class \u4ee3\u865f: coffee mup,kb, ms Ret2):  Class \u767e\u5206\u6bd4: class confident percentage rms_tf_node recognization ....  Install Keras (pip install keras) Install TensorFlow (see tensor flow install guide) Install ROS (see http://wiki.ros.org) InstallROSTX2 (see https://github.com/jetsonhacks/installROSTX2)  updateRepositories.sh installROS.sh setupCatkinWorkspace.sh   Install cv-bridge  $ sudo apt-get install ros-kinetic-cv-bridge ros-kinetic-opencv3  (Optional1) Install camera driver (for example, cv_camera , cv_camera_node)  $ sudo apt-get install ros-kinetic-cv-camera or $ sudo apt-get install ros-kinetic-libuvc-camera $ sudo apt-get install ros-kinetic-image-pipeline $ rosdep update or  (Optional2) Test Camera  $ rosrun libuvc_camera camera_node $ rosrun image_view image_view image:=image_raw  install Virtualenv for Python 2.7  $ sudo apt-get install --no-install-recommends python-pip python-dev python-virtualenv virtualenv $ virtualenv --system-site-packages /rms_root/tensorflow_py27 TensorFlow install note (with GPU) Please read official guide. This is a only note for me.  install tensorflow-gpu for Python 2.7  $ source /rms_root/tensorflow_py27/bin/activate $ pip install --upgrade pip $ pip install --upgrade imutils $ pip install --upgrade pytz $ pip install --upgrade https://github.com/eweill/Tensorflow-Jetson-TX2/releases/download/v1.4.1/tensorflow-1.4.1-cp27-cp27mu-linux_aarch64.whl $ or pip install --upgrade tensorflow-1.4.1-cp27-cp27mu-linux_aarch64.whl $ deactivate img_recognization.py  subscribe: /image  or /logitech_c922/image_raw (sensor_msgs/Image) publish:   /Imgnet_Tf  (sensor_msgs/Image) publish1:  /result (std_msgs/String)  Environmental requirements Opencv 3.2 or laster Tensorflow 1.4.1 Start up *git clone $ cd /root/catkin_ws/ or cd /rms_root/gitlab_proj $ git clone https://github.com/gilbert-sun/rms_tf_node.git $ cd ros_tf_node $ git checkout itri-devel (optional , default is no need) $ ln -s /rms_root/gitlab_proj/ros_tf_node /rms_root/catkin_ws/src (optional , default is no need)   build package  $ source /root/catkin_ws/devel/setup.bash or /opt/ros/kinetic/setup.bash $ cd  /root/catkin_ws or /rms_root/catkin_ws $ catkin_make -DCATKIN_ENABLE_TESTING=False -DCMAKE_BUILD_TYPE=Release -DCATKIN_WHITELIST_PACKAGES=\"rms_tf_node\"    test  $ source /rms_root/catkin_ws/devel/setup.bash $ roscore $ roslaunch usb_cam usb_cam-test.launch $ rosrun rms_tf_node img_recognization.py image:=/usb_cam/image_raw $ roslaunch rms_tf_node tf_imgNet_node.launch   Log cat result.log  Function Input & Output See img_recognization.py #Program execution python img_recognization.py image:=image_raw  #function output (Class name,%\u767e\u5206\u6bd4 ) == (Keyboard, 0.8253038)  Demo  Demo1  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://wiki.ros.org"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e38"}, "repo_url": "https://github.com/Jeffer-hua/keras_learning", "repo_name": "keras_learning", "repo_full_name": "Jeffer-hua/keras_learning", "repo_owner": "Jeffer-hua", "repo_desc": "deep_learning_framwork:keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T11:15:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T06:12:20Z", "homepage": null, "size": 0, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188965236, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e39"}, "repo_url": "https://github.com/blackwiz4rd/CoinsDetector", "repo_name": "CoinsDetector", "repo_full_name": "blackwiz4rd/CoinsDetector", "repo_owner": "blackwiz4rd", "repo_desc": "Detect coins using machine learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T20:07:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T22:51:16Z", "homepage": null, "size": 32194, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188918085, "is_fork": false, "readme_text": "Clone dataset The data set is structured suitable to read using Keras ImageDataGenerator.flow_from_directory() method. git clone https://github.com/kaa/coins-dataset  Generate coins images Most images only need to set two parameters: cannyThreshold, accumulatorThreshold Some images need to set more parameters to find coins in a more accurate way: rowsThreshold, cannyThreshold, accumulatorThreshold, minRadiusThreshold, maxRadiusThreshold chmod +x build/generate_coins.sh sh build/generate_coins.sh  Content of shell: ./Lab ../images_T2/1.jpg 255 145 ./Lab ../images_T2/2.jpg 194 151 201 5 236 ./Lab ../images_T2/3.jpg 194 152 201 5 219 ./Lab ../images_T2/4.jpg 133 92 ./Lab ../images_T2/5.jpg 230 200 ./Lab ../images_T2/6.jpg 159 159 ./Lab ../images_T2/7.jpg 170 78 ./Lab ../images_T2/8.jpg 210 124 ./Lab ../images_T2/9.jpg 162 120  Consider using this before training/testing find . -name '._*.jpg' -delete -type f  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/blackwiz4rd/CoinsDetector/blob/9908b80a611efd87a7a8b46fe2d7b6632162d9a2/17-22-52%7C2019-05-28.h5", "https://github.com/blackwiz4rd/CoinsDetector/blob/2311220dd73040c9569b3a9e17337398659851e6/17-44-57%7C2019-05-28.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e3a"}, "repo_url": "https://github.com/leigh-johnson/zero-to-tensorflow", "repo_name": "zero-to-tensorflow", "repo_full_name": "leigh-johnson/zero-to-tensorflow", "repo_owner": "leigh-johnson", "repo_desc": "From Zero to Reinforcement Learning: training a Space Invaders bot using Tensorflow, Keras, OpenAI Gym", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T02:14:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T02:13:21Z", "homepage": null, "size": 48340, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188755950, "is_fork": false, "readme_text": "====================================================================================================== From Zero to Tensorflow  Practical machine learning examples for curious software developers ======================================================================================================   .. image:: https://img.shields.io/pypi/v/zero_to_tensorflow.svg         :target: https://pypi.python.org/pypi/zero_to_tensorflow  .. image:: https://img.shields.io/travis/leigh-johnson/zero_to_tensorflow.svg         :target: https://travis-ci.org/leigh-johnson/zero_to_tensorflow  .. image:: https://readthedocs.org/projects/zero-to-spaceinvaders-ai/badge/?version=latest         :target: https://zero-to-spaceinvaders-ai.readthedocs.io/en/latest/?badge=latest         :alt: Documentation Status     From Zero to Reinforcement Learning: training a Space Invaders bot using Tensorflow, Keras, OpenAI Gym   * Free software: MIT license * Documentation: https://zero-to-tensorflow.readthedocs.io.   Features --------  * TODO  Credits -------  This package was created with Cookiecutter_ and the `audreyr/cookiecutter-pypackage`_ project template.  .. _Cookiecutter: https://github.com/audreyr/cookiecutter .. _`audreyr/cookiecutter-pypackage`: https://github.com/audreyr/cookiecutter-pypackage ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e3b"}, "repo_url": "https://github.com/AntoineSimon91/digit-recognizer", "repo_name": "digit-recognizer", "repo_full_name": "AntoineSimon91/digit-recognizer", "repo_owner": "AntoineSimon91", "repo_desc": "Computer vision fundamentals with the MNIST dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T15:21:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T10:09:12Z", "homepage": null, "size": 40, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 188667208, "is_fork": false, "readme_text": "digit-recognizer Kaggle Competition Competition Description MNIST (\"Modified National Institute of Standards and Technology\") is the  classic dataset of handwritten images has served as the basis for benchmarking classification algorithms. In this competition, the goal is to correctly identify digits from a dataset of tens of thousands of handwritten images. Setup pip install -r requirements.txt Run Script cd scripts python main.p  Command Line Interface -d (--dirpath)  Dataset directory path, default to datasets/  -n (--n_train)  Train dataset size, default to all file.  -t (--n_test)  Test dataset size, default to all file.  -e (--epochs)  Number of epochs, default to 3  -b (--batch_size)  Batch size, default to 86  Sumbmissions Create a submissions file in the submissions/ directory. Sources https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e3c"}, "repo_url": "https://github.com/LuoJiaji/SiamesNetwork_keras", "repo_name": "SiamesNetwork_keras", "repo_full_name": "LuoJiaji/SiamesNetwork_keras", "repo_owner": "LuoJiaji", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T02:07:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T11:46:27Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189017777, "is_fork": false, "readme_text": "SiamesNetwork_keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e3d"}, "repo_url": "https://github.com/snmsung716/Tensorflow_tf.Keras_Tutorial", "repo_name": "Tensorflow_tf.Keras_Tutorial", "repo_full_name": "snmsung716/Tensorflow_tf.Keras_Tutorial", "repo_owner": "snmsung716", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T04:39:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T09:03:34Z", "homepage": null, "size": 2679, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188661325, "is_fork": false, "readme_text": "Tensorflow_tf.Keras_Tutorial ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e3e"}, "repo_url": "https://github.com/Abezzam10/Deep-learning-TensorFlow_Keras", "repo_name": "Deep-learning-TensorFlow_Keras", "repo_full_name": "Abezzam10/Deep-learning-TensorFlow_Keras", "repo_owner": "Abezzam10", "repo_desc": "Neural networks to accomplish image classifiers and recognize patterns optimizing hidden layers", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T00:40:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T22:07:10Z", "homepage": null, "size": 805580, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188914542, "is_fork": false, "readme_text": "Deep-learning-TensorFlow_Keras Neural networks to accomplish image classifiers and recognize patterns optimizing hidden layers ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e3f"}, "repo_url": "https://github.com/asepboy/read-OCR-C1", "repo_name": "read-OCR-C1", "repo_full_name": "asepboy/read-OCR-C1", "repo_owner": "asepboy", "repo_desc": "trial test analysis of image processing form c1 on the desktop of the tanri abeng university apple using python with the CNN method, used for preparation worksop international journals", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T18:39:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T13:26:42Z", "homepage": "", "size": 40936, "language": "Python", "has_wiki": false, "license": null, "open_issues_count": 0, "github_id": 189033620, "is_fork": false, "readme_text": "C1 Form Reader Application of computer vision and convolutional neural network (CNN) for automatically reading hand written numbers on C1 form (Indonesia election's tally form). Prerequisites You need to have python 3.x and the following libraries must be installed to run c1 form reader:  OpenCV Keras Tensorflow Numpy Matplotlib  Installing Simplest way to install all the requirements is using anaconda. $ conda create -n c1reader python=3.7 anaconda $ source activate c1reader (c1reader) $ pip install opencv-python (c1reader) $ pip install keras (c1reader) $ pip install tensorflow  Once all requirements are installed, clone this repository. (c1reader) $ git clone https://github.com/rzwck/c1-form-reader.git (c1reader) $ cd c1-form-reader  Running From the c1-form-reader you can just run the script as the following example: (c1reader) $ python read-C1-form.py Using TensorFlow backend. 2019-05-05 11:19:51.089820: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA test_images/test1.jpg {'01': 12, '02': 115, 'valid': 122, 'invalid': 0, 'total': 127} test_images/test10.jpg {'01': 136, '02': 11, 'valid': 147, 'invalid': 6, 'total': 153} Form test_images/test11.jpg is unreadable: Unable to find digit positions for invalid ballots count test_images/test12.jpg {'01': 190, '02': 415, 'valid': 205, 'invalid': 4, 'total': 209} test_images/test13.jpg {'01': 44, '02': 61, 'valid': 109, 'invalid': 0, 'total': 150} Form test_images/test14.jpg is unreadable: Unable to find digit positions for valid ballots count test_images/test15.jpg {'01': 119, '02': 19, 'valid': 138, 'invalid': 0, 'total': 138} Form test_images/test16.jpg is unreadable: Unable to find digit positions for votes #01 test_images/test17.jpg {'01': 69, '02': 160, 'valid': 229, 'invalid': 7, 'total': 231} Form test_images/test18.jpg is unreadable: Unable to find digit positions for invalid ballots count Form test_images/test19.jpg is unreadable: Unable to find digit positions for votes #02 test_images/test2.jpg {'01': 125, '02': 57, 'valid': 182, 'invalid': 3, 'total': 185} Form test_images/test20.jpg is unreadable: Unable to find digits for total ballots count test_images/test21.jpg {'01': 72, '02': 164, 'valid': 236, 'invalid': 4, 'total': 240} test_images/test22.jpg {'01': 116, '02': 24, 'valid': 140, 'invalid': 3, 'total': 143} test_images/test23.jpg {'01': 59, '02': 150, 'valid': 209, 'invalid': 1, 'total': 210} test_images/test24.jpg {'01': 89, '02': 133, 'valid': 223, 'invalid': 5, 'total': 228} Form test_images/test25.jpg is unreadable: Unable to find digit positions for valid ballots count Form test_images/test3.jpg is unreadable: Unable to find digit positions for votes #01 test_images/test4.jpg {'01': 128, '02': 28, 'valid': 156, 'invalid': 5, 'total': 161} test_images/test5.jpg {'01': 121, '02': 3, 'valid': 124, 'invalid': 5, 'total': 129} Form test_images/test6.jpg is unreadable: Unable to find digit positions for votes #02 Form test_images/test7.jpg is unreadable: Unable to find digit positions for votes #02 test_images/test8.jpg {'01': 143, '02': 89, 'valid': 232, 'invalid': 7, 'total': 239} test_images/test9.jpg {'01': 139, '02': 7, 'valid': 146, 'invalid': 0, 'total': 146}  The script saves the output file on the same folder (test_images). The following screenshot shows some of the outputs produced by c1 form reader. Green boxes are area containing hand written digits, small black boxes with white written numbers are 28x28 hand written digits that will be fed to CNN classifiers. The final number recognized by this program is the white number written on the blue rectangles.  Built With This scripts utilizes codes from the following sites:  PyImageSearch - for perspective transformation Keras examples - for training hand written digits classifiers  Training dataset I trained two digits (0-9) classifiers from two different datasets:  MNIST database - samples of hand written digits Pilkada DKI 2017 - extracted hand written digits from pilkada DKI's C1 form, since local people might have local hand written styles that is not contained in standard MNIST dataset.  I also trained \"X\" classifiers and \"-\" hyphen classifiers for recognizing \"X\" and \"-\" characters that is commonly used to denote empty digit box. The sample for this \"X\" and \"-\" classifier also came from Pilkada DKI 2017. read-OCR-C1 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/asepboy/read-OCR-C1/blob/76fcbc1a53b13b6d221132c7efad65954ce1a3a0/classifiers/X_classifier.h5", "https://github.com/asepboy/read-OCR-C1/blob/76fcbc1a53b13b6d221132c7efad65954ce1a3a0/classifiers/digits_recognizer.h5", "https://github.com/asepboy/read-OCR-C1/blob/76fcbc1a53b13b6d221132c7efad65954ce1a3a0/classifiers/hyphen_classifier.h5", "https://github.com/asepboy/read-OCR-C1/blob/76fcbc1a53b13b6d221132c7efad65954ce1a3a0/classifiers/mnist_classifier.h5"], "see_also_links": ["http://yann.lecun.com/exdb/mnist/"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e40"}, "repo_url": "https://github.com/A3itor/3DUnetCNN", "repo_name": "3DUnetCNN", "repo_full_name": "A3itor/3DUnetCNN", "repo_owner": "A3itor", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T13:13:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T13:06:37Z", "homepage": null, "size": 12874, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188845120, "is_fork": false, "readme_text": "3D U-Net Convolution Neural Network with Keras  Background Originally designed after this paper on volumetric segmentation with a 3D U-Net. The code was written to be trained using the BRATS data set for brain tumors, but it can be easily modified to be used in other 3D applications. Tutorial using BRATS Data Training  Download the BRATS 2018 data by following the steps outlined on the BRATS 2018 competition page. Place the unzipped folders in the brats/data/original folder. (You can also get the older versions of the dataset without signing up: LGG GBM) Install Python 3 and dependencies:  nibabel, keras, pytables, nilearn, SimpleITK, nipype  (nipype is required for preprocessing only)   Install ANTs N4BiasFieldCorrection and add the location of the ANTs binaries to the PATH environmental variable.   Add the repository directory to the PYTONPATH system variable:   $ export PYTHONPATH=${PWD}:$PYTHONPATH   Convert the data to nifti format and perform image wise normalization and correction:  cd into the brats subdirectory: $ cd brats  Import the conversion function and run the preprocessing: $ python >>> from preprocess import convert_brats_data >>> convert_brats_data(\"data/original\", \"data/preprocessed\")   Run the training:  To run training using the original UNet model: $ python train.py  To run training using an improved UNet model (recommended): $ python train_isensee2017.py  If you run out of memory during training: try setting config['patch_shape`] = (64, 64, 64) for starters. Also, read the \"Configuration\" notes at the bottom of this page. Write prediction images from the validation data In the training above, part of the data was held out for validation purposes. To write the predicted label maps to file: $ python predict.py  The predictions will be written in the prediction folder along with the input data and ground truth labels for comparison. Results from patch-wise training using original UNet   In the box plot above, the 'whole tumor' area is any labeled area. The 'tumor core' area corresponds to the combination of labels 1 and 4. The 'enhancing tumor' area corresponds to the 4 label. This is how the BRATS competition is scored. The both the loss graph and the box plot were created by running the evaluate.py script in the 'brats' folder after training has been completed. Results from Isensee et al. 2017 model I also trained a model with the architecture as described in the 2017 BRATS proceedings  on page 100. This architecture employs a number of changes to the basic UNet including an equally weighted dice coefficient, residual weights, and deep supervision. This network was trained using the whole images rather than patches. As the results below show, this network performed much better than the original UNet.   Configuration Changing the configuration dictionary in the train.py or the train_isensee2017.py scripts, makes it easy to test out different model and training configurations. I would recommend trying out the Isensee et al. model first and then modifying the parameters until you have satisfactory results. If you are running out of memory, try training using (64, 64, 64) shaped patches. Reducing the \"batch_size\" and \"validation_batch_size\" parameters will also reduce the amount of memory required for training as smaller batch sizes feed smaller chunks of data to the CNN. If the batch size is reduced down to 1 and it still you are still running out of memory, you could also try changing the patch size to (32, 32, 32). Keep in mind, though, that a smaller patch sizes may not perform as well as larger patch sizes. Using this code on other 3D datasets If you want to train a 3D UNet on a different set of data, you can copy either the train.py or the train_isensee2017.py scripts and modify them to read in your data rather than the preprocessed BRATS data that they are currently setup to train on. Pre-trained Models The following Keras models were trained on the BRATS 2017 data:  Isensee et al. 2017: model (weights only) Original U-Net: model (weights only)  Citations GBM Data Citation:  Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin Kirby, John Freymann, Keyvan Farahani, and Christos Davatzikos. (2017) Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-GBM collection. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2017.KLXWJJ1Q  LGG Data Citation:  Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin Kirby, John Freymann, Keyvan Farahani, and Christos Davatzikos. (2017) Segmentation Labels and Radiomic Features for the Pre-operative Scans of the TCGA-LGG collection. The Cancer Imaging Archive. https://doi.org/10.7937/K9/TCIA.2017.GJQ7R0EF  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://lmb.informatik.uni-freiburg.de/Publications/2016/CABR16/cicek16miccai.pdf", "http://www.med.upenn.edu/sbia/brats2017.html"], "reference_list": ["https://arxiv.org/pdf/1409.5185.pdf"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e41"}, "repo_url": "https://github.com/sitonholy/toy_tf", "repo_name": "toy_tf", "repo_full_name": "sitonholy/toy_tf", "repo_owner": "sitonholy", "repo_desc": "\u7b80\u5355\u7684tensorflow\uff081.8.0\uff09\u6d4b\u8bd5", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-27T08:52:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T08:48:39Z", "homepage": null, "size": 5819, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188806621, "is_fork": false, "readme_text": "\u7b80\u5355\u7684tensorflow\u6d4b\u8bd5\uff0c\u4f7f\u7528\u7684tensorflow\u7248\u672c\u4e3a1.8.0 \u4f7f\u7528\u65b9\u6cd5 python FGK_attention_keras.py ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e42"}, "repo_url": "https://github.com/Sotaneum/ConfigHelper", "repo_name": "ConfigHelper", "repo_full_name": "Sotaneum/ConfigHelper", "repo_owner": "Sotaneum", "repo_desc": "Can use it to save or recall preferences from Python.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:17:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T07:14:27Z", "homepage": null, "size": 25, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188651715, "is_fork": false, "readme_text": "ConfigHelper Can use it to save or recall preferences from Python.   Copyright (c) 2019 InfoLab (Donggun LEE)   How to install  requirement # python 3.6 -- tensorflow pip install tensorflow==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo  # python 3.6 -- tensorflow-gpu pip install tensorflow-gpu==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo  install pip install ConfigHelper  Other version # 0.0.3 pip install ConfigHelper==0.0.3     Functions config = ConfigHelper(data:dict) config = ConfigHelper(path:str) # URL is not supported. config = ConfigHelper(cls:type) # Class config = ConfigHelper(obj) # is not None  # Returns the value. def getValue(key:str):     return Value  # Modify or add new value.  def setValue(key:str, value:object):     return None  # Create New Object. (Init Variable) def newObject(cls:Class):     return Object  # Put a value in Object def setObject(obj:Object):     pass  # Return as \"Dictionary\". def toDict():     return Dictionary  # Return as \"JSON\". def toJSON():     return String(=JSON)  # Return as \"File\". def toFile(path:string):     return file   How to use  Test Class # Test Class class Test:     def __init__(self):         self.name = \"Donggun LEE\"         self.age = 24          def __str__(self):         return \"name : {}, age : {}\".format(self.name, self.age)  Import ConfigHelper from ConfigHelper import Config  Config None Example # Config None Example print(\"Config None Example\") cfg_none = Config() cfg_none.setValue(\"Version\", \"0.0.1\") cfg_none.setValue(\"isTemporary\", 0) cfg_none.setValue(\"isUserMode\", 1)  print(cfg_none.isTemporary) \"\"\"     0 \"\"\" print(cfg_none.Version) \"\"\"     0.0.1 \"\"\"  print(cfg_none.toJSON()) \"\"\"     {         \"Version\": \"0.0.1\",         \"isTemporary\": 0,         \"isUserMode\": 1     } \"\"\"  test = Test() print(test) \"\"\"     name : Donggun LEE, age : 24 \"\"\" cfg_none.setValue(\"name\", \"LEE Donggun\") cfg_none.setObject(test) print(test) \"\"\"     name : LEE Donggun, age : 24 \"\"\" try:     print(test.Version)     \"\"\"     \"\"\" except Exception as e:     print(e)     \"\"\"         'Test' object has no attribute 'Version'     \"\"\"  print(cfg_none.Version) \"\"\"     0.0.1 \"\"\"  print(cfg_none.toDict()['Version']) \"\"\"     0.0.1 \"\"\"  cfg_none.toFile(\"d:/a/b/c/d/e/f/config.json\")  Config Dictionaray Example # Config Dictionaray Example print(\"Config Dictionaray Example\")  cfg_dict = Config({\"name\":\"LEE Donggun\", \"age\":40}) print(cfg_dict) \"\"\"     {         \"age\": 40,         \"name\": \"LEE Donggun\"     } \"\"\" test = cfg_dict.newObject(Test) print(test) \"\"\"     name : LEE Donggun, age : 40 \"\"\" cfg_dict.setValue(\"age\",70) cfg_dict.setObject(test) print(test) \"\"\"     name : LEE Donggun, age : 70 \"\"\"  Config File Example # Config File Example print(\"Config File Example\") cfg_file = Config(\"d:/a/b/c/d/e/f/config.json\") print(cfg_file) \"\"\" {         \"Version\": \"0.0.1\",         \"isTemporary\": 0,         \"isUserMode\": 1,         \"name\": \"LEE Donggun\" } \"\"\"  Config Class Example # Config Class Example print(\"Config Class Example\")  cfg_class = Config(Test) print(cfg_class) \"\"\"     {         \"age\": 24,         \"name\": \"Donggun LEE\"     } \"\"\"  Config Object Example # Config Object Example print(\"Config Object Example\")  cfg_obj = Config(Test()) print(cfg_class) \"\"\"     {         \"age\": 24,         \"name\": \"Donggun LEE\"     } \"\"\"     ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://duration.digimoon.net/", "http://infolab.kunsan.ac.kr"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e43"}, "repo_url": "https://github.com/Artem48/3D-Unet", "repo_name": "3D-Unet", "repo_full_name": "Artem48/3D-Unet", "repo_owner": "Artem48", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T17:37:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T15:01:12Z", "homepage": null, "size": 120, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188864111, "is_fork": false, "readme_text": "3D U-Net Convolution Neural Network with Keras Background Originally designed after this paper on volumetric segmentation with a 3D U-Net. The code was written to be trained using the BRATS data set for brain tumors, but it can be easily modified to be used in other 3D applications. Tutorial using BRATS Data Training  Download the BRATS 2018 data by following the steps outlined on the BRATS 2018 competition page. Place the unzipped folders in the brats/data/original folder. Install Python 3 and dependencies:  nibabel, keras, pytables, nilearn, SimpleITK, nipype  (nipype is required for preprocessing only)   Install ANTs N4BiasFieldCorrection and add the location of the ANTs binaries to the PATH environmental variable.   Add the repository directory to the PYTONPATH system variable:   $ export PYTHONPATH=${PWD}:$PYTHONPATH   Convert the data to nifti format and perform image wise normalization and correction:  cd into the brats subdirectory: $ cd brats  Import the conversion function and run the preprocessing: $ python >>> from preprocess import convert_brats_data >>> convert_brats_data(\"data/original\", \"data/preprocessed\")   Run the training:  To run training using the original UNet model: $ python train.py  To run training using an improved UNet model (recommended): $ python train_isensee2017.py  If you run out of memory during training: try setting config['patch_shape`] = (64, 64, 64) for starters. Also, read the \"Configuration\" notes at the bottom of this page. Write prediction images from the validation data In the training above, part of the data was held out for validation purposes. To write the predicted label maps to file: $ python predict.py  The predictions will be written in the prediction folder along with the input data and ground truth labels for comparison. Configuration Changing the configuration dictionary in the train.py or the train_isensee2017.py scripts, makes it easy to test out different model and training configurations. I would recommend trying out the Isensee et al. model first and then modifying the parameters until you have satisfactory results. If you are running out of memory, try training using (64, 64, 64) shaped patches. Reducing the \"batch_size\" and \"validation_batch_size\" parameters will also reduce the amount of memory required for training as smaller batch sizes feed smaller chunks of data to the CNN. If the batch size is reduced down to 1 and it still you are still running out of memory, you could also try changing the patch size to (32, 32, 32). Keep in mind, though, that a smaller patch sizes may not perform as well as larger patch sizes. Using this code on other 3D datasets If you want to train a 3D UNet on a different set of data, you can copy either the train.py or the train_isensee2017.py scripts and modify them to read in your data rather than the preprocessed BRATS data that they are currently setup to train on. Pretrained model on Brats2018+TCGA_LGG datasets. Pretrained model ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://lmb.informatik.uni-freiburg.de/Publications/2016/CABR16/cicek16miccai.pdf", "http://www.med.upenn.edu/sbia/brats2017.html"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e44"}, "repo_url": "https://github.com/Sotaneum/VideoToSMI-Server", "repo_name": "VideoToSMI-Server", "repo_full_name": "Sotaneum/VideoToSMI-Server", "repo_owner": "Sotaneum", "repo_desc": "Create a smi file in Web based on the video", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:14:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T17:05:06Z", "homepage": null, "size": 16, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188710411, "is_fork": false, "readme_text": "VideoToSMI-Server Create a smi file in Web based on the video   Copyright (c) 2019 InfoLab (Donggun LEE)   How to install  requirement # python 3.6 -- tensorflow or tensorflow-gpu pip install tensorflow-gpu==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo VideoToSMI ConfigHelper  install pip install VideoToSMI-Server  other version # 0.0.3 pip install VideoToSMI-Server==0.0.3 # 0.0.4 pip install VideoToSMI-Server==0.0.4     How to use from VideoToSMIServer import Server, ServerConfig  config = ServerConfig() config.MODEL_NAME = \"mscoco\" config.IP = \"127.0.0.1\" config.PORT=80 config.MODEL_CONFIG_PATH = \"D:/test/config.json\" config.MODEL_ENGINE = \"maskrcnn\" config.FILTER = ['truck','car','bus'] config.VIDEO_FOLDER = \"D:/test/videotosmi/\" server = Server(config) server.Run()  # POST VIDEO FILE -> http://127.0.0.1:80/ -> Reulst SMI FILE   How to Test   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://duration.digimoon.net", "http://infolab.kunsan.ac.kr"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e45"}, "repo_url": "https://github.com/pumudithaULPC/Pest-Detection-Module-CNN", "repo_name": "Pest-Detection-Module-CNN", "repo_full_name": "pumudithaULPC/Pest-Detection-Module-CNN", "repo_owner": "pumudithaULPC", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T18:34:34Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T08:00:29Z", "homepage": null, "size": 56181, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188655720, "is_fork": false, "readme_text": "Pest-Detection-Module-CNN This is a multi class classifier CNN based on Tensorflow, Keras and Python. This Module is capable of identifying Insects(or any other type of objects) and classify them accordingly. Steps To train this CNN with your own data:  Add the train images to a directory and classify them by adding different classes in different folders. In the getdata.py file, modify DATADIR with the path to your training images directory. Also add the names of the classes to CATEGORIES and modify. In the cnnbuild.py file modify the \"num of classes\" variable with the number of classes that are required to train the model. You can also change  other important variables like the number of epochs, optimiser and etc. Add the batch of test data into test.py and replace the CATEGORIES with the name of the classes you added in the 1st step. also modify the path_in_str_in_str with the path to the test data directory. Now run the project in the following order.  1.get_data.py 2.resize_data.py 3.building_training_data.py 4.pickle_builder.py 5.cnn_build.py 6.test.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/pumudithaULPC/Pest-Detection-Module-CNN/blob/92799a3f710aff618a028984576b34af0e53078d/my_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e46"}, "repo_url": "https://github.com/rangijayant15/Heart_Disease_UCI-Kaggle-", "repo_name": "Heart_Disease_UCI-Kaggle-", "repo_full_name": "rangijayant15/Heart_Disease_UCI-Kaggle-", "repo_owner": "rangijayant15", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T05:12:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T04:39:14Z", "homepage": null, "size": 9, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188771732, "is_fork": false, "readme_text": "Heart_Disease_UCI-Kaggle- Dataset-heart.csv This database contains 14 attributes, The \"goal\" field refers to the presence of heart disease in the patient. Link to the Dataset -> https://www.kaggle.com/ronitf/heart-disease-uci Attribute Information:   age sex chest pain type (4 values) resting blood pressure serum cholestoral in mg/dl fasting blood sugar > 120 mg/dl resting electrocardiographic results (values 0,1,2) maximum heart rate achieved exercise induced angina oldpeak = ST depression induced by exercise relative to rest the slope of the peak exercise ST segment number of major vessels (0-3) colored by flourosopy thal: 3 = normal; 6 = fixed defect; 7 = reversable defect   Model Information :  1.Constructed a neural network using Keras API. 2.Neural Network consists of an input,output and two hidden layers. 3.There are some features which needs to be pre-processed which is done according to the trends in bar corresponding to each     feature. 4.Split the data into training and test set in ratio 1:4 . 5.Achieved an accuracy of around 85% 6.Data is also trained on Random Forest Classifier which gives an accuracy around 90%.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e47"}, "repo_url": "https://github.com/grupoioa/keras_template", "repo_name": "keras_template", "repo_full_name": "grupoioa/keras_template", "repo_owner": "grupoioa", "repo_desc": "This repository contains a template to create deep learning algorithms on keras. It has examples for preprocessing, training, tensorboard, etc. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T15:29:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T13:46:06Z", "homepage": null, "size": 335, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189036925, "is_fork": false, "readme_text": "Keras Template The objective of this template is to provide an initial code when developing a new Deep Learning project and a suggestion in how to separate the modules, configuration, files etc. The current template contains the following files:  This project assumes 4 main steps in training a model preprocess, training, classification, and visualization. For every project you will normally have to modify the way your data is being read and visualized, and the DL models you want to use. But you should be able to re-use lot of code. The configuration for these 4 steps is made at MainConfig.py. The objective of separating the configuration in an external file is to ease the collaboration. The first file that need to be modified is inout\\readData, this should be able to read your own data. Then you need to modify 1_preproc in order to perform any preprocess necessary for your data. Here you can reuse some visualization functions as well and some common ways to normalize data from the preproc  folder. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e48"}, "repo_url": "https://github.com/kiharalab/Dove_Pred", "repo_name": "Dove_Pred", "repo_full_name": "kiharalab/Dove_Pred", "repo_owner": "kiharalab", "repo_desc": "Dove is a Docking Model Evaluation method based on 3D Deep Convolutional Neural Networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T22:46:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T19:13:30Z", "homepage": null, "size": 100071, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188722004, "is_fork": false, "readme_text": "Dove_Pred Dove Prediction Version Dependency: tensorflow, keras, numpy General Instructions python main.py: -h, --help            show this help message and exit -F F                  decoy example path --mode MODE, -M MODE  0: predicting for single docking model 1: predicting and sorting for a list of docking models --id ID               random id for the webserver notification, make sure corresponding --gpu GPU             Choose gpu id, example: '1,2'(specify use gpu 1 and 2) 1 Predict single pdb file python main.py --mode=0 -F [pdb_file] --id=888 --gpu=0 if you need more than 1 gpu, use --gpu=0,1 2 Predict pdb file lists python main.py --mode=1 -F [directory_path] --id=888 --gpu=0 Notice: All the docking models should use chain 'A' for receptors, use chain 'B' for ligands. Output record: Output will be saved in the subdirectory of your models' directory. For mode 0,the output will be kept as file_name[:-4]_jobid[id].txt. For mode 1,the output will be kept as RECORD_jobid[id].txt Output format(Example): complex.244440.pdb,0.80237,0.79943,0.90355,0.78516,-1.00000,-1.00000,0.91417,0.75000, (Explanation: first column is the file name, 2nd-9th column denotes the probability that model outputs, 2nd-ATOM20, 3rd-ATOM40 4th-GOAP 5th-ITScore 6th-ATOM+GOAP 7th-ATOM+ITScore 8th-GOAP+ITScore 9th-ATOM40+GOAP+ITScore. If it's -1, it means you do not have model weights files for corresponding deep learning model.) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/kiharalab/Dove_Pred/blob/ff23c011a0b369d9fb6ae6dca8a2c70dbca9b813/Best_Model/atom20/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ff23c011a0b369d9fb6ae6dca8a2c70dbca9b813/Best_Model/goapitscore/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ff23c011a0b369d9fb6ae6dca8a2c70dbca9b813/Best_Model/atomgoap/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ff23c011a0b369d9fb6ae6dca8a2c70dbca9b813/Best_Model/atomitscore/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ceea0e366a5f0794bcd3ede3c86425af6d9c6df0/Best_Model/atom40/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ceea0e366a5f0794bcd3ede3c86425af6d9c6df0/Best_Model/atomgoapitscore/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ceea0e366a5f0794bcd3ede3c86425af6d9c6df0/Best_Model/goap/Best_Model.h5", "https://github.com/kiharalab/Dove_Pred/blob/ceea0e366a5f0794bcd3ede3c86425af6d9c6df0/Best_Model/itscore/Best_Model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e49"}, "repo_url": "https://github.com/hri-unica/Nao-Zora-conversational-agent", "repo_name": "Nao-Zora-conversational-agent", "repo_full_name": "hri-unica/Nao-Zora-conversational-agent", "repo_owner": "hri-unica", "repo_desc": "Zora/Nao interprets and responds to statements made by users in ordinary natural language, using seq2seq.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T20:45:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T08:45:53Z", "homepage": null, "size": 311453, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188659671, "is_fork": false, "readme_text": "Chatbot (By Mattia Atzeni) Zora/Nao interprets and responds to statements made by users in ordinary natural language, using seq2seq. Prerequisites  Install modules  apt-get install graphviz &&\\ pip3 install flask_jsonpify &&\\ pip3 install keras &&\\ pip3 install theano &&\\ pip3 install pydot   Download nltk (from bash)  python << END import nltk nltk.download('punkt') END   Merge the files and unzip  cat my_model_weights.zip* > my_model_weights.zip && \\ cat my_model_weights_bot.zip* > my_model_weights_bot.zip && \\ cat my_model_weights20.zip* > my_model_weights20.zip && \\ unzip my_model_weights.zip && \\ unzip my_model_weights_bot.zip && \\ unzip my_model_weights20.zip  Usage with NAO/Zora  Start the webapp  python3 webapp.py   Open the Choregraphe project, right click on the Seq2Seq box, click Set parameter and set as URL the url of the preceded server (something like http://<IP>:4003/chatbot, where IP is the internet address of the computer where webapp.py is running) Start the behavior Say or write in the dialog box the text  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/hri-unica/Nao-Zora-conversational-agent/blob/3b0f05e87004f1e93694896c27b7592642f48542/my_model_weights_discriminator.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e4a"}, "repo_url": "https://github.com/bernardolemos/WaveNet_and_Variant", "repo_name": "WaveNet_and_Variant", "repo_full_name": "bernardolemos/WaveNet_and_Variant", "repo_owner": "bernardolemos", "repo_desc": "WaveNet and Variant - Temporal Modeling Using Dilated Convolution", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T11:34:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T09:25:16Z", "homepage": null, "size": 16192, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188996851, "is_fork": false, "readme_text": "WaveNet WaveNet and a variant of WaveNet implementation in Keras This work implements the WaveNet network architecture and small variant. The implementation is based on:  WaveNet: : A Generative Model for Raw Audio Temporal Modeling Using Dilated Convolution and Fating for Voice-Activity-Detection  Both works have the same author: A\u00e4ron van den Oord The models Both models use a stack of WaveNet blocks (wavenet_block.py). See wavenet_and_variant.py WaveNet [1]          WaveNet Architecture Image source: [1]    Predict next most likely value based on previous ones. Generat audio  Apply causal convolution to input Use parameterized skip-connections Output a softmax distribution (apply to parameterized skip-connecitons)  Variant (Gated Dilated 1D Convolution with Residual connections) [2]          Variant Architecture Image source: [2]    Assess the probability of input being speech or not.  Aplly dimension matching to input and stack's outputs Residual connections (accumulated input and output) - doesn't use skip-connections Aplly fully connected layer to stack's residual output Output probability of speech and non-speech  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1609.03499.pdf", "https://arxiv.org/pdf/1609.03499.pdf", "https://arxiv.org/pdf/1609.03499.pdf"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e4b"}, "repo_url": "https://github.com/kulasama/deepfashion", "repo_name": "deepfashion", "repo_full_name": "kulasama/deepfashion", "repo_owner": "kulasama", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T05:39:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T11:21:06Z", "homepage": null, "size": 22632, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188673661, "is_fork": false, "readme_text": "DEEP FASHION Setup Environment # Virtual environment (optional)  # Tensorflow (optional) sudo apt-get install python-dev libhdf5-dev libblas-dev  liblapack-dev conda create -n deepfashion python=3.7 source activate deepfashion conda install tensorflow-gpu keras  # for Python 2.7 and GPU  # Dependencies sudo apt install -y python-tk pip install -r requirements.txt  Download DeepFashion Dataset # http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion/AttributePrediction.html ./dataset_download.sh  # The directory structure after downloading and extracting dataset: # fashion_data/ # ---Anno # ------list_attr_cloth.txt # ------list_attr_img.txt # ------list_bbox.txt # ------list_category_cloth.txt # ------list_category_img.txt # ------list_landmarks.txt # ---Eval # ------list_eval_partition.txt # ---Img # ------img Create Dataset # For images in fashion_data, apply selective search algo to find ROI/bounding boxes. Crop and copy these ROI inside dataset python dataset_create.py Train python train.py Predict python predict.py Misc dataset - Contains images used for training, validation and testing. output - Contains trained weights and bottleneck features. logs    - Contains logs and events used by tensorboard. MODEL       -> Classification Head (Categories) InputImage -> VGG16 + Layers --       -> Regression Head (Confidnence in the Classification head prediction)  RESULTS  Acknowledgment  DeepFashion Dataset  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://mmlab.ie.cuhk.edu.hk/projects/DeepFashion.html"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e4c"}, "repo_url": "https://github.com/kingBabo/EDAN70-Job-Classifier", "repo_name": "EDAN70-Job-Classifier", "repo_full_name": "kingBabo/EDAN70-Job-Classifier", "repo_owner": "kingBabo", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T15:16:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T15:07:16Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189051509, "is_fork": false, "readme_text": "Text Classification - Predicting occupations (EDAN70 - Project) This is a neural network algorithm for predicting occupation based on text descriptions on persons. Getting Started Before running any files you need to download the docria wikipedia file from: http://fileadmin.cs.lth.se/nlp/wiki2018/enwiki.tar Put this file in corpus/enwiki. You also need to download the word embeddings from gloVe. Put the file name glove.6B.100d.txt in the root folder. Download it here: https://nlp.stanford.edu/projects/glove/ Also delete all temp files named temp in all folders. Prerequisites Install these libraries before moving on. pip3 install keras pip3 install pickle pip3 install sklearn  FETCHING DATA FROM WIKIDATA The first thing you need to do is extract data from wikidata and match it with the docria file you downloaded. To do this run first: python extract_wikidata.py  And then python match.py  Preprocessing Now you are ready to preprocess the data. Do this by running: python preprocess.py  You will now be prompted if you want to create a big or small model. Building the model Build the model by running: python build_model.py  Once again you will be prompted which set you want to use. Evaluating and ploting confusion matrix python evaluate_model.py  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://fileadmin.cs.lth.se/nlp/wiki2018/enwiki.tar"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e4d"}, "repo_url": "https://github.com/minkeshtu/Change-Background", "repo_name": "Change-Background", "repo_full_name": "minkeshtu/Change-Background", "repo_owner": "minkeshtu", "repo_desc": "Main characters (persons) extraction into new (desired) background - Used Mask R-CNN for object detection", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T15:28:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T07:50:42Z", "homepage": "", "size": 16762, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188980724, "is_fork": false, "readme_text": "Change-Background Main characters (persons) extraction into new (desired) background utilizing the Mask R-CNN Introduction This project mainly aims to change the background (Just need an image of your desired background) of input file (Image or video) while keeps the main charactors (person) same as input file. This project uses MASK R-CNN for object detection. Demo    Demo 1 Demo 2          Requirements numpy scipy cython h5py Pillow scikit-image tensorflow-gpu>=1.3 (Tested on 1.13) keras jupyter matplotlib imgaug (It requires shapely to be installed) IPython[all] opencv-python Getting started Ready the environment by installing all the packges. If you are facing any issues in setting up the environment then you can refer to Mask RCNN page. This project doesn't need any extra packages to run. Before running, please download mask_rcnn_coco.h5 file by clicking here and put it into the main directory Now, you can run the \"main_single_person_extraction.ipynb\" and follow the Instructions to process video and image files separately. Upcoming A general program will be added in which you can specify No of persons you want to extract from the input file. (Right now It extract only a single person who's area is biggest) Ref Mask RCNN ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e4e"}, "repo_url": "https://github.com/astrazeneca-cgr-publications/mantis-ml-release", "repo_name": "mantis-ml-release", "repo_full_name": "astrazeneca-cgr-publications/mantis-ml-release", "repo_owner": "astrazeneca-cgr-publications", "repo_desc": "mantis-ml: Stochastic semi-supervised learning to prioritise genes from high throughput genomic screens", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T13:24:32Z", "repo_watch": 3, "repo_forks": 2, "private": false, "repo_created_at": "2019-05-28T11:40:30Z", "homepage": "", "size": 154251, "language": "Python", "has_wiki": true, "license": {"key": "mpl-2.0", "name": "Mozilla Public License 2.0", "spdx_id": "MPL-2.0", "url": "https://api.github.com/licenses/mpl-2.0", "node_id": "MDc6TGljZW5zZTE0"}, "open_issues_count": 0, "github_id": 189016912, "is_fork": false, "readme_text": "mantis-ml (v 1.0)  Introduction Installation Run Output Workflow overview Package contents  Introduction mantis-ml is a disease-agnostic gene prioritisation framework, built on top of scikit-learn and keras (tensorflow-based). mantis-ml takes its name from the Greek word '\u03bc\u03ac\u03bd\u03c4\u03b7\u03c2' which means 'fortune teller', 'predicter'.  Installation Requirements:  Python (Python3, tested with v3.6.7) R (tested with v3.5.1)   1. Download mantis-ml-release GitHub repository: git clone https://github.com/astrazeneca-cgr-publications/mantis-ml-release.git  (Optional - for Anaconda users):  Install Anaconda Create a new environment with clean Python and R installations:  conda create -n mantis_ml python=3.6 r   2. Install Python library dependencies: pip install -r requirements.txt  or with conda: conda install --file requirements.txt   3. Install mantis-ml package: cd mantis-ml-release python setup.py install  and add based mantis-ml-release dir to PYTHONPATH: # in ~/.basrhc add: export PYTHONPATH=[FULL_PATH_TO_DIR]/mantis-ml-release:$PYTHONPATH   4. Install R library dependencies: # R  > install.packages('Boruta')   Run Prepare config file Basic parameters run parameters:  Tissue: primary tissue affected by disease additional_tissues: other tissues affected by disease seed_include_terms: patterns matching HPO phenotypes for annotation of known disease genes (seed genes) additional_include_terms: patterns used alongside seed_include_terms for disease-specific feature extraction exclude_terms: string patterns to exclude during seed gene selection and/or disease-specific feature extraction phenotype: user-defined descriptive term for the disease/phenotype run_id: output folder-name suffix  pu parameters:  classifiers: define list of classifiers to use for Positive-Unlabelled learning iterations: number L of stochastic iterations nthreads: number of threads to use (optimally assign one CPU per thread)  run_steps parameters:  run_boruta: True/False to run/or not the Boruta feature importance estimation algorithm run_unsupervised: True/False to run/or not unsupervised learning methods (PCA, t-SNE and UMAP) during the pre-processing step.   Config example (for Epilepsy): run:     tissue: Brain     additional_tissues: []     seed_include_terms: [epilep, seizure]     exclude_terms: []     additional_include_terms: [nerve, nervous, neuronal, cerebellum, cerebral, hippocampus, hypothalamus]     phenotype: Epilepsy      run_id: production pu_params:     classifiers: [ExtraTreesClassifier, RandomForestClassifier]     iterations: 2     nthreads: 4 run_steps:     run_boruta: False     run_unsupervised: True  Advanced parameters All other config parameters (Advanced) can be used with their default values (see mantis_ml/conf/config.yaml template).  Preview selected features based on input config parameters cd mantis_ml/bin python mantis_ml_profiler.py input_config.yaml [-v]             # use -v for verbose output  Example output available at: mantis_ml/bin/logs/profiling.out  Run mantis-ml cd mantis_ml/bin ./run_mantis_ml.sh [-c CONFIG_FILE]   Examples cd mantis_ml/bin ./run_mantis_ml.sh -c ../conf/config.yaml   Run on a SLURM cluster cd mantis_ml/bin sbatch [SBATCH_OPTIONS, e.g. -o, -t] ./submit_mantis_ml.sh [-h] [-c|--config CONFIG_FILE] [-m|--mem MEMORY]           [-t|--time TIME] [-n|nthredas NUM_THREADS]  Examples # generic sbatch -o generic.sbatch.out -t 24:0:0 ./submit_mantis_ml.sh -c ../conf/Generic_config.yaml -m 12G -n 10  # CKD sbatch -o ckd.sbatch.out -t 24:0:0 ./submit_mantis_ml.sh -c ../conf/CKD_config.yaml   Output The output folder's name is: out/[phenotype]/[run_id], where phenotype and run_id are the respective parameters in the input config file.  mantis-ml predictions (gene prediction probabilities and percentile scores) can be found at:  out/[OUTPUT_FOLDER]/supervised-learning/ranked_gene_predictions,   in the [CLASSIFIER].All_genes.mantis-ml_percentiles.csv files. Example mantis-ml scores output out/CKD-production/supervised-learning/ranked_gene_predictions/ExtraTreesClassifier.All_genes.mantis-ml_percentiles.csv     Gene_Name mantis_ml_proba mantis_ml_perc     MARK2 0.9510298564038976 100.0   MAPK8IP3 0.9404119640423988 99.99463116074305   CDH2 0.9342689255189254 99.9892623214861   PRKG1 0.9156746031746031 99.98389348222915   HES1 0.9133577533577534 99.97852464297219   CHRNA1 0.906381187440011 99.97315580371524   TFRC 0.9061122637525076 99.96778696445828   DDB1 0.8967073370255295 99.96241812520134   ... ... ...    All generated figures can be found at: out/[OUTPUT_FOLDER]/figs   Workflow overview mantis-ml follows and Automated Machine Learning (AutoML) approach for feature extraction (relevant to the disease of interest), feature compilation and pre-processing. The processed feature table is then fed to the main algorithm that lies in the core of mantis-ml: a stochastic semi-supervised approach that ranks genes based on their average performance in out-of-bag sets across random balanced datasets from the entire gene pool. A set of standard classifiers are used as part of the semi-supervised learning task:  Random Forests (RF) Extremely Randomised Trees (Extra Trees; ET) Gradien Boosting (GB) Extreme Gradient Boosting (XGBoost) Support Vector Classifier (SVC) Deep Neural Net (DNN) Stacking (1st layer: RF, ET, GB, SVC; 2nd layer: DNN)  The parameters from each classifier have been fine-tuned based on grid search using a subset of balanced datasets constructed on the basis of Chronic Kidney Disease example. Following the semi-supervised learning step of mantis-ml, predictions are overlapped with results from cohort-level rare-variant association studies. The final consensus results can then been visualised using a set of dimensionality reduction techniques:  PCA t-distributed Stochastic Neighbor Embedding (t-SNE) Uniform Manifold Approximation and Projection   Package contents The mantis-ml-release project contains:   the mantis-ml package which includes:  the mantis-ml main run scripts    .   \u2514\u2500\u2500mantis_ml    \u2514\u2500\u2500 bin   example input config files    .   \u2514\u2500\u2500mantis_ml    \u2514\u2500\u2500 conf   the mantis-ml modules      .       \u2514\u2500\u2500mantis_ml      \u2514\u2500\u2500 modules     \u251c\u2500\u2500 post_processing     \u251c\u2500\u2500 pre_processing     \u2502\u00a0\u00a0 \u2514\u2500\u2500 data_compilation     \u251c\u2500\u2500 supervised_learn     \u2502\u00a0\u00a0 \u251c\u2500\u2500 classifiers     \u2502\u00a0\u00a0 \u251c\u2500\u2500 core     \u2502\u00a0\u00a0 \u251c\u2500\u2500 feature_selection     \u2502\u00a0\u00a0 \u251c\u2500\u2500 model_selection     \u2502\u00a0\u00a0 \u2514\u2500\u2500 pu_learn     \u251c\u2500\u2500 unsupervised_learn     \u2514\u2500\u2500 validation   integrated feature data (mantis-ml/data/)    .   \u2514\u2500\u2500mantis_ml            \u2514\u2500\u2500 data     \u00a0\u00a0 \u251c\u2500\u2500 adipose_eqtl     \u00a0\u00a0 \u251c\u2500\u2500 ckddb     \u00a0\u00a0 \u251c\u2500\u2500 ensembl     \u00a0\u00a0 \u251c\u2500\u2500 essential_genes_for_mouse     \u00a0\u00a0 \u251c\u2500\u2500 exac-broadinstitute     \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 cnv     \u00a0\u00a0 \u251c\u2500\u2500 exSNP     \u00a0\u00a0 \u251c\u2500\u2500 genic-intolerance     \u00a0\u00a0 \u251c\u2500\u2500 gnomad     \u00a0\u00a0 \u251c\u2500\u2500 goa     \u00a0\u00a0 \u251c\u2500\u2500 gtex     \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 RNASeq     \u00a0\u00a0 \u251c\u2500\u2500 gwas_catalog     \u00a0\u00a0 \u251c\u2500\u2500 HPO     \u00a0\u00a0 \u251c\u2500\u2500 human_protein_atlas     \u00a0\u00a0 \u251c\u2500\u2500 in_web     \u00a0\u00a0 \u251c\u2500\u2500 mgi     \u00a0\u00a0 \u251c\u2500\u2500 msigdb     \u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tables_per_gene_set     \u00a0\u00a0 \u251c\u2500\u2500 neph_qtl     \u00a0\u00a0 \u251c\u2500\u2500 omim     \u00a0\u00a0 \u251c\u2500\u2500 platelets_eqtl     \u00a0\u00a0 \u2514\u2500\u2500 rvis_plosgen_2013    output structure examples (out/)      .    \u2514\u2500\u2500 out  \u2514\u2500\u2500 CKD-production   \u251c\u2500\u2500 data   \u2502\u00a0\u00a0 \u2514\u2500\u2500 compiled_feature_tables   \u251c\u2500\u2500 feature_selection   \u2502\u00a0\u00a0 \u2514\u2500\u2500 boruta   \u2502\u00a0\u00a0     \u251c\u2500\u2500 out   \u2502\u00a0\u00a0     \u2514\u2500\u2500 tmp   \u251c\u2500\u2500 figs   \u2502\u00a0\u00a0 \u251c\u2500\u2500 benchmarking   \u2502\u00a0\u00a0 \u251c\u2500\u2500 boruta   \u2502\u00a0\u00a0 \u251c\u2500\u2500 EDA   \u2502\u00a0\u00a0 \u251c\u2500\u2500 supervised-learning   \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 gene_proba_predictions   \u2502\u00a0\u00a0 \u2514\u2500\u2500 unsupervised-learning   \u251c\u2500\u2500 processed-feature-tables   \u251c\u2500\u2500 supervised-learning   \u2502\u00a0\u00a0 \u251c\u2500\u2500 gene_predictions   \u2502\u00a0\u00a0 \u251c\u2500\u2500 gene_proba_predictions   \u2502\u00a0\u00a0 \u2514\u2500\u2500 ranked_gene_predictions   \u2514\u2500\u2500 unsupervised-learning     Additional analyses modules: misc  module for overlap of mantis-ml predictions with cohort-level association studies        .       \u2514\u2500\u2500misc    \u2514\u2500\u2500 overlap-collapsing-analyses   module for estimation of Boruta feature importance scores across different disease examples        .       \u2514\u2500\u2500misc    \u2514\u2500\u2500 boruta-post-processing    ", "has_readme": true, "readme_language": "English", "repo_tags": ["machine-learning", "keras", "scikit-learn", "auto-ml", "genomics"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e4f"}, "repo_url": "https://github.com/Stick-To/Deep_Conv_Backone", "repo_name": "Deep_Conv_Backone", "repo_full_name": "Stick-To/Deep_Conv_Backone", "repo_owner": "Stick-To", "repo_desc": "deep conv backone for image classification", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:32:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T12:19:51Z", "homepage": "", "size": 21, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189022686, "is_fork": false, "readme_text": "Deep conv backone image classifier models resnet inception wide resnet densenet resnext dla dpn octconv-resnet pyramidnet residial attention network image classify   use keras.preprocessing.image.ImageDataGenerator   fill in dict 'config' in test-model.py   run test-model.py   Test model.test_one_batch() Experimental Environment python3.7 tensorflow1.13 ", "has_readme": true, "readme_language": "English", "repo_tags": ["resnet", "wrn", "inception", "pyramidnet", "dla", "dpn", "densenet", "resnext", "residualattentionnetwork", "octconv-resnet"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e50"}, "repo_url": "https://github.com/AkiIshikawa/techpitPython1", "repo_name": "techpitPython1", "repo_full_name": "AkiIshikawa/techpitPython1", "repo_owner": "AkiIshikawa", "repo_desc": "TensorFlow\u30fbKeras\u30fbPython\u30fbFlask\u3067\u4f5c\u308b\u6a5f\u68b0\u5b66\u7fd2\u30a2\u30d7\u30ea\u958b\u767a\u5165\u9580", "description_language": "Japanese", "repo_ext_links": null, "repo_last_mod": "2019-05-26T01:48:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T01:43:20Z", "homepage": null, "size": 115553, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188629023, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/AkiIshikawa/techpitPython1/blob/96d67e45af0f735d1bcee6f715aaab7825f616c4/animal_cnn.h5", "https://github.com/AkiIshikawa/techpitPython1/blob/96d67e45af0f735d1bcee6f715aaab7825f616c4/animal_cnn_aug.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e51"}, "repo_url": "https://github.com/deveshchatuphale7/Behavioral-Cloning", "repo_name": "Behavioral-Cloning", "repo_full_name": "deveshchatuphale7/Behavioral-Cloning", "repo_owner": "deveshchatuphale7", "repo_desc": "Project repository for Udacity self-driving car engineer behavioral cloning project", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T17:58:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T17:56:58Z", "homepage": null, "size": 30, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188887718, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e52"}, "repo_url": "https://github.com/Uttam95yadav/CarND-Behavioral-Cloning-P3", "repo_name": "CarND-Behavioral-Cloning-P3", "repo_full_name": "Uttam95yadav/CarND-Behavioral-Cloning-P3", "repo_owner": "Uttam95yadav", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T05:37:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T05:04:17Z", "homepage": null, "size": 81560, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188957266, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Uttam95yadav/CarND-Behavioral-Cloning-P3/blob/be67cb258b68c8a38265234747ffb244576085a9/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e53"}, "repo_url": "https://github.com/avivga/style-image-prior", "repo_name": "style-image-prior", "repo_full_name": "avivga/style-image-prior", "repo_owner": "avivga", "repo_desc": "Official Implementation of \"Style Generator Inversion for Image Enhancement and Animation\".", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T12:27:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T11:06:04Z", "homepage": "", "size": 7, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188827560, "is_fork": false, "readme_text": "Style Image Prior Implementation of the method described in the paper: Style Generator Inversion for Image Enhancement and Animation by Aviv Gabbay and Yedid Hoshen. Inpainting                Corrupted Ours GT    Super-Resolution (128x128 to 1024x1024)                Bicubic Ours GT    Re-animation: Animating Obama from a video of Trump                     Usage Dependencies  python >= 3.6 numpy >= 1.15.4 tensorflow-gpu >= 1.12.0 keras >= 2.2.4 opencv >= 3.4.4 tqdm >= 4.28.1  Getting started   Clone the official StyleGAN repository.   Add the local StyleGAN project to PYTHONPATH. For bash users:   export PYTHONPATH=<path-to-stylegan-project>  Style Image Prior for Inpainting Recovering missing parts of given images along with the respective latent codes can be done as follows: inpainting.py --imgs-dir <input-imgs-dir> --masks-dir <output-masks-dir>     --corruptions-dir <output-corruptions-dir> --restorations-dir <output-restorations-dir>     --latents-dir <output-latents-dir>     [--input-img-size INPUT_IMG_HEIGHT INPUT_IMG_WIDTH]     [--perceptual-img-size EFFECTIVE_IMG_HEIGHT EFFECTIVE_IMG_WIDTH]     [--mask-size MASK_HEIGHT MASK_WIDTH]     [--learning-rate LEARNING_RATE]     [--total-iterations TOTAL_ITERATIONS]  Style Image Prior for Super-Resolution Performing super-resolution on given images can be done as follows: super_resolution.py --lr-imgs-dir <input-imgs-dir> --hr-imgs-dir <output-imgs-dir>     --latents-dir <output-latents-dir>     [--lr-img-size LR_IMG_HEIGHT LR_IMG_WIDTH]     [--hr-img-size HR_IMG_HEIGHT HR_IMG_WIDTH]     [--learning-rate LEARNING_RATE]     [--total-iterations TOTAL_ITERATIONS]  Citing If you find this project useful for your research, please cite @article{gabbay2019styleimageprior,   author    = {Aviv Gabbay and Yedid Hoshen},   title     = {Style Generator Inversion for Image Enhancement and Animation},   journal   = {arXiv preprint},   year      = {2019} }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.vision.huji.ac.il/style-image-prior"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e54"}, "repo_url": "https://github.com/ml-tooling/ml-workspace", "repo_name": "ml-workspace", "repo_full_name": "ml-tooling/ml-workspace", "repo_owner": "ml-tooling", "repo_desc": "Machine Learning Workspace", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T06:15:09Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T16:55:15Z", "homepage": null, "size": 313, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188880197, "is_fork": false, "readme_text": "Machine Learning Workspace The ML workspace is an all-in-one web IDE specialized for machine learning and data science. It comes with Jupyter Notebook, VNC access, Git integration, hardware monitoring, GPU access, and many common ML libraries. The workspace is preinstalled with various common data science tools, libraries, and features, such as:  Runtimes: Anaconda 3, Java 8, NodeJS Tools: Jupyter, Visual Studio Code, ungit, netdata, noVNC ML libs: Tensorflow, Keras, Pytorch, Sklearn, CNTK, XGBoost, ...  Usage Deploy Workspace To start the workspace locally, execute: docker run -d --name ml-workspace -p 8091:8091 --restart always mltooling/ml-workspace Visit http://localhost:8091 Persist Data To persist the data, you need to mount a volume into /workspace. Configuration The container can be configured with following environment variables (--env):   Variable Description Default   WORKSPACE_BASE_URL The base URL under which the notebook server is reachable. E.g. setting it to /workspace, the workspace would be reachable under /workspace/tree /   Jupyter Configuration:   VNC Configuration:   VNC_PW Password of VNC Connection vncpassword   VNC_RESOLUTION Desktop Resolution of VNC Connection 1600x900   VNC_COL_DEPTH Color Depth of VNC Connection 24   Develop Build Execute this command in the project root folder to build the docker container: python build.py --version={MAJOR.MINOR.PATCH-TAG} The version has to be provided. The version format should follow the Semantic Versioning standard (MAJOR.MINOR.PATCH). For additional script options: python build.py --help Deploy Execute this command in the project root folder to push the container to the configured docker registry: python build.py --deploy --version={MAJOR.MINOR.PATCH-TAG} The version has to be provided. The version format should follow the Semantic Versioning standard (MAJOR.MINOR.PATCH). For additional script options: python build.py --help ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://localhost:8091"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e55"}, "repo_url": "https://github.com/shahabi8/Deep-Reinforcement-Learning", "repo_name": "Deep-Reinforcement-Learning", "repo_full_name": "shahabi8/Deep-Reinforcement-Learning", "repo_owner": "shahabi8", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T19:12:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T18:36:15Z", "homepage": null, "size": 17, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188892528, "is_fork": false, "readme_text": "Deep-Reinforcement-Learning Train a Quadcopter How to Fly The Quadcopter or Quadrotor Helicopter is becoming an increasingly popular aircraft for both personal and professional use. Its maneuverability lends itself to many applications, from last-mile delivery to cinematography, from acrobatics to search-and-rescue. Most quadcopters have 4 motors to provide thrust, although some other models with 6 or 8 motors are also sometimes referred to as quadcopters. Multiple points of thrust with the center of gravity in the middle improves stability and enables a variety of flying behaviors. But it also comes at a price\u2013the high complexity of controlling such an aircraft makes it almost impossible to manually control each individual motor's thrust. So, most commercial quadcopters try to simplify the flying controls by accepting a single thrust magnitude and yaw/pitch/roll controls, making it much more intuitive and fun. The next step in this evolution is to enable quadcopters to autonomously achieve desired control behaviors such as takeoff and landing. You could design these controls with a classic approach (say, by implementing PID controllers). Or, you can use reinforcement learning to build agents that can learn these behaviors on their own. Instruction  task.py: Define task (environment) for reinforcement learning. physics_sim.py: This file contains the simulator for the quadcopter. Replay_buffer.py: Most modern reinforcement learning algorithms benefit from using a replay memory or buffer to store and recall experience tuples. Actor.py DDPG: Actor (Value) Model. Critic.py DDPG: Critic (Value) Model. DDPG.py DDPG agent to put together actor and critic. OUnoise.py Ornstein\u2013Uhlenbeck Noise. Use this process to add some noise to our actions, in order to encourage exploratory behavior. And since our actions translate to force and torque being applied to a quadcopter, we want consecutive actions to not vary wildly.  Libaries Used  Keras csv  Additional Document Lillicrap, Timothy P., et al., 2015. Continuous Control with Deep Reinforcement Learning. (https://arxiv.org/pdf/1509.02971.pdf) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1509.02971.pdf"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e56"}, "repo_url": "https://github.com/sdhnt/trash.ai", "repo_name": "trash.ai", "repo_full_name": "sdhnt/trash.ai", "repo_owner": "sdhnt", "repo_desc": "An AI that can automatically classify trash for recycling - Built on Tensorflow and Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T18:58:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T18:17:02Z", "homepage": "", "size": 9, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189080388, "is_fork": false, "readme_text": "trash_ai ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e57"}, "repo_url": "https://github.com/bitsurgeon/CarND_BehavioralCloning", "repo_name": "CarND_BehavioralCloning", "repo_full_name": "bitsurgeon/CarND_BehavioralCloning", "repo_owner": "bitsurgeon", "repo_desc": "train a deep neural network to drive a car like you", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T03:08:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T06:54:06Z", "homepage": "https://bitsurgeon.github.io/udacity/BehavioralCloning.html", "size": 3877, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188650043, "is_fork": false, "readme_text": "Behavioral Cloning A project of Self-Driving Car Engineer Nanodegree  The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Build, a convolution neural network in Keras that predicts steering angles from images Train and validate the model with a training and validation set Test that the model successfully drives around track one without leaving the road   In this project, you, as a driver, need to train a neural network to keep a car driving in the center of the lane line. Udacity car simulator (Windows Mac Linux) is used to train the network and test how good the model behave on the road, well of course, in the simulator. The model architecture The model used in this project contains 9 layers, including a normalization layer, 5 convolutional layers, and 3 fully connected layers. It is inspired by PilotNet mentioned in a research paper published by Nvidia Self-driving Team.  the normalization layer  used to cut off unrelated information from the camera image, such as sky, trees, and car botnet, which will make the model more generalized the reset of the image data is then normalized with zero mean all these helps reduce the size of the model and speed up the training   the convolutional layers  the first 3 layers are processed with 3x3 stride with kernel size 5x5 the last 2 layers are processed by a 3x3 kernel with no stride these layers are used to extract features of from the camera image   the fully connected layers  designed to act as control layers to provide steering angle decision according to the features detected from previous convolutional layers   activation layer  ReLu activation is used through out the network to provide non-linearity of the model since this is a regression network, the final output is the steering angle, thus no activation layer is attached   dropout layer  to prevent overfitting, a dropout rate of 0.5 is applied to the output of the first 2 fully connected layers    Here is the Keras network summary: _________________________________________________________________ Layer (type)                 Output Shape              Param # ================================================================= cropping2d_1 (Cropping2D)    (None, 65, 320, 3)        0 _________________________________________________________________ lambda_1 (Lambda)            (None, 65, 320, 3)        0 _________________________________________________________________ conv2d_1 (Conv2D)            (None, 31, 158, 24)       1824 _________________________________________________________________ conv2d_2 (Conv2D)            (None, 14, 77, 36)        21636 _________________________________________________________________ conv2d_3 (Conv2D)            (None, 5, 37, 48)         43248 _________________________________________________________________ conv2d_4 (Conv2D)            (None, 3, 35, 64)         27712 _________________________________________________________________ conv2d_5 (Conv2D)            (None, 1, 33, 64)         36928 _________________________________________________________________ flatten_1 (Flatten)          (None, 2112)              0 _________________________________________________________________ dense_1 (Dense)              (None, 100)               211300 _________________________________________________________________ dropout_1 (Dropout)          (None, 100)               0 _________________________________________________________________ dense_2 (Dense)              (None, 50)                5050 _________________________________________________________________ dropout_2 (Dropout)          (None, 50)                0 _________________________________________________________________ dense_3 (Dense)              (None, 10)                510 _________________________________________________________________ dense_4 (Dense)              (None, 1)                 11 ================================================================= Total params: 348,219 Trainable params: 348,219 Non-trainable params: 0 _________________________________________________________________  As shown above, there are a total of 348,219 parameters to train in the network. Model training The model get trained by observing how you drive. The simulator has two modes: training and autonomous driving. You first drive on the track and capture driving data used for training. Then, the data, the captured images, will be used to train your network.  collect driving data  a total of 51,390 images are collected, and 20% of it are reserved for validation one lap of driving in the center of the lane  one lap of driving in the center of lane, but revers direction recovery driving to drive the car from side of the road back onto lane center      data collected from 3 cameras, e.g. left, center, right, are used      data preprocessing  all the collect images are left-right flipped to expend the training dataset for more generalized network the camera collected RBG images are converted to YUV color space before sent to the model the sequence of the images can affect the trained model, so, for each epoch of training, the dataset are shuffled randomly to eliminate the affect   training  as the model is trained on my laptop, which only has 4GB GPU memory to use, Python generator is used to keep the memory usage manageable, a batch size of 32 is used here an Adam optimizer is used so that manually training the learning rate wasn't necessary since we are predicting the steering angle, Mean Square Error (MSE) loss is used to determine the model quality    The model converges quite quickly. The validation dataset start to reach <0.015 after 3 epochs.  Running the project   Start the simulator in training mode, and collect driving examples. Alternertively, you can get a copy of the driving data from this link. Extract the data in to folder, so that you have the following folder structure: data   |   |-- IMG   |    |   |    |-- *.jpg   |   |-- driving_log.csv    Train model python model.py   Test model in simulator python drive.py model.h5 video Start simulator in autonomous driving mode. The car should start driving by itself. All copy of the driveing footage will be saved in the video folder.   Generate testing footage python video.py video video.mp4 will be generated.   Here is a sample output from my trained model.  Note:  The first 2 minutes of the video is to test how the car can recovery from the side of the road. You may find the footage is a bit stucky, which is due to I constantly pull the car towards the curbs manually. As shown in the video, when I pull the car to the side of the road in both straight lane and curves, the car can always recovery itself back onto the middle of the lane. The rest of the video demonstrated the car can self-drive for the whole lap. It navigate through the curves smoothly and keeps itself in the center of the lane all the time.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/bitsurgeon/CarND_BehavioralCloning/blob/4eb34e31ba4b757f95cc68fac84038e08b4c3295/model.h5"], "see_also_links": ["http://www.udacity.com/drive", "http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e58"}, "repo_url": "https://github.com/zobrathemanish/age-detection", "repo_name": "age-detection", "repo_full_name": "zobrathemanish/age-detection", "repo_owner": "zobrathemanish", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T06:35:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T06:34:49Z", "homepage": null, "size": 18277, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188648588, "is_fork": false, "readme_text": "Age and Gender Estimation This is a Keras implementation of a CNN for estimating age and gender from a face image [1, 2]. In training, the IMDB-WIKI dataset is used. Dependencies  Python2.7+ Keras2.0+ scipy, numpy, Pandas, tqdm, tables, h5py OpenCV3  Tested on:  Ubuntu 16.04, Python 3.5.2, Keras 2.0.3, Tensorflow(-gpu) 1.0.1,  CUDA 8.0, cuDNN 5.0  CPU: i7-7700 3.60GHz, GPU: GeForce GTX1080    Usage Face detect ''' here use opencv-face-detection. or you can use the mtcnn-caffemodel, I have packaged the mtcnn face model into a class, the link is here. ''' Use pretrained model Download pretrained model weights for TensorFlow backend: The age, gender and emotion pretrained models locate in the directory PRJ_ROOT/trained_models/  Run demo script (requires web cam) python demo.py Train a model using the IMDB-WIKI dataset Download the dataset The dataset is downloaded and extracted to the data directory. ./download.sh Create training data Filter out noise data and serialize images and labels for training into .mat file. Please check check_dataset.ipynb for the details of the dataset. python create_db.py --output data/imdb_db.mat --db imdb --img_size 64 usage: create_db.py [-h] --output OUTPUT [--db DB] [--img_size IMG_SIZE] [--min_score MIN_SCORE]  This script cleans-up noisy labels and creates database for training.  optional arguments:   -h, --help                 show this help message and exit   --output OUTPUT, -o OUTPUT path to output database mat file (default: None)   --db DB                    dataset; wiki or imdb (default: wiki)   --img_size IMG_SIZE        output image size (default: 32)   --min_score MIN_SCORE      minimum face_score (default: 1.0) Train network Train the network using the training data created above. python train.py --input data/imdb_db.mat Trained weight files are stored as checkpoints/weights.*.hdf5 for each epoch if the validation loss becomes minimum over previous epochs. usage: train.py [-h] --input INPUT [--batch_size BATCH_SIZE]                 [--nb_epochs NB_EPOCHS] [--depth DEPTH] [--width WIDTH]                 [--validation_split VALIDATION_SPLIT]  This script trains the CNN model for age and gender estimation.  optional arguments:   -h, --help                          show this help message and exit   --input INPUT, -i INPUT             path to input database mat file (default: None)   --batch_size BATCH_SIZE             batch size (default: 32)   --nb_epochs NB_EPOCHS               number of epochs (default: 30)   --depth DEPTH                       depth of network (should be 10, 16, 22, 28, ...) (default: 16)   --width WIDTH                       width of network (default: 8)   --validation_split VALIDATION_SPLIT validation split ratio (default: 0.1) Use the trained network python demo.py usage: demo.py [-h] [--weight_file WEIGHT_FILE] [--depth DEPTH] [--width WIDTH]  This script detects faces from web cam input, and estimates age and gender for the detected faces.  optional arguments:   -h, --help                show this help message and exit   --weight_file WEIGHT_FILE path to weight file (e.g. weights.18-4.06.hdf5) (default: None)   --depth DEPTH             depth of network (default: 16)   --width WIDTH             width of network (default: 8)  Please use the best model among checkpoints/weights.*.hdf5 for WEIGHT_FILE if you use your own trained models. Plot training curves from history file python plot_history.py --input models/history_16_8.h5    Network architecture In the original paper [1, 2], the pretrained VGG network is adopted. Here the Wide Residual Network (WideResNet) is trained from scratch. I modified the @asmith26's implementation of the WideResNet; two classification layers (for age and gender estimation) are added on the top of the WideResNet. Note that while age and gender are independently estimated by different two CNNs in [1, 2], in my implementation, they are simultaneously estimated using a single CNN. Results Trained on imdb, tested on wiki.  References [1] R. Rothe, R. Timofte, and L. V. Gool, \"DEX: Deep EXpectation of apparent age from a single image,\" ICCV, 2015. [2] R. Rothe, R. Timofte, and L. V. Gool, \"Deep expectation of real and apparent age from a single image without facial landmarks,\" IJCV, 2016. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/ag_models/WRN_16_2.h5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/ag_models/weights.25000-0.03.hdf5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/emotion_models/fer2013_mini_XCEPTION.110-0.65.hdf5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/gender_models/gender_mini_XCEPTION.176-0.91.hdf5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/gender_models/gender_mini_XCEPTION.2878-0.93.hdf5", "https://github.com/zobrathemanish/age-detection/blob/a1ff22b9d18759a267bae56e4adec374ffa16828/trained_models/gender_models/simple_CNN.81-0.96.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e59"}, "repo_url": "https://github.com/KomalTauqeer/MiST", "repo_name": "MiST", "repo_full_name": "KomalTauqeer/MiST", "repo_owner": "KomalTauqeer", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T10:39:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T10:26:13Z", "homepage": null, "size": 78, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189006494, "is_fork": false, "readme_text": "MiST - Machine learning in Single Top Providing an easy way to utilize state-of-the-art machine learning tools in HEP analyses Requirements To avoid dependency on local software installations, virtualenv is used to create a virtual python environment in which any additional python packages are installed using pip. The GPU is interfaced with the CUDA API, but in principle the code also runs on CPU with some minor adjustments (tensorflow-gpu -> tensorflow below). Other than that, only ROOT (v6) needs to be installed locally for the instructions below. Instructions   Connect to the ekpdeepthought machine to utilize GPU power:  ssh ekpdeepthought    To make use of the GPU, add the following to your ~/.bashrc or ~/.profile, you can also place it outside the function to source it automatically:  cuda_init() {       MYCUDAVERSION=\"cuda-9.0\"       export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/$MYCUDAVERSION/lib64:/usr/local/$MYCUDAVERSION/extras/CUPTI/lib64\"      export CUDA_HOME=\"/usr/local/$MYCUDAVERSION\"   }  You may also add something like this to automatically use the same ROOT version  if [ \"$HOSTNAME\" = \"ekpdeepthought\" ]; then      . /usr/local/bin/thisroot.sh  fi    Source your bashrc/profile again and call cuda_init (if necessary).   Clone the repo either using your ssh key:  git clone git@gitlab.ekp.kit.edu:tH/MiST.git  OR via https:  git clone https://gitlab.ekp.kit.edu/tH/MiST.git    Create a new virtualenv instance (only tested with python2) and cd to cloned repo  virtualenv venv  source venv/bin/activate  cd MiST    Install all required python packages:  pip install --upgrade --force-reinstall --no-cache argparse pandas numpy tensorflow-gpu==1.12.0 keras six tqdm scipy scikit-learn matplotlib h5py pydot pympler pandas root_pandas  Make sure that your ROOT installation is working and that the corresponding environment variables are set correctly!   The program is started with:  python run.py [-...]  Description of additional arguments is provided with:  python run.py --help  You can also use a config file which provides all arguments:  python run.py -c 'config/settings.config'  Options set with such a config file can be overwritten:  python run.py -c 'config/settings.config' --epochs 500    If the programm crashes during plots, execute the following: echo \"backend: Agg\" > ~/.config/matplotlib/matplotlibrc  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e5a"}, "repo_url": "https://github.com/Sotaneum/DeepGeo", "repo_name": "DeepGeo", "repo_full_name": "Sotaneum/DeepGeo", "repo_owner": "Sotaneum", "repo_desc": "Easy Deep Learning ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T03:04:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T05:52:40Z", "homepage": "", "size": 77, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188645527, "is_fork": false, "readme_text": "Deep Geo  Easy Deep Learning Copyright (c) 2019 InfoLab (Donggun LEE) PIP : https://pypi.org/project/deepgeo/ Demo  deepGeo.io DeepGeo.io   How to install pip install deepgeo  other version # 0.0.1 pip install deepgeo==0.0.1  requirement  Python 3.6  pip install tensorflow-gpu==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7    How to use  Import DeepGeo import deepgeo  detect engine = deepgeo.Engine()  # add model engine.add_model('mscoco','maskrcnn','D:/test/config.json')  # create image image = deepgeo.Image('uri') engine.detect('mscoco',image) print(image)  ... etc ...  extends function import sys,os,json  def init_loading_bar(max):   max_ = str(max)   sys.stdout.write(\"%s/0\" % max_)   sys.stdout.flush()   sys.stdout.write(\"\\b\")  def update_loading_bar(num):   sys.stdout.write(\"%s\" % str(num))   sys.stdout.flush()   sys.stdout.write(\"\\b\" * len(str(num)))  def final_loading_bar():   sys.stdout.write(\"\\n\")  def fjson_to_imgs(engine, dataset_name, path, image_path):   file_list = os.listdir(path)   file_list.sort()   init_loading_bar(len(file_list))   count = 0   for item in file_list:       count+=1       update_loading_bar(count)       if item.find('.json') is not -1:           data=None           with open(path+item) as data_file:                   data = json.load(data_file)           engine.add_data('mscoco',dataset_name,deepgeo.Image(data['uri'],image_path,data['annotations']))           del data   final_loading_bar()   del count   del file_list  train engine = deepgeo.Engine() engine.add_model('mscoco', 'maskrcnn', 'D:/test/config.json') engine.add_dataset('mscoco','train','maskrcnn') engine.add_dataset('mscoco','val','maskrcnn') fjson_to_imgs(engine, 'train','D:/test/val2017/json/','D:/test/val2017/images/') fjson_to_imgs(engine, 'val','D:/test/val2017/json/','D:/test/val2017/images/')  ## Training... print(\" > STEP3 : Fine tune all layers\") engine.set_config('mscoco',{\"EPOCHS\":160, \"LAYERS\":'all',\"LEARNING_RATE\":engine.get_config('mscoco',\"LEARNING_RATE\")/10}) engine.train('mscoco',\"train\",\"val\",None)  ... etc ...     ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://duration.digimoon.net", "http://deepGeo.io", "http://duration.digimoon.net/dev/deepGeo/test/", "http://infolab.kunsan.ac.kr"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e5b"}, "repo_url": "https://github.com/ntustison/ANTsPyNet", "repo_name": "ANTsPyNet", "repo_full_name": "ntustison/ANTsPyNet", "repo_owner": "ntustison", "repo_desc": "We are the pretty petty thieves, And you're standing on our street.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T01:02:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T16:44:24Z", "homepage": null, "size": 32, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189067098, "is_fork": false, "readme_text": "ANTsPyNet A collection of deep learning architectures ported to the python language and tools for basic medical image processing. Based on keras and tensorflow with cross-compatibility with our R analog ANTsRNet. Architectures Image voxelwise segmentation/regression  U-Net (2-D, 3-D)  O. Ronneberger, P. Fischer, and T. Brox.  U-Net: Convolutional Networks for Biomedical Image Segmentation.    Image classification/regression  ResNet/ResNeXt (2-D, 3-D)  Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.  Deep Residual Learning for Image Recognition. Saining Xie and Ross Girshick and Piotr Doll\u00e1r and Zhuowen Tu and Kaiming He.  Aggregated Residual Transformations for Deep Neural Networks.    Object detection Image super-resolution  Deep back-projection network (DBPN) (2-D, 3-D)  Muhammad Haris, Greg Shakhnarovich, Norimichi Ukita.  Deep Back-Projection Networks For Super-Resolution.    Registration and transforms Generative adverserial networks Clustering Miscellaneous  Installation  ANTsPyNet Installation:  Option 1:   Option 2:      Publications   Nicholas J. Tustison, Brian B. Avants, and James C. Gee. Learning image-based spatial transformations via convolutional neural networks: a review,  Magnetic Resonance Imaging.  (accepted)   Nicholas J. Tustison, Brian B. Avants, Zixuan Lin, Xue Feng, Nicholas Cullen, Jaime F. Mata, Lucia Flors, James C. Gee, Talissa A. Altes, John P. Mugler III, and Kun Qing.  Convolutional Neural Networks with Template-Based Data Augmentation for Functional Lung Image Quantification, Academic Radiology, 26(3):412-423, Mar 2019. (pubmed)   Andrew T. Grainger, Nicholas J. Tustison, Kun Qing, Rene Roy, Stuart S. Berr, and Weibin Shi.  Deep learning-based quantification of abdominal fat on magnetic resonance images. PLoS One, 13(9):e0204071, Sep 2018.  (pubmed)   Cullen N.C., Avants B.B. (2018) Convolutional Neural Networks for Rapid and Simultaneous Brain Extraction and Tissue Segmentation. In: Spalletta G., Piras F., Gili T. (eds) Brain Morphometry. Neuromethods, vol 136. Humana Press, New York, NY doi   Acknowledgments   We gratefully acknowledge the support of the NVIDIA Corporation with the donation of two Titan Xp GPUs used for this research.   We gratefully acknowledge the grant support of the Office of Naval Research and Cohen Veterans Bioscience.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1505.04597", "https://arxiv.org/abs/1512.03385", "https://arxiv.org/abs/1611.05431", "https://arxiv.org/abs/1803.02735"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e5c"}, "repo_url": "https://github.com/shinezzz/Test-MASK-RCNN", "repo_name": "Test-MASK-RCNN", "repo_full_name": "shinezzz/Test-MASK-RCNN", "repo_owner": "shinezzz", "repo_desc": "\u57fa\u4e8eMask R-CNN\u8fdb\u884c\u56fe\u7247\u5904\u7406\uff0c\u89c1[myTest]", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-28T04:23:10Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T02:42:44Z", "homepage": null, "size": 124442, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 188941377, "is_fork": false, "readme_text": "Mask R-CNN for Object Detection and Segmentation This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.  The repository includes:  Source code of Mask R-CNN built on FPN and ResNet101. Training code for MS COCO Pre-trained weights for MS COCO Jupyter notebooks to visualize the detection pipeline at every step ParallelModel class for multi-GPU training Evaluation on MS COCO metrics (AP) Example of training on your own dataset  The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here. Getting Started   demo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.   train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.   (model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.   inspect_data.ipynb. This notebook visualizes the different pre-processing steps to prepare the training data.   inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.   inspect_weights.ipynb This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.   Step by Step Detection To help with debugging and understanding the model, there are 3 notebooks (inspect_data.ipynb, inspect_model.ipynb, inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples: 1. Anchor sorting and filtering Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.  2. Bounding Box Refinement This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.  3. Mask Generation Examples of generated masks. These then get scaled and placed on the image in the right location.  4.Layer activations Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).  5. Weight Histograms Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.  6. Logging to TensorBoard TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.  6. Composing the different pieces into a final result  Training on MS COCO We're providing pre-trained weights for MS COCO to make it easier to start. You can use those weights as a starting point to train your own variation on the network. Training and evaluation code is in samples/coco/coco.py. You can import this module in Jupyter notebook (see the provided notebooks for examples) or you can run it directly from the command line as such: # Train a new model starting from pre-trained COCO weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  # Train a new model starting from ImageNet weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet  # Continue training a model that you had trained earlier python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5  # Continue training the last model you trained. This will find # the last trained weights in the model directory. python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last  You can also run the COCO evaluation code with: # Run COCO evaluation on the last trained model python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last  The training schedule, learning rate, and other parameters should be set in samples/coco/coco.py. Training on Your Own Dataset Start by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application. In summary, to train the model on your own dataset you'll need to extend two classes: Config This class contains the default configuration. Subclass it and modify the attributes you need to change. Dataset This class provides a consistent way to work with any dataset. It allows you to use new datasets for training without having to change the code of the model. It also supports loading multiple datasets at the same time, which is useful if the objects you want to detect are not all available in one dataset. See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py. Differences from the Official Paper This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.   Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.   Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation. To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, and only 0.01% differed by 10px or more.   Learning Rate: The paper uses a learning rate of 0.02, but we found that to be too high, and often causes the weights to explode, especially when using a small batch size. It might be related to differences between how Caffe and TensorFlow compute gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively. We found that smaller learning rates converge faster anyway so we go with that.   Citation Use this bibtex to cite this repository: @misc{matterport_maskrcnn_2017,   title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},   author={Waleed Abdulla},   year={2017},   publisher={Github},   journal={GitHub repository},   howpublished={\\url{https://github.com/matterport/Mask_RCNN}}, }  Contributing Contributions to this repository are welcome. Examples of things you can contribute:  Speed Improvements. Like re-writing some Python code in TensorFlow or Cython. Training on other datasets. Accuracy Improvements. Visualizations and examples.  You can also join our team and help us build even more projects like this one. Requirements Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt. MS COCO Requirements: To train or test on MS COCO, you'll also need:  pycocotools (installation instructions below) MS COCO Dataset Download the 5K minival and the 35K validation-minus-minival subsets. More details in the original Faster R-CNN implementation.  If you use Docker, the code has been verified to work on this Docker container. Installation   Clone this repository   Install dependencies pip3 install -r requirements.txt   Run setup from the repository root directory python3 setup.py install   Download pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.   (Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).  Linux: https://github.com/waleedka/coco Windows: https://github.com/philferriere/cocoapi. You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)    Projects Using this Model If you extend this model to other datasets or build projects that use it, we'd love to hear from you. 4K Video Demo by Karol Majek.  Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.  Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.  Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science Bowl Code is in the samples/nucleus directory.  Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.  Reconstructing 3D buildings from aerial LiDAR A proof of concept project by Esri, in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.  Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy A project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.   Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery Research project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.  Mask-RCNN Shiny A computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.  Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.  GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ond\u0159ej Pe\u0161ek.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cocodataset.org/#home", "http://www.mdpi.com/2072-4292/10/9/1487"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e5d"}, "repo_url": "https://github.com/danamyu/mask_rcnn_nucleus", "repo_name": "mask_rcnn_nucleus", "repo_full_name": "danamyu/mask_rcnn_nucleus", "repo_owner": "danamyu", "repo_desc": "Instance segmentation for cell nuclei", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T09:06:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T08:39:43Z", "homepage": null, "size": 74351, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 188805057, "is_fork": false, "readme_text": "Mask R-CNN for Object Detection and Segmentation Cloned from https://github.com/matterport/Mask_RCNN/blob/master/samples/nucleus/nucleus.py This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.  The repository includes:  Source code of Mask R-CNN built on FPN and ResNet101. Training code for MS COCO Pre-trained weights for MS COCO Jupyter notebooks to visualize the detection pipeline at every step ParallelModel class for multi-GPU training Evaluation on MS COCO metrics (AP) Example of training on your own dataset  The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here. Getting Started   demo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.   train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.   (model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.   inspect_data.ipynb. This notebook visualizes the different pre-processing steps to prepare the training data.   inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.   inspect_weights.ipynb This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.   Step by Step Detection To help with debugging and understanding the model, there are 3 notebooks (inspect_data.ipynb, inspect_model.ipynb, inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples: 1. Anchor sorting and filtering Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.  2. Bounding Box Refinement This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.  3. Mask Generation Examples of generated masks. These then get scaled and placed on the image in the right location.  4.Layer activations Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).  5. Weight Histograms Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.  6. Logging to TensorBoard TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.  6. Composing the different pieces into a final result  Training on MS COCO We're providing pre-trained weights for MS COCO to make it easier to start. You can use those weights as a starting point to train your own variation on the network. Training and evaluation code is in samples/coco/coco.py. You can import this module in Jupyter notebook (see the provided notebooks for examples) or you can run it directly from the command line as such: # Train a new model starting from pre-trained COCO weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  # Train a new model starting from ImageNet weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet  # Continue training a model that you had trained earlier python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5  # Continue training the last model you trained. This will find # the last trained weights in the model directory. python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last  You can also run the COCO evaluation code with: # Run COCO evaluation on the last trained model python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last  The training schedule, learning rate, and other parameters should be set in samples/coco/coco.py. Training on Your Own Dataset Start by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application. In summary, to train the model on your own dataset you'll need to extend two classes: Config This class contains the default configuration. Subclass it and modify the attributes you need to change. Dataset This class provides a consistent way to work with any dataset. It allows you to use new datasets for training without having to change the code of the model. It also supports loading multiple datasets at the same time, which is useful if the objects you want to detect are not all available in one dataset. See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py. Differences from the Official Paper This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.   Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.   Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation. To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, and only 0.01% differed by 10px or more.   Learning Rate: The paper uses a learning rate of 0.02, but we found that to be too high, and often causes the weights to explode, especially when using a small batch size. It might be related to differences between how Caffe and TensorFlow compute gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively. We found that smaller learning rates converge faster anyway so we go with that.   Citation Use this bibtex to cite this repository: @misc{matterport_maskrcnn_2017,   title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},   author={Abdulla, Waleed},   year={2017},   publisher={Github},   journal={GitHub repository},   howpublished={\\url{https://github.com/matterport/Mask_RCNN}}, }  Contributing Contributions to this repository are welcome. Examples of things you can contribute:  Speed Improvements. Like re-writing some Python code in TensorFlow or Cython. Training on other datasets. Accuracy Improvements. Visualizations and examples.  You can also join our team and help us build even more projects like this one. Requirements Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt. MS COCO Requirements: To train or test on MS COCO, you'll also need:  pycocotools (installation instructions below) MS COCO Dataset Download the 5K minival and the 35K validation-minus-minival subsets. More details in the original Faster R-CNN implementation.  If you use Docker, the code has been verified to work on this Docker container. Installation   Install dependencies pip3 install -r requirements.txt   Clone this repository   Run setup from the repository root directory python3 setup.py install   Download pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.   (Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).  Linux: https://github.com/waleedka/coco Windows: https://github.com/philferriere/cocoapi. You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)    Projects Using this Model If you extend this model to other datasets or build projects that use it, we'd love to hear from you. 4K Video Demo by Karol Majek.  Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.  Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.  Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science Bowl Code is in the samples/nucleus directory.  Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.  Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.  GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ond\u0159ej Pe\u0161ek.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cocodataset.org/#home"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e5e"}, "repo_url": "https://github.com/tuhinjubcse/ALW3-ACL2019", "repo_name": "ALW3-ACL2019", "repo_full_name": "tuhinjubcse/ALW3-ACL2019", "repo_owner": "tuhinjubcse", "repo_desc": "Pay \"Attention'' to your Context when Classifying Abusive Language", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T02:35:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T02:19:04Z", "homepage": null, "size": 17, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188631223, "is_fork": false, "readme_text": "ALW3-ACL2019 Repo for our paper Pay \"Attention'' to your Context when Classifying Abusive Language Third Proceedings of the Abusive Language Workshop, ACL 2019 , Florence Italy Download twitter datasets and preprocess using Ekphrasis There is some skeleton code on how to use Ekphrasis You can alter based on your file and how you have the tweet and label corresponding to it textClassifierHATT.py has code for both Self and Context Attention Self Attention ----> Class Name Attention Context Attention ---> Class Name AttLayer You can edit when adding in the model For running you have to use python2 and Theano as BACKEND to run the code Use command KERAS_BACKEND=theano python2 textClassifierHATT.py If you use our code or idea, please cite us ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e5f"}, "repo_url": "https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning", "repo_name": "UdacitySDN-Behavioural_Cloning", "repo_full_name": "BlazingFire/UdacitySDN-Behavioural_Cloning", "repo_owner": "BlazingFire", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T02:33:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T02:22:38Z", "homepage": null, "size": 96856, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188757119, "is_fork": false, "readme_text": "Behavioral Cloning Writeup Template You can use this file as a template for your writeup if you want to submit it as a markdown file, but feel free to use some other method and submit a pdf if you prefer.  Behavioral Cloning Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Build, a convolution neural network in Keras that predicts steering angles from images Train and validate the model with a training and validation set Test that the model successfully drives around track one without leaving the road Summarize the results with a written report  Rubric Points Here I will consider the rubric points individually and describe how I addressed each point in my implementation.  Files Submitted & Code Quality 1. Submission includes all required files and can be used to run the simulator in autonomous mode My project includes the following files:  model.py containing the script to create and train the model drive.py for driving the car in autonomous mode model.h5 containing a trained convolution neural network train.py containing the actual neural net architecture and training files writeup_report.md or writeup_report.pdf summarizing the results  2. Submission includes functional code Using the Udacity provided simulator and my drive.py file, the car can be driven autonomously around the track by executing python drive.py model.h5 3. Submission code is usable and readable The model.py and train.py file contains the code for training and saving the convolution neural network. The file shows the pipeline I used for training and validating the model, and it contains comments to explain how the code works. Model Architecture and Training Strategy 1. An appropriate model architecture has been employed My model consists of a convolution neural network with 5x5 and 3x3 filter sizes and depths between 24 and 64 (train.py lines 22-38) The model includes ELU layers to introduce nonlinearity , and the data is normalized in the model using a Keras lambda layer (code line 15). 2. Attempts to reduce overfitting in the model The model contains dropout layers in order to reduce overfitting (train.py lines 42 and 51). The model was trained and validated on different data sets to ensure that the model was not overfitting (model.py  lines 64-127). The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track. 3. Model parameter tuning The model used an adam optimizer, so the learning rate was not tuned manually but learning rate was reduced(0.00001) (train.py line 81). 4. Appropriate training data Training data was chosen to keep the vehicle driving on the road. I used a the full dataset provided as sample along with image augmentation, generating data on my own with simulator did not work very well so I skipped that part initially and then went on working with already provided data For details about how I created the training data, see the next section. Model Architecture and Training Strategy 1. Solution Design Approach The overall strategy for deriving a model architecture was to look into already existing working models My first step was to use a convolution neural network model similar to the Nvidia paper (https://devblogs.nvidia.com/deep-learning-self-driving-cars/), I thought this model might be appropriate because it was suggested and shown to be working well on our lake data in simulator In order to gauge how well the model was working, I split my image and steering angle data into a training and validation set. I found that my my inidial models has a little higher train accuracy than validation accuracy and the model was not following the lanes all the time, so I used a bigger network and increased the training data. To combat the overfitting, I modified the model so that it had dropout layers at two points. Then I used used opencv image flip function to reduce left-right bias and thus reduce overfitting. The final step was to run the simulator to see how well the car was driving around track one. There were a few spots where the vehicle fell off the track with the initial models but with the Nvidia type model this issue was rectified. At the end of the process, the vehicle is able to drive autonomously around the track without leaving the road. 2. Final Model Architecture  Layer (type)                 Output Shape              Param # lambda_1 (Lambda)            (None, 160, 320, 3)       0  cropping2d_1 (Cropping2D)    (None, 70, 320, 3)        0  conv2d_1 (Conv2D)            (None, 33, 158, 24)       1824  activation_1 (Activation)    (None, 33, 158, 24)       0  conv2d_2 (Conv2D)            (None, 15, 77, 36)        21636  activation_2 (Activation)    (None, 15, 77, 36)        0  conv2d_3 (Conv2D)            (None, 6, 37, 48)         43248  activation_3 (Activation)    (None, 6, 37, 48)         0  conv2d_4 (Conv2D)            (None, 4, 35, 64)         27712  activation_4 (Activation)    (None, 4, 35, 64)         0  conv2d_5 (Conv2D)            (None, 2, 33, 64)         36928  activation_5 (Activation)    (None, 2, 33, 64)         0  dropout_1 (Dropout)          (None, 2, 33, 64)         0  flatten_1 (Flatten)          (None, 4224)              0  dense_1 (Dense)              (None, 128)               540800  activation_6 (Activation)    (None, 128)               0  dropout_2 (Dropout)          (None, 128)               0  dense_2 (Dense)              (None, 64)                8256  activation_7 (Activation)    (None, 64)                0  dense_3 (Dense)              (None, 10)                650  activation_8 (Activation)    (None, 10)                0  dense_4 (Dense)              (None, 1)                 11 Total params: 681,065 Trainable params: 681,065 Non-trainable params: 0 Here is a visualization of the architecture (note: visualizing the architecture is optional according to the project rubric)  3. Creation of the Training Set & Training Process To capture good driving behavior, I used the full dataset provided as sample and used data augmentation techniques such as image flipping.  To augment the data sat, I also flipped images and angles thinking that this would it would remove the left-right bias present in image and give me more data After the collection process, I had around 40000 number of data points. I then preprocessed this data by by cropping and mean normalisation.   I finally randomly shuffled the data set and put 15% of the data into a validation set. I used this training data for training the model. The validation set helped determine if the model was over or under fitting. The ideal number of epochs was 5 as evidenced by converge of accuracy and loss vlaues. I used an adam optimizer so that manually training the learning rate wasn't necessary. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning/blob/d8d5f13d84756f868a36a72fdb7889eb0fc669d6/model.h5", "https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning/blob/d8d5f13d84756f868a36a72fdb7889eb0fc669d6/saved-model-low-05-0.18.h5", "https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning/blob/d8d5f13d84756f868a36a72fdb7889eb0fc669d6/saved-model-low-01-0.18.h5", "https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning/blob/d8d5f13d84756f868a36a72fdb7889eb0fc669d6/saved-model-low-02-0.18.h5", "https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning/blob/d8d5f13d84756f868a36a72fdb7889eb0fc669d6/saved-model-low-03-0.18.h5", "https://github.com/BlazingFire/UdacitySDN-Behavioural_Cloning/blob/d8d5f13d84756f868a36a72fdb7889eb0fc669d6/saved-model-low-04-0.18.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e60"}, "repo_url": "https://github.com/th4t-gi/CJ2", "repo_name": "CJ2", "repo_full_name": "th4t-gi/CJ2", "repo_owner": "th4t-gi", "repo_desc": "The rebirth of the FPS Computer Judge! (Cause I need to write neater code...)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T23:35:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T23:27:12Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189117048, "is_fork": false, "readme_text": "FPS Computer Judge A Machine Learning algorithm that grades Future Problem Solvers competition packets. The Basis of this Project: FPSPI FPSPI (Future Problem Solvers Program International) is a non-profit program with 3 competitions:  Global Issue Problem Solving Scenario Writing Community Problem Solving  This program just focuses on grading GIPS. GIPS In Global Issue Problem Solving (GIPS), you and your team of up to 4 people recieve a Future scene or Fuzzy. You go through a six step process to find the best solution to the Fuzzy:  Problems: Find 16 problems from the fuzzy Underlying Problem (UP): Find the biggest problem out of the 16 problems that you created Solutions: Make 16 solutions to the UP Select Criteria: Pick the 5 best ways to grade your solutions Apply Criteria: Grade the top 8 solutions (that you've chosen) based on your 5 selected Criteria Action Plan (AP): Create a more in-detail passage about the best solution  A judge then grades your packet based on creativity, futuristic thinking, and elaboration. The goal of the FPS Computer Judge is to have a comuter that is able to do the same thing. The program The Framework of Neural Nets To grade a challenge, the Computer Judge (CJ) has to be able to decide which category the challenge is in. If it is relevent to the fuzzy - if it isn't relevent, why? And if the challenge is a logical cause or effect of the fuzzy. Therefore, there are multiple independent networks to grade just a challenge. So far, I have developed (or am developing) the folowing Neural Networks:  Categorizing RNN  And that's it... FAQ Where's all the Training data? Well, FPSPI says that no one can post the Future scene of any packet online until 4 years after the packet was graded/used/completed... so let me know when it's 2023, the training data from 2018 will be open source then. If you want to help, and you need the training data, the best I can do is show you the JSON format that I use. It is located under the template packet directory. How can I give you Training data? At the moment, I'm developing a data entry app with the Ionic Framework for this purpose exactly. If you know Angular by any chance and want to help develop this app, here's the repo for it! Acknowledgements Thank you to FPSPI for inspiration and generating the training data for this project. Not to mention, it's the reason I get up at 6 am on Friday mornings. I'd also like to thank datalogue and their repository keras-attention for building a model that I am attempting to implement in my ML algs. ", "has_readme": true, "readme_language": "English", "repo_tags": ["python-3", "text-classification", "data-analytics"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.fpspi.org/sw.html", "http://www.fpspi.org/gips.html", "http://www.fpspi.org/cmps.html"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e61"}, "repo_url": "https://github.com/AsimJalwana/LUTA", "repo_name": "LUTA", "repo_full_name": "AsimJalwana/LUTA", "repo_owner": "AsimJalwana", "repo_desc": "Label Universal Targetted Attacks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T02:33:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T05:41:38Z", "homepage": null, "size": 91, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188777943, "is_fork": false, "readme_text": "LUTA Label Universal Targetted Attacks Software Requirements  Python 3.5.2 TensorFlow 1.9.0 Keras 2.2.4 Foolbox 1.8.0 (https://github.com/bethgelab/foolbox)  All the development/testing was done on Ubuntu 16.04. Once the software requirements are met. We need to download the data and arrange it in the desired hiearchy. Data Downloading Download the ImageNet ILSVRC2012 dataset from http://image-net.org/download (training and validation dataset). One needs to register before downloading.In order to setup the data for our code, we will create directories named by the wnids and inside each directory there will be two folders training and testing. The training images will be inside training folder and the validation images will become part of the testing folder. The testing images will come from the imagenet validation dataset, which provides 50 samples for each class. The training images will come from the imageNet training data. One also needs to create a directory for the images to be used for inhibition of leakage as discussed in the paper. This dataset can be created by picking 50 samples from each class (wnid) which get correctly predicted by VGG16 by confidence greater than 60%. Once the data is in this shape, you can open the code folder and and find the file config.ini, this file has to be modified to include the relevant paths. Below, there is a description of each field present in the config.ini.    S/N Field Field Description     1 saveDir Path of the directory where the results will be saved. The results are saved as \u201cAdversarialAttackResults.db\u201d   2 datasetDir Path of the directory where the dataset will be present, folders named by wnids and inside each folder we should have testing and training folder.   3 inhibitionDatsetDir Path of the directory where samples used for inhibition will be present. Should include samples from all the classes.   4 imageNetValidationDir Path of the directory where imagenet validation images can be found. There are 50000 images.   5 imageNet2012ValidationGroundTruthFile Path of the file \u201cILSVRC2012_validation_ground_truth.txt\u201d. This comes with ImageNet2012 validation dataset.   6 imageNet2012LabelMapFile Path of the file \u201cimagenet_2012_challenge_label_map_proto.pbtxt\u201d. This comes with the imageNet2012 validation dataset.   7 sourceIdentities It is a comma separated Wnids that will be taken as source classes. Note the data will be picked based on these wnids and the path of the dataset set in datasetDir.   8 targetIdentities It is comma separated Wnids that will be taken as target classes.   9 attackModels Comma separated attack Model Ids. It represents the deep model for launching the target attack. You can find the table below to select it.   10 etas Comma separated values of eta for each algorithm id.   11 algorithmId Comma separated Algorithm IDs. These algorithms will be launched one by one on each deep Models that you have select on each pair of source and target Identities. Please see the table below to find the algorithm ids.    The algorithmIds can be selected from table below.    S/N Algorithm Description Algorithm ID     1 LinfinityBounded 3   2 L2Bounded 4    and the AttackModels can be selected from below table    S/N AttackModel Description Attack Model ID     1 VGG16 1   2 ResNet50 2   3 InceptionV3 3   4 MobileNetV2 4    Once you have setup the config.ini file, you run the code by running the script as python MainScriptToRunAttacksWithInhibition.py    to run attacks with inhibition as described in the paper. One can also run python MainScriptToRunAttacksWithoutInhibition.py   to run the attacks without inhibition. The results are saved in the database. One can check the tables attacktrainingperformance and attacktestingperformance to find the training and testing accuracy. The perturbations are saved in attack table, in the column perturbedimage. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://image-net.org/download"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e62"}, "repo_url": "https://github.com/fredxfred/compressml", "repo_name": "compressml", "repo_full_name": "fredxfred/compressml", "repo_owner": "fredxfred", "repo_desc": "Image compression, upscaling using Machine learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T01:52:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T19:50:12Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188901146, "is_fork": false, "readme_text": "compressml Image compression and resolution enhancement using Machine learning. Create a compression model specific to your data with configurable models. Models This library makes use of several models for different use cases. Detailed descriptions and explanations can be found in resources/docs/ at a later date. MLLAE MLLAE is a (Mostly) LossLess AutoEncoder model. It is trained by a neural network that attempts to map an image X -> X. The compression is accomplished by the network architecture: there is a bottleneck layer Lk where bytes(Lk) < bytes(X). Compressing an image applies the first portion of the network architecture (the encoder), with the output of Lk being the compressed representation. Decompression requires applying the remaining part of the network, the decoder. MLLAE-N MLLAE-N lets you train a new MLLAE model that compresses images to a fixed size of at most N bytes. Training a new MLLAE-N model will attempt to optimize compression on the training dataset for that fixed size; note that there are theoretical limits on lossless compression in the general case, and that you will probably overfit to your training data. This is essentially a more restricted case of LAE-g. LAE LAE is a Lossy AutoEncoder model. Its encryption works the same way as with MLLAE. It is trained differently: rather than attempting to minimize data loss for a fixed bottleneck layer size, it maximizes reward as a function g(L, N) where L is data loss and N is the size. This way, the model makes tradeoffs where loss is acceptable if it can be justified by a reduced compression size. The default LAE model will use a well-tuned function g. LAE-g LAE-g lets you train your own LAE model using your own definition of the model loss function g. You can enforce maximum loss, minimum compression size, etc. by tuning g. RED RED is a Resolution Enhancement Decoder. It takes low resolution images and attempts to resolve them to images of higher resolution. It is trained by lowering the resolution of images, and then creating a neural network to map images back to their original resolutions. APIs A long term goal of this project is to provide simple apis in a variety of programming languages to use pretrained or custom trained models. Setup The long term goal is to enable python environment setup by running a single script, and to have premade models able to be downloaded and used using well-known package managers. Currently, this is not implemented. Guide Please download and install Anaconda for Python 3.X (https://www.anaconda.com/distribution/). Then, run conda install keras and conda install tensorflow. Jupyter is included in Anaconda by default, and with properly installed anaconda, can be started with jupyter notebook from the terminal; this is used to open .ipynb files. There will probably be other third party modules that in various files; unless otherwise noted, these will be available through conda or pip. Please feel free to contribute set up scripts that make using, training, or reconfiguring these models simpler. Operating Systems Initially, the only supported operating systems will be commonly used linux distributions (Ubuntu) and macOS. If you have a fix to restore functionality for a specific operating system (preferably by using more generic code), please submit a PR. If something is not working on macOS or a linux distribution such as Ubuntu, please raise an issue. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e63"}, "repo_url": "https://github.com/mohaEs/Train-Predict-Landmarks-by-flat", "repo_name": "Train-Predict-Landmarks-by-flat", "repo_full_name": "mohaEs/Train-Predict-Landmarks-by-flat", "repo_owner": "mohaEs", "repo_desc": "Train-Predict-Landmarks-by-flat", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T13:58:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T13:03:19Z", "homepage": null, "size": 144, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188844628, "is_fork": false, "readme_text": "Train-Predict-Landmarks-by-flat This network architecture is presented in which detects the locations of landmarks for vocal tract area. The network is based on heatmap generation and location of the argmax. set up code is based on the tensorflow 1.14 which the embedded keras is also used. data it is assumed that, the png files are stored in a folder and corresponding cvs files with same filenames are located in another folder and contains the x,y locations of the landmarks.  For example one image and corresponding landmark file is placed in data folder. The landmark file contanis 20 landmarks. training The mentioned network were able to handle just 5 heatmaps for 256x256 images and batch size of 10. Therefore for having a full machine predicting 20 landmarks we need to train 4 different netwroks. The desired landmark for each network can be set by arguments. the input arguments are as follow:  \"--mode\", choices=[\"train\", \"test\"]) \"--input_dir\", the path of the directory contains png files  \"--target_dir\",  the path of directory contains csv files of corressponding ladnmarks  \"--checkpoint\",  the path of directorry for the trained model  \"--output_dir\",  the path of output, in training mode the trained model would be saved here and in testing mode the results. \"--landmarks\",  the string contains the number of desired landmarks for example for a machine:   target_dir='../Images_Data/CV_10_Folding/Fold_1/temp_train_lm/'  input_dir='../Images_Data/CV_10_Folding/Fold_1/temp_train_png/'  checkpoint='../Images_Data/CV_10_Folding/Fold_1/Models/Models_flat/Models_lm_2/'  output_dir='../Images_Data/CV_10_Folding/Fold_1/Models/Models_flat/Models_lm_2/'  landmarks='2,12,11,10,5'  python3 Pr_LandMarkDetection_FlatArc+HeatMap.py --mode 'train'   --input_dir   $input_dir     --target_dir  $target_dir    --checkpoint  $checkpoint     --output_dir  $output_dir     --landmarks  $landmarks   of the checkpoint is empty, the new training session would be done otherwise the training would be continued from previous session. Do not forget to set the suitable number of epochs in the python scripts. predicting  target_dir='../Images_Data/CV_10_Folding/Fold_1/temp_test_lm/'  input_dir='../Images_Data/CV_10_Folding/Fold_1/temp_test_png/'  checkpoint='../Images_Data/CV_10_Folding/Fold_1/Models/Models_flat/Models_lm_2/'  output_dir='../Images_Data/CV_10_Folding/Fold_1/Results/Models_flat/Models_lm_2/'  landmarks='2,12,11,10,5'  python3 Pr_LandMarkDetection_FlatArc+HeatMap.py --mode 'test'   --input_dir   $input_dir     --target_dir  $target_dir    --checkpoint  $checkpoint     --output_dir  $output_dir     --landmarks  $landmarks  The predicted landmarks plus the truth locations would be saved in csv files. Also a visualization image of the prediction will be saved:  Notice that, in the save csv of output folder, the x and y columns may changed from the original ones in targer_dir: x->y y->x Network Architecture  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e64"}, "repo_url": "https://github.com/mohaEs/Train-Predict-Landmarks-by-pix2pix", "repo_name": "Train-Predict-Landmarks-by-pix2pix", "repo_full_name": "mohaEs/Train-Predict-Landmarks-by-pix2pix", "repo_owner": "mohaEs", "repo_desc": "Train-Predict-Landmarks-by-pix2pix", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T14:12:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T13:58:55Z", "homepage": null, "size": 78, "language": "Python", "has_wiki": false, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188853569, "is_fork": false, "readme_text": "Train-Predict-Landmarks-by-pix2pix This network architecture is presented in which detects the locations of landmarks for vocal tract area. The method is based on heatmap generation and location of the argmax. The network architecuture is same as the autoencoder with skip connections in the generator of the pix2pix network. set up code is based on the tensorflow 1.14 which the embedded keras is also used. data it is assumed that, the png files are stored in a folder and corresponding cvs files with same filenames are located in another folder and contains the x,y locations of the landmarks.  For example one image and corresponding landmark file is placed in data folder. The landmark file contanis 20 landmarks. training The mentioned network were able to handle just 3 heatmaps for 256x256 images and batch size of 10. Therefore for having a full machine predicting 20 landmarks we need to train 7 different netwroks. The desired landmark for each network can be set by arguments. the input arguments are as follow:  \"--mode\", choices=[\"train\", \"test\"]) \"--input_dir\", the path of the directory contains png files  \"--target_dir\",  the path of directory contains csv files of corressponding ladnmarks  \"--checkpoint\",  the path of directorry for the trained model  \"--output_dir\",  the path of output, in training mode the trained model would be saved here and in testing mode the results. \"--landmarks\",  the string contains the number of desired landmarks for example for a machine:   target_dir='../Images_Data/LoSo_1/temp_train_lm/'  input_dir='../Images_Data/LoSo_1/temp_train_png/'  checkpoint='../Images_Data/LoSo_1/Models/Models_pix2pix/Models_lm_6/'  output_dir='../Images_Data/LoSo_1/Models/Models_pix2pix/Models_lm_6/'  landmarks='18,2,14' python3 Pr_LandMarkDetection_pix2pixArc+HeatMap.py --mode 'train'   --input_dir   $input_dir     --target_dir  $target_dir    --checkpoint  $checkpoint     --output_dir  $output_dir     --landmarks  $landmarks  of the checkpoint is empty, the new training session would be done otherwise the training would be continued from previous session. Do not forget to set the suitable number of epochs in the python scripts. predicting  target_dir='../Images_Data/LoSo_1/temp_test_lm/'  input_dir='../Images_Data/LoSo_1/temp_test_png/'  checkpoint='../Images_Data/LoSo_1/Models/Models_pix2pix/Models_lm_6/'  output_dir='../Images_Data/LoSo_1/Results/Models_pix2pix/Models_lm_6/'  landmarks='18,2,14'  python3 Pr_LandMarkDetection_pix2pixArc+HeatMap.py --mode 'test'   --input_dir   $input_dir     --target_dir  $target_dir    --checkpoint  $checkpoint     --output_dir  $output_dir     --landmarks  $landmarks  The predicted landmarks plus the truth locations would be saved in csv files. Also a visualization image of the prediction will be saved:  Notice that, in the save csv of output folder, the x and y columns may changed from the original ones in targer_dir: x->y, y->x ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e65"}, "repo_url": "https://github.com/arthw/mnist_offline", "repo_name": "mnist_offline", "repo_full_name": "arthw/mnist_offline", "repo_owner": "arthw", "repo_desc": "This mnist is baded on keras of Tensorflow with the local dataset. No need to access the networ to download data.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T02:43:05Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T02:24:18Z", "homepage": null, "size": 17588, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188757321, "is_fork": false, "readme_text": "##Purpose## It's used to compare the performance of tensorflow-mkl and legacy tensorflow. The legacy tensorflow also could be optimized for Intel CPU.  tensorflow in PIP is optimized by OpenBLAS. tensorflow in Conda is optimized by Intel-MKL.*  We recommend to use PIP's tensorflow as legacy, or you build your own tensorflow to disable such optimize method. In Windows 10, the tf-mkl is increased about 10-20% than legacy tensorflow by PIP (optimized by OpenBLAS) ##Usage## (This example of tensorflow needn't to download dataset from internet.) run in tf-mkl python fully_connected_feed.py --mkl=1 run in legacy tf python fully_connected_feed.py --mkl=0 ##Steps## Both actions below should be executed in same folder. They will share the result of each other and show them. ###1.run in legacy tf### python fully_connected_feed.py --mkl=0 ###2. run in tf-mkl### Entry conda with Intel Tensorflow activate tf-mkl (tf-mkl) tf> python fully_connected_feed.py --mkl=1 ##Result##  ##CNN## Run in different tf enverionment. Please refer to ###Steps### python lenet5.py Following is the optimized record (it's only used for internal training, not use it for customer demo/training) Processor Name: Intel(R) Core(TM) i5-8350U CPU @ 1.70GHz    tf-mkl tf-legacy MKL_NUM_THREADS inter_op_parallelism_threads intra_op_parallelism_threads     240 210 default default default   240 510 default 8 1   210 510 1 4 1   210 220 1 default default   220 210 2 default default   240 210 4 default default   260 240 8 default default   240 200 8 4 2    Intel(R) Xeon(R) Platinum 8180M CPU @ 2.50GHz    tf-mkl tf-legacy MKL_NUM_THREADS inter_op_parallelism_threads intra_op_parallelism_threads     75 67 default default default    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e66"}, "repo_url": "https://github.com/rosieyu1995/Diabetic-Retinopathy", "repo_name": "Diabetic-Retinopathy", "repo_full_name": "rosieyu1995/Diabetic-Retinopathy", "repo_owner": "rosieyu1995", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T05:44:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T23:22:21Z", "homepage": null, "size": 32, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189116626, "is_fork": false, "readme_text": "Diabetic Retinopathy Detection by deep learning Diabetic retinopathy is a progressive disease that is classified into one of 5 stages by an ophthalmologist, based on severity. The disease and its risk factors, as well as its symptoms, are well understood. It is diagnosed by an ophthalmologist, who examines features discovered via a visual examination and/or fundus photography (photography of the back of the eye), or other forms of optical imaging. Data The data is from Kaggle(https://www.kaggle.com/c/classroom-diabetic-retinopathy-detection-competition/data). The data contains training and validation sets. This data contains images that are taken of different people, some are flipped and some are not, and the brightness of each image is different. Therefore, we do some preprocessing steps and data argumentation to get all images to a useable format for training a model.    Class Training Data Counts Validation Data Counts     Class 0 23229 2581   Class 1 2199 244   Class 2 4763 529   Class 3 786 87   Class 4 639 70    Prerequisites  Python 2.7 or 3.6 with tensorflow as backend Keras Numpy  Data Preprocessing and Data Argumentation By modifying and standardizing the images we had obtained, we used ImageDataGenerator. From the ImageDataGenerator\u2019s rescale argument, we passed (1. / 255) to normalize values from 0 to 1 in a grid of 256. We also set shear_range to equal 0.2, which is the shear intensity \u2013 the amount of image that we will shear off. We then set zoom_range to be 0.2, which is the range for random zoom \u2013 an image may be zoomed in or out on from a range of 0.80 to 1.2, at random. We also set horizontal_flip = True, which will randomly flip some of our images horizontally \u2013 this is useful in this case in particular because fundus photographs of the left and right eyes are mirrored, so we dealt with this by flipping a portion of the images at random. We then defined train_generator to equal train_datagen.flow_from_directory( ), which will generate labels for images from a target directory. Our target_size is (224, 224), the dimensions of the images we would like to generate from our directory of images. We set batch_size to equal the default of 32, and we set class_mode to equal \u201ccategorical\u201d, which determines that the type of label array returned is a 2D array. We also defined validation_generator in a similar way, but changed the directory to the directory for our validation images. Neural Network Architectures We defined our model as Sequential( ), with the first layer being a Conv2D layer of 32 neurons and a (3x3) kernel with padding = 'same', and our input shape correspondingly being [224, 224, 3]. For this and all subsequent Conv2D layers, kernel_regularizer was set to equal regularizers.12((0.0001) \u2013 this was done to apply a penalty on our layer parameters during its optimization, which would be included in our loss function. Our activation function for this layer was relu. The next layer was a MaxPooling2D layer of pool size (2, 2), followed by another Conv2D layer of (32, (3x3)) and another MaxPooling2D layer with the same pool size as before.  Our final Conv2D layer was of (64, (3x3)) and was followed by one more MaxPooling2D layer. We then flattened the output of this MaxPoolingLayer and fed it into a Dense layer of 256 neurons with a relu activation function. We used a Dropout layer with 0.5 dropout, and then one more Dense layer of 5 neurons as our final layer. The activation function for this layer was the softmax function, since we were looking to output 5 continuous probabilities from 0 to 1 for each of 5 classes. Our final model had 12.875 million trainable parameters. Results We used an early stopping function that monitored loss in our validation set with a patience of 2 (the number of epochs with no improvement that would lead to stopping training). After compiled and fitting the model, our testing accuracy on our withheld images using this model was 73.73%. Project Participants  Hsin-Jung(Rosie) Yu Ho Huang Spencer Glass  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e67"}, "repo_url": "https://github.com/suyukun666/ssd_pytorch", "repo_name": "ssd_pytorch", "repo_full_name": "suyukun666/ssd_pytorch", "repo_owner": "suyukun666", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-01T06:26:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T11:25:39Z", "homepage": null, "size": 6347, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189014749, "is_fork": false, "readme_text": "SSD: Single Shot MultiBox Object Detector, in PyTorch A PyTorch implementation of Single Shot MultiBox Detector from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found here.  Table of Contents  Installation Datasets Train Evaluate Performance Demos Future Work Reference  \u00a0 \u00a0 \u00a0 \u00a0 Installation  Install PyTorch by selecting your environment on the website and running the appropriate command. Clone this repository.  Note: We currently only support Python 3+.   Then download the dataset by following the instructions below. We now support Visdom for real-time loss visualization during training!  To use Visdom in the browser:  # First install Python server and client pip install visdom # Start the server (probably in a screen or tmux) python -m visdom.server  Then (during training) navigate to http://localhost:8097/ (see the Train section below for training details).   Note: For training, we currently support VOC and COCO, and aim to add ImageNet support soon.  Datasets To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit torch.utils.data.Dataset, making them fully compatible with the torchvision.datasets API. COCO Microsoft COCO: Common Objects in Context Download COCO 2014 # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/COCO2014.sh VOC Dataset PASCAL VOC: Visual Object Classes Download VOC2007 trainval & test # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2007.sh # <directory> Download VOC2012 trainval # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2012.sh # <directory> Training SSD  First download the fc-reduced VGG-16 PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth By default, we assume you have downloaded the file in the ssd.pytorch/weights dir:  mkdir weights cd weights wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth  To train SSD using the train script simply specify the parameters listed in train.py as a flag or manually change them.  python train.py  Note:  For training, an NVIDIA GPU is strongly recommended for speed. For instructions on Visdom usage/installation, see the Installation section. You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see train.py for options)    Evaluation To evaluate a trained network: python eval.py You can specify the parameters listed in the eval.py file by flagging them or manually changing them.  Performance VOC2007 Test mAP    Original Converted weiliu89 weights From scratch w/o data aug From scratch w/ data aug     77.2 % 77.26 % 58.12% 77.43 %    FPS GTX 1060: ~45.45 FPS Demos Use a pre-trained SSD network for detection Download a pre-trained network  We are trying to provide PyTorch state_dicts (dict of weight tensors) of the latest SSD model definitions trained on different datasets. Currently, we provide the following PyTorch models:  SSD300 trained on VOC0712 (newest PyTorch weights)  https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth   SSD300 trained on VOC0712 (original Caffe weights)  https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth     Our goal is to reproduce this table from the original paper    Try the demo notebook  Make sure you have jupyter notebook installed. Two alternatives for installing jupyter notebook:   If you installed PyTorch with conda (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run): jupyter notebook   If using pip:     # make sure pip is upgraded pip3 install --upgrade pip # install jupyter notebook pip install jupyter # Run this inside ssd.pytorch jupyter notebook  Now navigate to demo/demo.ipynb at http://localhost:8888 (by default) and have at it!  Try the webcam demo  Works on CPU (may have to tweak cv2.waitkey for optimal fps) or on an NVIDIA GPU This demo currently requires opencv2+ w/ python bindings and an onboard webcam  You can change the default webcam in demo/live.py   Install the imutils package to leverage multi-threading on CPU:  pip install imutils   Running python -m demo.live opens the webcam and begins detecting!  TODO We have accumulated the following to-do list, which we hope to complete in the near future  Still to come:   Support for the MS COCO dataset  Support for SSD512 training and testing  Support for training on custom datasets    Authors  Max deGroot Ellis Brown  Note: Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible. References  Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" ECCV2016. Original Implementation (CAFFE) A huge thank you to Alex Koltun and his team at Webyclip for their help in finishing the data augmentation portion. A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):  Chainer, Keras, MXNet, Tensorflow    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pytorch.org/", "http://localhost:8097/", "http://localhost:8888", "http://www.image-net.org/", "http://pytorch.org/docs/torchvision/datasets.html", "http://github.com/ellisbrown", "http://jupyter.readthedocs.io/en/latest/install.html", "http://mscoco.org/", "http://host.robots.ox.ac.uk/pascal/VOC/"], "reference_list": ["http://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1409.1556", "http://arxiv.org/abs/1512.02325"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e68"}, "repo_url": "https://github.com/daQuincy/Variational-Autoencoder-for-Texts", "repo_name": "Variational-Autoencoder-for-Texts", "repo_full_name": "daQuincy/Variational-Autoencoder-for-Texts", "repo_owner": "daQuincy", "repo_desc": "Loose implementation of Gerating Sentences from a Continuous Space using Tensorflow", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T12:56:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T07:49:35Z", "homepage": null, "size": 92, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188654775, "is_fork": false, "readme_text": "Variational Autoencoder for Texts Very loose implementation of Generating Sentences from a Continuous Space. Dataset and Preprocessing The dataset used in this repo is IMDB Review Dataset. The dataset is preprocessed as follows:  Each string is split by sentence. Characters are all lower-cased. No words are removed, common punctuations (comma, period, question mark) are preserved and treated as a word. Dataset is split into training and testing set. Only the top 20000 most frequent words in the training set are stored in vocabulary, other words are replaced with an unknown token. The training set, testing set and Keras word Tokenizer are saved as pickle files.  Example of preprocessed sentence: Original: Do you think Batman lives under Bruce Wayne's manor? Preprocessed: do you think batman lives under bruce wayne s manor ? Check dataset.py for more details. Training The model is described in seq_vae.py. The Kullback-Leibler divergence term is multiplied by a weight beta as described in the paper. Beta is annealed from 0 to 1 following a sigmoid schedule as desribed in the paper. The model is trained for 20 epochs.  Check seq_vae.py and train.py for more details and tuning hyperparameters. Results usage: test.py [-h] [-m MODE] [-s SENTENCE] [-s1 SENTENCE_1] [-s2 SENTENCE_2]                [-b BEAM_SIZE] [-r RESTORE_PATH]  optional arguments:   -h, --help            show this help message and exit   -m MODE, --mode MODE   -s SENTENCE, --sentence SENTENCE   -s1 SENTENCE_1, --sentence_1 SENTENCE_1   -s2 SENTENCE_2, --sentence_2 SENTENCE_2   -b BEAM_SIZE, --beam_size BEAM_SIZE   -r RESTORE_PATH, --restore_path RESTORE_PATH    Reconstruction python test.py --mode reconstruct --sentence \"the actors are too young for their characters\" --beam_size 8 --restore_path vae/vae-20    1. the actors are too young for them . 2. the actors are too young for their . 3. the songs are too young for them . 4. the actors are too young for her . 5. the actors are too young for some . 6. the songs are not too young . 7. the actors are not too young . 8. the characters are too young for them .    Generating random sentences python test.py --mode generate --beam_size 8 --restore_path vae/vae-20    1. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , not a good , dark comedy . 2. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , especially for a good , dark comedy . 3. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , it was a good , classy thriller . 4. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , it was a good , classy movie . 5. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , especially for a good , classy thriller . 6. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , especially for a good , classy comedy . 7. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , it was a good , classy fashion . 8. that would be the best thing about the previous films today , but this film is packed , and it was a great movie , it was a good , classy , inspirational movie .    Interpolating between sentences First and last sentences are the reconstruction of the originals python test.py --mode interpolate --sentence_1 \"the movie was not as good as the prequel\" --sentence_2 \"the actors of the film are too young for their characters\" --restore_path vae/vae-20     1. the movie was not as good as a . 2. the movie was not as good as a . 3. the movie was not as good as <unk> . 4. the acting is great as good as <unk> . 5. the actors in the movie are quite good . 6. the actors of the film are too young for their . 7. the actors of the film are too young for their .    Check out test.py to vary the number of interpolations, 5 interpolations is a good number to see significant changes between each interpolation. Requirements  Tensorflow/Tensorflow-gpu 1.13.1 NumPy 1.16.3  ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "variational-autoencoder", "vae", "tensorflow", "sequences", "nlp"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1511.06349"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e69"}, "repo_url": "https://github.com/Kunika21/Ads_Classifier", "repo_name": "Ads_Classifier", "repo_full_name": "Kunika21/Ads_Classifier", "repo_owner": "Kunika21", "repo_desc": "Adverts Sieving based on svm, decision trees, random forest etc.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T12:26:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T01:35:58Z", "homepage": "", "size": 332, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188628540, "is_fork": false, "readme_text": "Ads_Classifier The code in this repository can be modified to be compatible with other data sets too. Dataset Information We use and compare various different methods for sentiment analysis on comments (a binary classification problem). The training dataset is expected to be a csv file of type comment_id,class,content where the comment_id is a unique integer identifying the comment, class is either 1 (positive) or 0 (negative), and content is the comment enclosed in \"\". Similarly, the test dataset is a csv file of type comment_id,content. Please note that csv headers are not expected and should be removed from the training and test datasets. Requirements There are some general library requirements for the project and some which are specific to individual methods. The general requirements are as follows.  numpy scikit-learn scipy nltk  The library requirements specific to some methods are:  keras with TensorFlow backend for Logistic Regression, MLP, RNN (LSTM), and CNN. xgboost for XGBoost.  Usage Preprocessing  Run preprocess.py <raw-csv-path> on both train and test data. This will generate a preprocessed version of the dataset. Run stats.py <preprocessed-csv-path> where <preprocessed-csv-path> is the path of csv generated from preprocess.py. This gives general statistical information about the dataset and will two pickle files which are the frequency distribution of unigrams and bigrams in the training dataset.  After the above steps, you should have four files in total: <preprocessed-train-csv>, <preprocessed-test-csv>, <freqdist>, and <freqdist-bi> which are preprocessed train dataset, preprocessed test dataset, frequency distribution of unigrams and frequency distribution of bigrams respectively. For all the methods that follow, change the values of TRAIN_PROCESSED_FILE, TEST_PROCESSED_FILE, FREQ_DIST_FILE, and BI_FREQ_DIST_FILE to your own paths in the respective files. Wherever applicable, values of USE_BIGRAMS and FEAT_TYPE can be changed to obtain results using different types of features as described in report. Baseline  Run baseline.py. With TRAIN = True it will show the accuracy results on training dataset.  Naive Bayes  Run naivebayes.py. With TRAIN = True it will show the accuracy results on 10% validation dataset.  Maximum Entropy  Run logistic.py to run logistic regression model OR run maxent-nltk.py <> to run MaxEnt model of NLTK. With TRAIN = True it will show the accuracy results on 10% validation dataset.  Decision Tree  Run decisiontree.py. With TRAIN = True it will show the accuracy results on 10% validation dataset.  Random Forest  Run randomforest.py. With TRAIN = True it will show the accuracy results on 10% validation dataset.  XGBoost  Run xgboost.py. With TRAIN = True it will show the accuracy results on 10% validation dataset.  SVM  Run svm.py. With TRAIN = True it will show the accuracy results on 10% validation dataset.  Multi-Layer Perceptron  Run neuralnet.py. Will validate using 10% data and save the best model to best_mlp_model.h5.  Reccurent Neural Networks  Run lstm.py. Will validate using 10% data and save models for each epock in ./models/. (Please make sure this directory exists before running lstm.py).  Convolutional Neural Networks  Run cnn.py. This will run the 4-Conv-NN (4 conv layers neural network) model as described in the report. To run other versions of CNN, just comment or remove the lines where Conv layers are added. Will validate using 10% data and save models for each epoch in ./models/. (Please make sure this directory exists before running cnn.py).  Majority Vote Ensemble  To extract penultimate layer features for the training dataset, run extract-cnn-feats.py <saved-model>. This will generate 3 files, train-feats.npy, train-labels.txt and test-feats.npy. Run cnn-feats-svm.py which uses files from the previous step to perform SVM classification on features extracted from CNN model.  Information about other files  dataset/positive-words.txt: List of positive words. dataset/negative-words.txt: List of negative words. dataset/glove-seeds.txt: GloVe words vectors from StanfordNLP which match our dataset for seeding word embeddings.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e6a"}, "repo_url": "https://github.com/cheapkai/EmaiAuthorId", "repo_name": "EmaiAuthorId", "repo_full_name": "cheapkai/EmaiAuthorId", "repo_owner": "cheapkai", "repo_desc": "SMAI - author identification from emails", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T17:33:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T17:29:09Z", "homepage": null, "size": 843, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 189073513, "is_fork": false, "readme_text": "E-Mail Author Identification SMAI@IIIT-H (Monsoon 2017) Team 15  Kritika Prakash (20161039) Aditya Srivastava (201425112) Karthik Chintapalli (201501207) Vighnesh Chenthil Kumar (201501201)  Course Instructor  Dr. Vineet Gandhi  Project Mentor  Ishit Mehta  Table of Contents  Overview Method  Enron Email Dataset Data Preparation  Cleaning   Models  CNN implementation Bi-LSTM implementation Hierarchical Bi-LSTM implementation Augmented Hierarchical Bi-LSTM implementation  Stylometry       Dependencies Project Structure References  Overview Classify emails from the Enron email dataset based on their predicted authorship, and used the trained classifier to identify authors of test samples. Method Enron Email Dataset Available here, the dataset contains 0.5 million emails from about 150 users, who were employees of Enron. The classifers use the authors as classess and the emails as samples to be assigned to those classes by authorship. Data Preparation The number of author classes were fixed while maximising the number of emails per author, and while keeping the emails-per-author ratio similar for every author class. This number was found to be 10 authors with 800-1000 emails each. Cleaning The Enron corpus contains all emails in raw form, including not only the message but also all the email metadata. The data is cleaned to keep only the subject and body of the mails. All attached forward chains are removed, including forwarded threads, and salutations. The data is also tokenised by word, sentence and paragraph, and is case normalised. Models The following different models have been implemented and tested: CNN implementation The CNN can identify commonly used groups of words and phrases by an author. Also, the CNN captures localized chunks of information which is useful for finding phrasal units within long texts. There are three layers to the CNN  First, the embedding layer generates a sequence of word-embeddings from a sequence of words Second, the conv layer performs the convolution operation using 128 5x5 filter Third, the dense layer is used for classification  Bi-LSTM implementation The Bi-LSTM is a commonly used technique for text classification. LSTMs are a special kind of RNN which are more capable of remembering long term dependencies in a sequence. This gives more context to the classifier which helps in author identification while processing a sequence of text. There are three layers to the model  First, the embedding layer generates a sequence of word-embeddings from a sequence of words Second, the bidirectional LSTM generates email embeddings from the sequence of word embeddings Third, the dense layer is performs the classification  Hierarchical Bi-LSTM implementation LSTMs are known to work best for a sequence of length of 10-15 elements. However, in this implementation the model can take the entire document, increasing the length and hence the overall context for classification. There are four layers to this model  First, the embedding layer generates a sequence of word-embeddings from a sequence of words Second, the first bidirectional LSTM generates sentence embeddings from the sequence of word embeddings Third second bidirectional LSTM generates email embeddings from sentence embeddings Fourth, the dense layer is performs the classification  Augmented Hierarchical Bi-LSTM implementation This model appends stylometric features to the final document embedding in the hierarchical Bi-LSTM, right before it is passed on to the dense layer. The classification is now performed these augmented documenting-embeddings. Stylometry The stylometric features extracted from the data and experimented with are  Lexical  Average sentence length Average word length total number of words Ratio of unique words to total number of words Total number of characters   Syntactic  Total number of function words Total number of personal pronouns Total number of adjectives    Dependencies  python2 numpy cPickle keras tensorflow nltk MySQL and mysqldb  Project Structure root/      | data_preprocessing_scripts/         - dataProcessing.py      | extracted_features/         - adjperemail.txt         - avgsentlenperemail.txt         - avgwordlenperemail.txt         - charsperemail.txt         - funcwordsperemail.txt         - perpronperemail.txt         - stylometricVector.txt         - uniqbytotperemail.txt         - wordsperemail.txt      | feature_extraction_scripts/         - adjperemail.py         - avgsentlenperemail.py         - avgwordlenperemail.py         - charsperemail.py         - funcwordsperemail.py         - perpronperemail.py         - stylometricVector.py         - uniqbytotperemail.py         - wordsperemail.py      | models/         - CNN.py         - HierLSTM_withStylometry.py         - HierLSTM.py         - LSTM_final.py      - README.md  References   CEAI: CCM-based email authorship identification model - Serwat Nizamani, Nasrullah   Writeprints: A stylometric approach to identity-level identification and similarity detection in cyberspace - Ahmed Abbasi, Hsinchun Chen   Detection of Fraudulent Emails by Authorship Extraction - A. Pandian, Mohamed Abdul Karim   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://research.ijcaonline.org/volume41/number7/pxc3877619.pdf"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e6b"}, "repo_url": "https://github.com/DoubleT136/uttt-deepq", "repo_name": "uttt-deepq", "repo_full_name": "DoubleT136/uttt-deepq", "repo_owner": "DoubleT136", "repo_desc": "Deep-Q based agent that plays Ultimate Tick-Tack-Toe.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T23:22:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T22:57:45Z", "homepage": null, "size": 5920, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 189114460, "is_fork": false, "readme_text": "The Ultimate Tic Tac Toe Player Bot - with Reinforcement Learning Reinforcement Learning based Ultimate Tic Tac Toe player  Background For more details on the game of Ultimate Tic Tac Toe and why I started this project, refer to my blog article This project is meant for others to test their learning algorithms on an existing infrastructure for the Ultimate Tic Tac Toe game. This project has two implemented reinforcement learning algorithms, a reinforcement learning bot (which can use any provided learning algorithm of your choice), and a random bot (that pick moves at random) and they are good for testing against one another for benchmarking performance. Credit to this blog post for helping me understand the rules of the game with a lot of whiteboard drawings. Board To instantiate and play a game of ultimate tic tac toe: from ultimateboard import UTTTBoard from board import GridStates b = UTTTBoard() b.makeMove(GridStates.PLAYER_X, (1,1), (1,1)) b.makeMove(GridStates.PLAYER_O, b.getNextBoardLocation(), (1, 2)) b.makeMove(GridStates.PLAYER_X, b.getNextBoardLocation(), (1, 1)) The co-ordinate system is shown below, and is the same for the master board, as well as any tile within it:  E.g. co-ordinates of (1,1), (1,1) as in the first move above represents the center square of the center tile. To view the state of the board at any given time (you'll get a console output): b.printBoard() Players There are two implemented bots for playing the game  RandomUTTTPlayer who makes moves at random RLUTTTPlayer who makes moves based on a user-supplied learning algorithm  To play the game with one or a combination of these bots, use the SingleGame class. E.g. with two random players from game import SingleGame from ultimateplayer import RandomUTTTPlayer from ultimateboard import UTTTBoard, UTTTBoardDecision  player1, player2 = RandomUTTTPlayer(), RandomUTTTPlayer() game = SingleGame(player1, player2, UTTTBoard, UTTTBoardDecision) result = game.playAGame() When using the RL player, it will need to be initialized with a learning algorithm of your choice. I've already provided two sample learning algorithms: TableLearning and NNUltimateLearning from game import SingleGame from learning import TableLearning from ultimateplayer import RandomUTTTPlayer, RLUTTTPlayer from ultimateboard import UTTTBoard, UTTTBoardDecision  player1, player2 = RLUTTTPlayer(TableLearning(UTTTBoardDecision)), RandomUTTTPlayer()  game = SingleGame(player1, player2, UTTTBoard, UTTTBoardDecision) result = game.playAGame() Learning Algorithm The reinforcement learning (RL) player uses a learning algorithm to improve its chances of winning as it plays a number of games and learns about different positions. The learning algorithm is the key piece to the puzzle for making the RL bot improve its chances of winning over time. There is a generic template provided for the learning algorithm: class GenericLearning(object):     def getBoardStateValue(self, player, board, boardState):         # Return the perceived `value` of a given board state         raise NotImplementedError      def learnFromMove(self, player, board, prevBoardState):         # Learn from the previous board state and the current state of the board         raise NotImplementedError              def resetForNewGame(self):         # Optional to implement. Reinitialize some form of state for each new game played         pass              def gameOver(self):         # Option to implement. When a game is completed, run some sort of learning e.g. train a neural network         pass Any learning model must inherit from this class and implement the above methods. For examples see TableLearning for a lookup table based solution, and NNUltimateLearning for a neural network based solution. Every board state is an 81-character string which represents a raster scan of the entire 9x9 board (row-wise). You can map this to numeric entries as necessary. Using your own learning algorithm Simply implement your learning model e.g. MyLearningModel by inheriting from GenericLearning. Then instantiate the provided reinforcement learning bot with an instance of this model: from ultimateboard import UTTTBoardDecision from learning import GenericLearning import random from ultimateplayer import RLUTTTPlayer  class MyLearningModel(GenericLearning):    def getBoardStateValue(self, player, board, boardState):        # Your implementation here        value = random.uniform() # As an example (and a very poor one)        return value   # Must be a numeric value        def learnFromMove(self, player, board, prevBoardState):        # Your implementation here - learn some value for the previousBoardState        pass  learningModel = MyLearningModel(UTTTBoardDecision) learningPlayer = RLUTTTPlayer(learningModel) Sequence of games More often than not you will want to just play a sequence of games and observe the learning over time. Code samples for that have been provided and uses the GameSequence class from ultimateplayer import RLUTTTPlayer, RandomUTTTPlayer from game import GameSequence from ultimateboard import UTTTBoard, UTTTBoardDecision  learningPlayer = RLUTTTPlayer() randomPlayer = RandomUTTTPlayer() results = [] numberOfSetsOfGames = 40 for i in range(numberOfSetsOfGames):     games = GameSequence(100, learningPlayer, randomPlayer, BoardClass=UTTTBoard, BoardDecisionClass=UTTTBoardDecision)     results.append(games.playGamesAndGetWinPercent()) Prerequisites You will need to have numpy installed to work with this code. If using the neural network based learner in the examples provided, you will also need to have keras installed. This will require one of Tensorflow, Theano or CNTK. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/DoubleT136/uttt-deepq/blob/93d28e9626c2573a7c12a69b4b3c96bf4a5f1398/ultimate_player_nn1.h5"], "see_also_links": ["http://www.numpy.org"], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e6c"}, "repo_url": "https://github.com/opalr/nn_data_plugin", "repo_name": "nn_data_plugin", "repo_full_name": "opalr/nn_data_plugin", "repo_owner": "opalr", "repo_desc": "Help train a neural network for Rocket League", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T03:36:06Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T23:51:32Z", "homepage": "", "size": 12944, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188922600, "is_fork": false, "readme_text": "nn_data_plugin This plugin saves Rocket League 1v1 data to help train a neural network. The uploaded data is available here. This tool records features like controller state which are not available in common replay databases. This is in early development. Please report any issues. Getting Started These instructions will get this plugin up and running on your local machine. Prerequisites You will need to download Bakkes Mod from here, unzip the folder, and run BakkesMod.exe. This will create a bakkesmod folder located at {STEAM INSTALLATION FOLDER}\\steamapps\\common\\rocketleague\\Binaries\\Win32\\bakkesmod\\ Installing After running BakkesMod.exe, download nn_data_plugin.dll from this github repo and place it in the folder ...\\bakkesmod\\plugins\\ Open plugins.cfg located at ...\\bakkesmod\\cfg\\plugins.cfg in a text editor and add the following line at the bottom: nn_data_plugin  Using the Plugin The data recording process is fully automated. Every time you play a 1v1 match in casual or ranked modes, a new data file is saved to ...\\bakkesmod\\plugins\\nn_data\\ on your local machine and uploaded to the shared dropbox folder. The name of the file contains your steam ID and a number which increments with each new upload from your account. Training a Neural Network The python script keras_sample.py in this repo shows an example of how this data can be used to train a neural network to play Rocket League. Data Format The list below describes each column of the data. All position values have an origin at the ball spawn location at the center of the arena. 1. my_team_ID (0 for blue team, 1 for orange team) 2. my_steamID 3. my_mmr (may be useful to take the highest MMR's to train the neural net) 4. my_score (as in goals scored) 5. my_x (x position of your vehicle, positive to the right, if you're in your spawn position) 6. my_y (y position of your vehicle, positive behind you, if you're in your spawn position) 7. my_z (z position of your vehicle, positive up) 8. my_rotx (rotational position of your vehicle) 9. my_roty 10. my_rotz 11. my_vx (velocity of your vehicle) 12. my_vy 13. my_vz 14. my_avx (angular velocity of your vehicle) 15. my_avy 16. my_avz 17. my_supersonic (boolean to tell when you are supersonic) 18. my_throttle (-1 for full reverse, 1 for full forward) 19. my_steer (-1 for full left, 1 for full right) 20. my_pitch (-1 for nose down, 1 for nose up) 21. my_yaw (-1 for full left, 1 for full right) 22. my_roll (-1 for roll left, 1 for roll right) 23. my_jump (true if jump button is pressed) 24. my_activateboost (true if boost is activated) 25. my_handbrake (true if handbrake is activated) 26. my_jumped (true if player has jumped) 27. my_boostamount 28. opponent_steamid (a unique ID is still created for cross-platform players) 29. opponent_mmr 30. opponent_score 31. opponent_x (positive to the opponent's right, if opponent is in their spawn position) 32. opponent_y (positive behind the opponent, if opponent is in their spawn position) 33. opponent_z 34. opponent_rotx 35. opponent_roty 36. opponent_rotz 37. opponent_vx 38. opponent_vy 39. opponent_vz 40. opponent_avx 41. opponent_avy 42. opponent_avz 43. opponent_supersonic 44. opponent_throttle 45. opponent_steer 46. opponent_pitch 47. opponent_yaw 48. opponent_roll 49. opponent_jump 50. opponent_activateboost 51. opponent_handbrake 52. opponent_jumped 53. opponent_boostamount 54. ball_x (same coordinate frame as my_x, my_y, my_z) 55. ball_y 56. ball_z 57. ball_vx 58. ball_vy 59. ball_vz 60. ball_avx 61. ball_avy 62. ball_avz 63. my_ball_touches (an integer which increments every time you touch the ball) 64. opponent_ball_touches 65. game_countdown (game countdown which updates every second, from 300 to 0)  ", "has_readme": true, "readme_language": "English", "repo_tags": ["rocket-league"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e6d"}, "repo_url": "https://github.com/bing-w/image-generation-and-transformation-GAN", "repo_name": "image-generation-and-transformation-GAN", "repo_full_name": "bing-w/image-generation-and-transformation-GAN", "repo_owner": "bing-w", "repo_desc": "This repository is the term project for Big Data and Artificial Intelligence for Business(758B) course", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T01:22:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T00:53:49Z", "homepage": null, "size": 15326, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 188927987, "is_fork": false, "readme_text": "Image-generation-and-transformation-GAN This repository is the term project for Big Data and Artificial Intelligence for Business(758B) course 1.Introduction The recent unprecedented advances in computer vision and machine learning are mainly due to the development of deep neural architectures, existence of abundant labeled data and more powerful computer. On the one hand, deep convolutional neural networks trained on large numbers of labeled images provide researchers powerful image representation that can be used for a wide variety of tasks including recognition, detection, and segmentation. On the other hand, there is still an emerging need to generate images using computer vision technology for game production and obtaining abundant annotated data for image to image transformation remains a cumbersome and expensive process in the majority of application such as autonomous driving, where a semantic segmentation network is required to be trained to perform roads, cars, pedestrians detection. Generative Adversarial Network, called GAN, is a method of unsupervised machine learning algorithms. Two neural networks contest with each other in a zero-sum game framework. This technique can generate photographs that look at least superficially authentic to human observers, having many realistic characteristics. Our objective is to use GAN method to generate digits and then use conditional-GAN method to perform image to image translation. We use the traditional GAN method to perform digits generation using the data from MNIST digit dataset. In an unconditioned generative model, there is no control on models of the data being generated. However, by conditioning the model on additional information it is possible to direct the data generation process. Therefore, we explore the performance of conditional-GAN in image to image translation task using maps dataset. 2.Dataset 2.1. MNIST dataset We use the digit-recognizer dataset obtained from the Kaggle website in previous phase. The data files, train.csv and test.csv, contain gray-scale images of hand-drawn digits, from zero to nine. The dataset contains 42000 digit images in the training set and 28000 digit images in the testing set. Each image is a black-and-white picture and is with 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255, inclusive. The training data set, (train.csv), has 785 columns. The first column, called \"label\", is the digit that was drawn by the user. The rest of the columns contain the pixel values of the associated image. 2.2. Maps dataset In phase two, we use maps dataset from website. It includes three sub-datasets: train, validation and test. The data are in the format of paired aerial and map view of the same region scraped from Google Maps. The dataset consists of 1097 training image pairs, 1098 validation image paris, and 1098 test image pairs. Each image composed of RGB channels and of size 600 pixels * 600 pixels.  For example: Figure 1: A sample of Maps training dataset (source: https://people.eecs.berkeley.edu/~tinghuiz/projects/pix2pix/datasets) Data Preprocessing: The original image size is 3 x 600 x 600. We first resize the input images to 3 x 256 x 256 to reduce computation cost. 3. Methodology 3.1. Highly Effective Platform 3.1.1. Instance for MNIST We use EC2 from Amazon Web Service (AWS) to implement our code running for MNIST dataset, because it is more powerful and time-saving, compared to our local computer system with only 3 CPUs. The instance we created includes following details: Table 1: Details of computational instance used Name GPUs vCPUs Memory(GiB) GPU Memory(GiB) g3.16xlarge 4 64 488 32 3.1.2. Instance for Maps We also use EC2 from AWS to implement our code running for Maps dataset and created one instance with more CPUs to speed up image processing. The instance includes following details: Table 2: Details of computational instance used Name vCPUs Mem(GiB) Storage(GB) Network Performance(Gbps) m5.24xlarge 96 384 EBS-only 25 3.2. Algorithm Explanation We used generative adversarial networks (GAN) as our model structure for generating pictures. The GAN mainly consists of two parts: generator and discriminator. The task of generator is to produce a fake image which is as much indistinguishable from a real image as possible and confuse the discriminator. The generator takes a random noise as input and tries to produce samples in such a way that Discriminator is unable to determine if they come from training data or Generator. The task of discriminator is to distinguish between real image and fake image from the generator, given the reference input image. It learns in a supervised manner by looking at real and generated samples and labels telling where those samples came from. In some sense, Discriminator component replaces the fixed loss function and tries to learn one that is relevant for the distribution from which the training data comes. Its input parameter is X, which represents a picture, and its output is D(X), which represents the probability that X is the real picture. If the output is 1, then the probability that x is the real picture is 100%. If the output is 0, then the probability that x is the real picture is 0%. For image translation task, we used conditional GAN to approach this task. Conditional GAN is an extension of original GAN if both generator and discriminator are conditioned on additional information Y. Y is supplementary information for input data in any types. In conditional GAN, Y is regarded as additional input fed into both the discriminator and generator. In the generator, the noise Z and extra information Y are combined in joint hidden representation, and the network framework allows for huge flexibility in hidden representations. In the discriminator, output of the generator and target output are regarded as input of discriminative function. The objective function of the conditional GAN is: (G*,D*) = arg min max(L(G, D) + \u03bbL(G)) Figure 2: Structure of conditional GAN 4. Network Structure We develop our training framework using Keras in Python 3. 4.1. Structure for generating MNIST digits 4.1.1. Generator The input of generator is a list consisting of an input image sample of size (28,28,1) and a latent variable (noise), and the output of the generator is an image with the same size of (28,28,1). Weights are initialized from the normal distribution with the mean of 0 and a standard deviation of 0.02. The activation functions of the generator are LeakyReLU for hidden layers and tanh for the output layer. Figure 3: Generator network 4.1.2. Discriminator We use convolutional neural network as the architecture of the discriminator. Weights are initialized from the normal distribution with the mean of 0 and a standard deviation of 0.02. The input dimension is 28x28x1 and the output is a list of a vector of 1 (probability of fakeness) and vector of 10 (digit classification). We use dropout with a rate of 0.3 to avoid overfitting. The activation functions of discriminator are LeakyReLU for hidden layers, linear and Softmax for output layer. Figure 4: Discriminator network 4.1.3. Compiling Model Then, we use optimizer \u2018RMSprop\u2019 with a learning rate 0.00005 and loss function \u2018sparse categorical cross-entropy\u2019 to compile the generator model and discriminator model together. 4.2. Structure of Conditional-GAN We use U-Net structure to for the generator in conditional-GAN. The U-Net generator is an encoder-decoder network with symmetrical long skip connections. The network consists of 7 encoding layers and 7 decoding layers. Each encoding and decoding block follows the form of convolution/deconvolution-BatchNorm-LeakyReLU. Figure 5 illustrates the U-Net architecture. Figure 5: U-Net generator structure 5.Result 5.1. For digits image generation 5.1.1. Performance overview We can see the result summary from the table and plot below. We will choose the three representative outputs for process demonstration. Table 3: Mnist model performance for 14000th (last) result Discriminator loss Accuracy Generator loss 0.819696 95.31% 0.735431 The first plot below is the result from the 200th (first) epoch, which is very messy and blurry so that we cannot find any information from generated images. The second plot is the result from the 2600th epoch. We can see the result is pretty good and the digits generated by our model is recognizable. The third plot is the result from the 8000th epoch, in which we get the highest testing accuracy. We can see the all digits very clearly. Figure 6: Main results from MINIST GAN model 5.1.2. Loss Plot Figure 7: Loss plot from MINIST GAN model The plot above is the loss change of Generator network (left) and Discriminator network (right). As we can see from the loss plot, after 8000 training iterations, loss becomes close to zero and the performance of our model is getting more stable. 5.1.3. Accuracy Plot Figure 8: Accuracy plot of MINIST model As we can see from the accuracy plot, the result is corresponding to the result we observed in loss plot. The generated digits image is performing well to fool the discriminator. 5.2. For maps image generation 5.2.1. Performance overview We can see the result summary from the table and plot below. We run 23 epochs with 200 batches each. We will choose the three representative outputs for process demonstration. Table 4: Maps model performance for 23th (last) epoch result Discriminator loss Accuracy Generator loss 0.000405 100% 0.026529 Main results review:  The plot below is the result from the 0 (first) epoch, which is very blurry so that we cannot find any information from generated images.  Figure 9: Output from 1st epoch  The plot below is the result from the 10th epoch. We can see the result is better and the maps generated in the middle is recognizable.  Figure 10: Output from 10th epoch  The plot below is the result from the 23rd epoch, in which we get the highest accuracy. The generated maps are quite desired and we can see the all maps very clearly.  Figure 11: Output from 23th epoch 5.2.2. Loss Plot Figure 12: Loss plot from Maps model As we can see, Discriminator loss decreases as batches are processed. When the batch reaches 15000th, the loss tends to become very low and closer to 0. For Generator loss, during each epoch (1097 batches), the loss decreases dramatically. 5.2.3. Accuracy Plot The plot in right is the accuracy performance. We can see the accuracy can quickly reach to 100%, during each epoch. Figure 13: Accuracy plot from Maps model 6.Business value 6.1. Fashion Industry product image generation or style transformation Our project can be implied in fashion industry. Pursuing the fashion, the high-quality life of having intension is the world trends. Apparel companies focus on designing unique and fashionable products for customers. From skirts to coats, hats and handbags and boots, the variety and style are dazzling. Our model can help generate unlimited apparel picture samples, including different shapes, colors, characters, textures, so designers can use them for inspirations. For example, they can choose specific raw materials according to their explanation of pictures. Especially, customized cloth industry is profitable, lots of customers, such as celebrities want to choose appropriate match in terms of color, cloth, and style for different events and purpose. Figure 14:  Implementation for fashion designing 6.2. Autonomous vehicles object detection Image to image transformation could also be implied in autonomous vehicles. This could be applied to studying the road conditions and provide information for human drivers and self-drive cars. In addition, image to image translation can be used for generating realistic environments in simulation for training self-driving car policies. Figure 15: Implementation for auto-vehicle image detection 7.Challenges 7.1. Limited computational power and high time costs and risk As we processed GAN on computers, we found that the speed of computation was very low. It usually operated several hours to finish one complete network. Especially for Maps model implementation. It would spend 1219 hours to run the whole epochs with 3 CPUs in our local computer system. We thought this is a problem because we might have to spend much long time in running models but the results might not be good. Thus, we first used a small subset of data to test whether the structure was correct and whether the outputs are what we expected. After we confirmed that our model was correct, we then applied the whole dataset via servers in AWS platform. 7.2. Dataset not large enough We thought that our dataset is not very large. In general, the more data we have, the better for our model. As there are only about 128 MB of digit recognizer file, we concern that the data is not convictive that our model is good. Thus, we also train our networks by using a new dataset, 1.5 GB in map dataset. 7.3. Gap between research and real application After we completed GAN network, we faced a problem that there is a gap between theory and practical application. GAN has the high potential commercial value and the worldwide market foreground. We do hope more researchers would make image to image more applicable to different users. 8.Future work 8.1. Try other improved GAN methods There are many other GAN methods for different tasks. Learning new ideas from other\u2019s methods might help us to improve the performance of our model. For example, DiscoGAN is designed specifically for both customer and company by successfully transferring style from one domain to another while preserving key attributes such as orientation to recommend the perfect color match. Using this GAN could significantly improve the quality and efficiency of generating new images with different style. 8.2. Try other Generator network structures and training frequency We would explore residual-based network for discriminator, and experiment with dynamic training frequency to allow generator train more often than discriminator in the beginning and gradually slow down. Reference  [1] M. Arjovsky, and L. Bottou: Towards Principled Methods for Training Generative Adversarial Networks (2017) [2] M. Arjovsky, S. Chintala, and L. Bottou: Wasserstein GAN (2017) [3] I. Gulrajani , F. Ahmed, M. Arjovsky, V. Dumoulin, and Aaron Courville: Improved Training of Wasserstein GANs (2017) [4] Ian Goodfellow: NIPS 2016 Tutorial: Generative Adversarial Networks (2016) [5] Luke de Oliveira, Augustus Odena, Christopher Olah, Jonathon Shlens: Conditional Image Synthesis with Auxiliary Classifier GANs 5. Keras ACGAN implementation (2016) [6] Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A., Bengio, Y.: Generative adversarial nets (2014) [7] Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep convolutional generative adversarial networks (2016) [8] Mehdi Mirza, Simon Osindero: Conditional Generative Adversarial Nets (2014)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e6e"}, "repo_url": "https://github.com/crimsonFig/AudioSurfer_Game_AI", "repo_name": "AudioSurfer_Game_AI", "repo_full_name": "crimsonFig/AudioSurfer_Game_AI", "repo_owner": "crimsonFig", "repo_desc": "A 'bot' composed of two AI models that can see and play the video game called 'Audio Surfer'", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T09:46:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-26T05:59:47Z", "homepage": null, "size": 20562, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 188646050, "is_fork": false, "readme_text": "AudioSurfer_Game_AI A 'bot' composed of two AI models that can see and play the video game called 'Audio Surfer'. Purpose This project was created as part of an independent study course that had the goal of being able to create neural networks that could play a given video game of my choice using computer vision that was granted only by screenshots and keyboard inputs. Requirements Requires the following python3 modules:  pandas numpy cv2 mss keyboard tensorflow  keras object_detection.utils (this will likely require protobuf if you haven't set up the models module)    How It Works The project uses two neural network models (AI) and scripts that extract, transform, and load data between the source (the computer peripherals) and the models. To summarize what happens: one model determines where the objects we need to know about when making our choices are located on the screen, then the second model will make a prediction of what arrow-key to press based on where these objects are on screen. For any given arrangement of objects on the screen, it will make a prediction based on the gameplay it was trained on and mimics that gameplay (i.e. the AI will mimic all the choices i made during the gameplay i trained it on, so it's a copycat from watching me play). The predicted arrow-key is then pushed for you. This can be repeated constantly, allowing the bot to play the game on its own. This bot's two AI models is:  an object detection model (ODM) that takes input from the screen then outputs object locations a long short-term memory model (LSTMM) that takes in a feature vector and outputs a predicted feature vector  The full loop of data goes as follows:  capture an image from the screen feed the image into the ODM and get the resulting object locations filter and transform the objects location data into a feature vector  the feature vector's composition can be found in the section below (Information and Notes for Geeks)   capture the keyboard arrow-key input and add it to the current feature vector feed the feature vector to the LSTMM and get the resulting predicted feature vector extract the predicted arrow-key to be pushed from the predicted feature vector give the predicted arrow-key as a keyboard input to the computer on our behalf repeat  Information and Notes for Geeks The game rules of Audio Surfer is very simple. We have three lanes that extend vertically with the our single ship occupying a single lane at the very bottom. The ship can choose to be in the left, middle, or right lane by pushing the left arrow key, no arrow key, or the right arrow key respectively. block objects can appear at the top and slide down a given lane, which we want collide our ship with for points. spike objects can appear at the top and slide down a given lane as well, but we want to avoid touching them with our ship. multiple block and spike objects can appear on the screen at the same time, but they won't overlap and must be aligned with the center of any given lane. the feature vector is an array composed of 4 features: the location of a block, spike, ship, and the arrow-key pressed.  the current 4 features is represented as 10 values:   the x coordinate of the lowest detected block the y coordinate of the lowest detected block the x coordinate of the lowest detected spike the y coordinate of the lowest detected spike (5.,6.,7.) the location of the ship represented as 'one hot encoding' for left lane, middle lane, and right lane  a one hot array is used to have a binary representation for all possible catagories the purpose of this is to make predictions easier and more accurate by having 'off' and 'on' values for each catagory, which can be further explained here   (8.,9.,10.) the key input represented as 'one hot encoding' for left arrow-key, no key, and right arrow key  These values were chosen as both blocks and spikes can appear on screen and the vertical order they appear can affect what choice we make, along with the x coordinate determining what lane they may be in (however we cannot one-hot encode this easily due to parallax effecting x coordinates when the ship changes lane). The ship can be one-hot encodded as it tends to be at the same x coordinates for each lane and uneffected by any parallax and will always be at the very bottom (so no consideration is needed for the y coordinate). The ODM has currently an overall accuracy of >80%, with typically >95% confidence scores when it recognizes the actual object (i.e. it has very high confidence for true-positive matches, but infrequently has false-negatives for an expected block or spike object, and very rarely a false-negative for the ship object.) False positives with high confidence are very, very rare. The LSTMM has a training accuracy of ~85% and testing accuracy of ~80%. This inaccuracy typically resulted from predicting to enter or leave the expected lane slightly sooner than expected or slightly later than expected, but this perfectly acceptable as it is in the expected/desired lane when it mattered and had negligable impact on gameplay results. details about the respective models can be found documented within the python files of the project. Personal Take on the Project This project has taught me quite a lot about python, computer environments, as well as the various parts in configuring neural networks and training them. Overall, I feel the models I trained were a success with my object detection model having a high accuracy rating without too many anomalies that I had faced in previous model attempts. My LSTM model also feels rather successful as the accuracy of my second version ended up around 83% accuracy, which is rather impeccable as after I compared the prediction, they were notably similar where it mattered with a few variations (mostly being inaccurate due to a slight delay or quickness in changing lanes compared to when I would've, but this is overall good as it followed the major trend and didn't predict weird or unbelievably.) Notably, only the test data accuracy consistently converged on a '66.7% accuracy' for all configurations, which jumped to 83% accuracy after I shifted the expected keys column up by one row before comparing to the predicted keys column. The hardest parts faced during this project mostly was figuring out how to set up my environments in order to get tensorflow and its modules to work, but after enough dedicated effort and research I was able to work through it successfully. The longest part was probably labeling all the images for training the object detection model and determining what my dependencies were for creating a useful feature vector that would give me accurate results. Going Foward Going forward, I hope to make further optimizations that may give better results. I plan on adding a script that will implement the full loop as well as adding a setup script for installing the dependencies that my scripts require. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/crimsonFig/AudioSurfer_Game_AI/blob/97e562bc70705d2bff75b37caba787c6237d34a4/models/audio_surfer_LSTM_V1_20B_50N.h5", "https://github.com/crimsonFig/AudioSurfer_Game_AI/blob/97e562bc70705d2bff75b37caba787c6237d34a4/models/audio_surfer_LSTM_V2_5B_35N.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf424d47eb8d63dac258e6f"}, "repo_url": "https://github.com/maloletnik/exploring-blackbox-attacks", "repo_name": "exploring-blackbox-attacks", "repo_full_name": "maloletnik/exploring-blackbox-attacks", "repo_owner": "maloletnik", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T07:10:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-28T07:07:53Z", "homepage": null, "size": 81727, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188973712, "is_fork": false, "readme_text": "Query-based black-box attacks This repository contains code to reproduce results from the paper: Exploring the Space of Black-box Attacks on Deep Neural Networks  Arjun Nitin Bhagoji, Warren He, Bo Li and Dawn Song  ArXiv report: https://arxiv.org/abs/1712.09491 REQUIREMENTS The code was tested with Python 2.7.12, Tensorflow 1.3.0 and Keras 1.2.2. EXPERIMENTS TRAINING MNIST MODELS STANDARD TRAINING To train Models A through D on the MNIST dataset, create a models directory and run train.py as follows: python train.py models/modelA --type=0 --num_epochs=6  python train.py models/modelB --type=1 --num_epochs=6  python train.py models/modelC --type=2 --num_epochs=6  python train.py models/modelD --type=3 --num_epochs=6   ADVERSARIAL TRAINING To obtain variants of Models A through D which are trained using standard adversarial training, run train_adv.py as follows: python train_adv.py models/modelA_adv --type=0 --num_epochs=12  python train_adv.py models/modelB_adv --type=1 --num_epochs=12 python train_adv.py models/modelC_adv --type=2 --num_epochs=12 python train_adv.py models/modelD_adv --type=3 --num_epochs=12  Note that the number of epochs is increased from 6 to 12. The magnitude of the adversarial perturbation used for the training images can be controlled with the -eps flag. The default value used for MNIST is 0.3. ENSEMBLE ADVERSARIAL TRAINING The train_adv.py script can also be run to obtain variants of Models A through D trained using ensemble adversarial training. For example, to train a variant of Model A that uses adversarial samples generated from Models A, B and C, as well samples generated from the current state of the model (as in standard adversarial training), run: python train_adv.py models/modelA_ens models/modelA models/modelC models/modelD --type=0 --num_epochs=12  ITERATIVE ADVERSARIAL TRAINING Using the --iter flag with train_adv.py allows for the training of variants of Models A through D using iterative adversarial training. For example, a variant of Model A with iterative adversarial training can be trained as follows: python train_adv.py models/modelA_adv --type=0 --iter=1 --num_epochs=64  Note that this form of training needs a much higher number of epochs for the training to converge. Iterative adversarial samples are generated using 40 steps of magnitude 0.01 each by default. This can be changed in the train_adv.py script. The maximum perturbation magnitude is still set to 0.3. To train using only the adversarial loss, set the --ben flag to 0. PRETRAINED CIFAR-10 MODELS CIFAR-10 models are trained using the same techniques. We have uploaded a set of pretrained weights. ATTACKING MNIST MODELS For all attacks except the Random Pertubations attack, setting --targeted_flag=1 enables a targeted attack to be carried out. By default, the target for each sample is chosen uniformly at random from the set of class labels except the true label of that sample. For attacks that generate adversarial examples (not the baseline attacks), two loss functions can be used: the standard cross-entropy loss (use --loss_type=xent) and a logit-based loss (use --loss_type=cw). BASELINE ATTACKS In order to carry out an untargeted Difference of Means attack (on Model A for example), run the baseline_attacks.py script as follows: python baseline_attacks.py models/modelA  This will run the attack for a pre-specified list of perturbation values. For attacks constrained using the infinity-norm, the maximum perturbation value is 0.5 and for attacks constrained using the 2-norm, it is 9.0. To carry out an untargeted Random Perturbations attack, the --alpha parameter is set to 0.6 (infinity-norm constraint) or 9.1 (2-norm constraint). TRANSFERABILITY-BASED ATTACKS To carry out a transferability-based attack on a single model (say Model B) using FGS adversarial examples generated for another single model (say Model A), run the transfer_attack_w_ensemble.py script as follows: python transfer_attack_w_ensemble.py fgs models/modelA --target_model=models/modelB  An ensemble-based transferability attack can also carried out. For example, if the ensemble of local models consists of Models A, C and D, and the model being attacked is Model B, run transfer_attack_w_ensemble.py as follows: python transfer_attack_w_ensemble.py fgs models/modelA models/modelC models/modelC --target_model=models/modelB  The transfer_attack_w_ensemble.py script also supports a number of other attacks including Iterative FGS, FGS with an initial random step and the Carlini-Wagner attack, all of which can be carried out for either a single local model or an ensemble of models. If the --target_model option is not specified, then just the white-box attack success rates will be reported. Note that if the perturbation magnitude is not specified using the --eps option, then a default set of perturbation values will be used. QUERY-BASED ATTACKS These attacks are carried out directly on a target model assuming query access to the output probabilities. An untargeted query-based attack with no query reduction using Single Step Finite Differences can be carried out on Model A as follows: python query_based_attack.py models/modelA --method=query_based_un  A parameter --delta can be used to control the perturbations in input space used to estimate gradients. It is set to a default value of 0.01. To run a targeted version, set the --method option to 'query_based'. The_untargeted Iterative Finite Differences_ attack can be run as follows: python query_based_attack.py models/modelA --method=query_based_un_iter  The number of iterations is set using the --num_iter flag and the step size per iteration is set using the --beta flag. The default values of these are 40 and 0.1 respectively. Targeted attacks can be run by setting the --method option to 'query_based_iter'. To run an attack using the technique of Simultaneous Perturbation Stochastic Approximation (SPSA), the --method option is set to 'spsa_iter' for targeted attacks and to 'spsa_un_iter' for untargeted attacks. USING QUERY-REDUCTION TECHNIQUES To reduce the number of queries, two methods are implemented in the query_based_attack.py script. These can be used along with any of the Finite Difference methods ('query_based', 'query_based_un', 'query_based_iter' and 'query_based_un_iter') To use the Random Grouping method with 8 pixels grouped together, for example, with the untargeted Single Step Gradient Estimation method, run python query_based_attack.py models/modelA --method=query_based_un --group_size=8  Similarly, to use the PCA component based query reduction with 100 components, for example, with the same attack as above, run python query_based_attack.py models/modelA --method=query_based_un --num_comp=100  These query-reduction techniques can be used with targeted, untargeted, Single-Step and Iterative Gradient Estimation methods. ATTACKS ON CLARIFAI To run attacks on models hosted by Clarifai, first follow the instructions given here to install their Python client. You will need to obtain your own API key and set it using clarifai config. The two models currently supported for attack are the 'moderation' and 'nsfw-v1.0' models. To obtain an adversarial example for the 'moderation' model starting with my_image.jpg, run the attack_clarifai.py script as follows: python attack_clarifai.py my_image --target_model=moderation  The default attack used is Gradient Estimation with query reduction using Random Grouping. The available options are the magnitude of perturbation (--eps), number of iterations (--num_iter), group size for random grouping (--group_size) and gradient estimation parameter (--delta). Only the logit loss is used since it is found to perform well. The target image must be an RGB image in the JPEG format. The Resizing_clarifai.ipynb notebook allows for interactive re-sizing of images in case the target image is too large. CONTACT This repository is under active development. Questions and suggestions can be sent to abhagoji@princeton.edu ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1712.09491", "https://arxiv.org/abs/1705.07204", "https://arxiv.org/abs/1706.06083", "https://arxiv.org/abs/1608.04644", "https://arxiv.org/abs/1611.02770", "https://arxiv.org/abs/1608.04644"]}, {"_id": {"$oid": "5cf424d47eb8d63dac258e70"}, "repo_url": "https://github.com/OthmanEmpire/project_captcha", "repo_name": "project_captcha", "repo_full_name": "OthmanEmpire/project_captcha", "repo_owner": "OthmanEmpire", "repo_desc": "Solving Captchas Using Convolution Neural Networks (CNN)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T22:49:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-27T01:15:37Z", "homepage": null, "size": 16100, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 188749448, "is_fork": false, "readme_text": "Project Captcha: Captcha The Hill!      The objective of project captcha is three-fold: solving simple captchas that are generated from a real system (albeit ancient), learning more about neural networks for my own interest, and writing a guide for those who have just started doing data science and/or neural networks. Above is a summary animation of the results of this project: The top window is the raw captcha, the bottom left window is the captcha with Otsu's algorithm applied, and the bottom right window is the solved captcha with letter tracking. In short, the method used here to solve the captchas was to split the captcha images into single letters, and then solve for each individual letter using a convolution neural network (CNN). For those interested in the low level details and a bit of a guide, see below (Part 0: Introduction). Part 0: Introduction Sometimes learning about the thought process involved is more important than the solution itself; this guide is tailored for those who have just begun exploring data science and/or neural networks. My small hope is to encourage those people since at the time of this writing I am one of them. I am not by any means a qualified data scientist nor do I claim to be, however, I am a believer in that going through the struggles of someone who has just started is entertaining (that sounds rather mean) and useful in its own right. Table of Contents:  Part 0: Introduction Part 1: Analysing the Problem Part 2: Studying the Data Part 3: Labelling the Data Part 4: Recycling Old Code Part 5: Letter Detection Algorithm Part 6: Neural Networks Part 7: Conclusion  Part 1: Analysing the Problem (Four Step Plan) Let's sketch out the problem. The problem statement boils down to a single line: \"Given a captcha image, return the letters contained in that captcha.\"  But how? Do we know of any system that can already solve these captchas? Humans! It would be reasonable to ask our humans friends for 'inspiration'. How would a human solve this captcha? If we can break those steps down, it would be a good start. What do these captchas even look like?      So as human, how would we solve the captcha above? (its blurry because the original image is a tiny 20x60 pixels). The steps I would take for instance: 1. I see a red background that looks rather useless so I would ignore it. 2. I see four distinct letters in black and would assume the captcha is only four letters. 3. Going from left to right, the letters look like '0' (or maybe 'O' if the font is potato?), '1', 'Z', 'U'.  4. So the captcha is '01ZU'!  Alright, this is great. We now have a simple algorithm to solve these captchas. This algorithm is good enough for other humans to use (although by my standards this is completely unacceptable; where are the details man?!) but what about for our machine friends? Would they understand what 'ignoring' the background is? Can they distinguish between '0' and 'O'? Do they even know what an '0' even is? Let's break down our algorithm into more details to help our machine friends (and some human friends too). Step 1 1. I see a red background that looks rather useless so I would ignore it.   1.1 Remove any pixel that has a good degree of red or white in it.  Using GIMP (similar to Photoshop but open source), we would remove the red background and have a black and white image like the one below. We can use this new image in the next step.      Step 2 2. I see four distinct letters in black and would assume the captcha is only four letters.   2.1 From our black and white image (step 1.1), I can extract four black regions that represent four letters.  In the previous black and white image, we can see four black regions that so happen to map onto four letters. Let us 'extract' those four black regions (see below). All that is left now is to find what each letter is per block!         Step 3 3. Going from left to right, the letters look like '0' (or maybe 'O' if the font is potato?), '1', 'Z', 'U'.      3.1 Using my brains limited memory capacity, I am doing a simple pattern matching.  For step 3.1, the pattern matching that the brain does is actually rather complex and you will see why once we implement the neural network! Though for step 3 as a whole, we are doing some pattern matching and there various approaches here (e.g. neural networks, template matching, key feature matching) however we will only explore neural networks in this guide even though later we will see the limitations of neural networks for this dataset. Step 4 4. So the captcha is '01ZU'!  Just glue all the letters we guessed from step 3.1 together from left to right and viola! This is great, we should now have a much better idea of how to implement our solution. We have some keywords and topics we can now Google as well to learn more of the lower level implementation details. I would highly recommend to check out the guides mentioned in credits of which this guide is heavily inspired by including direct code snippets. Part 2: Studying the Data Now we can dip our hands into some data. At this point, I just headed to the data directory, opened ImageGlass (alternative to Windows Photos), and shuffled back and forth between various images to get a feel of how the data is 'structured'.        Some of my observations were:  The captchas are always 20x60 pixels. The red background is not static and changes every time. The letters are uniformly spaced but vary (and sometimes get merged). The same exact font for each captcha (e.g. see the 'R' letter above in both captchas) There is always 3 pixels of spacing above and below a letter (except for 'J' which I found out only later!)  Bullet point 4 is rather interesting. This means that we don't even need to use neural networks for this problem! We can just store the images of each letter as templates, and then do some form of sliding templates across the captcha until a perfect match is found. But no! Neural networks or nothing! So let us move on. Bullet point 2 implies that we can't just subtract a static background image from the captchas, rather, we will have to do something more clever like thresholding some pixel values. Bullet point 3 is an indication of imminent pain. We can't extract the letters by just uniformly splitting our captcha. We will want to do something more clever like extracting of contours. Part 3: Labelling the Data At this point I should mention that while I happily obtained 10k captcha images, it went over my head that I would have to manually label these captchas with the solutions, since the neural network we are going to build uses supervised learning (we need to teach it what the correct answer is first!). The amount of data we have exactly is 10k x 4 letters = 40k letters. The captchas seem rather simple so perhaps we don't need all that data to train our network. But how would I know how much is enough? The answer is when I get tired of labelling. Actually, I'm not even the one labelling, I have empowered a member of my household to do this dutiful task. So when they get tired its over. And... we have 500 labelled captchas to start with. If we need more, we can come back to labelling them. Part 4: Recycling Old Code To implement our four step plan from part 1, we need to code two major components: an algorithm to extract letters (this would involve step 1 and step 2 of the plan), and a neural network to feed the extracted letters (this would involve step 3 and step 4). For letter extraction algorithm, I decided it was a good day to recycle some code that I wrote two two years ago. The idea was to use a ready GUI interface (e.g. GIF at the start of this guide) and a tracking algorithm that would need little modification to do letter extraction. The incentive was that it would save time. It was a good incentive... I had a fight with the docstrings, the awkward IO class, the funny naming conventions, and truth be told, the code I wrote 2 years ago surprisingly wasn't bad at all, but my slight OCD to refactor code kicked in and... no time was saved, only lost. Part 5: Letter Detection Algorithm (Step 1 & Step 2) Now it's time to actually implement step 1 and step 2 of our plan, the letter detection algorithm. Step 1 (implementation) 1. I see a red background that looks rather useless so I would ignore it.   1.1 Remove any pixel that has a good degree of red or white in it.  To filter out the noisy background, we need to define exactly what is considered noise. As before, diving into GIMP and analysing the image should prove to be fruitful. Converting the image to grayscale and then analysing the histogram of pixel values (features of GIMP), we see that there is a clean separation between noise and useful data (see below). So it would be safe to say our letter detection algorithm should convert a captcha image to grayscale, then extract pixel values of 127 to 255 for any captcha.      Step 2 (implementation) 2. I see four distinct letters in black and would assume the captcha is only four letters.   2.1 From our black and white image (step 1.1), I can extract four black regions that represent four letters.  Now to extract the letters themselves, since they are not uniformly spaced as noted before, we can use OpenCV's contour detection algorithm in Python. Simply put, a contour is a curve that encompasses pixels of the same value, and in our case, the image like the one above should only have two colours (black and white), so our contours should be black regions that map onto a letter. Studying the letters again, the smallest sized letter (and consequently contour) would be 'I' (13x4 pixels) and largest it seems would be 'W' (13x13 pixels). We can use this information. In our OpenCV contour algorithm, cv2.findContours function, let us ignore any contours that are smaller than the letter 'I' and larger than 'W'. If we run our algorithm now, it works great for most captchas but it breaks for some captchas:        What's going on here? Let's look case by case. For the 1st captcha, 'OM4' is not even detected! Notice that the letters are merged... Wait, this violates a recent assumption of ours! If letters are merged, the largest contour size is no longer 'W' (13x13 pixels), but it would be four 'W' letters stuck together which is 13 x 13 x 4 pixels.      That is much better, though there is still an issue. 'OM4' is treated as one big contour. A solution to fix this issue is that if we detect a contour that is larger than one letter, say by 25%, then we split that contour uniformly into two  smaller contours right in the middle. This approach is prone to creating  contours with overlapping regions, but that might be ok, we will let our  upcoming neural network handle the issue (though it would be best to fix it  here but it is too much work for now, we can revisit this later if need be).  The same logic can be applied for two, three or even four letters joined together. If all fails, for instance in the case when you have four 'I' letters joined together (their width would roughly equal height), then we can have an if statement that manually slices the captcha into four uniform sections if less than four contours are detected (this required a lot of trail and error to find optimal slicing!). Not the most robust or ideal solution, but it is quick and dirty enough for our needs. Implementing this, our results are as follows:      Two more cases left to examine:       Notice that our solution we just implemented is already at work; the merged 'KP' in the 3rd captcha is now being detected as two separate letters. Great. But why isn't the 'J' and 'Q' being recognised? To be frank, I was confused at this point so I just shuffled around other captchas with 'J' and 'Q' and then noticed a trend... When a letter touches a black pixel that is part of the border of the captcha (see all those black pixels at the borders?), this letter automatically merges with the contour of the entire border, thus making it a super big contour. This is not what we want, we want to isolate the captcha border contour from the contour of the letters. A quick fix to this issue is to override the border with white pixels (we don't care about the border after all). This will ensure separation.       At this point, skimming through the output images of our letter detection algorithm, our algorithm is more than ready to go. It looks like it works well for more than 90-95% cases. Any further improvement is not worth the time and can be revisited later. So to recap, our algorithm does the following:  Convert the image to grayscale. Filter out the noisy red background by thresholding for pixel values of 127-255. 'Colour' in the border to let the contour extraction algorithm pick up letters like 'J' that touch the borders. Extract contours of black pixels which would map onto letters. Slice contours that are large since most likely they contain multiple letters.  Part 6: Neural Network (Step 3 & Step 4) The most exciting part of the entire project. My hopes and dreams, I see them all. Not that they lasted very long. Despite neural networks being the crux of the entire project, this part was the quickest. Building the neural network was pretty quick in Python with Keras and recycling code from Adam Geitgey. Step 3 (implementation) 3. Going from left to right, the letters look like '0' (or maybe 'O' if the font is potato?), '1', 'Z', 'U'.      3.1 Using my brains limited memory capacity, I am doing a simple pattern matching.  We've already extracted our letters from the captchas, which are labelled, so the letter images can be labelled easily too. We have 500 * 4 = 2,000 letters worth of training data. Our neural network is going to be a generic convolution neural network:      Why one convolution layers and not two or three? Trail and error indicates that one layer produces the highest accuracy. It seems that one convolution layer is sufficient to capture enough details about the captcha and anymore captures too many unnecessary details that confuse matters. Playing around with the hidden layer (2nd dense layer in the diagram) does some good:     50 Nodes ----> 40-65% accuracy     500 Nodes ---> 85-90% accuracy     1000 Nodes --> 85-90% accuracy     2000 Nodes --> 90-95% accuracy  The more nodes, the more the neural network learns of the intricacies of the captcha image structure. Anything more than 2000 nodes ends up eating too much RAM on my PC so here is a good point to stop. Varying the size of the pooling window on the max pooling layer to (3, 3) further bumps up the accuracy by 5-10%. It seems that considering a bigger chunk of the convoluted image is better representative in identifying the letter in the image. Step 4 (implementation) 4. So the captcha is '01ZU'!  Is it though? The accuracy of our convolution neural network (CNN) is about ~90%, that is, it solves ~90% of the captchas correctly. And of the cases it failed? How far away from the correct answer was it? Let's examine some cases to see ways to improve this:          From left to right, let us examine case by case. Case 1      Can you spot the error here? None! The solved captcha in green is the same as the actual captcha. So what went wrong? The human solved solution of the captcha is wrong! The filename was mislabelled as \"QVAS\". Tsk, tsk. Our CNN is picking up on my human nature. Next case! Case 2      Here is where we face a real issue. The '0' is being recognised as an 'O' by our CNN. How to remedy this? To ensure our training data is not misleading our CNN, I quickly delved into the training data which are the images of letters extracted from the captchas using our letter detection algorithm. The training data looked fine, except for a handful of mislabelled cases (shouldn't affect our network that much). So the training data is good. Perhaps we need to feed it more data of 'O' and '0' so that it learns the difference? They look really similar after all. This is left as an exercise for another day. Case 3      The captcha 'QW1P' is being recognised as 'QWJP'. Why? The 'W1' are merged, and if we flip the image around, the '1' consequently does look like a 'J'. One property to keep in mind about CNNs as that they are rotational and transitional invariant. So a 'W' would like a 'M', and a 'J' would look like a funny merged 'W1'. A solution to combat this is to use the same tactic used in the card game of Uno: draw a line underneath every letter (e.g. a '9' and a '6' in Uno are distinguished by just that). Case 4      Our letter detection algorithm hasn't done such a good job here, so the CNN consequently couldn't either. What the CNN is seeing is a 'V' and not a 'W' because the 'W' has been sliced incorrectly. To solve these type of issues, we need to patch up our letter detection algorithm. Case 5      No idea. This one is rather mysterious... Part 7: Conclusion Hopefully you found that journey fruitful. In the end, the best our CNN performed was 95% accuracy which is more than sufficient for my needs. I would have hoped for it to be closer to 99% considering how simple these captchas actually are. Perhaps CNN was not the best approach as discussed earlier, not because better solutions like letter template matching exist for this dataset (the letter shapes are always the same so this solution can achieve 100% accuracy with some quirks involved!), but because this CNN is highly dependant on the input from the letter detection algorithm, which has its limitations when it comes to merged letters. So the CNN's true powers are held back by the previous input. There is still room for improvement as discussed in part 6 with some solutions briefly discussed, but for now, I will consider this project done and dusted. Now would be a good time to say this: If there is anything more (or anything less) that you would have liked me to mention, please let me know! Our fleshed out algorithm that we implemented now is: 1. I see a red background that looks rather useless so I would ignore it.     1.1 Convert image to grayscale.     1.2 Threshold for pixel values of 127 to 255. 2. I see four distinct letters in black and would assume the captcha is only four letters.     2.1 Fill in the border so the contour extraction algorithm picks up letters like 'J' that touch the border.     2.2 Extract contours of black pixels which would map onto letters.     2.3 Slice contours that are large since most likely they contain multiple letters. 3. Going from left to right, the letters look like '0' (or maybe 'O' if the font is potato?), '1', 'Z', 'U'.      3.1 Train our convolutional neural network to build its 'memory'.     3.2 Feed in the captcha letters one by one and let the CNN classify. 4. So the captcha is '01ZU'!     4.1 Validate the output manually.     4.2 Run on all captchas and measure accuracy.     4.3 Pick incorrectly solved cases and analyse to improve entire process or give up.  Instructions How to Use  Install all dependencies with pip install -r requirements.txt Original data is located in data/dataset (10k captchas, 1041 labelled). Training data is already unzipped and named appropriately in data/training. Validation data is already unzipped and named appropriately in data/validation. Navigate to captcha/main.py and comment code as need be (e.g. only enable GUI analysis tool). Run python main.py from the captcha directory. See results in data/output.  Note: Useful command for renaming input directory, 'let i=0; for f in $(ls); do mv $f `printf %06d_$f $i`; ((i++)); done' Key Features  Solves captchas using a convolutional neural network (accuracy 85-90%). Allows quick analysis of processed captcha images using a custom GUI tool. Contains two core algorithms: captcha letter detection & neural network captcha solver. Dumps all processed images in a directory for later analysis (e.g. 'solved_incorrect')  File Structure Core Logic:  data.py: responsible for handling IO operations on captcha images filter.py: Contains algorithms for analysing captchas (e.g. letter extraction algorithm). main.py: Highest level script to run the project.  GUI:  controller.py: contains controller class for the GUI (MVC model) gui.py: Contains the GUI tool used to analyse captchas.  Neural Networks:  neural.py: Responsible for training and building the CNN. solver.py: Responsible for solving captchas using the trained CNN.  Credits   Adam Geitgey, a beautiful guide and code for solving captchas in 15 minutes with machine learning: https://medium.com/@ageitgey/how-to-break-a-captcha-system-in-15-minutes-with-machine-learning-dbebb035a710   Lutzroeder, NETRON, visualisation of the neural network: https://github.com/lutzroeder/Netron   Marco Radic & Mario Mann, Novatec, another beautiful guide for solving more complex captchas: https://www.novatec-gmbh.de/en/blog/deep-learning-for-end-to-end-captcha-solving/   Sumit Saha, Towards Data Science, an in-depth guide on CNNs: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53   ", "has_readme": true, "readme_language": "English", "repo_tags": ["machine-learning", "convolutional-neural-networks", "captcha", "data-science"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6477"}, "repo_url": "https://github.com/zeppaman/KerasUI", "repo_name": "KerasUI", "repo_full_name": "zeppaman/KerasUI", "repo_owner": "zeppaman", "repo_desc": "UI for Keras to implement image classification written in python and django", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T17:24:43Z", "repo_watch": 13, "repo_forks": 6, "private": false, "repo_created_at": "2019-05-05T17:45:12Z", "homepage": null, "size": 3711, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 185063480, "is_fork": false, "readme_text": "Keras UI: Visual tool from image classification KerasUI is a visual tool to allow easy training of model in image classification and allow to consume model as a service just calling API. Main features:  authenticated with oauth2 allow full model customization you can upload yet trained model and consume via API test form and visual to check how the net works bulk upload of the training set  usage  run standalone.bat or sh standalone.bat (this will install requirements apply migrations and run the server, the same script works on UNIX and windows) create the admin user using python manage.py createsuperuser navigate to http://127.0.0.1:8000/  this requires python 3+, if you have multiple version installed please change the script according (i.e. pip3). How to manage dataset Keras UI allows uploading dataset items (image) into the web application. You can do it one by one or adding a zip file with many images in one shot. It manages multiple datasets so you can keep things separates. After you have the images loaded, you can click the training button and run the training process. This will train the model you have defined without any interaction from you. You will get back training result and if you are finicky you can go to the log file and see what the system output  How to test using web UI Tho avoid to lose sleep over, I provided a simple form where you can upload your image and get the result.  How to use API UI or postman to test API All you have seen until now in the web UI can be replicated using API.  API usage This application use oauth2 to authenticate requests, so the first step you need is to get the token. This is a simple example for password flow. Please remember you have to enable the app (this is not created by default at first run). Assuming client hUiSQJcR9ZrmWSecwh1gloi7pqGTOclss4GwIt1o secret ZuuLK21sQ2uZxk8dVG7k6pO474FBlM6DEQs7FQvDh28gdLtbCDJwFFi0YlTlLsbz9ddjUa7Lun6ifYwkfwyGMD95WsCuzibFWIMpsZHMA039RIv1mOsYUO5nK5ZVv1hB  POST to http://127.0.0.1:8000/o/token/  Headers: Authorization: Basic czZCaGRSa3F0MzpnWDFmQmF0M2JW Content-Type: application/x-www-form-urlencoded  Body: grant_type:password username:admin password:admin2019!   Response is {     \"access_token\": \"h6WeZwYwqahFDqGDRr6mcToyAm3Eae\",     \"expires_in\": 36000,     \"token_type\": \"Bearer\",     \"scope\": \"read write\",     \"refresh_token\": \"eg97atDWMfqC1lYKW81XCvltj0sism\" }  The API to get the prediction works in json post or form post. In json post the image is sent as base64 string. This double way to consume the service is useful because you may link it to a form or use with wget or curl tool directly as well you can use it from your application. POST http://127.0.0.1:8000/api/test/  Headers: Content-Type:application/json Authorization:Bearer <token>  Body {     \"image\":\"<base 64 image\",     \"dataset\":1 }   The response {     \"result\": \"<LABEL PREDICTED>\" }  for a full api documentation you can refer to the postman file Tutorial This project is part of the image classification context on Codeproject. Here a walkthrough on the technical part to explain how it is built and how it works. The project stack:  Python django framework keras, tensorflow,numpy sqlite (or other databases you like)  Tools used:  Visual studio code Postman A web browser  Project setup The project is based on Django, so the first thing to do is to create a Django project using CLI. This requires to install Django from pip. django-admin startproject kerasui ' create the project  This command will produce the following structure: kerasui/     manage.py     kerasui/         __init__.py         settings.py         urls.py         wsgi.py  These files are:  The outer kerasui/ root directory is just a container for your project. The inner mysite/ directory is the actual Python package for your project. Its name is the Python package name you\u2019ll need to use to import anything inside it (e.g. mysite.urls). manage.py: A command-line utility that lets you interact with this Django project in various ways. You can read all the details about manage.py in jango-admin and manage.py. __init__.py: An empty file that tells Python that this directory should be considered a Python package. If you\u2019re a Python beginner, read more about packages in the official Python docs. kerasui/settings.py: Settings/configuration for this Django project. Django settings will tell you all about how settings work. kerasui/urls.py: The URL declarations for this Django project; a \u201ctable of contents\u201d of your Django-powered site. You can read more about URLs in the URL dispatcher. kerasui/wsgi.py: An entry-point for WSGI-compatible web servers to serve your project. See How to deploy with WSGI for more details.  Run it To check if all works, just run django with the built-in server (in production we will use wsgi interface to integrate with our favourite web server) python manage.py runserver  You can also use  setup visual studio code to run django /  This is the django configuration: {     // Use IntelliSense to learn about possible attributes.     // Hover to view descriptions of existing attributes.     // For more information, visit: https://go.microsoft.com/fwlink/?linkid=830387     \"version\": \"0.2.0\",     \"configurations\": [         {             \"name\": \"Python: Django\",             \"type\": \"python\",             \"request\": \"launch\",             \"program\": \"${workspaceFolder}\\\\kerasui\\\\manage.py\",             \"args\": [                 \"runserver\",                 \"--noreload\",                 \"--nothreading\"             ],             \"django\": true         }     ] } Settings configuration Here the basic part of the configuration that tell:  to use oauth 2 and session authentication so that: regular web user login and use the web site and rest sandbox, API user get the token and query the API services to use SQLite (you can change to move to any other DB) to add all Django modules (and our two custom: management UI and API) enable cors  INSTALLED_APPS = [     'python_field',     'django.contrib.admin',     'django.contrib.auth',     'django.contrib.contenttypes',     'django.contrib.sessions',     'django.contrib.messages',     'django.contrib.staticfiles',     'oauth2_provider',     'corsheaders',     'rest_framework',       'management',     'api', ]  MIDDLEWARE = [     'django.middleware.security.SecurityMiddleware',     'django.contrib.sessions.middleware.SessionMiddleware',     'django.middleware.common.CommonMiddleware',    # 'django.middleware.csrf.CsrfViewMiddleware',     'django.contrib.auth.middleware.AuthenticationMiddleware',     'django.contrib.messages.middleware.MessageMiddleware',     'django.middleware.clickjacking.XFrameOptionsMiddleware',     'django.middleware.security.SecurityMiddleware',     'corsheaders.middleware.CorsMiddleware', ]  ROOT_URLCONF = 'kerasui.urls'   REST_FRAMEWORK = {     'DEFAULT_AUTHENTICATION_CLASSES': (         'rest_framework.authentication.SessionAuthentication',         'rest_framework.authentication.BasicAuthentication',         'oauth2_provider.contrib.rest_framework.OAuth2Authentication',     ),     'DEFAULT_PERMISSION_CLASSES': (         'rest_framework.permissions.IsAuthenticated',     ),        'DEFAULT_PAGINATION_CLASS': 'rest_framework.pagination.LimitOffsetPagination',     'PAGE_SIZE': 10, }  DATABASES = {     'default': {         'ENGINE': 'django.db.backends.sqlite3',         'NAME': os.path.join(BASE_DIR, 'db.sqlite3'),     } } First run Django uses a migration system that produces migration files from the model you defined. To apply migrations you just need to run the migrate command (makemigration to create migration files from model). The user database start empty, so you need to create the admin user to login. This is done by the createsuperadmin command python manage.py migrate python manage.py createsuperuser admin\\admin2019!  How it's built The app is separated into 3 modules:  Management part: the web UI, the modules and all the core stuff Background worker: is a Django command that can be executed in background and is used to train models against the dataset API: this part exposes API to interact with application from outside. In example, this allows to add items to dataset from a third party application. Moreover, the most common usage is to send an image and get the prediction result  Management To create an app on Django: python manage.py startapp management   This will create the main files for you. In this module the most we use is about Model and Model representation:  module.py: here are all models with field specifications. By such class definition, all is set to have a working CRUD over entities admin.py: this layer describes how to show and edit data with forms.  The data model Our data model is very simple. Assuming that we want to train only one model per dataset (this may be a limit if you would reuse dataset with multiple models...), we have  DataSet: this contains the model, the model settings, and the name of the dataset. DataSetItem: this contains the dataset items, so one image per row with the label attached.  Here just a sample of models and model representation: #from admin.py class DataSetForm( forms.ModelForm ):        process =forms.CharField( widget=forms.Textarea(attrs={'rows':40, 'cols':115}), initial=settings.PROCESS_TEMPLATE )     model_labels =forms.CharField(initial=\"[]\")     class Meta:         model = DataSet         fields = ['name', 'process','epochs','batchSize','verbose','model_labels','model']         widgets = {           'process': forms.Textarea(attrs={'rows':20, 'cols':200}),           }      def train(modeladmin, request, queryset):        for dataset in queryset:         DataSetAdmin.train_async(dataset.id)  class DataSetAdmin(admin.ModelAdmin):     list_display = ('name','epochs','batchSize','verbose','progress')     inlines = [       #  DataSetItemInline,     ]     form=DataSetForm     actions = [train]     change_list_template = \"dataset_changelist.html\"       @staticmethod     def train(datasetid):         call_command('train',datasetid)     @staticmethod     def train_async(datasetid):         t = threading.Thread(target=DataSetAdmin.train, args=(datasetid,))         t.setDaemon(True)         t.start()   admin.site.register(DataSet,DataSetAdmin)  #from model.py  class DataSet(models.Model):     name= models.CharField(max_length=200)     process = models.CharField(max_length=5000, default=settings.PROCESS_TEMPLATE)     model = models.ImageField(upload_to=path_model_name,max_length=300,db_column='modelPath',blank=True, null=True)     #weights = models.ImageField(upload_to=path_model_name,max_length=300,db_column='weightPath',blank=True, null=True)     batchSize = models.IntegerField(validators=[MaxValueValidator(100), MinValueValidator(1)],default=10)     epochs = models.IntegerField(validators=[MaxValueValidator(100), MinValueValidator(1)],default=10)     verbose = models.BooleanField(default=True)     progress = models.FloatField(default=0)         model_labels= models.CharField(max_length=200)     def __str__(self):         return self.name      Django works in code-first approach, so we will need to run python manage.py makemigrations to generate migration files that will be applied to the database. python manage.py makemigrations  Background worker To create the background worker we need a module to host it, and I used the management module. Inside it, we need to create a management folder (sorry for the name that is the same as the main module, I hope this is not a threat). Each file on it can be run via python manage.py commandname or via API. In our case, we start the command in a background process via regular Django action This is the relevant part: class DataSetAdmin(admin.ModelAdmin):         actions = [train]       # ....          @staticmethod     def train(datasetid):         call_command('train',datasetid)     @staticmethod     def train_async(datasetid):         t = threading.Thread(target=DataSetAdmin.train, args=(datasetid,))         t.setDaemon(True)         t.start() API The API is created in a separated app python manage.py startapp API   Basically all CRUD model can be exposed by API, however, you need to specify how to serialize it class DataSetItemSerializer(serializers.HyperlinkedModelSerializer):     image = Base64ImageField()     dataset=   serializers.PrimaryKeyRelatedField(many=False, read_only=True)     class Meta:         model = DataSetItem          # Fields to expose via API         fields = ('label', 'image', 'dataset')   class DataSetSerializer(serializers.HyperlinkedModelSerializer):           class Meta:         model = DataSet         fields = ('name', 'process') You need also to create ViewSet (mapping between the model and the data presentation: class DataSetItemViewSet(viewsets.ModelViewSet):         queryset = DataSetItem.objects.all()     serializer_class = DataSetItemSerializer  class DataSetViewSet(viewsets.ModelViewSet):         queryset = DataSet.objects.all()     serializer_class = DataSetSerializer Finally, you need to define all routes and map viwset to url. This will be enough to consume model as api router = routers.DefaultRouter() router.register(r'users', views.UserViewSet) router.register(r'datasetitem', views.DataSetItemViewSet) router.register(r'dataset', views.DataSetViewSet) router.register(r'test', views.TestItemViewSet, basename='test')  # Wire up our API using automatic URL routing. # Additionally, we include login URLs for the browsable API. urlpatterns = [     url(r'^', include(router.urls)),     url(r'^api-auth/', include('rest_framework.urls', namespace='rest_framework')), ]  urlpatterns += staticfiles_urlpatterns()  The training The algorithm is very easy:  Take all images from the dataset Normalize them and add to a labeled list Create the model how it is specified into the dataset model train it  This is the piece of code that query dataset items and load images: def load_data(self, datasetid):         self.stdout.write(\"loading images\")         train_data = []                  images = DataSetItem.objects.filter(dataset=datasetid)         labels = [x['label'] for x in  DataSetItem.objects.values('label').distinct()]                for image in images:             self.stdout.write(\"Loading {0}\".format(image.image))             image_path = image.image.path             if \"DS_Store\" not in image_path:                            index=[x for x in range(len(labels)) if labels[x]==image.label]                 label = to_categorical([index,],len(labels))                                  img = Image.open(image_path)                 img = img.convert('L')                 img = img.resize((self.IMAGE_SIZE, self.IMAGE_SIZE), Image.ANTIALIAS)                 train_data.append([np.array(img), np.array(label[0])])                      return train_data Take a look at: labels = [x['label'] for x in  DataSetItem.objects.values('label').distinct()] label = to_categorical([index,],len(labels)) this assigns an order to all the labels, i.e. [\"CAT\",\"DOGS\"] then to_categorical convert the positional index to the one-hot representation. To tell in simpler words, this make CAT =[1,0] and DOG=[0,1] To train the model    model=Sequential()    exec(dataset.process)    model.add(Dense(len(labels), activation = 'softmax'))    model.fit(training_images, training_labels, batch_size=dataset.batchSize, epochs=dataset.epochs, verbose=dataset.verbose) Note that the dataset.process is the python model definition you entered into web admin and you can tune as much you want. The last layer is added outside the user callback to be sure to match the array size. The fit method just runs the train using all data (keras automatically make a heuristic separation of test and training set, for now, it's enough, in future we can plan to let the user choose percentages of data to use in each part or mark items one by one). Finally we store the trained model: datasetToSave=DataSet.objects.get(pk=datasetid) datasetToSave.progress=100 datasetToSave.model_labels=json.dumps(labels) temp_file_name=str(uuid.uuid4())+'.h5' model.save(temp_file_name) datasetToSave.model.save('weights.h5',File(open(temp_file_name, mode='rb'))) os.remove(temp_file_name) datasetToSave.save() Note that I save also the label order beacuse must be the same of the model to match the one-hot convention. the prediction There is a common method that, given the sample and the dataset, retrieve the model, load it and make the prediction. This is the piece of code: def predict(image_path,datasetid):                      dataset=DataSet.objects.get(pk=datasetid)             modelpath=dataset.model.path             model=load_model(modelpath)             labels=json.loads(dataset.model_labels)                          img = Image.open(image_path)             img = img.convert('L')             img = img.resize((256, 256), Image.ANTIALIAS)              result= model.predict(np.array(img).reshape(-1,256,256, 1))             max=result[0]             idx=0             for i in range(1,len(result)):                 if max<result[i]:                     max=result[i]                     idx=i               return labels[idx] The model is loaded using load_model(modelpath) and the labels are from the database. The model prediction output as a list of values, it is chosen the higher index and used to retrieve the correct label assigned to the network output at the training time. Acknowledgements This article is part of the image classification challenge. Thanks to the article Cat or not of Ryan Peden where I find the basics to manage the training process and images to test the tool. ", "has_readme": true, "readme_language": "English", "repo_tags": ["django", "keras", "ui", "image-classification"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://127.0.0.1:8000/"], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6478"}, "repo_url": "https://github.com/jgraving/deepposekit", "repo_name": "deepposekit", "repo_full_name": "jgraving/deepposekit", "repo_owner": "jgraving", "repo_desc": "an API for pose estimation", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T02:01:36Z", "repo_watch": 14, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-05T09:48:51Z", "homepage": "http://deepposekit.org", "size": 362, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185012179, "is_fork": false, "readme_text": "DeepPoseKit: an API for pose estimation    You have just found DeepPoseKit. DeepPoseKit is a high-level API for 2D pose estimation with deep learning written in Python and built using Keras and Tensorflow. Use DeepPoseKit if you need:  tools for annotating images or video frames with user-defined keypoints a straightforward but flexible data augmentation pipeline using the imgaug package a Keras-based interface for initializing, training, and evaluating pose estimation models easy-to-use methods for saving and loading models and making predictions on new data  DeepPoseKit is designed with a focus on usability and extensibility, as being able to go from idea to result with the least possible delay is key to doing good research. DeepPoseKit is currently limited to individual pose esimation, but can be extended to multiple individuals by first localizing and cropping individuals with additional tracking software such as idtracker.ai, pinpoint, or Tracktor. Check out our preprint to find out more. Note: This software is still in early-release development. Expect some adventures.     How to use DeepPoseKit DeepPoseKit is designed for easy use. For example, training and saving a model requires only a few lines of code: from deepposekit import TrainingGenerator from deepposekit.models import StackedDenseNet  train_generator = TrainingGenerator('/path/to/data.h5') model = StackedDenseNet(train_generator) model.compile('adam', 'mse') model.fit(batch_size=16, n_workers=8) model.save('/path/to/model.h5') Loading a trained model and running predictions on new data is also straightforward: from deepposekit.models import load_model  model = load_model('/path/to/model.h5') new_data = load_new_data('/path/to/new/data.h5') predictions = model.predict(new_data) See our example notebooks for more details on how to use DeepPoseKit. Installation DeepPoseKit requires Tensorflow and Keras for training and using pose estimation models. These should be manually installed, along with dependencies such as CUDA and cuDNN, before installing DeepPoseKit:  Keras Installation Instructions Tensorflow Installation Instructions Note: Tensorflow 2.0 is not yet supported.  DeepPoseKit has only been tested on Ubuntu 18.04, which is the recommended system for using the toolkit. Install the development version with pip: pip install git+https://www.github.com/jgraving/deepposekit.git To use the annotation toolkit you must install the DeepPoseKit Annotator package: pip install git+https://www.github.com/jgraving/deepposekit-annotator.git You can download example datasets from our DeepPoseKit Data repository: git clone https://www.github.com/jgraving/deepposekit-data Citation If you use DeepPoseKit for your research please cite our preprint: @article{graving2019fast,          title={Fast and robust animal pose estimation},          author={Graving, Jacob M and Chae, Daniel and Naik, Hemal and Li, Liang and Koger, Benjamin and Costelloe, Blair R and Couzin, Iain D},          journal={bioRxiv},          pages={620245},          year={2019},          publisher={Cold Spring Harbor Laboratory}          }  Development Please submit bugs or feature requests to the GitHub issue tracker. Please limit reported issues to the DeepPoseKit codebase and provide as much detail as you can with a minimal working example if possible. If you experience problems with Tensorflow or Keras, such as installing CUDA or cuDNN dependencies, then please direct issues to those development teams. Contributors DeepPoseKit was developed by Jake Graving and Daniel Chae, and is still being actively developed. We welcome public contributions to the toolkit. If you wish to contribute, please fork the repository to make your modifications and submit a pull request. License Released under a Apache 2.0 License. See LICENSE for details. ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "python", "keras", "pose-estimation", "behavior-analysis", "machine-learning", "tensorflow", "neural-networks", "deepposekit", "animal-pose-estimation", "animal", "pose", "estimation"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6479"}, "repo_url": "https://github.com/BitFloyd/deepsbd", "repo_name": "deepsbd", "repo_full_name": "BitFloyd/deepsbd", "repo_owner": "BitFloyd", "repo_desc": "This is a package to do shot boundary detection on videos. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T16:04:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T07:27:16Z", "homepage": null, "size": 68977, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184995402, "is_fork": false, "readme_text": "deepsbd This is a package to do shot boundary detection on videos. Required packages: keras-squeezenet (https://github.com/BitFloyd/keras-squeezenet) cv2 skimage Keras Tensorflow clockshortenstream (https://github.com/BitFloyd/clockshortenstream) Installation Navigate to the deepsbd folder pip install -e . Usage Example:  from deepsbd.video_to_shots import VideoToShots  video_path = 'video.mp4' directory_to_save_shots_in = 'shots' vts = VideoToShots(video_path)  #Perform fit vts.fit()  print \"Transition Candidates: \" vts.candidates print \"Cut Transitions: \",vts.cuts print \"Gradual Transitions:\", vts.grads  print \"##############################################\" print \"ALL_TRANSITION_FRAMES:\" print vts.full_trans print \"##############################################\"  vts.save_video_as_shots(directory_to_save_shots_in)   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/BitFloyd/deepsbd/blob/286ec79331394d11474ff84206139e09d85101c4/deepsbd/cut_detector.h5", "https://github.com/BitFloyd/deepsbd/blob/286ec79331394d11474ff84206139e09d85101c4/deepsbd/grad_detector.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df647a"}, "repo_url": "https://github.com/LucaCappelletti94/plot_keras_history", "repo_name": "plot_keras_history", "repo_full_name": "LucaCappelletti94/plot_keras_history", "repo_owner": "LucaCappelletti94", "repo_desc": "A simple python package to print a keras NN training history.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-03T12:46:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T16:50:09Z", "homepage": null, "size": 752, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185058056, "is_fork": false, "readme_text": "plot_keras_history        A simple python package to print a keras NN training history.  How do I install this package? As usual, just download it using pip: pip install plot_keras_history  Tests Coverage Since some software handling coverages sometime get slightly different results, here's three of them:     Examples Here's how two histories can be visualized: from plot_keras_history import plot_history import matplolib.pyplot as plt  history = model.fit(...).history plot_history(history) plt.show()   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df647b"}, "repo_url": "https://github.com/fengwang/subpixel_conv2d", "repo_name": "subpixel_conv2d", "repo_full_name": "fengwang/subpixel_conv2d", "repo_owner": "fengwang", "repo_desc": "Sub-Pixel Convolutional Layer with Tensorflow/Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T12:09:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T11:22:04Z", "homepage": "", "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185021648, "is_fork": false, "readme_text": "SubpixelConv2D Subpixel convolution with keras and tensorflow.  Example usage A simple model upsampling a layer of dimension ( 32, 32, 16 ) to ( 128, 128, 1 ), with save/load functionality enabled.. from subpixel_conv2d import SubpixelConv2D from keras.layers import Input from keras.models import Model, load_model ip = Input(shape=(32, 32, 16)) x = SubpixelConv2D(upsampling_factor=4)(ip) model = Model(ip, x) model.summary() model.save( 'model.h5' )  print( '*'*80 ) nm = load_model( 'model.h5' ) print( 'new model loaded successfully' ) produces output of Using TensorFlow backend. _________________________________________________________________ Layer (type)                 Output Shape              Param # ================================================================= input_1 (InputLayer)         (None, 32, 32, 16)        0 _________________________________________________________________ subpixel_conv2d_1 (SubpixelC (None, 128, 128, 1)       0 ================================================================= Total params: 0 Trainable params: 0 Non-trainable params: 0 _________________________________________________________________ ******************************************************************************** /usr/lib/python3.7/site-packages/keras/engine/saving.py:269: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.   warnings.warn('No training configuration found in save file: ' new model loaded successfully  Note Before loading a model designed with SubpixelConv2D, do load from subpixel_conv2d import SubpixelConv2D. Reference Shi, Wenzhe, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P. Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. \u201cReal-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network.\u201d ArXiv:1609.05158 [Cs, Stat], September 16, 2016. http://arxiv.org/abs/1609.05158. License AGPL-3 ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "super-resolution", "tensorflow"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["http://arxiv.org/abs/1609.05158"]}, {"_id": {"$oid": "5cf5186d7eb8d666b0df647c"}, "repo_url": "https://github.com/hvardhan878/SANDP500-Predictor-Using-ANN", "repo_name": "SANDP500-Predictor-Using-ANN", "repo_full_name": "hvardhan878/SANDP500-Predictor-Using-ANN", "repo_owner": "hvardhan878", "repo_desc": "S&P500-Predictor-Using-ANN With Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T07:02:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T06:57:34Z", "homepage": "", "size": 276, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184992214, "is_fork": false, "readme_text": "S-P500-Predictor-Using-ANN ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df647d"}, "repo_url": "https://github.com/yanxinhao/YOLO", "repo_name": "YOLO", "repo_full_name": "yanxinhao/YOLO", "repo_owner": "yanxinhao", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T21:21:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T01:09:46Z", "homepage": null, "size": 426, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184960381, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   YOLO ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df647e"}, "repo_url": "https://github.com/parya-j/LungNoduleClassification", "repo_name": "LungNoduleClassification", "repo_full_name": "parya-j/LungNoduleClassification", "repo_owner": "parya-j", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T17:20:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T23:28:36Z", "homepage": null, "size": 2041, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185093797, "is_fork": false, "readme_text": "Lung Nodule Classification Reguirements:  Keras Tensorflow Numpy  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df647f"}, "repo_url": "https://github.com/JohnBoxAnn/TensorFlow-Project", "repo_name": "TensorFlow-Project", "repo_full_name": "JohnBoxAnn/TensorFlow-Project", "repo_owner": "JohnBoxAnn", "repo_desc": "Practicing Codes", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T13:22:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T13:48:17Z", "homepage": null, "size": 24753, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185037316, "is_fork": false, "readme_text": "Tensorflow-Keras-project Some Practicing Codes ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/JohnBoxAnn/TensorFlow-Project/blob/e606f23cd2827f9c9f639893292bab7604694030/LSTM/modelsave.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6480"}, "repo_url": "https://github.com/vselleby/nn_xor", "repo_name": "nn_xor", "repo_full_name": "vselleby/nn_xor", "repo_owner": "vselleby", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T19:42:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T19:38:10Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185074384, "is_fork": false, "readme_text": "nn_xor Simple demo to show differences in neural network implementations of xor using Keras, Tensorflow and Numpy ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6481"}, "repo_url": "https://github.com/augusyan/Kaggle-Competition", "repo_name": "Kaggle-Competition", "repo_full_name": "augusyan/Kaggle-Competition", "repo_owner": "augusyan", "repo_desc": "kaggle", "description_language": "Inupiaq", "repo_ext_links": null, "repo_last_mod": "2019-05-05T01:43:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T01:26:54Z", "homepage": null, "size": 20107, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 184961831, "is_fork": false, "readme_text": "Kaggle-Competition This repo is code of Public project of my Kaggle Competition based on sklearn&Keras&Tensorflow. This repo only for learning. Environment  Operating system: Ubuntu 16.04 or CentOS 7 Data would take up to 10GB disk memory Memory cost would be around 16GB Dependencies:  CUDA and cuDNN with GPU Tensorflow with packages (Keras) installed    Prerequisites   Download this repo git clone https://git@github.com:augusyan/Kaggle-Competition.git cd forward_mahjong_processiong   Install requirements pip3 install -r requirements.txt   (Unnecessary)   Usage The training and testing scripts come with several options, which can be listed with the --help flag. python3 main.py --help To run the training and testing, simply run main.py. By default, the script runs resnet34 on attribute 'coat_length_labels' with 50 epochs. To training and testing resnet34 on attribute 'collar_design_labels' with 100 epochs and some learning parameters: python3 main.py --model 'resnet34' --attribute 'collar_design_labels' --epochs 100 --batch-size 128 --lr 0.01 --momentum 0.5 Every epoch trained model will be saved in the folder save/[attribute]/[model]. License The code is licensed with the Apache license. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6482"}, "repo_url": "https://github.com/ShreyasJothish/picassopaintedit", "repo_name": "picassopaintedit", "repo_full_name": "ShreyasJothish/picassopaintedit", "repo_owner": "ShreyasJothish", "repo_desc": "Neural style transfer on images. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T21:17:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T18:09:33Z", "homepage": "https://picasso-frontend.netlify.com/", "size": 24, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185065870, "is_fork": false, "readme_text": "picassopaintedit Neural style transfer on images using DeepAI API and Tensorflow Example This application supports neural style transfer on images in two modes.  fasttransform - Using DeepAI API. deeptransform - Using Neural Style Transfer with tf.keras  Major Dependency:  DeepAI API   https://deepai.org/api-docs/#neural-style   TensorFlow and Keras Celery   http://www.celeryproject.org/   AWS EC2 instance for running deeptransform and S3 for storing images.  app.py  Flask application to support fasttransform and deeptransform endpoints.  Input:  key - Neural style transform request key. style_url - URL of the style image used neural style transformation. content_url - URL of the content image used neural style transformation.  Output  key - Neural style transform request key. tranformed_image_url - URL of the neural style transformed image.  flask_celery.py  deeptransform is handled asynchronously using flask_celery.  deeptransformimpl.py  Starting point for handling asynchronous task for flask_celery.  neuralstyle.py  Implementation of deeptransform based on Neural Style Transfer with tf.keras.  util.py  Implements util functions for storing neural style transformed images on AWS S3 and sending deeptransform notification.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://deepai.org", "http://www.celeryproject.org/"], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6483"}, "repo_url": "https://github.com/LordZorgoth/apparel-practice", "repo_name": "apparel-practice", "repo_full_name": "LordZorgoth/apparel-practice", "repo_owner": "LordZorgoth", "repo_desc": "Image classification code for practice dataset consisting of apparel", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T21:10:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T23:28:31Z", "homepage": "", "size": 78235, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185093788, "is_fork": false, "readme_text": "Introduction This is my work on a practice problem that can be found at https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/. I am new to machine learning, and this is a very early phase of my project. I ultimately hope to carry out a detailed exploration of image classification methodology using this dataset and others, for the purpose of improving my own understanding. The code works and is documented, and the scripts code/train_conv_nn_augment_in_memory.py and code/train_dense_nn_augment_in_memory.py can be run successfully. Further documentation can be found in master_plan.pdf. On Windows, one may need to load docs\\\\master_plan.pdf, as I do not know that the symbolic link master_plan.pdf will work. How to Use I recommend using the IPython console when running scripts so that you can explore the results of each run. I am running my code using Keras 2.2.4 and Tensorflow 1.13.1 from Anaconda. It has been tested on my personal laptop (MacOS 10.14, on CPU), and on a remote server (Ubuntu 19.04, on GPU). It also requires numpy, scipy, and Matplotlib, and it makes use of sklearn.model_selection.train_test_split. In order for the models to build successfully at this time (5/5/2019) for training on a GPU, an updated version of  keras/backend/tensorflow_backend.py may need to be downloaded from the Keras GitHub repository. This is due to a bug in Keras which has been resolved, but the fix for which has not yet been added to the Anaconda repository. The zip file containing the training data must be extracted before the data can be loaded. This is because it is much faster to download one zip file than 60000 small image files, so I left the training data folder out of the repository. In the future, I will replace the train folder with a single file that can be loaded more efficiently and does not require additional steps on the part of the user. Development I am prioritizing documentation and reproducibility over rushing to tune hyperparameters and get on the leaderboard. The next step in my project is to create a pdf in which I will outline my strategy for testing and parameter tuning. After that, I will create code to compile, save, and visualize detailed information about test runs. I have other code that I have written and various tests that I have already run, but I am only adding fully documented material to the GitHub repository. There are not yet any documented test runs, but there will be. The reader can infer what hyperparameters I'm likely to focus on by looking at the parameters of my functions, especially augmentation.randomize_image and the model construction functions in code/model_building.py. I may, however, eliminate some of these parameters: my early testing will focus on which parameters are and aren't useful to explore. Workflow This code is developed in Emacs on my personal laptop, and run primarily on my \"server,\" a borrowed gaming laptop based in Idaho. I connect to the server via ssh over a VPN. The server works about 20 times faster during training due to its GPU. I generally work on the code and do minor tests and debugging on my laptop, and then sync the server's code with the GitHub repository before running larger-scale tests using a combination of ssh, GNU Screen, and IPython. Notes I am fully aware that my convolutional neural network model should outperform my fully connected neural network model. However, I am interested in whether this image set being centered and generally neat will mean that the advantages of convolutional neural networks will be smaller than they would have been with a \"messier\" dataset. I am also aware that Keras has built-in support for dataset augmentation. I have tested keras.preprocessing.ImageDataGenerator (without documentation as of yet) and am going to compare the performance of this option to in-memory augmentation during the testing process. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6484"}, "repo_url": "https://github.com/Gal1eo/lab3_speech", "repo_name": "lab3_speech", "repo_full_name": "Gal1eo/lab3_speech", "repo_owner": "Gal1eo", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-14T17:13:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T10:46:32Z", "homepage": null, "size": 181, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185018142, "is_fork": false, "readme_text": "lab3_speech Task Train and test a phone recogniser based on digit speech material from the TIDIGIT database: \u2022 using predefined Gaussian-emission HMM phonetic models, create time aligned phonetic transcriptions of the TIDIGITS database, \u2022 define appropriate DNN models for phoneme recognition using Keras, \u2022 train and evaluate the DNN models on a frame-by-frame recognition score, \u2022 repeat the training by varying model parameters and input features Optional: \u2022 perform and evaluate continuous speech recognition at the phoneme and word level using Gaussian-emission HMM models \u2022 perform and evaluate continuous speech recognition at the phoneme and word level using DNN-HMM models ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6485"}, "repo_url": "https://github.com/zhx-main/ConvsPPIS", "repo_name": "ConvsPPIS", "repo_full_name": "zhx-main/ConvsPPIS", "repo_owner": "zhx-main", "repo_desc": "The Prediction of Protein-protein Interaction Sites", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T07:37:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T09:04:12Z", "homepage": null, "size": 980746, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185006829, "is_fork": false, "readme_text": "ConvsPPIS: Identifying Protein-protein Interaction Sites by an Ensemble Convolutional Neural Network with Feature Graph Directory -Dataset The protein sequence of DBv5-Sel and CAPRI-Alone and the corresponding label for each residues of the protein. -Model The model architecture we used in our expriments: CNN(Convolutional Neural Network) and FCN(Fully-Connected Network). -Train The Pipeline of training process with 10-fold cross validation for PSSM, PhyChem, and PSAIA respectively. -DifferentRadius The trained model of different sliding window size (including the responding radius 5,6,7,8,9,10) with 10-fold cross validation.  Enviorment  Python 3.5  scikit-learn 0.20.1  tensorflow 1.9.0 keras 2.2.4  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_1.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_10.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_2.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_3.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_4.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_5.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_6.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_7.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_8.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem10_9.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_1.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_10.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_2.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_3.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_4.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_5.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_6.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_7.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_8.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem5_9.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_1.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_10.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_2.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_3.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_4.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_5.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_6.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_7.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_8.h5", "https://github.com/zhx-main/ConvsPPIS/blob/f85ed08679489324182d213261370d6804aabb4d/DifferentRadius/phychem/phychem6_9.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6486"}, "repo_url": "https://github.com/ozturkaslii/analyze-turkish-sentiment", "repo_name": "analyze-turkish-sentiment", "repo_full_name": "ozturkaslii/analyze-turkish-sentiment", "repo_owner": "ozturkaslii", "repo_desc": "Sentiment Analysis on Turkish Texts using LSTM with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T18:05:25Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T18:58:30Z", "homepage": "http://www.turkish-sentiment-analysis.com/", "size": 24647, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 185070666, "is_fork": false, "readme_text": "Sentiment Analysis on Turkish Texts In this project, I have made a simple web application to analyze sentiments on Turkish texts. I used an e-commerce dataset which had \"Rating\" and \"Review\" columns and contains roughly 250.000 commments. Rating column was tagged as 1 for positive comments, 0 for negative comments. In the model, 3-layered LSTM architecture was used. At the end of training, 95% accuracy was obtained. All codes were written in Python and Flask was used as a web server. For details, please check the documentation Instructions git clone https://github.com/ozturkaslii/analyze-turkish-sentiment.git  cd analyze-turkish-sentiment  pip install -r requirements.txt ", "has_readme": true, "readme_language": "English", "repo_tags": ["natural-language-processing", "sentiment-analysis", "lstm", "rnn", "tensorflow", "keras", "flask", "python", "heroku", "ci-cd", "pandas"], "has_h5": true, "h5_files_links": ["https://github.com/ozturkaslii/analyze-turkish-sentiment/blob/b9fa6c2c0c11060a41f3aa828a76d830b13965a1/lstm_nlp1.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6487"}, "repo_url": "https://github.com/gruckion/digitrecognizerpykeras", "repo_name": "digitrecognizerpykeras", "repo_full_name": "gruckion/digitrecognizerpykeras", "repo_owner": "gruckion", "repo_desc": "Digit OCR using a convolutional neural network in keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T09:25:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T13:45:04Z", "homepage": null, "size": 17, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185036921, "is_fork": false, "readme_text": "Digit Recognition Multi Layer Perceptron This repo follows the guide on multi class classifier for Digit Recognition using a multi layer perceptron in Python. This is on the MNist dataset loaded through Keras. Result  13s - loss: 0.0082 - acc: 0.9984 - val_loss: 0.0594 - val_acc: 0.9816 Baseline Error: 1.84%  Keywords and linked articles for learning  Perceptron model  A simple perceptron consists of an input and output layer with weights and a bias. This is capable of binary classification since it is only capable of classifying items between a linear decision boundary.  Multi-layer perceptron  In addition to input and output layers the multi-layer perceptron has one or more hidden layers.  Single hidden layer  The hidden layer allows for non-linear classification. Since a digit recognition system has 10 classes a hidden layer is needed to allow for non-linear classifications.  np.reshape()  For reshaping multi dimensional arrays  multi-class classification  `A perceptron is a binary classifer, multi-class classification requires non-linear decision boundaries.  One hot encoding`  One hot encoding is the process of transforming your mutually exclusive catagoracal data into a feture vectors of the same size {1, 0, 0} {0, 1, 0}. If we use integer encoding e.g. x = 1orx = 2 this will lead to results where we get a predicion mid way between the two values. Clearly with digits a two is always a two and is not a three or a nine. We do have percentage certainities on each but a cataegorical value can not belong to two classes, there is no ordinal relationship. This exclusive relationship can be easily described with one hot encoding.  Limitations of one hot encodidng  `The main limiation is that the representation size will grow with the corpus. With digits the corpus contains 10 classes, but with natura language this this could easily become 50 million. In this case we would then want to use distributed encoding.  Rectifier activation function for neurons in hidden layer (Rectified linear unit - relu for short) Each layer in a neural network will have an activation function, this is a treshhold that must be achieved for the ouput to be classified in the range of [0,1]. For the rectifier activation function (c) below, this is the  se ction where the value is non zero. (a) is the sigmoid and (b) is the tanh.`    Softmax (softplus) activation function Used on the output layer to turn outputs into probability-like values for more complex networks.    Other activation functions    Logarithmic loss function (Keras categorical_crossentrpoy)  A loss function is used for back propergation when updating the weights within the multiple layers of a perceptron. For a single perceptron the output is a function of the input and the weights are updated to get to the desired output. In a MLP the output is allowed to change along with the weights whilst holding the inputs constant, this allows the weights to adjust by an amount represented by the loss function. The loss function is the amount by which the output is off from the target vallue. The type of loss function depends on the problem and varies for catagorical / continuous classificationsl.  ADAM gradient descent algorithm for learning weights  When updating the weights in an MLP calculus is used to get the gradient at a point to see which direction to move to head towards a minimum. This may or may not be a local minima. This process is called gradient decent. By minimising the loss function above through gradient decent the weights can be adjusted to get the correct target output.  Model fitting epochs- False 100% accuracy using Kaggle  When adjusting the weights to improve the accuracy of a learning algorithm multiple parses will be required to minimise the loss function. The number of parses to get to the minima is known as epochs.   Epoch vs Batch size vs itteration  We can divide the dataset of 2000 examples into batches of 500 then it will take 4 iterations to complete 1 epoch. More [here](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)  [Stats on the best performing model on MNIST]  (http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354) testing mergify 2 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://docs.scipy.org/doc/numpy-1.10.1/reference/generated/numpy.reshape.html", "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354"], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6488"}, "repo_url": "https://github.com/atharvakadethankar/try_chexpert", "repo_name": "try_chexpert", "repo_full_name": "atharvakadethankar/try_chexpert", "repo_owner": "atharvakadethankar", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T15:06:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T14:57:51Z", "homepage": null, "size": 74727, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185045429, "is_fork": false, "readme_text": "CheXpert-Keras This project is a tool to build CheXpert-like models, written in Keras. What is CheXpert? CheXpert is a large dataset of chest X-rays and competition for automated chest x-ray interpretation, which features uncertainty labels and radiologist-labeled reference standard evaluation sets. In this project, you can  Train/test a baseline model by following the quickstart. You can get a model with performance close to the paper. Run class activation mapping to see the localization of your model. Modify multiply parameter in config.ini or design your own class weighting to see if you can get better performance. Modify weights.py to customize your weights in loss function. If you find something useful, feel free to make that an option and fire a PR. Every time you do a new experiment, make sure you modify output_dir in config.ini otherwise previous training results might be overwritten. For more options check the parameter description in config.ini.  Quickstart Note that currently this project can only be executed in Linux and macOS. You might run into some issues in Windows.  Download all tar files, train.csv and valid.csv of CheXpert dataset from Stanford Mirror. Put them under ./data/default_split folder and untar all tar files. Create & source a new virtualenv. Python >= 3.6 is required. Install dependencies by running pip3 install -r requirements.txt. Copy sample_config.ini to config.ini, you may customize batch_size and training parameters here. Make sure config.ini is configured before you run training or testing Run python train.py to train a new model. If you want to run the training using multiple GPUs, just prepend CUDA_VISIBLE_DEVICES=0,1,... to restrict the GPU devices. nvidia-smi command will be helpful if you don't know which device are available. Run python test.py to evaluate your model on the test set.  Important notice for CUDA 9 users If you use >= CUDA 9, make sure you set tensorflow_gpu >= 1.5. Author Bruce Chou (brucechou1983@gmail.com) Editor Ali Gholami (ali.gholami@sharif.edu) License MIT ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/atharvakadethankar/try_chexpert/blob/30c06835c32121039613ca48a8e8e7f015836f0e/experiments/1/best_weights.h5", "https://github.com/atharvakadethankar/try_chexpert/blob/30c06835c32121039613ca48a8e8e7f015836f0e/experiments/1/weights.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1901.07031"]}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6489"}, "repo_url": "https://github.com/jianz94/e-lstm-d", "repo_name": "e-lstm-d", "repo_full_name": "jianz94/e-lstm-d", "repo_owner": "jianz94", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T16:19:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T08:40:12Z", "homepage": null, "size": 264, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185003964, "is_fork": false, "readme_text": "e-lstm-d This is a TensorFlow implementation of the paper: E-LSTM-D: A Deep Learning Framework for Dynamic Network Link Prediction. The baselines used in the paper will be released as a toolbox soon. Requirements  tensorflow (1.3.0) keras (2.2.4) scikit-learn (0.19.0) numpy (1.14.2)  run the demo  prepare the data  mkdir data tar -xzvf contact.tar.gz ./data   train model  python train_model.py --dataset contact --encoder [128] --lstm [256,256] --decoder [274] --num_epochs 1600 --batch_size 32 --BETA 10 --learning_rate 0.001  Cite Please cite our paper if you use this code in your own work: @article{chen2019lstm,   title={E-LSTM-D: A Deep Learning Framework for Dynamic Network Link Prediction},   author={Chen, Jinyin and Zhang, Jian and Xu, Xuanheng and Fu, Chengbo and Zhang, Dan and Zhang, Qingpeng and Xuan, Qi},   journal={arXiv preprint arXiv:1902.08329},   year={2019} }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1902.08329"]}, {"_id": {"$oid": "5cf5186d7eb8d666b0df648a"}, "repo_url": "https://github.com/craigdsouza97/Handwriting-Recognition-using-CNN-", "repo_name": "Handwriting-Recognition-using-CNN-", "repo_full_name": "craigdsouza97/Handwriting-Recognition-using-CNN-", "repo_owner": "craigdsouza97", "repo_desc": "Handwritten Character Recognition using CNN with 89% accuracy", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T22:28:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T19:49:10Z", "homepage": "", "size": 88538, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185075438, "is_fork": false, "readme_text": "Description This is a Convolutional Neural Network with 28 Convolutional Layers that has been trained using the Emnist_Balanced Dataset to recognise Handwritten Characters and has an accuracy of 89%. Usage  Using Pre-trained models  First extract the models from Models/Models.rar The models require a 28X28 size binary image   Using it as an api  First create an empty folder named 'Data' Then extract the models from Models/Models.rar into Models Run Main.py It requires a normal Handwrtten Document with a plain background as input. Images need to be encoded into base64 strings and sent via post preprocessing.py is used to perform preprocessing as well as segment the image for character recognition.    Requirements  python 3.6 tensorflow keras flask (for using it as an api) opencv 4.0 Emnist_Balanced Dataset which can be downloaded from https://www.nist.gov/node/1298471/emnist-dataset or https://www.kaggle.com/crawford/emnist and must be added to the folder Train  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df648b"}, "repo_url": "https://github.com/piginzoo/captcha.v2", "repo_name": "captcha.v2", "repo_full_name": "piginzoo/captcha.v2", "repo_owner": "piginzoo", "repo_desc": "captcha.v2", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T06:43:17Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T03:23:17Z", "homepage": null, "size": 86, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184973724, "is_fork": false, "readme_text": "\u4f7f\u7528CNN\u8bc6\u522b\u9a8c\u8bc1\u7801 \u4f7f\u7528\u65b9\u6cd5 \u6ce8\uff1a  \u8bad\u7ec3\u6570\u636e\u5b58\u653e\u5728data/validate/ \u9a8c\u8bc1\u6570\u636e\u5b58\u653e\u5728data/validate/ \u8bad\u7ec3\u548c\u8bc6\u522b\u65f6\uff0c\u4e0d\u7528\u6572\u76ee\u5f55\u540d\uff0c\u4f1a\u6839\u636ebank\u81ea\u52a8\u8bc6\u522b  \u8bad\u7ec3\uff1a py train.py --bank renfa|jianhang|nonghang \u5982\u9700\u5e2e\u52a9\u8bf7\u8f93\u5165\uff1apy train.py -h \u8bc6\u522b\u4e00\u5f20\u56fe\u7247\uff1a py predict.py --bank renfa --image abcd.jpg \u968f\u673a\u8bc6\u522b100\u5f20\uff1a py predict.py --bank renfa --test 100 \u751f\u6210\u4e00\u5f20\u56fe\u7247\uff1a\uff08\u5230out\u76ee\u5f55\uff0c\u7528\u4e8e\u89c2\u5bdf\u56fe\u50cf\u5904\u7406\u60c5\u51b5\uff09 py lib/image_process.py  --bank renfa --image abcd.jpg \u6587\u4ef6\u8bf4\u660e  /     \u6839\u76ee\u5f55\u4e0b\u662f\u51e0\u4e2a\u5165\u53e3\u6587\u4ef6 lib   \u76f8\u5173\u7684python\u6587\u4ef6 model \u5b58\u653e\u7740\u8bad\u7ec3\u5b8c\u7684\u6587\u4ef6 log   \u8bad\u7ec3\u7684\u7528\u7684\u65e5\u5fd7\u6587\u4ef6 web   Restful\u7684JSon API Web\u670d\u52a1 docker docker\u955c\u50cf\u548c\u5bb9\u5668\u4ee5\u53ca\u90e8\u7f72\u76f8\u5173\u7684\u811a\u672c data\u662f\u8bad\u7ec3\u548c\u9a8c\u8bc1\u6570\u636e  data/validate/ \u9a8c\u8bc1\u6570\u636e\uff0c\u5404\u4e2a\u5b50\u76ee\u5f55\u662f\u6bcf\u4e2abank\u7684 data/train/    \u8bad\u7ec3\u6570\u636e\uff0c\u5404\u4e2a\u5b50\u76ee\u5f55\u662f\u6bcf\u4e2abank\u7684   renfa jianhang nonghang   bin   \u5e38\u7528\u7684\u4e00\u4e9b\u6279\u5904\u7406  \u914d\u7f6e\u6587\u4ef6config.json \u4e3a\u4e86\u65b9\u4fbf\u914d\u7f6e\uff0c\u6211\u4eec\u505a\u4e86\u4e00\u4e2ajson\u7684\u914d\u7f6e\u6587\u4ef6\uff0c\u65b9\u4fbf\u52a0\u5165\u65b0\u7684bank  ''' \"name\": \"renfa\",    \u54ea\u4e2abank \"width\":160,        \u56fe\u50cf\u5bbd\u5ea6 \"height\":70,        \u56fe\u50cf\u9ad8\u5ea6 \"number\":4,         \u8bc6\u522b\u7801\u957f\u5ea6\uff08\u51e0\u4e2a\u5b57\u7b26\uff09 \"charset\": \"0aA\",   \u5bf9\u5e94\u7684\u5b57\u7b26\u96c6\uff08\u6bd4\u5982\u662f0:0-9,a:a-z,A:A-Z) \"img_binary_threshold\":240  \u4e8c\u503c\u5316\u65f6\u5019\u7684\u9608\u503c\uff0c\u5927\u90e8\u5206bank\u4e0d\u9700\u8981\u5b9a\u4e49\uff0c\u53ea\u5bf9\u5c11\u6570bank\u9700\u8981 \"img_remove_area\"   \u56fe\u50cf\u53bb\u71e5\u7684\u65f6\u5019\uff0c\u8981\u53bb\u6389\u7684\u56fe\u50cf\u566a\u70b9\u7684\u5927\u5c0f '''  \u5173\u4e8e\u90e8\u7f72 \u5728\u670d\u52a1\u5668\u4e0a\u91c7\u7528\u4e86docker\u65b9\u5f0f\u90e8\u7f72\uff0c\u53ea\u8981\u8fd0\u884c docker/build_run.sh \u5c31\u53ef\u4ee5build\u51fa\u6765\u4e00\u4e2aimage\uff0c\u5e76\u751f\u6210\u4e00\u4e2a\u5bb9\u5668\u3002 \u76ee\u5f55\u7ea6\u5b9a\uff1a\u4ee3\u7801\u4e00\u5b9a\u8981\u90e8\u7f72\u5728/home/captcha\u76ee\u5f55\u4e0b\uff0c\u6a21\u578b\u653e\u7f6e\u5728/home/captcha/model\u5185\u3002 \u5bb9\u5668\u4e2d\u4f9d\u8d56\u5916\u90e8\u4e24\u4e2a\u5185\u5bb9\uff1a 1\u3001\u65e5\u5fd7\uff1a\u672c\u8003\u8651\u7528run -v\u65b9\u5f0fmount\u4e00\u4e2a\u65e5\u5fd7\u76ee\u5f55\u3001\u6587\u4ef6\u4e0a\u53bb\uff0c\u4f46\u662f\u8fc7\u4e8e\u9ebb\u70e6\uff0c\u73b0\u5728\u91c7\u7528\u76f4\u63a5\u8f93\u51fa\u5230\u6807\u51c6\u63a7\u5236\u53f0\u4e0a\uff0c\u9700\u8981\u67e5\u770b\u7684\u65f6\u5019\uff0c\u4f7f\u7528docker logs -f/--tail N\u65b9\u5f0f\u67e5\u770b 2\u3001\u6a21\u578b\u6587\u4ef6\uff1a\u4f7f\u7528run -v\u65b9\u5f0f\u6302\u67b6/home/captcha/model\u76ee\u5f55\u5230\u5bb9\u5668\u5185\u7684/home/captcha/model\u76ee\u5f55\uff0c\u91cc\u9762\u5b58\u653e\u7740\u751f\u4ea7\u7528\u7684\u6a21\u578b\u6587\u4ef6 \u6279\u6b21\u6587\u4ef6 bin/image.sh    \u7528\u4e8e\u6d4b\u8bd5\u56fe\u7247\u5904\u7406\u60c5\u51b5\uff08\u5982\u4e8c\u503c\u5316\uff0c\u53bb\u566a\uff09 bin/predict.sh  \u7528\u4e8e\u9884\u6d4b\u5355\u5f20\u56fe\u7247\u6216\u8005\u9884\u6d4b\u6279\u91cf\u56fe\u7247 bin/stress.sh   \u7528\u4e8e\u505a\u538b\u529b\u6d4b\u8bd5 bin/train.sh    \u7528\u4e8e\u505a\u8bad\u7ec3 docker/build_run.sh     \u7528\u4e8e\u6784\u5efa\u4e00\u4e2a\u751f\u4ea7\u955c\u50cf(image)\u5e76\u521b\u5efa\u8fd0\u884c\u6001\u7684\u5bb9\u5668(container) docker/deploy.sh        \u7528\u4e8e\u91cd\u65b0\u90e8\u7f72\u4ee3\u7801 web/startup.sh  \u7528\u4e8e\u542f\u52a8Gunicorn Web\u670d\u52a1\u5668  \u73af\u5883\u4f9d\u8d56 opencv      3.4.0   https://opencv.org/ tensorflow  1.5     https://tensorflow.org/ skimage     0.13.1  http://scikit-image.org/ keras       2.1.3   https://keras.io/ flask       0.12.2  http://flask.pocoo.org/  \u8bbe\u8ba1\u601d\u8def \u95ee\u9898\u63d0\u51fa\uff1a \u8fc7\u53bb\u91c7\u7528\u5207\u5272\u7684\u65b9\u6cd5\uff0c\u4f7f\u7528\u4f20\u7edf\u7684\u56fe\u50cf\u8bc6\u522b\u7b97\u6cd5\u8fdb\u884c\u9a8c\u8bc1\u7801\u8bc6\u522b\uff0c\u6548\u679c\u4e0d\u662f\u7279\u522b\u597d\uff0c\u8fd9\u6b21\u91c7\u7528\u6df1\u5ea6\u5b66\u4e60\u7684\u65b9\u6cd5\u6765\u8fdb\u884c\u8bc6\u522b\uff0c\u5e76\u6ca1\u6709\u5206\u5272\uff0c\u800c\u662f\u76f4\u63a5\u4e00\u4f53\u6254\u7ed9CNN\u53bb\u8bc6\u522b\u3002 \u8bbe\u8ba1\u601d\u8def\uff1a \u672c\u6253\u7b97\u8fd8\u662f\u91c7\u7528\u5207\u7247\u7684\u65b9\u5f0f\uff0c\u5373\u5148\u5bf9\u56fe\u7247\u8fdb\u884c\u5206\u5272\uff0c\u7136\u540e\u8fdb\u884c\u5355\u72ec\u7684\u8bc6\u522b\uff0c\u4ea4\u6d41\u540e\uff0c\u4ed6\u4eec\u4e4b\u524d\u5df2\u7ecf\u8fdb\u884c\u8fc7\u5927\u91cf\u7684\u5c1d\u8bd5\uff0c\u62c6\u5206\u56fe\u7247\u6548\u679c\u4e0d\u597d\uff0c\u6240\u4ee5\u679c\u65ad\u653e\u5f03\uff0c\u76f4\u63a5\u91c7\u7528\u4e00\u4f53\u8bc6\u522b\uff0c\u5373\u628a5\u4e2a\u5b57\u7b26\u4e00\u8d77\u8bc6\u522b\u7684\u65b9\u5f0f\u3002\u53e6\u5916\uff0c\u56fe\u7247\u8fdb\u884c\u4e86\u9884\u5904\u7406\uff0c\u7070\u5ea6\u5316->\u4e8c\u503c\u5316->\u53bb\u566a\u70b9\u56e0\u4e3a\u62c5\u5fc3\u8bc6\u522b\u6548\u679c\uff0c\u8fd8\u662f\u53bb\u7f51\u4e0a\u505a\u4e86\u4e00\u756a\u8c03\u7814\uff0c\u53d1\u73b0\u4e86\u4e00\u4e9b\u89e3\u51b3\u65b9\u6848\uff0c\u90fd\u53ef\u4ee5\u652f\u6301\uff1a \u53c2\u8003\uff1a    http://blog.csdn.net/Gavin__Zhou/article/details/52071797    \u8fd9\u4e2a\u8c8c\u4f3c\u4e0d\u9519\uff0c\u6709github\u4ee3\u7801\uff0c\u8003\u4e86\u7c98\u8fde\uff1ahttps://github.com/iamGavinZhou/py-captcha-breaking zxinbloom@gmail.com \u4e0d\u8fc7\uff0c\u8fd9\u4e2a\u54e5\u4eec\u540e\u6765\u53c8\u64b8\u4e86\u4e00\u4e2a\u56fa\u5b9a\u957f\u5ea6\u7684\uff0chttp://blog.csdn.net/gavin__zhou/article/details/69049663 \u6211\u90fd\u5f97\u770b\u770b\u3002    https://www.jianshu.com/p/86489f1afd36 \u8fd9\u4e2a\u662f\u7aef\u5230\u7aef\u7684\u505a\u6cd5\uff0c\u4e0d\u5207\u5272\u4e86\uff0c    (1). \u628aOCR\u7684\u95ee\u9898\u5f53\u505a\u4e00\u4e2a\u591a\u6807\u7b7e\u5b66\u4e60\u7684\u95ee\u9898\u30024\u4e2a\u6570\u5b57\u7ec4\u6210\u7684\u9a8c\u8bc1\u7801\u5c31\u76f8\u5f53\u4e8e\u67094\u4e2a\u6807\u7b7e\u7684\u56fe\u7247\u8bc6\u522b\u95ee\u9898\uff08\u8fd9\u91cc\u7684\u6807\u7b7e\u8fd8\u662f\u6709\u5e8f\u7684\uff09\uff0c\u7528CNN\u6765\u89e3\u51b3\u3002 (2). \u628aOCR\u7684\u95ee\u9898\u5f53\u505a\u4e00\u4e2a\u8bed\u97f3\u8bc6\u522b\u7684\u95ee\u9898\uff0c\u8bed\u97f3\u8bc6\u522b\u662f\u628a\u8fde\u7eed\u7684\u97f3\u9891\u8f6c\u5316\u4e3a\u6587\u672c\uff0c\u9a8c\u8bc1\u7801\u8bc6\u522b\u5c31\u662f\u628a\u8fde\u7eed\u7684\u56fe\u7247\u8f6c\u5316\u4e3a\u6587\u672c\uff0c\u7528    CNN+LSTM+CTC\u6765\u89e3\u51b3\u3002\u4f5c\u8005\u73a9\u7684\u662f\u7b2c\u4e00\u79cd\u65b9\u6cd5\uff0c\u6211\u4eec\u53ef\u4ee5\u8bd5\u8bd5\u4e5f\u3002    https://github.com/moxiegushi/zhihu \u4ece\u77e5\u4e4e\u4e0a\u722c\u53d6\uff0c\u6d4b\u8bd5\u7684\uff0c\u6709\u70b9\u610f\u601d\uff0c\u5148\u8bb0\u4e0b     http://www.bubuko.com/infodetail-1877150.html \u4e2d\u6587\u7684\uff0c\u7559\u7740\u770b\uff0c    5.http://blog.csdn.net/yinchuandong2/article/details/40340735 \u8fd9\u7bc7\u867d\u7136\u8001\u70b9\uff0c\u4f46\u662f\u6709cfs\u7684\u5207\u5272\uff0c\u56fe\u50cf\u5207\u5272\u7684\u529e\u6cd5\uff0c\u53ef\u4ee5\u53c2\u8003    http://www.doc88.com/p-9874925209090.html \u8fd9\u7bc7\u662f\u4e00\u4e2a\u4f20\u7edf\u65b9\u6cd5\u7684\uff0c\u4f46\u662f\u7528\u5230\u5f88\u591a\u65b9\u6cd5\uff0c\u53ef\u4ee5\u53c2\u8003\uff0c\u4f46\u662f\u4ec5\u80fd\u5728\u7ebf\uff0c\u65e0\u6cd5\u4e0b\u8f7dpdf    7.https://www.baitiwu.com/t/topic/16027 \u6700\u540e\u770b\u7684\u8fd9\u7bc7\uff0c\u633a\u795e\u5947\uff0c\u601d\u8def\u7b80\u5355\u7c97\u66b4\u554a\uff0c\u76f4\u63a5\u66b4\u529b\u7834\u89e3\uff0c\u72e0\u559c\u6b22    https://www.cnblogs.com/SeekHit/p/7806953.html \u53d7\u4e0d\u4e86\u4e86\uff0c\u8fd9\u7bc7\u66f4\u66b4\u529b\uff0c\u8981\u8dd1\u6b7b\u7684\u8282\u594f\u554a\uff0c\u603b\u5171 62^4 \u79cd\u7c7b\u578b\uff0c\u91c7\u7528 4 \u4e2a one-hot \u7f16\u7801\u5206\u522b\u8868\u793a 4 \u4e2a\u5b57\u7b26\u53d6\u503c\u3002\u5662\uff0c\u6655\u4e86\uff0c\u5176\u5b9e\u548c\u4e0a\u4e00\u7bc7\u662f\u540c\u6837\u7684\u601d\u8def    9.http://blog.topspeedsnail.com/archives/10858 \u518d\u6765\u4e00\u7bc7\uff0c\u64b8\u8d77\u6765\uff0c\u8fd9\u4e2a\u5f88\u8d5e\uff0c\u6211\u559c\u6b22\u7684\u601d\u8def\uff0c\u53c8\u662f\u4e0d\u5206\u5272\u7684\uff0c\u54c8\u54c8 \u8fd9\u54e5\u4eec\u7684\u4ee3\u7801\u8c8c\u4f3c\u9760\u8c31\uff0c\u4f46\u662f\u6709\u4eba\u53cd\u9988resize\u540e\u5c31\u6709\u95ee\u9898\u4e86\u3002\u6211\u8bd5\u8bd5\u3002   10.https://github.com/luyishisi/Anti-Anti-Spider/tree/master/1.%E9%AA%8C%E8%AF%81%E7%A0%81/tensorflow_cnn   \u5176\u4e2d\uff0c9\u300110\u7684\u601d\u8def\u6700\u63a5\u8fd1\uff0c\u4e8e\u662f\u679c\u65ad\u91c7\u7528\uff0c\u5f00\u59cb\u7f16\u5199\u4ee3\u7801\u3002 \u8be6\u7ec6\u8bbe\u8ba1 \u8bbe\u8ba1\u601d\u8def\u6bd4\u8f83\u6e05\u6670\uff0c\u5c31\u662f\u628a\u6574\u4e2a5\u4f4d\u7684\u9a8c\u8bc1\u7801\u56fe\u50cf\uff0c\u4f5c\u4e3a\u4e00\u4e2a\u6574\u4f53\u5582\u7ed9CNN\u7f51\u7edc\uff0c\u6574\u4e2a\u662f\u8f93\u5165\u7684x\uff0c\u800c\u5bf9\u5e94\u7684y\u662f\u4e00\u4e2a36(\u5c0f\u5199\u5b57\u7b26+\u6570\u5b57\uff09x\u8bc6\u522b\u5b57\u7b26\u6570\uff085\uff09=180\u7684\u4e00\u4e2aOne-hot\u5411\u91cf\uff0c\u9047\u5230\u7f3a\u5c11\u7684\u5b57\u7b26\uff0c\u90a336\u4f4d\u5c31\u5168\u90e8\u7f6e\u96f6\u3002 \u6837\u5b50\u5982\u4e0b\uff1a  [0,0,0\u2026.1,\u2026.,0,0, | 0,0,0.1,\u2026.,0,0,...|,0,0,0\u2026.1,\u2026...,0, |,0,0,0\u2026.1,\u2026..,0,0]  \u704c\u5165\u540e\uff0c\u5728CNN\u7f51\u7edc\u7684\u6700\u540e\u4e00\u5c42\uff0c\u8981\u4f7f\u7528sigmod\u51fd\u6570\uff0c\u800c\u6ca1\u6709\u4f7f\u7528softmax\uff0c\u800c\u635f\u5931\u51fd\u6570\u4f9d\u7136\u662f\u4ea4\u53c9\u71b5\uff1f\u4e3a\u4ec0\u4e48\u8981\u8fd9\u4e48\u4e00\u4e2a\u8bbe\u8ba1\u5462\uff1f  model.add(Dense(num_classes, activation='sigmoid'))   model.compile( loss='binary_crossentropy',  \u56e0\u4e3asoftmax\u5f80\u5f80\u6700\u540e\u5f97\u5230\u7684\u662f\u591a\u5206\u7c7b\u4e2d1/N\uff08\u6211\u4eec\u8fd9\u91cc\u5c31\u662f1/180\uff09\u7684\u7ed3\u679c\uff0c\u4f46\u662f\u6211\u4eec\u7684\u8fd9\u4e2a\u5b9e\u9645\u4e0a\u8981\u5f97\u5230\u7684\u662f5/N\uff08\u6211\u4eec\u8fd9\u91cc\u662f5/180\uff09\u7684\u5206\u7c7b\u95ee\u9898\uff0c\u4e5f\u5c31\u662f\u591a\u4e2a\u5206\u7c7b\u90fd\u6ee1\u8db3\u8981\u6c42\u7684\u7ed3\u679c\u3002\u8fd9\u79cd\u60c5\u51b5\u4e0b\uff0c\u5c31\u5fc5\u987b\u8981\u91c7\u7528binary_crossentropy\u7684\u635f\u5931\u51fd\u6570\u3002\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\uff0c\u9700\u8981\u6211\u4eec\u5728\u5582\u7ed9\u8fd9\u4e2a\u635f\u5931\u51fd\u6570\u4e4b\u524d\u7684\u5f20\u91cf\u8981\u505a\u4e00\u6b21sigmod\u53d8\u6362\u3002 \u6700\u540e\u5728\u8bad\u7ec3\u7684\u8bc4\u4ef7\u51fd\u6570\u4e0a\uff0c\u901a\u8fc7\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684\u635f\u5931\u51fd\u6570\uff0c\u6765\u5bf9\u6bd4\u6807\u7b7e\u76845/180\u7684\u7ed3\u679c\uff0c\u548c\u6211\u7684\u6982\u7387\u5206\u5e03\u7684\u7ed3\u679c\uff0c\u6211\u7684\u6982\u7387\u5206\u5e03\u4e5f\u662f180\u7ef4\u5ea6\u7684\u5411\u91cf\uff0c\u4f46\u662f\u662f\u6bcf\u4e2a\u7ef4\u5ea6\u90fd\u6709\u503c\uff0c\u6211\u9700\u8981\u514836\u4e2a\u5206\u5272\u4e00\u4e2a\uff0c\u5206\u5272\u51fa5\u4e2a36\u7ef4\u5ea6\u7684\u5411\u91cf\uff0c\u7136\u540e\u627e\u51fa36\u7ef4\u5ea6\u91cc\u9762\u6700\u9ad8\u7684\u90a3\u4e2a\u7ef4\u5ea6\uff0c\u7f6e\u4e3a1\uff0c\u5176\u4ed635\u7ef4\u5ea6\u7f6e\u4e3a0\uff0c\u5f62\u6210\u4e00\u4e2a36\u7ef4\u5ea6\u7684one-hot\u5411\u91cf\uff0c\u7136\u540e\u62fc\u63a5\u6210180\u7ef4\u5ea6\u76845/180\uff0c\u548c\u6807\u7b7e5/180\u505a\u5bf9\u6bd4\uff0c\u4e00\u81f4\u5c31\u8ba4\u4e3aok\uff0c\u4e0d\u4e00\u81f4\u5c31\u8ba4\u4e3a\u4e0dok\u3002\u6240\u4ee5\uff0c\u8fd9\u4e2a\u8fc7\u7a0b\uff0c\u9700\u8981\u81ea\u5b9a\u4e49\u4e00\u4e2ametrics\u51fd\u6570\u3002 \u9047\u5230\u95ee\u9898  \u9996\u5148\u662f\u56fe\u50cf\u9884\u5904\u7406\uff0c\u91c7\u7528opencv\uff0c\u505a\u4e8c\u503c\u5316\u5904\u7406\uff1b\u7136\u540e\u4f7f\u7528skimage\u5bf9\u56fe\u50cf\u505a\u4e86\u566a\u70b9\u53bb\u9664\uff1b\u4f46\u662f\uff0c\u5bf9\u5e72\u6270\u7ebf\u5b9e\u5728\u662f\u53bb\u4e0d\u6389\uff0c\u548c\u4e5f\u8fdb\u884c\u8fc7\u4ea4\u6d41\uff0c\u56e0\u4e3ajianhang\u7684\u56fe\u7247\u7684\u5e72\u6270\u7ebf\u548c\u56fe\u50cf\u7684\u989c\u8272\u5f88\u76f8\u8fd1\uff0c\u56e0\u6b64\u5f88\u96be\u53bb\u6389\uff0c\u6240\u4ee5\u6700\u540e\u4f5c\u7f62 9\u300110\u4e2d\u4f7f\u7528\u7684\u662f\u76f4\u63a5\u7684tensorflow\uff0c\u6211\u6ca1\u6709\u91c7\u7528\u4ed6\u4eec\u7684\u4ee3\u7801\uff0c\u800c\u662f\u4f7f\u7528keras\uff0c\u4e00\u4e2a\u662f\u6211\u60f3\u5bf9\u719f\u6089\u4e00\u4e9b\uff0c\u53e6\u5916\u4e00\u4e2a\u539f\u56e0\u662fkeras\u7684\u4ee3\u7801\u66f4\u7b80\u6d01\u548c\u5bb9\u6613\u7ef4\u62a4 \u76ee\u524d\u5076\u5c14\u9047\u5230\u4e00\u4e9b\u6570\u636e\u662f4\u4f4d\u7684\uff0c\u6807\u51c6\u662f5\u4f4d\u7684\uff0c\u5bf9\u4e8e\u7f3a\u4f4d\u7684\u5904\u7406\u662f\u5728\u5b57\u7b26\u9636\u6bb5\u7528\"_\"\u4ee3\u66ff\uff0c\u5728 \u8fd9\u91cc\u6211\u6709\u4e00\u4e2a\u975e\u5e38\u56f0\u60d1\u7684\u5730\u65b9\uff1a  \u56e0\u4e3a\u6211\u7406\u89e3\u7684\u4e00\u822c\u5728\u4ea4\u53c9\u71b5\u7684\u8ba1\u7b97\u4e0a\uff0c\u90fd\u662f\u4e24\u4e2a\u6982\u7387\u5206\u5e03\u7684\u8ba1\u7b97\uff0c\u65e2\u7136\u662f\u6982\u7387\u5206\u5e03\uff0c\u4e00\u5b9a\u662f\u5f52\u4e00\u5316\u5f97\uff0c\u800c\u6211\u4eec\u7684\u6837\u672c\uff0c\u4ee5\u53ca\u6211\u4eec\u7684CNN\u8ba1\u7b97\u7ed3\u679c\uff0c\u663e\u7136\u90fd\u4e0d\u662f\u5f52\u4e00\u5316\u7684\u3002\u6211\u4eec\u7684\u6837\u672c\u662f180\u7ef4\u5ea6\u4e0a\u67095\u4e2a1\uff0c\u6211\u4eec\u7684\u7ed3\u679c\u4e0a\u66f4\u662f\u5938\u5f20\uff0c\u7ed3\u679csum\u5230\u4e00\u8d77\u8fde5\u90fd\u4e0d\u662f\uff0c\u800c\u662f\u7ea6\u7b49\u4e8e7\u3002  [DEBUG] \u6a21\u578b\u9884\u6d4b\u51fa\u6765\u7684\u7ed3\u679c\u662f\uff1a  array([[2.27676210e-05, 1.47138140e-03, 2.10353086e-04, 6.11234969e-03,         2.60898820e-03, 8.84016603e-03, 5.93697906e-01, 2.26049320e-04,         1.70289129e-01, 7.46237068e-03, 9.22823325e-03, 3.46448243e-04,         6.10056275e-04, 1.16427680e-02, 5.61858434e-03, 1.01848654e-02,         2.52747093e-04, 1.50831360e-02, 8.88945724e-05, 7.16042996e-04, [DEBUG] \u9884\u6d4b\u7ed3\u679cshape\u662f\uff1a(1, 180) [DEBUG] \u9884\u6d4b\u7ed3\u679cSum\u4e3a:6.8608074   \u5728\u81ea\u5b9a\u4e49metrics\u51fd\u6570\u7684\u7f16\u5236\u8fc7\u7a0b\u4e2d\uff0c\u9047\u5230\u4e00\u4e9b\u95ee\u9898\uff0c\u56e0\u4e3a\u8fd9\u4e2a\u51fd\u6570\u5b9e\u5728\u7f16\u8bd1\u9636\u6bb5\u8981\u6784\u4ef6\u597d\uff0c\u800c\u5728\u8fd0\u884c\u6001\u624d\u6267\u884c\u7684\uff0c\u6240\u4ee5\uff0c\u4e0d\u80fd\u76f4\u63a5\u4f7f\u7528\u8bf8\u5982ndarray\u7684\u4e00\u4e9b\u51fd\u6570\u6765\u505aone-hot\u8f6c\u5316\uff0c\u6bd4\u5982to_categorical\u51fd\u6570\uff0c\u5426\u5219\uff0c\u5728\u7f16\u8bd1\u9636\u6bb5\u5c31\u4f1a\u62a5\u9519\u3002\u540e\u6765\u5168\u90e8\u91c7\u7528tf\u7684\u51fd\u6570\uff0c\u5982tf.shape/tf.argmax/tf.reduce_mean\uff0c\u4fdd\u8bc1\u4ed6\u53ea\u662f\u64cd\u4f5c\u8fd9\u4e9b\u5f20\u91cf\u53d8\u91cf\uff0c\u4fdd\u8bc1\u7f16\u8bd1\u5668\u901a\u8fc7\u3002 \u53e6\u5916metrics\u7684\u8c03\u8bd5\u6781\u5176\u8d39\u52b2\uff0c\u65e0\u6cd5\u770b\u5230\u7ed3\u679c\uff0c\u5c1d\u8bd5\u4e86tf.Print\u6765\u6253\u5370\uff0c\u4f46\u662f\u6ca1\u51d1\u6548\uff0c\u4e0d\u77e5\u9053\u4ec0\u4e48\u539f\u56e0\uff0c\u4e0b\u4e2a\u9636\u6bb5\u8fd8\u662f\u8981\u7ee7\u7eed\u6df1\u5165  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://blog.csdn.net/Gavin__Zhou/article/details/52071797", "http://blog.topspeedsnail.com/archives/10858", "http://www.doc88.com/p-9874925209090.html", "http://www.bubuko.com/infodetail-1877150.html", "http://blog.csdn.net/yinchuandong2/article/details/40340735", "http://blog.csdn.net/gavin__zhou/article/details/69049663"], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df648c"}, "repo_url": "https://github.com/nicolas-racchi/digit_recognizer", "repo_name": "digit_recognizer", "repo_full_name": "nicolas-racchi/digit_recognizer", "repo_owner": "nicolas-racchi", "repo_desc": "A Python implementation of the popular MNIST dataset application of Digit Recognition with Convolutional Neural Networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-17T20:57:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T14:01:37Z", "homepage": null, "size": 2240, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185038812, "is_fork": false, "readme_text": "Digit Recognizer A Python implementation of the popular MNIST dataset application of Digit Recognition with Convolutional Neural Networks This project was made to demonstrate a Python implementation of web inference capability. The network was trained on the MNIST database in Jupyter Notebook with the Keras library with Tensorflow backend. The recognition error on the test data set is 0.75% after 12 epochs, without hyperparameter tuning. The convolutional neural network I used is an 8-layer Sequential model. This project is available as a full featured demo here: http://www.digitrecognition.nicolasracchi.com In this demo there is also a brief explanation of how the neural network works. Installation  Just cd in the flask_3 folder, activate your virtual environment, and run:  pip install -r requirements.txt flask run ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/nicolas-racchi/digit_recognizer/blob/1389113debfaad7049d2141a6191a1a0f326d8cd/flask_3/model/model.h5"], "see_also_links": ["http://www.digitrecognition.nicolasracchi.com"], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df648d"}, "repo_url": "https://github.com/tatsuya068/keras_sample", "repo_name": "keras_sample", "repo_full_name": "tatsuya068/keras_sample", "repo_owner": "tatsuya068", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T08:05:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T13:22:37Z", "homepage": null, "size": 1661, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185034347, "is_fork": false, "readme_text": "keras_sample ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df648e"}, "repo_url": "https://github.com/josehoras/LSTM-Frameworks", "repo_name": "LSTM-Frameworks", "repo_full_name": "josehoras/LSTM-Frameworks", "repo_owner": "josehoras", "repo_desc": "An investigation of LSTM  resurrent networks in pure Python, TensorFlow, and Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T17:55:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T17:52:14Z", "homepage": null, "size": 2162, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185064173, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df648f"}, "repo_url": "https://github.com/aplusplusproject/realTime_face_analysis", "repo_name": "realTime_face_analysis", "repo_full_name": "aplusplusproject/realTime_face_analysis", "repo_owner": "aplusplusproject", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-13T12:43:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T16:46:03Z", "homepage": null, "size": 81664, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185057634, "is_fork": false, "readme_text": "Real-time Facial Recognition using YOLO and FaceNet Available Funtions  Face Alignment: - YOLO v3. Training on FaceNet: Transfer learning Real-time Facial Recognition: OpenCV rendering  Configuration  OS: Windows 10 / Ubuntu 18.04 GPU: NVIDIA GeForce GTX 1060 CUDA TOOLKIT: v9.0 cuDNN SDK: v7.5 (corresponding to CUDA TOOLKIT v9.0) Python: 3.x tensorflow-gpu: 1.10.1    Face Alignment. You can use align_dataset_yolo_gpu.py. NEED TO DOWNLOAD MANUALLY First, use get_models.sh in \\align\\yolo_weights\\ to get the pre-trained model of YOLO. Then create a folder in \\align and name it as \"unaligned_faces\", put all your images in this folder. In \\align\\unaligned_faces, one person has one folder with his/her name as folder name and all his/her images should be put in the corresponding folder. Finally run $ python align_dataset_yolo_gpu.py The result will be generated in \\aligned_faces folder, copy all the results with the class folder to /output folder for later use.   Training FaceNet Model Currently Skipped   If you want to directly use a pre-trained model for facial recognition, just skip this step.   If you want to implement a tranfer learning with a pre-trained model and your own dataset, you need to first download this pre-trained model, put it in /models and unzip it. Make sure that the directory /models/20170512-110547 has 4 files. Then run $ python train_tripletloss.py The trained model will be in the /models/facenet.   If you want to train your own model from scratch. In train_tripletloss.py line 433, there is an optional argument named \"--pretrained_model\", delete its default value. Then run again $ python train_tripletloss.py The trained model will also be in the /models/facenet.     Training SVM Model In Make_classifier_knnForFaces.py / Make_classifier_svmForFaces.py,  change the \"modeldir\" variable to your own FaceNet model path. If you have trained a model already, just use the corresponding path, otherwise there are several pre-trained model you can use: VGGFace2: 20180402-114759 Then run   $ python Make_classifier_svmForFaces.py The SVM model will be generated in \\myclassifier.   Real-time Facial Recognition There are two versions \u2014  realtime_facenet_yolo_gpu_2.py and realtime_facenet_yolo_gpu_http_streaming2.py . First Modify the \"url_of_ip_camera\" variable in realtime_facenet_yolo_gpu_http_streaming2.py to do real time streamming with an IP camera. Then run   $ python realtime_facenet_yolo_gpu_2.py or   $ python realtime_facenet_yolo_gpu.py   References A special thanks to the following:   davidsandberg https://github.com/davidsandberg/facenet Provided FaceNet code for training and embedding generation   sthanhng https://github.com/sthanhng/yoloface Provided a YOLO model trained on WIDER FACE for real-time facial detection   cryer https://github.com/cryer/face_recognition Provided a framework for moving images from webcam to model, model to real-time on-screen bounding boxes and names   https://github.com/Tony607/Keras_age_gender?fbclid=IwAR3g_k362ygrSRIKMttW0vzFP_G00juSUmUQywuu5BHsZ9p41u5JuDkpAbI Provided a framework for easy real time gender age prediction from webcam video with Keras   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/aplusplusproject/realTime_face_analysis/blob/c9845f3ebcef73f296d6a2b1346ca8cd71c10fff/align/yolo_weights/model-weights/__MACOSX/._YOLO_Face.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6490"}, "repo_url": "https://github.com/Bob-Chou/skelenet", "repo_name": "skelenet", "repo_full_name": "Bob-Chou/skelenet", "repo_owner": "Bob-Chou", "repo_desc": "Skeleton-based human action recognition with Tensorflow / Keras implementation of spatial-temporal graphic convolutional network (ST-GCN)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T12:16:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T14:13:12Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185040109, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6491"}, "repo_url": "https://github.com/bmehighday/bone-age-algorIthm", "repo_name": "bone-age-algorIthm", "repo_full_name": "bmehighday/bone-age-algorIthm", "repo_owner": "bmehighday", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T08:22:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T03:56:04Z", "homepage": null, "size": 54, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184976916, "is_fork": false, "readme_text": "Bone-age-algorithm This repository contains the code used for the paper- Diagnostic Performance of Convolutional Neural Network-Based TW3 Bone Age Assessment system. The code mainly contains the following modules:  Pre-processing module Positioning module for key points detection of an X-ray image, including training and prediction Cropping module for epiphysis ROIs Classifying module for epiphysis ROIs, including training and prediction Bone age calculating module  Dependencies The main programs and third-party libraries included in the operating environment:  Operating environment:  Python 3.7.3 anaconda3-4.3.14   Third-party libraries:  image 1.5.27 Keras 2.2.4 opencv 4.1.0.25 tensorflow 1.13.1 tqdm 4.31.1    How to use it? Run the script in turn after the data is stored in the data directory in a certain format. The configuration file is named as config.json, and the index path of the related files can be modified by modifying this file. Data storage  Input image directory  Input the directory where the image is placed, and the placement location is\uff1aconfig.json -> input_img_dir  Input name list  Input the name list of the image, and the placement location is\uff1aconfig.json -> annos -> img_name_csv As the following format: | img_name | | :--- : | | file name 1 | | file name 2 | | ...... |  Input gender list  Input the children's gender mapping table corresponding to the input images, and the placement location is: config.json -> annos -> gender_csv As the following format:    img_name gender     file name 1 gender 1\uff08M for male and F for female\uff09   file name 2 gender 2   ...... ......     Epiphysis classifying table  This is the epiphysis classifying table corresponding to TW3-method for the training dataset, and the placement location is: config.json -> annos -> bone_label_csv As the following format:    img_name bone_name label     image name 1 epiphysis A score of epiphysis A   image name 1 epiphysis B score of epiphysis B   ...... ...... ......    The score is in the form of a number, such as 0-A, 1-B...  Key annotation lookup table  This is the key points annotation positioning table for the training dataset, and the placement location is: config.json -> annos -> align_point_json_csv As the following format:    img_name json_path     image name 1 json path 1   image name 2 json path 2   ...... ......     Key points numbering list  Key points numbering name can be same as './crop/point_align'. Script running sequence After the data is properly placed and the dependencies installation is complete, run the script in order\uff1a  Prepare operating environment: python 1a_build_wksp.py Positioning module\uff1a  Train the module: python 2a_alignment_train.py Use the module to get positioning results: python 2b_alignment_infer.py   Crop ROIs based on positioning results\uff1a python 3a_crop_roi.py Classifying module\uff1a  Train the module\uff1a python 4a_cls_train.py Use the module to get classifying results: python 4b_cls_infer.py   Calculate bone age values based on classifying results and gender information\uff1a python 5a_calc_bone_age.py  It should be noted that, the folder named 'common' contains some public code and the folder named 'crop' contains some code for cropping purpose. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6492"}, "repo_url": "https://github.com/shailinhshah/CS365FinalProject", "repo_name": "CS365FinalProject", "repo_full_name": "shailinhshah/CS365FinalProject", "repo_owner": "shailinhshah", "repo_desc": "We trained our own neural net using Keras and Tensorflow and used it to recognize people's emotions and replaced their face with corresponding emojis.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T20:37:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T19:46:54Z", "homepage": null, "size": 51163, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185075227, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shailinhshah/CS365FinalProject/blob/a28d3a93cd11345de8d807bfb38edbf63b578fb0/model_2layer_2_2_pool.h5", "https://github.com/shailinhshah/CS365FinalProject/blob/a28d3a93cd11345de8d807bfb38edbf63b578fb0/model_4layer_2_2_pool.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6493"}, "repo_url": "https://github.com/Jp-Lin/ssd.pytorch", "repo_name": "ssd.pytorch", "repo_full_name": "Jp-Lin/ssd.pytorch", "repo_owner": "Jp-Lin", "repo_desc": "Deep learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T20:51:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T19:04:59Z", "homepage": null, "size": 494013, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185071263, "is_fork": false, "readme_text": "SSD: Single Shot MultiBox Object Detector, in PyTorch A PyTorch implementation of Single Shot MultiBox Detector from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found here.  Table of Contents  Installation Datasets Train Evaluate Performance Demos Future Work Reference  \u00a0 \u00a0 \u00a0 \u00a0 Installation  Install PyTorch by selecting your environment on the website and running the appropriate command. Clone this repository.  Note: We currently only support Python 3+.   Then download the dataset by following the instructions below. We now support Visdom for real-time loss visualization during training!  To use Visdom in the browser:  # First install Python server and client pip install visdom # Start the server (probably in a screen or tmux) python -m visdom.server  Then (during training) navigate to http://localhost:8097/ (see the Train section below for training details).   Note: For training, we currently support VOC and COCO, and aim to add ImageNet support soon.  Datasets To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit torch.utils.data.Dataset, making them fully compatible with the torchvision.datasets API. COCO Microsoft COCO: Common Objects in Context Download COCO 2014 # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/COCO2014.sh VOC Dataset PASCAL VOC: Visual Object Classes Download VOC2007 trainval & test # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2007.sh # <directory> Download VOC2012 trainval # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2012.sh # <directory> Training SSD  First download the fc-reduced VGG-16 PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth By default, we assume you have downloaded the file in the ssd.pytorch/weights dir:  mkdir weights cd weights wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth  To train SSD using the train script simply specify the parameters listed in train.py as a flag or manually change them.  python train.py  Note:  For training, an NVIDIA GPU is strongly recommended for speed. For instructions on Visdom usage/installation, see the Installation section. You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see train.py for options)    Evaluation To evaluate a trained network: python eval.py You can specify the parameters listed in the eval.py file by flagging them or manually changing them.  Performance VOC2007 Test mAP    Original Converted weiliu89 weights From scratch w/o data aug From scratch w/ data aug     77.2 % 77.26 % 58.12% 77.43 %    FPS GTX 1060: ~45.45 FPS Demos Use a pre-trained SSD network for detection Download a pre-trained network  We are trying to provide PyTorch state_dicts (dict of weight tensors) of the latest SSD model definitions trained on different datasets. Currently, we provide the following PyTorch models:  SSD300 trained on VOC0712 (newest PyTorch weights)  https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth   SSD300 trained on VOC0712 (original Caffe weights)  https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth     Our goal is to reproduce this table from the original paper    Try the demo notebook  Make sure you have jupyter notebook installed. Two alternatives for installing jupyter notebook:   If you installed PyTorch with conda (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run): jupyter notebook   If using pip:     # make sure pip is upgraded pip3 install --upgrade pip # install jupyter notebook pip install jupyter # Run this inside ssd.pytorch jupyter notebook  Now navigate to demo/demo.ipynb at http://localhost:8888 (by default) and have at it!  Try the webcam demo  Works on CPU (may have to tweak cv2.waitkey for optimal fps) or on an NVIDIA GPU This demo currently requires opencv2+ w/ python bindings and an onboard webcam  You can change the default webcam in demo/live.py   Install the imutils package to leverage multi-threading on CPU:  pip install imutils   Running python -m demo.live opens the webcam and begins detecting!  TODO We have accumulated the following to-do list, which we hope to complete in the near future  Still to come:   Support for the MS COCO dataset  Support for SSD512 training and testing  Support for training on custom datasets    Authors  Max deGroot Ellis Brown  Note: Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible. References  Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" ECCV2016. Original Implementation (CAFFE) A huge thank you to Alex Koltun and his team at Webyclip for their help in finishing the data augmentation portion. A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):  Chainer, Keras, MXNet, Tensorflow    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.image-net.org/", "http://pytorch.org/", "http://localhost:8888", "http://localhost:8097/", "http://mscoco.org/", "http://github.com/ellisbrown", "http://www.webyclip.com", "http://host.robots.ox.ac.uk/pascal/VOC/", "http://jupyter.readthedocs.io/en/latest/install.html", "http://pytorch.org/docs/torchvision/datasets.html"], "reference_list": ["http://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1409.1556", "http://arxiv.org/abs/1512.02325"]}, {"_id": {"$oid": "5cf5186d7eb8d666b0df6494"}, "repo_url": "https://github.com/prosperfields/Neural-Network-Trading-Bot", "repo_name": "Neural-Network-Trading-Bot", "repo_full_name": "prosperfields/Neural-Network-Trading-Bot", "repo_owner": "prosperfields", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T03:19:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-05T01:02:49Z", "homepage": null, "size": 824, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184959884, "is_fork": false, "readme_text": "Neural-Network-Trading-Bot Resources used: Nameko, Numpy, Oanda API, MongoDB, Keras (with Tensorflow backend), Datetime This is a currency (or other instrument) trading bot using Nameko microservices. It can be used quite easily, with the exception of a possible compatability bet on Python 3.7 and Nameko. DATA.PY The Oanda API is the first step. It provides the connection to get instrument prices. It has a 5000 call limit, which can be make training the neural network tedious. An authentification is required to connect, and after that the API is quite simple to use. We used MongoDB as a database to store candle information. We begin with the \"Data\" class. A nameko @timer decorator is used with the first function get_ohlc. This function returns the current hour candle. It has to return two candles because the current 1 hour candle will be incomplete, and we want the last completed candle. We also check to see if there any doubles just in case so that it would not train on the same candlestick if the nameko service was running and set to train on new candlesticks. Candlesticks are how many instrument prices are shown. They are effective because they can detail four different aspects of the price rather than just the price in that time interval. Candlesticks show the open, high, low, close prices during the time interval. Candlesticks come in different time intervals. 1 minute, 5 minutes, 1 hour, 1 day, 1 month are the usual candlestick time frames.  The second function, get_historical_data, is nearly identical to the get_ohlc function except that it currently is being used as a call to train the neural network in the next file. It also fetches candlesticks, however, it grabs the past 5000 candles. A neural network won't be useful with 5000 data points. 5000 one hour candles is 208.33 days. The to and from parameters in instruments.InsturmentCandles can specify dates to call candles from Oanda. get_historical_data also saves the candles to MongoDB. Although I currently bypass MongoDB (next file), if the bot were to be used for real trades, it would be advisable to save all the candles to MongoDB and then use the database to train the neural network. TRAINER.PY class Trainer also has two functions. The first, get_model, handles the neural network model. There you can add more or less nodes or change the neural network as wanted. The input shape is set to 4. The function returns the model. The second function, trainer preprocesses and cleans the data from the get_historical_data using a nameko proxy, and will then train the model. The data from Oanda is a large JSON, and 'mid' is the key for open, high, low, close price. Afterwards, the data is then put into the ratios: High to low, low to open, close to open, high to low. The neural network will train on these ratios, which really is used to standardize the prices. As we recently have learned, it is probably better to use an LSTM network rather than a regular artificial neural network (check future work). The retrain function trains the model and also saves it the model so we can use it in the Trader file. The network we use has a ReLu activation layer, and the next two layers are SoftMax activation layers. We were suprised to see that the sigmoid function did not work. This was a surprise because the sigmoid function gives binary values which is helpful to determine whether the price will rise or not. Instead, we worked with two SoftMax layers instead. We reduced the hidden layer nodes to 16 as the standard 64 is too many considering we only pull in the limit of 5000 candles. The process function processes both the candles and sets up for the output. It delivers the prediction based on a list of three numbers. The first number tells the percentage of a gain, it's really used to tell the chance that there will be no gain (price will stay the same). The second output states the percentage chance the neural network believes the price will increase. The third is the chance it wil decrease.  TRADER.PY class Trader brings in the data from MongoDB. It processes the data again, the same exact way it was processed earlier. It then loads the model, which is an interesting part of the project. The  model, as mentioned earlier, was saved and then called (hardcoded) and used to predict! The  numbers below shows the output!  FUTURE Perform trade or visualize it! We planned to visualize the data. The next method of guessing the price would be to see the chance probability of the three outputs. For example, if the output was [.0001,.0001,.80], it would signal that the price will definitely drop. We planned to visualize the data, however, the neural network took quite a while to get right. OBSTACLES We were unable to find much documentation on the GreenSSL Socket issue we were having when using the nameko service, so we instead created another Anaconda environment with Python 3.6 where I had to redownload all the necessary packages. That seem to be a quick work around. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d5a"}, "repo_url": "https://github.com/rzwck/c1-form-reader", "repo_name": "c1-form-reader", "repo_full_name": "rzwck/c1-form-reader", "repo_owner": "rzwck", "repo_desc": "Indonesia Presidential Election tally form (form C1) reader using deep learning (CNN) and computer vision", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T08:22:02Z", "repo_watch": 9, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-04T00:25:51Z", "homepage": "", "size": 42184, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184835257, "is_fork": false, "readme_text": "C1 Form Reader Application of computer vision and convolutional neural network (CNN) for automatically reading hand written numbers on C1 form (Indonesia election's tally form). Prerequisites You need to have python 3.x and the following libraries must be installed to run c1 form reader:  OpenCV Keras Tensorflow Numpy Matplotlib  Installing Simplest way to install all the requirements is using anaconda. $ conda create -n c1reader python=3.7 anaconda $ source activate c1reader (c1reader) $ pip install opencv-python (c1reader) $ pip install keras (c1reader) $ pip install tensorflow  Once all requirements are installed, clone this repository. (c1reader) $ git clone https://github.com/rzwck/c1-form-reader.git (c1reader) $ cd c1-form-reader  Running From the c1-form-reader you can just run the script as the following example: (c1reader) $ python read-C1-form.py Using TensorFlow backend. 2019-05-05 11:19:51.089820: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA test_images/test1.jpg {'01': 12, '02': 115, 'valid': 122, 'invalid': 0, 'total': 127} test_images/test10.jpg {'01': 136, '02': 11, 'valid': 147, 'invalid': 6, 'total': 153} Form test_images/test11.jpg is unreadable: Unable to find digit positions for invalid ballots count test_images/test12.jpg {'01': 190, '02': 415, 'valid': 205, 'invalid': 4, 'total': 209} test_images/test13.jpg {'01': 44, '02': 61, 'valid': 109, 'invalid': 0, 'total': 150} Form test_images/test14.jpg is unreadable: Unable to find digit positions for valid ballots count test_images/test15.jpg {'01': 119, '02': 19, 'valid': 138, 'invalid': 0, 'total': 138} Form test_images/test16.jpg is unreadable: Unable to find digit positions for votes #01 test_images/test17.jpg {'01': 69, '02': 160, 'valid': 229, 'invalid': 7, 'total': 231} Form test_images/test18.jpg is unreadable: Unable to find digit positions for invalid ballots count Form test_images/test19.jpg is unreadable: Unable to find digit positions for votes #02 test_images/test2.jpg {'01': 125, '02': 57, 'valid': 182, 'invalid': 3, 'total': 185} Form test_images/test20.jpg is unreadable: Unable to find digits for total ballots count test_images/test21.jpg {'01': 72, '02': 164, 'valid': 236, 'invalid': 4, 'total': 240} test_images/test22.jpg {'01': 116, '02': 24, 'valid': 140, 'invalid': 3, 'total': 143} test_images/test23.jpg {'01': 59, '02': 150, 'valid': 209, 'invalid': 1, 'total': 210} test_images/test24.jpg {'01': 89, '02': 133, 'valid': 223, 'invalid': 5, 'total': 228} Form test_images/test25.jpg is unreadable: Unable to find digit positions for valid ballots count Form test_images/test3.jpg is unreadable: Unable to find digit positions for votes #01 test_images/test4.jpg {'01': 128, '02': 28, 'valid': 156, 'invalid': 5, 'total': 161} test_images/test5.jpg {'01': 121, '02': 3, 'valid': 124, 'invalid': 5, 'total': 129} Form test_images/test6.jpg is unreadable: Unable to find digit positions for votes #02 Form test_images/test7.jpg is unreadable: Unable to find digit positions for votes #02 test_images/test8.jpg {'01': 143, '02': 89, 'valid': 232, 'invalid': 7, 'total': 239} test_images/test9.jpg {'01': 139, '02': 7, 'valid': 146, 'invalid': 0, 'total': 146}  The script saves the output file on the same folder (test_images). The following screenshot shows some of the outputs produced by c1 form reader. Green boxes are area containing hand written digits, small black boxes with white written numbers are 28x28 hand written digits that will be fed to CNN classifiers. The final number recognized by this program is the white number written on the blue rectangles.  Built With This scripts utilizes codes from the following sites:  PyImageSearch - for perspective transformation Keras examples - for training hand written digits classifiers  Training dataset I trained two digits (0-9) classifiers from two different datasets:  MNIST database - samples of hand written digits Pilkada DKI 2017 - extracted hand written digits from pilkada DKI's C1 form, since local people might have local hand written styles that is not contained in standard MNIST dataset.  I also trained \"X\" classifiers and \"-\" hyphen classifiers for recognizing \"X\" and \"-\" characters that is commonly used to denote empty digit box. The sample for this \"X\" and \"-\" classifier also came from Pilkada DKI 2017. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/rzwck/c1-form-reader/blob/b83c673abaebc5cdee8b8492b4a6f26034931370/classifiers/X_classifier.h5", "https://github.com/rzwck/c1-form-reader/blob/b83c673abaebc5cdee8b8492b4a6f26034931370/classifiers/digits_recognizer.h5", "https://github.com/rzwck/c1-form-reader/blob/b83c673abaebc5cdee8b8492b4a6f26034931370/classifiers/hyphen_classifier.h5", "https://github.com/rzwck/c1-form-reader/blob/b83c673abaebc5cdee8b8492b4a6f26034931370/classifiers/mnist_classifier.h5"], "see_also_links": ["http://yann.lecun.com/exdb/mnist/"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d5b"}, "repo_url": "https://github.com/chenpf1025/noisy_label_understanding_utilizing", "repo_name": "noisy_label_understanding_utilizing", "repo_full_name": "chenpf1025/noisy_label_understanding_utilizing", "repo_owner": "chenpf1025", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T03:24:29Z", "repo_watch": 4, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T07:57:30Z", "homepage": null, "size": 1946, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184867519, "is_fork": false, "readme_text": "Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels This is a Keras implementation for the paper 'Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels' (Proceedings of ICML, 2019). @inproceedings{chen2019understanding,   title={Understanding and Utilizing Deep Neural Networks Trained with Noisy Labels},   author={Chen, Pengfei and Liao, Ben Ben and Chen, Guangyong and Zhang, Shengyu},   booktitle={International Conference on Machine Learning},   pages={1062--1070},   year={2019} }  Dependencies Python 3, Keras, Tensorflow, numpy, sklearn Setup To set up experiments, we need to download the CIFAR-10 data and extract it to: data/cifar-10-batches-py  The code will automatically add noise to CIFAR-10 by randomly flipping original labels. Understanding noisy labels Note To quantitatively characterize the generalization performance of deep neural networks normally trained with noisy labels, we split the noisy dataset into two halves and perform cross-validation: training on a subset and testing on the other. We firstly theoretically characterize on the test set the confusion matrix (w.r.t. ground-truth labels) and test accuracy (w.r.t. noisy labels). We then propose to select a testing sample as a clean one, if the trained model predict the same label with its observed label. The performance is evaluated by label precision and label recall, which can be theoretically estimated using the noise ratio according to our paper. Train Experimrental resluts justify our theoretical analysis. To reproduce the experimental results, we can run Verify_Theory.py and specify the noise pattern and noise ratio, e.g.,   Symmetric noise with ratio 0.5: python Verify_Theory.py --noise_pattern sym --noisy_ratio 0.5   Asymmetric noise with ratio 0.4: python Verify_Theory.py --noise_pattern asym --noisy_ratio 0.4   Results Test accuracy, label precision and label recall w.r.t noise ratio on manually corrupted CIFAR-10.  Confusion matrix M approximates noise transistion matrix T.  Identifying clean labels and robustly train deep neural networks Note We present the Iterative Noisy Cross-Validation (INCV) to select a subset of clean samples, then modify the Co-teaching strategy to train noise-robust deep neural networks. Train E.g., use our method to train on CIFAR-10 with   50% symmetric noise: python INCV_main.py --noise_pattern sym --noisy_ratio 0.5 --dataset cifar10   40% asymmetric noise: python INCV_main.py --noise_pattern asym --noisy_ratio 0.4 --dataset cifar10   Results Average test accuracy (%, 5 runs) with standard deviation:    Method Sym. 0.2 Sym. 0.5 Sym. 0.8 Aym. 0.4     F-correction 85.08\u00b10.43 76.02\u00b10.19 34.76\u00b14.53 83.55\u00b12.15   Decoupling 86.72\u00b10.32 79.31\u00b10.62 36.90\u00b14.61 75.27\u00b10.83   Co-teaching 89.05\u00b10.32 82.12\u00b10.59 16.21\u00b13.02 84.55\u00b12.81   MentorNet 88.36\u00b10.46 77.10\u00b10.44 28.89\u00b12.29 77.33\u00b10.79   D2L 86.12\u00b10.43 67.39\u00b113.62 10.02\u00b10.04 85.57\u00b11.21   Ours 89.71\u00b10.18 84.78\u00b10.33 52.27\u00b13.50 86.04\u00b10.54    Average test accuracy (%, 5 runs) during training:  Cite Please cite our paper if you use this code in your research work. Questions/Bugs Please submit a Github issue or contact pfchen@cse.cuhk.edu.hk if you have any questions or find any bugs. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://proceedings.mlr.press/v97/chen19g.html"], "reference_list": ["https://arxiv.org/abs/1804.06872", "https://arxiv.org/abs/1609.03683", "https://arxiv.org/abs/1706.02613", "https://arxiv.org/abs/1804.06872", "https://arxiv.org/abs/1712.05055", "https://arxiv.org/abs/1806.02612"]}, {"_id": {"$oid": "5cf518767eb8d660a8f73d5c"}, "repo_url": "https://github.com/minhncedutw/prac-keras-yolo3", "repo_name": "prac-keras-yolo3", "repo_full_name": "minhncedutw/prac-keras-yolo3", "repo_owner": "minhncedutw", "repo_desc": "practice yolo3 keras version", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T11:02:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T18:06:20Z", "homepage": null, "size": 92, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184928698, "is_fork": false, "readme_text": "Practice YOLO3 (Detection, Training, and Evaluation)  This project is based on: https://github.com/experiencor/keras-yolo3  Training guide: 1. Data preparation Download the Raccoon dataset from from https://github.com/experiencor/raccoon_dataset. 2. Edit the configuration file You have to modify the parameters in the file config.json: labels, train_image_folder, train_annot_folder, cache_name. To modify other parameters are optional. The configuration file is a json file, which looks like this: {     \"model\" : {         \"min_input_size\":       352,         \"max_input_size\":       448,         \"anchors\":              [117,142, 151,233, 187,341, 245,377, 248,223, 288,302, 324,379, 374,274, 383,387],         \"labels\":               [\"raccoon\"]     },      \"train\": {         \"train_image_folder\":   \"/home/minhnc-lab/WORKSPACES/AI/data/raccoon_dataset/images/\",         \"train_annot_folder\":   \"/home/minhnc-lab/WORKSPACES/AI/data/raccoon_dataset/annotations/\",         \"cache_name\":           \"raccoon_train.pkl\",          \"train_times\":          10,         \"pretrained_weights\":   \"\",         \"batch_size\":           4,         \"learning_rate\":        1e-4,         \"nb_epochs\":             50,         \"warmup_epochs\":        3,         \"ignore_thresh\":        0.5,         \"gpus\":                 \"0\",          \"grid_scales\":          [1,1,1],         \"obj_scale\":            5,         \"noobj_scale\":          1,         \"xywh_scale\":           1,         \"class_scale\":          1,          \"tensorboard_dir\":      \"logs\",         \"saved_weights_name\":   \"yolo3.weights\",         \"debug\":                true     },      \"valid\": {         \"valid_image_folder\":   \"\",         \"valid_annot_folder\":   \"\",         \"cache_name\":           \"\",          \"valid_times\":          1     } } Download pretrained weights for backend at: http://www.mediafire.com/file/l1b96fk7j18yi7v/backend.h5 This weights must be put in the root folder of the repository. They are the pretrained weights for the backend only and will be loaded during model creation. The code does not work without this weights. 3. Generate anchors for your dataset (optional) python gen_anchors.py -c config.json Copy the generated anchors printed on the terminal to the anchors setting in config.json. 4. Start the training process python train.py -c config.json By the end of this process, the code will write the weights of the best model to file best_weights.h5 (or whatever name specified in the setting \"saved_weights_name\" in the config.json file). The training process stops when the loss on the validation set is not improved in 3 consecutive epoches. 5. Perform detection using trained weights on image, set of images, video, or webcam python predict.py -c config.json -i /path/to/image/or/video It carries out detection on the image and write the image with detected bounding boxes to the same folder. Evaluation python evaluate.py -c config.json Compute the mAP performance of the model defined in saved_weights_name on the validation dataset defined in valid_image_folder and valid_annot_folder. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.mediafire.com/file/l1b96fk7j18yi7v/backend.h5"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d5d"}, "repo_url": "https://github.com/bijeshos/machine-learning-demo", "repo_name": "machine-learning-demo", "repo_full_name": "bijeshos/machine-learning-demo", "repo_owner": "bijeshos", "repo_desc": "This repository contains a set of machine learning examples", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-05T11:14:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T17:13:46Z", "homepage": null, "size": 20, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184923597, "is_fork": false, "readme_text": "Machine Learning Examples This repo contains a set of machine learning examples using TensorFlow. (Examples using other frameworks will be added later.) Prerequisites It is assumed that the following is already configured in the local machine:  Python Virtualenv  Environment used for development/testing  OS: Ubuntu 18.04 LTS  Instructions   Install pre-requisites  python3  On Ubuntu 18.04, python3 is installed by default   pip3  $ sudo apt install python3-pip   vitualenv  $ pip3 install virtualenv or $ sudo apt install virtualenv (preferred)      Verify prerequisites installation by running the following commands:  $ python3 --version $ pip3 --version $ virtualenv --version    Create virtualenv  $ virtualenv --system-site-packages -p python3 ./venv $ source ./venv/bin/activate $ pip install --upgrade pip $ pip list    Install tensorflow  $ pip install --upgrade tensorflow or pip install tensorflow==2.0.0-alpha0    Verify tensorflow installation  v1  $ python -c \"import tensorflow as tf; tf.enable_eager_execution(); print(tf.reduce_sum(tf.random_normal([1000, 1000])))\"      How to use the examples The TensorFlow examples has two flavours: v1 and v2 (alpha). Following are details of the examples and corresponding program files.  v1  Basic Evaluation  basic-evaluation.py     v2 (alpha)   Hello World  Basic  basic-hello-world.py   Advanced  advanced-hello-world.py      Classification Examples  Text Classification  text-classification.py   Image Classification  image-classification.py   Structured Data Classification  structured-data-classification.py      Exploring Datasets  dataset-example.py    Using Keras API  Keras API Basic  keras-basics.py   Keras API Overview  keras-overview.py      Regression Example  fuel-efficiency-prediction-regression.py    Tensorboard Usage  tensorboard-example.py      Reference https://www.tensorflow.org/alpha ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d5e"}, "repo_url": "https://github.com/jbeougher7924/conn4", "repo_name": "conn4", "repo_full_name": "jbeougher7924/conn4", "repo_owner": "jbeougher7924", "repo_desc": "Connect 4 learning with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T20:13:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T12:19:32Z", "homepage": null, "size": 9790, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184890943, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d5f"}, "repo_url": "https://github.com/kumi123/ss", "repo_name": "ss", "repo_full_name": "kumi123/ss", "repo_owner": "kumi123", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-04T13:24:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T13:23:04Z", "homepage": null, "size": 17089, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184897861, "is_fork": false, "readme_text": "YOLOv3 Keras(TF backend) implementation of yolo v3 objects detection. According to the paper YOLOv3: An Incremental Improvement. Requirement  OpenCV 3.4 Python 3.6 Tensorflow-gpu 1.5.0 Keras 2.1.3  Quick start   Download official yolov3.weights and put it on top floder of project.   Run the follow command to convert darknet weight file to keras h5 file. The yad2k.py was modified from allanzelener/YAD2K.   python yad2k.py cfg\\yolo.cfg yolov3.weights data\\yolo.h5   run follow command to show the demo. The result can be found in images\\res\\ floder.  python demo.py  Demo result It can be seen that yolo v3 has a better classification ability than yolo v2.  TODO  Train the model.  Reference @article{YOLOv3,     title={YOLOv3: An Incremental Improvement},     author={J Redmon, A Farhadi },   year={2018}  Copyright See LICENSE for details. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d60"}, "repo_url": "https://github.com/bryanfisk/Machine-Learning-Competition-Keras", "repo_name": "Machine-Learning-Competition-Keras", "repo_full_name": "bryanfisk/Machine-Learning-Competition-Keras", "repo_owner": "bryanfisk", "repo_desc": "Keras implementation of machine learning project", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T02:32:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T03:41:55Z", "homepage": null, "size": 30491, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184848704, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d61"}, "repo_url": "https://github.com/gdut3118007086/VGG16-", "repo_name": "VGG16-", "repo_full_name": "gdut3118007086/VGG16-", "repo_owner": "gdut3118007086", "repo_desc": "\u4e00\u79cd\u591a\u5206\u7c7b\u65b9\u6cd5", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-04T07:10:40Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T06:56:16Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184862444, "is_fork": false, "readme_text": "VGG16- \u4e00\u79cd\u591a\u5206\u7c7b\u65b9\u6cd5 request TensorFlow keras pyqt5\u7b49 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d62"}, "repo_url": "https://github.com/Harphies1/MNIST-DATA", "repo_name": "MNIST-DATA", "repo_full_name": "Harphies1/MNIST-DATA", "repo_owner": "Harphies1", "repo_desc": "Digit recognition Neural Network Approach", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T12:20:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T12:12:51Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184890314, "is_fork": false, "readme_text": "MNIST-DATA Digit recognition Neural Network Approach A Neural-Network Approach To train Digit Data with 2 Hidden layers with Keras API on top of Tensorflow ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d63"}, "repo_url": "https://github.com/monkeydsandy/Signature_Recognition_DeepLearning", "repo_name": "Signature_Recognition_DeepLearning", "repo_full_name": "monkeydsandy/Signature_Recognition_DeepLearning", "repo_owner": "monkeydsandy", "repo_desc": "Signature_Recognition_DeepLearning Signature recognition with Keras,Deep learning  This repo predict new signature class with %98.04 error rate in 10 epoch  <img src=\"http://i.hizliresim.com/1LZP2b.png\">", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T18:40:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T18:37:36Z", "homepage": null, "size": 152, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184931693, "is_fork": false, "readme_text": "Signature_Recognition_DeepLearning Signature recognition with Keras,Deep learning This repo predict new signature class with %98.04 error rate in 10 epoch  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d64"}, "repo_url": "https://github.com/chenwe73/deep_active_learning_segmentation", "repo_name": "deep_active_learning_segmentation", "repo_full_name": "chenwe73/deep_active_learning_segmentation", "repo_owner": "chenwe73", "repo_desc": "Deep Active Learning with Semantic Segmentation on Automotive Dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T20:34:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T20:33:01Z", "homepage": null, "size": 21, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184941669, "is_fork": false, "readme_text": "deep_active_learning_segmentation Deep Active Learning with Semantic Segmentation on Automotive Dataset based on repository https://github.com/divamgupta/image-segmentation-keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d65"}, "repo_url": "https://github.com/minhncedutw/prac-keras-ssd", "repo_name": "prac-keras-ssd", "repo_full_name": "minhncedutw/prac-keras-ssd", "repo_owner": "minhncedutw", "repo_desc": "practice ssd", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T17:57:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T17:38:58Z", "homepage": null, "size": 20195, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 184926111, "is_fork": false, "readme_text": "SSD: Practice Single-Shot MultiBox Detector implementation in Keras   This project is based on: https://github.com/pierluigiferrari/ssd_keras  Training details To train the original SSD300 model on Pascal VOC:  Download the datasets:  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar  Modify the directory to your dataset in the file ssd300_training.py such as:     VOC_2007_images_dir = '.../VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/JPEGImages'    VOC_2007_annotations_dir = '.../VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/Annotations'    VOC_2007_train_image_set_filename = '.../VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/ImageSets/Main/train.txt'    VOC_2007_val_image_set_filename = '.../VOCtrainval_06-Nov-2007/VOCdevkit/VOC2007/ImageSets/Main/val.txt'  Run the file ssd300_training.py.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d66"}, "repo_url": "https://github.com/AnanthK1998/PALM-Challenge", "repo_name": "PALM-Challenge", "repo_full_name": "AnanthK1998/PALM-Challenge", "repo_owner": "AnanthK1998", "repo_desc": "This was a challenge in ISBI 2019 conference. This challenge has four subtasks : Image classification, Disc Segmentation, Lesion Segmentation and Fovea Localization. I have done the first three subtasks in this repository.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T14:03:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T13:49:51Z", "homepage": null, "size": 36, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184900796, "is_fork": false, "readme_text": "PALM-Challenge This was a challenge in ISBI 2019 conference. This challenge has four subtasks : Image classification, Disc Segmentation, Lesion Segmentation and Fovea Localization. I have done the first three subtasks in this repository. I have approached the classification subtask with the ResNet 50 architecture. The segmentation subtasks were approached with the UNet architecture with ResNeXt-101 encoder using Keras. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d67"}, "repo_url": "https://github.com/seantrinh/SeeFood", "repo_name": "SeeFood", "repo_full_name": "seantrinh/SeeFood", "repo_owner": "seantrinh", "repo_desc": "Is it a hot dog? Or is it not a hot dog? ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T21:34:47Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T20:47:55Z", "homepage": "", "size": 47266, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184942965, "is_fork": false, "readme_text": "SeeFood Is it a hot dog? Or is it not a hot dog? About From HBO's Silicon Valley, the SeeFood app is able to distinguish between photos of hot dogs and photos of things that are not hot dogs. This is my attempt at recreating the backend components of the app, primarily the model that makes predictions, using Python and machine learning frameworks like Keras and TensorFlow. Acknowledgements The data used to train and test this model was downloaded from Kaggle's Hot Dog - Not Hot Dog Competition. This data was also extracted from the Food 101 dataset. Accuracy The most recent model produced a test accuracy of 76%. ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "binary-classification", "image-classification"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d68"}, "repo_url": "https://github.com/ashish1dev/FACE-RFCN-Project", "repo_name": "FACE-RFCN-Project", "repo_full_name": "ashish1dev/FACE-RFCN-Project", "repo_owner": "ashish1dev", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-19T22:09:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T19:34:08Z", "homepage": null, "size": 96292, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184936750, "is_fork": false, "readme_text": "FACE-RFCN-Project Face-RFCN implementation, on top of RFCN code by  https://github.com/parap1uie-s/Keras-RFCN ##Steps to execute the project:   Change the path of WIDER FACE dataset files in the .py files 1.1) set the variable name  (job_dir) in the FaceR_FCN_Train.py file 1.2) set the variable names ( file, file1, file2 and file3) to point to correct folder names   To Run and Train the model, execute the following command: python3 FaceR_FCN_Train.py   To Test the model, Change the path stored in the variable named 'modelPath', and then execute this command python3 FaceR_FCN_Test.py   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d69"}, "repo_url": "https://github.com/kylelo/music-genre-cnn-classifier", "repo_name": "music-genre-cnn-classifier", "repo_full_name": "kylelo/music-genre-cnn-classifier", "repo_owner": "kylelo", "repo_desc": "A two-class music genre classfier based on CNN (Convolutional Neural Network)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T19:25:31Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T01:59:01Z", "homepage": "", "size": 106, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184841461, "is_fork": false, "readme_text": "Music Genre CNN Classifier A two-class music genre classifier based on CNN (Convolutional Neuron Network).    What does this project do? 1. Extract Mel-spectrogram as the training set from two groups of songs with different genre 2. Train the CNN model (on your PC or Google TPU) 3. Evaluate the CNN model using the validation set 4. Visualize each kernel (For learning purpose)  NOTE that this project takes the classification of prograssive rock and non-prograssive rock musics as an example. Users can feel free to change them to other music genres.  | |- code |   | |   |-- generatePatterns.py |   |-- trainModel.py |   |-- evaluateModel.py |   |-- toolbox |         |-- featureExtractTool.py (helper functions) |         |-- evaluateModelTool.py  (helper functions) |         |-- trainModelTPU.py   (optional) |         |-- generateTestset.py (optional) |         |-- visualizeKernel.py (optional) |  |- training songs (create manually) |    | |    |-- class1 |    |-- class2 | |- validation songs (create manually)      |      |-- class1      |-- class2   Prerequisites 1. Python3 Linux sudo apt-get update sudo apt-get install python3.6  MacOS brew install python3  2. Feature extraction tool pip3 install librosa  3. Model training tools pip3 install keras tensorflow   Extract Mel-spectograms from Songs as Training set  Skip this step by downloading default training sets (Class1: Prop songs and Class2: Non-prog songs)  1. Put two groups of songs (.mp3) into ./training song/class1 and ./training song/class2 2. Run python3 generatePatterns.py  3. After extraction, user will get training sets Class1Patterns.pkl and Class2Patterns.pkl  Train CNN Model  Skip this step by using the default trained model to try validation set  1. Run python3 trainModel.py  2. After training, user will get cnnModel.h5 and Scalers.sav  Scalers are used for training set normalization (make values in each channel between -1 and +1). It is necessary to use the same scalers to normalize validation songs.   Train CNN Model using TPU (Optional) 1. Create a notebook .ipynb on Colab 2. Copy the code in trainModelTPU.py to notebook 3. Upload class1Patterns.pkl and class2Patterns.pkl to ./Colab Notebooks on your Google drive 4. Run and get cnnModel.h5 on the Google drive   Validate CNN Model 1. Put two groups of songs (.mp3) into ./validation songs/class1 and ./validation songs/class2  Songs for validation should be different from the ones for training  2. Run python3 evaluateModel.py  3. See results in console   Visualize CNN Kernels (Optional) This feature is for learning purpose. This part of the code is referenced from an amazing post Visualization of Filters with Keras (Yumi's Blog) python3 visualizeKernel.py     Feel free to contact me if you find a bug or have any question :D ", "has_readme": true, "readme_language": "English", "repo_tags": ["cnn", "genre-classification", "music-classification", "machine-learning"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d6a"}, "repo_url": "https://github.com/mabertuc/bhatkhande-classifier", "repo_name": "bhatkhande-classifier", "repo_full_name": "mabertuc/bhatkhande-classifier", "repo_owner": "mabertuc", "repo_desc": "A CNN that classifies characters with chromatic alterations in a Bhatkhande corpus.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T23:31:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T21:08:35Z", "homepage": "", "size": 42, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184944500, "is_fork": false, "readme_text": "bhatkhande-classifier A CNN that classifies characters with chromatic alterations in a Bhatkhande corpus. Installation: This project requires Python 3.7.2, Keras 2.2.4, Pillow 6.0.0, and TensorFlow 1.13.1. Run pip install -e .  in your environment from the root directory of the project to install Keras, Pillow, and TensorFlow. Usage: Running python classifier/train.py  from the root directory of the project will train the model. classifier/train.py requires that a directory containing all the sample character images exists in the classifier directory (e.g., there is currently a samples directory in the classifier directory containing all the sample character images); modify line 228 in classifier/train.py so that the name of the directory is correct, if necessary. classifier/train.py will then generate the desired number of character images for each character (specified in line 226 of classifier/train.py) and distribute them to the classifier/train and classifier/validation directories according to the character type and the percentage of images used for training (specified in line 227 of classifier/train.py); all the relevant directories (classifier/train and classifier/validation) and files (classifier/model.json and classifier/model.h5) will be generated by classifier/train.py. Print messages throughout classifier/train.py display the current status. Running python classifier/predict.py  from the root directory of the project will make predictions based on the model that was previously created. classifier/predict.py requires that model.json, model.h5, and a directory containing all the images used for the predictions are in the classifier directory; modify line 73 in classifier/predict.py so that the name of the directory containing all the images used for the predictions is correct. For each image, classifier/predict.py will print the prediction for the image as well as a probability assigned to that prediction. IMPORTANT: running classifier/predict.py will modify the images in the directory containing all the images used for the predictions so that they can be processed by the model. Avoid using originals if you do not want the images to be modified. Testing: Run pip install -e .[dev]  from the root directory of the project to install pytest in addition to the other packages. Then run pytest tests  from the root directory of the project to run all the tests. Authors: If you have any questions, please contact me at\u00a0mabertuc.dev@gmail.com. License: This project is licensed under the MIT license. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d6b"}, "repo_url": "https://github.com/prams628/Digit-Recognizer", "repo_name": "Digit-Recognizer", "repo_full_name": "prams628/Digit-Recognizer", "repo_owner": "prams628", "repo_desc": "A program to recognize the digits spoken in audio files", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T14:44:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T02:30:39Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184843617, "is_fork": false, "readme_text": "Digit-Recognizer A program to recognize the digits spoken in audio files Input: The input file will consist of numbers spoken from 0 - 9 just as we tell phone numbers (99 can be told as \"double nine\"). Between every word a gap of about one-tenth of a second is expected. Dataset: The dataset has been stored as a collection of .wav files for every word which can be spoken. The recordings were made in a specific format as illustrated in the code to facilitate the comparison between two audio files. A program was written to achieve the same. Libraries used: Keras Librosa PyAudio PyDub Output: The output shall consist of the set of numbers being spoken Result: A prediction accuracy of around 75% for men and around 60% for women has been obtained (The dataset is biased towards men's voice due to insufficient resources for women's voice) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d6c"}, "repo_url": "https://github.com/djuxdj/AML-Project", "repo_name": "AML-Project", "repo_full_name": "djuxdj/AML-Project", "repo_owner": "djuxdj", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-04T01:32:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T01:06:37Z", "homepage": null, "size": 455, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184837997, "is_fork": false, "readme_text": "Multiclass Multilabel Untrimmed Video Event Detection (AML-Project)   You can download the dataset using downloader.py, it takes command line parameter as txt files that conatins YouTube video links and destination directory to store the downloaded videos   In order to extract audio features first download this file: VGGish model checkpoint   Then run gen_audioset_feature.py to obtain 128 dimensional embedding size for every 960ms   Before running preprocessing download this files   ResNet50 pretrained on ImageNet [PyTorch version]   C3D pretrained on Sports1M [ported from Keras]   Run preprocess_frames.py present in preprocess/train and preprocess/test directory. Similarly you can run preprocess/{test/train}/create_output_super.py to obtain segment wise labels and preprocess/{test/train}/create_output_ws.py to obtain per video labels.   For running the models you can go into the respective model directory and directly run model.py file.   Similarly to run UCF-101 models, follow the same instructions as above. The model files are present in UCF-101-models directory   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://imagelab.ing.unimore.it/files/c3d_pytorch/c3d.pickle"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d6d"}, "repo_url": "https://github.com/rtflynn/NLP-Sentiment", "repo_name": "NLP-Sentiment", "repo_full_name": "rtflynn/NLP-Sentiment", "repo_owner": "rtflynn", "repo_desc": "Sentiment analysis for amazon product reviews using NLTK, Scikit-Learn, and Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T04:39:24Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T16:35:42Z", "homepage": null, "size": 86, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 184919772, "is_fork": false, "readme_text": "Amazon Review Sentiment Analysis Sentiment analysis has been on the rise - both because of the availability of new analysis techniques in deep learning, and because there is an incomprehensibly large amount of data being generated everywhere.  Every product review, every tweet, every reddit post, etc, contains subjective information which we'd like to be able to process and understand. For example, say you're Netflix.  Then you're very interested in what your customers have to say about your service and your TV show/movie selection, and you might look toward mining facebook posts and tweets and IMDB reviews and so on to gauge public opinion.  If you're a politician, then you're interested (hopefully) in what your constituents are thinking, what they want, what set of values they hold dear, etc, and so you might have a team which analyzes public sentiment in these areas.  If you're an entrepreneur, then you're interested in public opinion as it concerns your niche, your product, and your competition, since this allows you to make more informed decisions moving forward. In fact, it's difficult to think of industries which would not benefit from having a bit of machine-learnt sentiment analysis in place.  This makes sentiment analysis (and more generally, natural language processing, or NLP) a valuable skill to possess.  Plus, at times it's just plain fun! In this project we'll see how to train binary classifiers to the task of reading in an Amazon product review, and outputting whether the review was positive or negative.  Once you've worked through this, you should have the necessary background to go into other NLP tutorials and have a good idea what's going on at all times.  Some interesting popular projects are: design a neural network which outputs original text in the style of a book or author of your choice... design a model which tokenizes sentences in a larger text as happy, sad, angry, etc... design a chat bot... Finally, a quick note on the style of this writeup:  It's not so much a tutorial as it is a finished project with clear references to the topics involved.  For example, we might briefly discuss (say) word tokenizing, in that we'll describe in a sentence or two what it is and we'll provide python code to carry it out, but interested readers should look elsewhere for detailed explanations of any new concepts.  It has to be this way --- otherwise this readme would be chapters long and not-at-all fun to read!  The intention here is to have a completed project all in one place, to serve as a quick and easy go-to reference/starred project for readers to come back to when they need a reminder on how something works. Topics Covered We'll be covering the following topics in varied amounts of detail:  NLTK (The Natural Language Toolkit for Python) Word tokenizing techniques Preprocessing training data for NLP \"Old-fashioned\" NLP techniques  (don't be fooled - these are still used today!) Building and training models using Scikit-Learn RNN (specifically, LSTM) Word embeddings Building and training models using Keras Multithreading using fit_generator Hyperparameter searching using Talos  Preliminary: I'm using the Amazon Reviews data set which can be found at https://www.kaggle.com/bittlingmayer/amazonreviews .  You'll want to click the download button, unzip the downloaded file, and then unzip the files within the newly unzipped file (7-zip if you get stuck).  You'll want test.ft.txt and train.ft.txt in the same folder as LSTM.py.  You'll notice that these data sets are very large!  In fact, the testing set has 400,000 training examples, so while we're playing around with different models it suffices to take some subset of test.ft.txt as our full train/test/validation set. All the data in train.ft.txt and test.ft.txt files has already been shuffled. We're using Python 3 throughout  (Version 3.6.5 to be specific). Getting Started: Data Formatting Open a command prompt, navigate to the folder containing train.ft.txt, and type python.  We first want to get a feel for the data: current_file = open(\"test.ft.txt\", \"rb\") x = current_file.read()                      current_file.close() So now x holds the entire contents of test.ft.txt. len(x) 177376193                      That's a lot!  Well, what does this data look like? print(x[:1000]) b'__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I\\'m in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life\\'s hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"\\n__label__2 One of the best game music soundtracks - for a game I didn\\'t really play: Despite the fact that I have only played a small portion of the game, the music I heard (plus the connection to Chrono Trigger which was great as well) led me to purchase the soundtrack, and it remains one of my favorite albums. There is an incredible mix of fun, epic, and emotional songs. Those sad and beautiful tracks I especially like, as there\\'s not too many of th' OK.  Mostly english text... some excape characters like \\' and \\n... a mysterious b at the beginning of it all, and __label__1, __label__2 at the beginning of each review.  We're going to want to separate this into individual reviews and clean them all up a little, so at some point we're going to want to do something like x = x.splitlines() since each review seems to end with a newline.  Feel free to try this - you'll find that each line now has that mysterious b at the beginning! Well, this is because x is being read as a so-called byte-literal instead of as a string.  Let's fix this before splitting: x = x.decode(\"utf-8\")       # now type(x) tells us that x is a string.   x = x.splitlines()  ##### OPTIONAL  but recommended for quick iteration  ##### ##### x = x[:10000]                                  ##### #####                                                ##### Good.  Now x is a list of strings.  Let's look into it a bit more: len(x) 400000 x[0] '__label__2 Great CD: My lovely Pat has one of the GREAT voices of her generation. I have listened to this CD for YEARS and I still LOVE IT. When I\\'m in a good mood it makes me feel better. A bad mood just evaporates like sugar in the rain. This CD just oozes LIFE. Vocals are jusat STUUNNING and lyrics just kill. One of life\\'s hidden gems. This is a desert isle CD in my book. Why she never made it big is just beyond me. Everytime I play this, no matter black, white, young, old, male, female EVERYBODY says one thing \"Who was that singing ?\"' Looking good.  Each element of x consists of a label (__label__2), a title(Great CD), and a review (everything after the colon).  We're going to want to separate the label from the review.  I've carried this out in two different ways: (1) Create three lists:  labels, titles, and reviews.  My thinking here had to do with the fact that the title (Great CD) of the first review is enough to classify the sentiment, and with the fact that the non RNN approaches I used employed something called bag-of-words, which means that the word 'Great' would be lost in the rest of the review... More on this later. (2) Create two lists:  labels and reviews.  This is the more straightforward option and it's what we'll do here. Look through a few elements of x if necessary.  You'll notice that the label is always separated from the review by a single space. labels = [] reviews = [] for i in x:   separated = i.split(\" \",1)   labels.append(separated[0])   reviews.append(separated[1]) Go ahead and check that things look correct:  labels and reviews are both python lists of length 400000, labels[i] always takes the value __label__1 or __label__2, and reviews[0] gives us the familiar review we've already read a few times.  Looking through a few reviews, it's easy to tell that __label__1 corresponds to bad reviews and __label__2 corresponds to good reviews.  Let's go ahead and incorporate this so we've got one less thing to think about in the future: for i in range(len(labels)):   if labels[i] == '__label__1':     labels[i] = 'Bad'   elif labels[i] == '__label__2':     labels[i] = 'Good'   else:     print(\"uh oh\")  for i in range(5):   print(labels[i]) good good bad good good Great.  We've got labels and reviews separated.  Now let's start talking about what to do with these sets. Bag-of-Words Models A bag-of-words model is one in which each input (in our case, each review) is first reduced to an unordered set of the words which occur within the input.  For example, the reviews \"Great product but the customer service was terrible\" and \"Terrible product but the customer service was great\" have exactly the same bag-of-words reductions ( the set {'but', 'customer', 'great', 'product', 'service', 'terrible', 'the', 'was'} ), even though the sentiments are very different.  Even though this bag-of-words idea is terribly naive, it works relatively well.  Good reviews will have more positive words in them and bad reviews will have more negative words in them.  Moreover there are some negative words which don't often show up in positive reviews at all, and vice-versa. By the way, this is the reason that above, when we were splitting our data into labels and reviews, I mentioned the idea of also separating out the titles.  A bag-of-words reduction of a review title is much less likely to lose much information, and important keywords like 'great' and 'terrible' are less likely to be lost in the noise of a giant bag of words.  We will revisit this idea later, and in particular we'll create a model out of two smaller models - one trained on titles and one trained on review texts.  This model will work by first trying to classify the review based on title alone, and if it returns a label with very high confidence, we use that answer; otherwise we look at the review text.  This separation of one feature into two is a form of 'feature engineering' and is a common practice in real-world classification and regression tasks. Our first sentiment analyzer will be a bag-of-words model.  We'll need to process our feature set in various important ways:  We'll need to 'tokenize' each review down to constituent words. We'll probably want to reduce the possible vocabulary down to a few thousand words, both to decrease training time and to decrease overfitting (some words may appear in only one review and thus be unfairly attributed a positive/negative score).  We'll use the most commonly occuring 3000 words for now. We'll want to make sure each word is lowercase so we don't end up counting the same word as several different words. We'll want to 'stem' or 'lemmatize' our words so that words which are essentially the same ('is', 'are', 'were', 'was', 'be') or ('happy', 'happily') are put into a standard form, again to avoid counting essentially the same word several times.  In general, stemming is a more crude procedure which has the advantage of usually saving disk space, whereas lemmatizing is a more intelligent procedure which better preserves the original meaning of the text.  Both, however, lose some amount of information. We may be interested in tagging the part of speech of each word, or in identifying certain commonly-occurring groupings of parts of speech such as an 'adverb-verb' pair or an 'adjective-noun' pair.  We won't do this in this project, but we point it out as a possibility because NLTK has powerful built-in tools for doing all sorts of tagging of this form. Finally, we'll want to loop through our reviews set and for each review output a bag-of-words representation of that review.  Let's get to it. Preparing the features import nltk from nltk.tokenize import word_tokenize, RegexpTokenizer  print(word_tokenize(reviews[0])) reTokenizer = RegexpTokenizer(r'\\w+') print(reTokenizer.tokenize(reviews[0]))   If you run the above lines, you'll see that word_tokenize separates the review into a list of words, but considers punctuation as words.  This might be desirable for some sorts of models (for example, an attention model may learn to look at what comes before an exclamation mark or a set of ellipses), but for a bag-of-words model it seems like punctuation will simply add noise. A RegexpTokenizer can be built to tokenize according to any regular expression you hand it.  The regular expression r'\\w+' matches any pattern consisting of one or more consecutive letters, so works fine for our purposes.  Note that this matcher will miss certain things like Ph.D. (i.e. it'll tokenize this as two words) and hyphenated words and contractions, but it should still work fine for our task.  We'll use the RegexpTokenizer going forward. Now let's work on getting everything stemmed and in lowercase.  from nltk.stem import PorterStemmer    ps = PorterStemmer()  temp = reTokenizer.tokenize(reviews[0])  for i in range(10):   print(ps.stem( temp[i].lower() )) OK... lots of information has been lost by stemming.  Let's try lemmatizing instead: from nltk.stem import WordNetLemmatizer  lemmatizer = WordNetLemmatizer() temp = reTokenizer.tokenize(reviews[0]) for i in range(10):   print(lemmatizer.lemmatize( temp[i].lower() ))  Better.  Let's use this.  Finally, we need to collect all the words from all the reviews into one list and keep the 3000 most common words as our bag-of-words vocabulary. all_words = [] for i in range(len(reviews)):   tokens = reTokenizer.tokenize(reviews[i])   reviews[i] = []   for word in tokens:     word = word.lower     word = lemmatizer.lemmatize(word)     all_words.append(word)     reviews[i].append(word) All our words are in the list all_words.  NLTK conveniently provides functionality for extracting the most common words from a list. from nltk import FreqDist  all_words = FreqDist(all_words) most_common_words = all_words.most_common(3000) word_features = [] for w in most_common_words:   word_features.append(w[0]) most_common_words is a python list of tuples like ('good', 32655) if the word 'good' happens to have appeared 32655 times in our data.  This list consists of the 3000 most common words and they're sorted in order of most- to least- frequent.  We collect all the words into our word_features list.  We can now iterate through each review in reviews and create a vector of 1's and 0's for a given review depending on which words from our chosen 3000 show up in that review.  However we should think ahead a little --- which ML algorithm will we use, and what format does it prefer its data in? NLTK includes some classifiers off the shelf, so let's keep things simple and use one of those.  We'll use the nltk.NaiveBayesClassifier() for now.  Naive Bayes is, as the name suggests, quite a naive method.  It simply correlates individual words with probability distributions for labels, so that (for instance) the word 'good' might correlate to a probability distribution like  90% 'Good', 10% 'Bad' .  It does this in a manner which treats all word occurences as independent from one another.  A consequence is the fact that the word 'not' has an associated label distribution which only has to do with how many times the word 'not' shows up in positive vs. negative reviews.  A naive Bayes classifier will never, ever be able to understand the phrase 'not good' to mean 'bad'.  But if you think about it, this is the perfect classifier to use with a bag-of-words model, since we've already thrown away all the interconnections between the words anyway! It's good that we've decided on which classifier to use, because now we know what format it wants its training data in.  Looking at the documentation real quick (or looking directly at the code for nltk.NaiveBayesClassifier()), we see that it wants its training data to be packaged as a python list of ordered pairs (feature_dict, label), where feature_dict is a dictionary with (key, value) pairs of the form ('some_word', 0 or 1). def make_feature_dict(word_list):   feature_dict = {}   for w in word_features:     if w in word_list:       feature_dict[w] = 1     else:       feature_dict[w] = 0     return feature_dict   nltk_data_set = [] for i in range(len(labels)):   nltk_data_set.append( (make_feature_dict(reviews[i]), labels[i]) )  train_proportion = 0.9 train_set_size = int(train_proportion * len(labels))  training_set = nltk_data_set[:train_set_size] testing_set = nltk_data_set[train_set_size:] Training and Testing the Model Believe it or not, we're done with the hard part.  All that's left is to instantiate a classifier, train it, and see how it does on the test set after training.  This will take a while if the optional line of code  x = x[:10000] was ommitted at the beginning of the project. print(\"Beginning classifier training...\") classifier = nltk.NaiveBayesClassifier.train(training_set) print(\"Finished classifier training... its accuracy on the testing set is: \") print(nltk.classify.accuracy(classifier, testing_set))  print(\"The most informative features were: \") classifier.show_most_informative_features(30) And there you have it. Results will vary based on vocabulary size and number of test/train examples.  On my machine the first run gave accuracy 0.811 and most informative feature 'refund' with a 44.6 to 1 proportion of 'Bad' to 'Good'.  The next most informative features were 'publisher', 'waste', 'worst', 'zero', 'pathetic', and 'elevator', 'awful', and 'defective', all of which point toward 'Bad'.  Many of these seem correct, but publisher and elevator don't make much sense.  This is simply what happens with naive Bayes on small data sets.  The first informative feature which points towards 'Good' is 'refreshing'. In fact, after training many models on this problem, I've noticed that the set of informative features which point toward 'Bad' vastly outnumbers the set of features which point toward 'Good'.  This introduces its own problems to the classifier, but that's an issue for another day. We can check this setup's average performance with the following: import random  train_proportion = 0.9 train_set_size = int(train_proportion * len(labels))   accuracies = 0 for i in range(10):     random.shuffle(nltk_data_set)      training_set = nltk_data_set[:train_set_size]     testing_set = nltk_data_set[train_set_size:]      print(\"Beginning classifier training...\")     classifier = nltk.NaiveBayesClassifier.train(training_set)     print(\"Finished classifier training... its accuracy on the test set is: \")     print(nltk.classify.accuracy(classifier, testing_set))     accuracies += nltk.classify.accuracy(classifier, testing_set)  print(\"Average accuracy across 10 models: \") print(accuracies/10) With total train/test size 10,000, we got an average accuracy of .824.  This is pretty good!  But let's not get too excited - the data sets we used were small and may have had too many similar reviews by chance.  To be sure, we repeated the experiment with 160,000 test/train examples and got an average accuracy of .818. I'd originally planned on discussing how to get nltk and scikit-learn working together --- and may do so on some future update to this project.  For now, however, let's move on to LSTM models. Sequential Models The very naive bag-of-words model worked surprisingly well, achieving over 80% accuracy on the sentiment classification task we gave it.  Let's turn our attention to neural networks and see how they fare. We'll be using a special kind of neural network architecture known as LSTM.  LSTM stands for 'Long Short-Term Memory', and is an example of a recurrent neural network (RNN), i.e. a network which analyses data over multiple time steps.  There are many amazing LSTM tutorials out there, and this isn't one of them, so let's content ourselves with a review of the very basics: The beginning of the process is the same as before:  We tokenize our reviews into individual words, count the number of occurences of each word, and make a vocabulary out of the most common 3000 (say) words.  We build our training and test data by going through a review and keeping all words which belong to our vocabulary, but importantly this time we keep them in order (!). We start with the following neural network:  First, there's an 'embedding' layer which does something special we'll discuss in a bit.  Next, we feed our words into an LSTM cell.  We actually feed information to an LSTM cell one letter at a time, or one word at a time, or one sentence at a time... one X at a time, where X is decided on before we create the network (the choice of X determines what the 'embedding' layer alluded to above looks like). LSTM cells do remarkable things.  Essentially, an LSTM cell is able to remember previous data, and it's able to forget previous data (within a given training or test example).  More than this, an LSTM cell is able to decide which previous data is important to remember and which previous data is OK to forget, and it's able to learn to make this decision better and better (i.e. it makes these decisions based on internal parameters and is able to tweak these parameters to decrease a loss function).  Going back to an earlier example, an LSTM would have absolutely no trouble distinguishing the statements \"Great product but the customer service was terrible\" and \"Terrible product but the customer service was great\" from one another. A nice property of LSTM which we won't use here is that LSTM cells actually learn sequence-to-sequence mappings, meaning they're well-suited to tasks like: given a paragraph of text from a story book, list all characters mentioned in that paragraph and what they did; given a sentence with the last word omitted, predict what the last word is; translate a passage from English to French. Back to this magical 'embedding' from before:  The idea is similar to lemmatizing.  When we lemmatize the words 'great', 'greater', 'greatest', they all map to the same word, 'great'.  However, the words 'good' and 'best' are not recognized as being close in meaning to 'great' via this procedure.  An embedding layer works as follows:  We take our vocabulary (which has size 3000), and one-hot encode our individual words into it.  Now our words are sitting incredibly sparsely and quite wastefully in a 3000-dimensional space.  We map this space down to (say) a 50-dimensional space via some linear map, and we learn the parameters of this map over time via gradient descent. Note:  This isn't how Keras does it --- Keras skips the one-hot embedding and instead uses a more-efficient dictionary lookup. What makes this work is the fact that in 50-dimensional space, there's not enough room for each word to get its own axis.  Therefore a bunch of words need to point in similar directions, and we get more accurate models when similar words get similar direction vectors associated to them.  The directions are initialized randomly and modified over time through backpropogation, so that eventually words like 'good', 'great', and 'best' are all pointing in similar directions to one another. This affords some other very, very nice properties --- for example, a good embedding might have \"car\" as an almost-linear-combination of \"fast\" and \"vehicle\", because \"car\" is more similar to \"fast\" and \"vehicle\" than it is to, say, \"transparent\" or \"hamper\".  This requires a little thought, but one can imagine that in a high-dimensional space, being close to two vectors means being close to the plane spanned by them. Similarly, one might take the word embedding for 'king', subtract the word embedding for 'man', add the word embedding for 'woman', and end up awfully close to the word embedding for 'queen'.  This whole topic is actually really cool and very worthy of a few afternoons on Wikipedia and YouTube. In any case, our model will look like:  Embedding Layer -->  LSTM Cell --> Single Unit (sigmoid) . Preparing the Features We'll need to prepare our features in a different format for Keras.  For one thing, Keras models need numpy arrays to be passed to them.  For another, since our embedding layer will take care of word similarity, we may want to revisit whether we lemmatize or not.  It may well be the case that the particular form of a word changes the meaning of the sentence it's in enough to matter, so we should experiment both with and without lemmatizing our input. Another thing to think about:  With the bag-of-words model, all our test/train examples automatically had the same length (3000) because each example was just a binary-valued dictionary whose key set was the set of most common words.  But here, our reviews have different lengths and so our examples potentially have different lengths.  This isn't necessarily problematic but things will be easier if we prepare all inputs to be the same length (say 500).  To do this we'll cut longer inputs down to length 500, and we'll pad shorter reviews at the beginning with 0's.  We'll do this with a preprocessing module from keras called 'sequence'. Let's start from scratch.  This time we'll put all our imports up front. from keras.layers import Embedding, Dense, LSTM  # CuDNNLSTM instead of LSTM if you've got tensorflow-gpu from keras.models import Sequential from keras.preprocessing import sequence import numpy as np from nltk.tokenize import RegexpTokenizer from nltk.stem import WordNetLemmatizer from nltk import FreqDist  vocab_size = 3000 max_length = 500  current_file = open(\"test.ft.txt\", \"rb\") x = current_file.load() current_file.close()  x = x.decode(\"utf-8\") x = x.splitlines() x = x[:10000]     ### For quick iteration.  Recall this dataset has 400,000 examples.  labels = [] reviews = []  for i in x:   separated = i.split(\" \",1)   labels.append(separated[0])   reviews.append(separated[1])  for i in range(len(labels)):   labels[i] = int(labels[i] == '__label__2')  reTokenizer = RegexpTokenizer(r'\\w+') all_words = []  for i in range(len(reviews)):   tokens = reTokenizer.tokenize(reviews[i])   reviews[i] = []   for word in tokens:     word = word.lower()     all_words.append(word)     reviews[i].append(word)      all_words = FreqDist(all_words) all_words = all_words.most_common(vocab_size) for i in range(len(all_words)):   all_words[i] = all_words[i][0] Everything up to this point should make sense.  Thinking ahead a bit, a Keras embedding layer will take as input a numpy array of positive integers - for example [52, 11, 641, 330, 1066, 12, 1, ..., 18] - where the integers are essentially tokens of words.  So, we'll want a dictionary which takes distinct words to distint positive integers.  It might be helpful to have the reverse dictionary handy, both for debugging and for analysis. word2int = {all_words[i] : i+1  for i in range(vocab_size)}   # i+1 because we want to start at 1 int2word = {x : y  for  y, x in word2int.items()} dict_as_list = list(word2int)  def review2intlist(review):   int_list = []   for i in review:     if i in word2int.keys():       int_list.append(word2int(i))   return int_list  lstm_input = [] for rev in reviews:   lstm_input.append(np.asarray(review2intlist(rev), dtype=int)) lstm_input = sequence.pad_sequences(lstm_input, maxlen=max_length)  train_proportion = 0.9 train_size = int(train_proportion * len(labels))  x_train, y_train = lstm_input[:train_size], labels[:train_size] x_test, y_test = lstm_input[train_size:], labels[train_size:] Building the Model Building a Keras model is a pretty straightforward process.  We'll use the Sequential API because it's simpler and we don't need the flexibility of the functional API.  Most of the following code should be easy enough to parse for meaning, even for people who aren't necessarily used to Keras.  There is one subtle point, though: the Embedding layer has input_dim=vocab_size+1 because we have 3001 input tokens: our 3000 words, and our 'padding symbol' 0. model = Sequential() model.add(Embedding(input_dim=vocab_size+1, output_dim=64, input_length=max_length)) model.add(LSTM(100)) Model.add(Dense(1, activation='sigmoid'))  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  model.fit(x_train, y_train, validation_data=(x_test, y_test), batch_size=64, epochs=3, verbose=2)  loss, accuracy = model.evaluate(x_test, y_test)     # Better: split the data into train/test/validate sets print(accuracy) This model achieves an accuracy of .848.  This is already better than a bag-of-words model trained on 160,000 data points, and we've only used 10,000 here.  One of the most well-known principles regarding neural networks is that they seem to improve, across the board, with more data.  I've gone ahead and trained models on larger data sets and found the following accuracies (in the following, the number given is the total size of the train/test/validation sets combined, and we've switched out \"test.ft.txt\" for \"train.ft.txt\" because the former has 'only' 400,000 examples): 400,000 examples:  accuracy .92525 800,000 examples:  accuracy .9353 1,600,000 examples:  accuracy .9444 2,500,000 examples:  accuracy .9469 After this point the computation takes too much memory to fit in RAM.  I decided to train models with more LSTM hidden units and 2,500,000 examples to the following results: 200 LSTM units:  accuracy .9498 300 LSTM units:  accuracy .9502 400 LSTM units:  accuracy .9493 It looks like there is some upside to increasing the number of units, but we should always be careful not to use too many as we want to avoid overfitting.  There's also the practical issue that if we want to someday deploy this model, fewer units is better since it takes less space to store the neural network and fewer operations to evaluate.  Just to check, I trained a model with 50 hidden LSTM units and got an accuracy of .9454.  So there's definitely a case to be made for using smaller networks. Improving the Model The previous computations would take a pretty long time on a CPU.  I ran all this on a GPU but at any given time the computation only used about 4% of GPU power.  We've also hit the wall on the number of training examples we can store in memory --- train.ft.txt contains 3,600,000 examples and test.ft.txt contains 400,000, so if we use these as our train/test set, we should be able to improve our model a bit more.  Thankfully there's a way to hit two birds with one stone here, and that's what we'll discuss next. But first, this whole thing is getting rather large.  It will be useful to refactor this code into several .py files to keep it modular and manageable.  I'll do this without listing all the code here - rather, you'll be able to find the refactored version in the folder 'Improved LSTM'.  Once again for anyone who's cloning this, you'll want to add test.ft.txt and train.ft.txt to this folder, or change the paths in the .py files to wherever your test and train data are sitting. Here's a brief list of the changes:  Move the data preparation to another file Add a file for the building of 'generators' Add a file for a 'shutup' module whose purpose is to squelch all the tensorflow announcements which appear in the terminal Added a separate pair of files to take care of Talos hyperparameter search Kept the LSTM in the main 'LSTM.py' file, and altered how it runs to take advantage of 'generators'  model.fit_generator() Keras provides an alternative method to model.fit(args) which goes by model.fit_generator(args).  The main difference is that with model.fit(x_train, y_train, ...), we have to provide the training set all at once, and the training set must fit in memory.  fit_generator, on the other hand, takes in a so-called data generator, which is an object of class keras.utils.Sequence that implements a __getitem__(index) method whose job it is to return the index'th training batch in a training set.  With this done, we can train our model via model.fit_generator(generator=myGenerator, workers=4, use_multiprocessing=True, ...). The generator feeds in the training data as it's needed; in particular we don't need to hold the entire training set in memory at once.  An added bonus comes with the second and third arguments of fit_generator:  the use_multiprocessing flag allows us to spawn several generators on distinct threads, and workers dictates how many threads to use.  This allows us to push more data to our GPU since we can have all our CPU cores doing this instead of just one. For my implementation of a generator, I decided to simply pre-generate all the training/testing data as we've done before and to save it to disk in numpy files, each containing batch_size training examples.  This is expensive on disk space but allows for very quick iteration through different models, especially because when using a large portion of the data set, data preparation takes a nontrivial amount of time. Oddly enough, when using workers > 1, each thread tries to generate all the data from scratch, and it does so every epoch!  So if I try to train a model with 8 workers and 10 epochs, it will try to prepare our data 80 times!  This runs significantly slower than using plain-old model.fit() AND uses all system memory by the time the second or third thread has started data preparation. I got around this by having my data preparation method first check a boolean to see whether data prep has been done already, and to pass if it has already been completed.  Before even defining the model I prepare the data so that in the first epoch we don't run out of memory.  After defining the model, Keras still tries to create the training set 80 times, but each time stops as soon as it hits the boolean flag. A quick warning to anyone cloning and running this repo:  Each distinct choice of (batch_size, vocab_size, max_length) causes the data preparation module to create a new directory on disk and populate it with the relevant data.  This can get out of hand quickly so be careful. Talos and Hyperparameter Search The previous improvements allowed for much quicker iteration through different model sizes and architectures, but each training run had to be set up 'by hand' - i.e. we'd have to go in and change vocab_size or num_units manually and train the model again to see whether it does better or worse than before.  Luckily there are some nice python modules out there which automate this task to an extent.  I decided to go with 'Talos'.  Installation is simple:  pip install talos or conda install talos. Since this isn't a tutorial, I won't go through how to use Talos, except to note that it's very easy to pick up.  We only have to do two things: (1) Create a python dictionary which contains the various hyperparameter options we want to try out: params = {'vocab_size': [3000, 6000, 9000], 'max_length': [300, 500, 800], 'num_data_points': [100000],                   'embedding_size': [64, 128, 256], 'batch_size': [128, 256, 512], 'optimizer': [Adam, Nadam],                   'loss': [binary_crossentropy, categorical_hinge, mean_squared_error], 'num_units': [50, 100, 200],                    'multiple_LSTM_layers': [False, True], 'lr': [0.001, .01, .1, 1, 10]} (2) Slightly alter the code which builds our model.  As an example, we change model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_size, input_length=max_length)) to model.add(Embedding(input_dim=params['vocab_size'] + 1, output_dim=params['embedding_size'], input_length=params['max_length'])). Finally, call the Talos.Scan() method on the newly-written model.  It will scan through all possibilities (or a user-provided proportion of them, chosen randomly) and output training results to a .csv file. One downside is that (it seems) Talos doesn't play nice with fit_generator, so I had to use model.fit() and pass in the entire training set at once.  I ran this overnight on a training set of size 100,000 and found the following about our models:  larger values of max_length and vocab_size improve model performance (on the validation set, here and in the following) large embedding sizes actually decrease performance.  This makes some sense: we want the embedding space to be small enough to force similar words to stick together. large batch sizes also decrease performance.  Granted, we only tested batch_sizes of 128, 256, and 512.  If we tested smaller batch sizes as well, we'd likely find a reasonable, medium-sized batch size works best. More LSTM units and additional LSTM layers help performance. It doesn't seem to make a difference whether we use Adam or Nadam. TO DO:  rerun to see which works best between loss functions.  Accidentally only ran it with binary_crossentropy the first time!  Although mean_squared_error is almost guaranteed to have abysmal performance, categorical_hinge might work nicely.  This Talos search was not exhaustive --- it only went through one one-hundredth of all possible combinations of the hyperparameters we fed it, and this took several hours.  But it was still worthwhile.  If we wanted, we could make changes to our params dictionary to reflect what we discovered and try to zero-in a bit more closely to an optimal set of hyperparameters.  At this level we can also change the proportion of combinations to try from .01 to .1.  Care must be taken, however, not to 'overfit to the validation set'.  That is, some models will by chance do better on the validation set, even though they're not actually better at the classification task.  For this reason it's good to have a separate test set hidden away somewhere, and it's also not a bad idea to stop the hyperparameter search after one or two iterations of this entire process. Pulling Everything Together We've gotten a good set of hyperparameters for our Amazon Review Sentiment Classifier, but this was only trained on 100,000 data points.  It's time to see how it does when trained on the entire training set (using fit_generator and multiple cores + GPU). The final architecture we're going with (which we found with help from Talos) is: vocab_size=9000 embedding_size=100 max_length=500  model = Sequential() model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_size, input_length=max_length)) model.add(CuDNNLSTM(200, return_sequences=True)) model.add(CuDNNLSTM(100)) model.add(Dense(1, activation='sigmoid')) model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) After training for 3 epochs, the test loss/accuracy are .1170, .9580. We could probably eke out another half to full percentage point of accuracy by using a pretrained word embedding, using dropout layers, using an attention mechanism, and iterating a bit more on our hyperparameter search. Questions, comments, suggestions and corrections are very welcome.  Just raise an issue. ", "has_readme": true, "readme_language": "English", "repo_tags": ["natural-language-processing", "machine-learning", "lstm", "scikit-learn", "nltk", "keras", "sentiment-analysis", "bag-of-words", "sequence-models", "hyperparameter-search", "tokenize", "binary-classification"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d6e"}, "repo_url": "https://github.com/raijisse/Udacity-SDC-P4-Behavioral-Cloning", "repo_name": "Udacity-SDC-P4-Behavioral-Cloning", "repo_full_name": "raijisse/Udacity-SDC-P4-Behavioral-Cloning", "repo_owner": "raijisse", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T10:32:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T17:08:02Z", "homepage": null, "size": 17524, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184923016, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/raijisse/Udacity-SDC-P4-Behavioral-Cloning/blob/a9323413515854b843b4f8690393b7f5e8e1bb50/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d6f"}, "repo_url": "https://github.com/olbedo/CarND-Behavioral-Cloning", "repo_name": "CarND-Behavioral-Cloning", "repo_full_name": "olbedo/CarND-Behavioral-Cloning", "repo_owner": "olbedo", "repo_desc": "Submission for Udacity Self-Driving Car Engineer Nanodegree Behavioral Cloning project", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T17:18:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T16:57:21Z", "homepage": null, "size": 610, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184921947, "is_fork": false, "readme_text": "Behaviorial Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d70"}, "repo_url": "https://github.com/mpc6/AudioRNN", "repo_name": "AudioRNN", "repo_full_name": "mpc6/AudioRNN", "repo_owner": "mpc6", "repo_desc": "Time domain trained audio generator", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T18:49:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T23:31:25Z", "homepage": null, "size": 253, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184954076, "is_fork": false, "readme_text": "AudioRNN A recurrent neural audio generation alogrithm that trains on and predicts PCM samples. Requirements(Pipfile included for creating virtual environment with all requirements):  keras numpy soundfile scipy matplotlib  Introduction Neural audio generation alogrithms require generating large datasets based on the long time context needed to accurately predict subsequent samples. Training to predict time series data with audio is a highly computationally complex problem due to the large sampling rates of high quality audio. Accuray is also a problem since the output range of PCM samples is (-32768, 32767). So AudioRNN predicts a probability distribution over the values 0-255 which is treated as a \u03bc-law encoded LPCM sample, which is then transformed into a PCM sample with post-processing. This allows the model to train categorically instead of a more complex regresssion problem. AudioRNN is a highly parameterized model that allows for the loading of data at any sample rate and converting it to a specified training sample rate. It also allows for generating datasets and training with a user specified time context. All input and targets are quantized to 8-bit representations by way of the \u03bc-law encoding. Since the \u03bc-law encoding is susceptible to noise in the higher frequencies, a pre-emphasis IR filter is applied to the audio prior to the encoding to increase the SNR at higher frequencies. The output of the model is decoded back to PCM and passed through the inverse of the pre-emphasis filter to undo the processing done on the input. Scripts   AudioRNN.py Master script used for generating data, training the model and inference (generating audio).   AudioRNNData.py Helper script with audio dataset generation functions, can also be run stand alone.   AudioRNNEval.py Evaluation script for plotting a trained model's loss and accuracy.   GRU Network  Model Metrics Sample Audio  Audio Sample with 1000@8kHz sample time context Audio Sample with 128@8kHz sample time context  Future Work ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d71"}, "repo_url": "https://github.com/dsamruddhi/MalariaDetection", "repo_name": "MalariaDetection", "repo_full_name": "dsamruddhi/MalariaDetection", "repo_owner": "dsamruddhi", "repo_desc": "Detecting malaria by classifying cell images into Infected / Not infected classes using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-04T18:44:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T18:43:27Z", "homepage": null, "size": 670, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184932219, "is_fork": false, "readme_text": "Malaria Cell Classification Project to identify malaria by classifying cell images into two classes - Infected / Not infected Data The dataset used for this project can be obtained from kaggle: https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria Usage Install the requirements pip install -r requirements.txt  Run the project python main.py  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/dsamruddhi/MalariaDetection/blob/2fe860b6a0a5d676736523b6f97a93f644ed8ec1/models/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d72"}, "repo_url": "https://github.com/hankkkwu/SDCND-P4-Behavioral_cloning", "repo_name": "SDCND-P4-Behavioral_cloning", "repo_full_name": "hankkkwu/SDCND-P4-Behavioral_cloning", "repo_owner": "hankkkwu", "repo_desc": "code, videos and trained models for project 4 - behavioral cloning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T08:23:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T15:10:12Z", "homepage": "", "size": 37567, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184910204, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. The Project My project includes the following files:  model.py containing the script to create and train the model drive.py for driving the car in autonomous mode model.h5 containing a trained convolution neural network writeup_report.md summarizing the results video.mp4 recording vehicle driving autonomously around the track one. track2.mp4 recording vehicle driving autonomously around the track two.  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  ", "has_readme": true, "readme_language": "English", "repo_tags": ["behavioral-cloning", "sdcnd-udacity"], "has_h5": true, "h5_files_links": ["https://github.com/hankkkwu/SDCND-P4-Behavioral_cloning/blob/b7f4ad9b5e26b8e07e6c9f3b55ee3f744b5c53b6/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d73"}, "repo_url": "https://github.com/reconjohn/dev", "repo_name": "dev", "repo_full_name": "reconjohn/dev", "repo_owner": "reconjohn", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T01:45:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-04T21:16:37Z", "homepage": null, "size": 14823, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184945115, "is_fork": false, "readme_text": "   pythonic package development tutorial There are great resources out there for getting your projects documented and distributed. I got started using shablona from the eScience institute at U Washington. I would still recommend that to folks. I've found, however, that it can be helpful to start bare-bones and walk through a tutorial to build your package up, to really understand how everything is working together. So in the following, if you follow the tutorial, you'll start with a basic directory structure, and proced to add on documentation, web hosting, continuous integration, coverage, and finally deploy your package on pypi. Overview By the grace of open-source-dev there are several free lunches you should know of:  sphinx  sphinx can be a bit finicky. The most important feature to introduce to you to today will be autodocs where we generate documentation from just your docstrings super cool!   github pages  Eventually I'll add a segment on getting your documentation on read the docs. But while code is still in development I've found rtd to be overkill. GH Pages is a simple alternative that hosts your static html files and doesn't require building your site on a remote server.   travis CI  \"CI\" stands for continuous integration. These folks provide you with a free service -- up to 1 hour of CPU time on their servers to run all of your unit tests.   coveralls  how much of that passed build is covered?!   pypi  You want people using your code as fast as possible, right?     Sphinx back to top Basic directory structure Clone this repo and cd into the main directory. Checkout the package organization: $ tree . \u251c\u2500\u2500 ECS_demo \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 data \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 climate_sentiment_m1.h5 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tweet_global_warming.csv \u2502\u00a0\u00a0 \u251c\u2500\u2500 input.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 tests \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_ECS_demo.py \u2502\u00a0\u00a0 \u2514\u2500\u2500 version.py \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 README.md \u251c\u2500\u2500 docs \u2502\u00a0\u00a0 \u251c\u2500\u2500 Makefile \u2502\u00a0\u00a0 \u251c\u2500\u2500 _static \u2502\u00a0\u00a0 \u251c\u2500\u2500 conf.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.rst \u2502\u00a0\u00a0 \u2514\u2500\u2500 source \u2502\u00a0\u00a0     \u251c\u2500\u2500 ECS_demo.core.rst \u2502\u00a0\u00a0     \u251c\u2500\u2500 ECS_demo.rst \u2502\u00a0\u00a0     \u2514\u2500\u2500 ECS_demo.tests.rst \u251c\u2500\u2500 examples \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.ipynb \u2514\u2500\u2500 setup.py  We're about to find out just how busy this directory structure can be with these added open source features. But for now, the main project lives under ECS_demo/ with tests/ and data/ subdirectories. Go ahead and inspect the contents of the core.py and test_ECS_demo.py files, in case you're interested. There's some common elements here in the package development world. core.py contains, well, the core code of the package. In a larger package you might have other modules living here such as analysis.py or visualize.py, depending on how you want to organize your code. For now, the core.py file contains four functions: load_data, data_setup, baseline_model and one class: Benchmark. You can learn more about pythonic naming conventions from the pep8 documentation. Makefile Time to get to Sphinx! cd over to the docs directory. In this tutorial, I've setup the appropriate rst files already. I haven't had excellent luck with using sphinx-quickstart or sphinx-autogen, personally. And so I will always start with a template such as this and modify the .rst files as needed. Suffice to say, if you are interested in creating your documentation from scratch I found this source helpful. All you need to do is type make html in the docs/ directory where your Makefile is sitting. and Sphinx will generate static html documents of your site. $ tree -L 2 . \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 _build \u2502\u00a0\u00a0 \u251c\u2500\u2500 doctrees \u2502\u00a0\u00a0 \u2514\u2500\u2500 html \u251c\u2500\u2500 _static \u251c\u2500\u2500 conf.py \u251c\u2500\u2500 index.rst \u2514\u2500\u2500 source     \u251c\u2500\u2500 ECS_demo.core.rst     \u251c\u2500\u2500 ECS_demo.rst     \u2514\u2500\u2500 ECS_demo.tests.rst  Use your preferred browser to checkout your site: open _build/html/index.html. If you navigate to the API you'll see how Sphinx autmoatically formats your docstrings for you, super neat!  Github Pages back to top At this point we're ready to make our documentation live. Personally, my favorite method while I'm still developing a project is Github pages. This is because github allows us to directly host our statically generated files with Sphinx. As your code grows up, you'll want to migrate to something more robust like readthedocs.io. The downside of doing this initially is that readthedocs compiles your website as you push to github and little changes in your code structure or prerequisites can break the build. It's just easier to put this work off until you're at version 0.0.1. Github UI In your browser, navigate to the settings folder for your cloned github project and scroll down until you see Github Pages. Change the Source option to master branch /docs folder and hit save. Local changes Now in your local repo we're going to do a bit of a juggling act. We've been working in docs/ directory we'll want to move this sphinx stuff to its own home and make sure our statically generated files live here. So in your main directory this would like: $ mkdir sphinx $ mv docs/* sphinx/ $ mv sphinx/_build/html/* docs/ $ tree -L 1 docs/ \u251c\u2500\u2500 docs \u2502\u00a0\u00a0 \u251c\u2500\u2500 _sources \u2502\u00a0\u00a0 \u251c\u2500\u2500 _static \u2502\u00a0\u00a0 \u251c\u2500\u2500 genindex.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 objects.inv \u2502\u00a0\u00a0 \u251c\u2500\u2500 py-modindex.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 search.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 searchindex.js \u2502\u00a0\u00a0 \u2514\u2500\u2500 source  By default, github uses jekyll to build its' sites. We'll want to turn this feature off since our site is already built. In the docs/ directory simply type touch .nojekyll. Now add/commit/push your changes. Your project is live under the url (yourgihtubname).io/ECS  Travis CI back to top Makefile We're going to use travis to run our unit tests. Good coding practice dictates that we also check our code for readability. This is done using pep8. Before we do this with travis we'll want to test our code locally. Same as with the autodocumentation, a Makefile makes this job easier for this. You'll create a new Makefile in the main directory and add the following: flake8:         @if command -v flake8 > /dev/null; then \\                 echo \"Running flake8\"; \\                 flake8 flake8 --ignore N802,N806,F401 `find . -name \\*.py | grep -v setup.py | grep -v /docs/ | grep -v /sphinx/`; \\         else \\                 echo \"flake8 not found, please install it!\"; \\                 exit 1; \\         fi;         @echo \"flake8 passed\"  Basically, we're asking flake8 to run some but not all, tests on some but not all, files. at the end of that file we'll also add: test:         py.test  You can now run make flake8 and make test and see that your package passes your unittests, locally. requirements.txt Travis CI needs to know how to build the environment to run our code. We'll do this with a requirements.txt file, also in the main directory: keras tensorflow scikit-learn pandas  .travis.yml If you haven't already, navigate over to travis-ci.org and create an account with them, then import the ECS project. To get this up and running with travis we'll need to add a .travis.yml file: language: python sudo: false  deploy:   provider: pypi   user: wesleybeckner   password:     secure:   on:     tags: true     repo: ECS  env:   global:     - PIP_DEPS=\"pytest coveralls pytest-cov flake8\"  python:   - '3.6'   install: - travis_retry pip install $PIP_DEPS - travis_retry pip install numpy cython - travis_retry pip install -r requirements.txt - travis_retry pip install -e .  before_script: # configure a headless display to test plot generation - \"export DISPLAY=:99.0\" - \"sh -e /etc/init.d/xvfb start\" - sleep 3 # give xvfb some time to start  script: - flake8 --ignore N802,N806,W503,F401 `find . -name \\*.py | grep -v setup.py | grep -v version.py | grep -v __init__.py | grep -v /docs/ | grep -v /sphinx/` - mkdir for_test - cd for_test - py.test --pyargs ECS_demo --cov-report term-missing --cov=ECS_demo  you'll notice that the .travis.yml file contains the same flake8 and py.test commands. git add/commit/push and checkout your passing travis build.  Coveralls back to top Other than setting up an account with coveralls and linking it to your github account, you don't have much to do here. At the end of your .travis.yml file add the following:  after_success: - coveralls  travis ci will now pass your build to coveralls.  pypi back to top setup.py contains the information that will launch our project on pypi. cd to your main directory and issue the command: python setup.py sdist upload (you'll need to have registered an email address + account with them before hand) At the end of this tutorial, your directory structure will have grown substantially! $ tree -L 3 . \u251c\u2500\u2500 ECS_demo \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 __pycache__ \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.cpython-36.pyc \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.cpython-36.pyc \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 input.cpython-36.pyc \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 version.cpython-36.pyc \u2502\u00a0\u00a0 \u251c\u2500\u2500 core.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 data \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 climate_sentiment_m1.h5 \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 tweet_global_warming.csv \u2502\u00a0\u00a0 \u251c\u2500\u2500 tests \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 __pycache__ \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 test_ECS_demo.py \u2502\u00a0\u00a0 \u2514\u2500\u2500 version.py \u251c\u2500\u2500 ECS_demo.egg-info \u2502\u00a0\u00a0 \u251c\u2500\u2500 PKG-INFO \u2502\u00a0\u00a0 \u251c\u2500\u2500 SOURCES.txt \u2502\u00a0\u00a0 \u251c\u2500\u2500 dependency_links.txt \u2502\u00a0\u00a0 \u251c\u2500\u2500 requires.txt \u2502\u00a0\u00a0 \u2514\u2500\u2500 top_level.txt \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 Makefile \u251c\u2500\u2500 README.md \u251c\u2500\u2500 dist \u2502\u00a0\u00a0 \u2514\u2500\u2500 ECS_demo-0.0.dev0.tar.gz \u251c\u2500\u2500 docs \u2502\u00a0\u00a0 \u251c\u2500\u2500 _sources \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.rst.txt \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 source \u2502\u00a0\u00a0 \u251c\u2500\u2500 _static \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 ajax-loader.gif \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 basic.css \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 comment-bright.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 comment-close.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 comment.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 css \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 doctools.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 documentation_options.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 down-pressed.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 down.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 file.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 fonts \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jquery-3.2.1.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 jquery.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 minus.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 plus.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 pygments.css \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 searchtools.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 underscore-1.3.1.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 underscore.js \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 up-pressed.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 up.png \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 websupport.js \u2502\u00a0\u00a0 \u251c\u2500\u2500 genindex.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 index.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 objects.inv \u2502\u00a0\u00a0 \u251c\u2500\u2500 py-modindex.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 search.html \u2502\u00a0\u00a0 \u251c\u2500\u2500 searchindex.js \u2502\u00a0\u00a0 \u2514\u2500\u2500 source \u2502\u00a0\u00a0     \u251c\u2500\u2500 ECS_demo.core.html \u2502\u00a0\u00a0     \u251c\u2500\u2500 ECS_demo.html \u2502\u00a0\u00a0     \u2514\u2500\u2500 ECS_demo.tests.html \u251c\u2500\u2500 examples \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.ipynb \u2502\u00a0\u00a0 \u251c\u2500\u2500 README.txt \u2502\u00a0\u00a0 \u251c\u2500\u2500 demo1.png \u2502\u00a0\u00a0 \u2514\u2500\u2500 demo2.png \u251c\u2500\u2500 requirements.txt \u251c\u2500\u2500 setup.py \u2514\u2500\u2500 sphinx     \u251c\u2500\u2500 Makefile     \u251c\u2500\u2500 _build     \u2502\u00a0\u00a0 \u251c\u2500\u2500 doctrees     \u2502\u00a0\u00a0 \u2514\u2500\u2500 html     \u251c\u2500\u2500 _static     \u251c\u2500\u2500 conf.py     \u251c\u2500\u2500 index.rst     \u2514\u2500\u2500 source         \u251c\u2500\u2500 ECS_demo.core.rst         \u251c\u2500\u2500 ECS_demo.rst         \u2514\u2500\u2500 ECS_demo.tests.rst  More to come  sdist vs bdist Google cloud platform Twitter API  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/reconjohn/dev/blob/5cf54c1c239ee43014d2846341c30b3a0d71bc97/ECS_demo/data/climate_sentiment_m1.h5"], "see_also_links": ["http://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html"], "reference_list": []}, {"_id": {"$oid": "5cf518767eb8d660a8f73d74"}, "repo_url": "https://github.com/MartingaleField/MartingaleField.github.io", "repo_name": "MartingaleField.github.io", "repo_full_name": "MartingaleField/MartingaleField.github.io", "repo_owner": "MartingaleField", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T12:14:02Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-04T17:27:42Z", "homepage": "", "size": 5985, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184925039, "is_fork": false, "readme_text": "  Table of Contents  Table of Contents Array  Two Sum Container With Most Water 3Sum 3Sum Closest 4Sum 4Sum II Remove Duplicates from Sorted Array Remove Duplicates from Sorted Array II Find Missing Positive Insert Interval Majority Element Majority Element II Kth Largest Element in an Array Minimum Size Subarray Sum Product of Array Except Self Missing Number Contains Duplicate III H-Index   Binary Search  Basics H-Index II   Linked List Binary Tree  Binary Tree Inorder Traversal Binary Tree Preorder Traversal Binary Tree Postorder Traversal Binary Tree Level Order Traversal Binary Tree Zigzag Level Order Traversal Same Tree Construct Binary Tree from Preorder and Inorder Traversal Construct Binary Tree from Inorder and Postorder Traversal   Binary Search Tree  Validate Binary Search Tree Recover Binary Search Tree Minimum Distance Between BST Nodes   Depth First Search  Generate Parentheses Sudoku Solver Combination Sum Combination Sum II Combination Sum III Permutations Permutations II N-Queens N-Queens II Combinations Subsets Subsets II   Design  LRU Cache   Pandas  Time Series Data Manipulation  Reindexing and NA-Filling Moving average Returns      Array Two Sum Given an array of integers, return indices of the two numbers such that they add up to a specific target. You may assume that each input would have exactly one solution, and you may not use the same element twice. Solution  vector<int> twoSum(vector<int> &nums, int target) {     vector<int> ans;     if (nums.empty()) return ans;      unordered_map<int, int> num_to_idx;     for (int i = 0; i < nums.size(); ++i) {         int gap = target - nums[i];         if (num_to_idx.find(gap) != num_to_idx.end()) {             ans.emplace_back(i);             ans.emplace_back(num_to_idx[gap]);             break;         }         num_to_idx.emplace(nums[i], i);     }     return ans; }  def twoSum(nums: List[int], target: int) -> List[int]:     num_to_idx = {}     for i, num in enumerate(nums):         gap = target - num         if gap not in num_to_idx:             num_to_idx[num] = i         else:             return i, num_to_idx[gap]   Container With Most Water Given n non-negative integers a1, a2, ..., an , where each represents a point at coordinate (i, ai). n vertical lines are drawn such that the two endpoints of line i is at (i, ai) and (i, 0). Find two lines, which together with x-axis forms a container, such that the container contains the most water.  Solution Use two pointers. Pointer i points to the first element and j to the last. The water volume is (j - i) * h where h = min(height[i], height[j]).  If there exists taller bar on the right of i than h, move i to it and check if we have a better result. If there exists taller bar on the left of j than h, move j to it and check if we have a better result.   int maxArea(vector<int> &height) {     int water = 0;     int i = 0, j = height.size() - 1;     while (i < j) {         int h = min(height[i], height[j]);         water = max(water, (j - i) * h);         while (height[i] <= h && i < j) i++;         while (height[j] <= h && i < j) j--;     }     return water; }  def maxArea(self, height: List[int]) -> int:     i, j = 0, len(height) - 1     ans = 0     while i < j:         h = min(height[i], height[j])         ans = max(ans, (j - i) * h)         while height[i] <= h and i < j:             i += 1         while height[j] <= h and i < j:             j -= 1     return ans   3Sum Given an array nums of n integers, are there elements a, b, c in nums such that a + b + c = 0? Find all unique triplets in the array which gives the sum of zero. The solution set must not contain duplicate triplets. Solution  vector<vector<int>> threeSum(vector<int> &nums) {     vector<vector<int>> result;     sort(nums.begin(), nums.end());     int n = nums.size();     for (int i = 0; i < n - 2; ++i) {         if (i > 0 && nums[i] == nums[i - 1]) continue;         int j = i + 1, k = n - 1;         while (j < k) {             int sum = nums[i] + nums[j] + nums[k];             if (sum < 0) {                 ++j;                 while (j < k && nums[j] == nums[j - 1]) ++j;             } else if (sum > 0) {                 --k;                 while (j < k && nums[k] == nums[k + 1]) --k;             } else {                 result.push_back({nums[i], nums[j++], nums[k--]});                 while (j < k && nums[j] == nums[j - 1] && nums[k] == nums[k + 1])                     ++j, --k;             }         }     }     return result; }  def threeSum(nums: 'List[int]') -> 'List[List[int]]':     ans = []     nums.sort()     n = len(nums)     for i in range(n - 2):         j, k = i + 1, n - 1         if i > 0 and nums[i] == nums[i - 1]:             continue         while j < k:             sum = nums[i] + nums[j] + nums[k]             if sum < 0:                 j += 1                 while j < k and nums[j] == nums[j - 1]:                     j += 1             elif sum > 0:                 k -= 1                 while j < k and nums[k] == nums[k + 1]:                     k -= 1             else:                 ans.append([nums[i], nums[j], nums[k]])                 j += 1                 k -= 1                 while j < k and nums[j] == nums[j - 1] and nums[k] == nums[k + 1]:                     j += 1                     k -= 1     return ans   3Sum Closest Given an array nums of n integers and an integer target, find three integers in nums such that the sum is closest to target. Return the sum of the three integers. You may assume that each input would have exactly one solution. Solution  int threeSumClosest(vector<int> &nums, int target) {     int res = nums[0] + nums[1] + nums[2], n = nums.size();     sort(nums.begin(), nums.end());     for (int i = 0; i < n; ++i) {         int j = i + 1, k = n - 1;         while (j < k) {             int diff = target - nums[i] - nums[j] - nums[k];             if (diff == 0)                 return target;             if (abs(diff) < abs(res - target)) {                 res = nums[i] + nums[j] + nums[k];             } else if (diff < 0) {                 k--;             } else {                 j++;             }         }     }     return res; }  def threeSumClosest(nums: 'List[int]', target: 'int') -> 'int':     nums.sort()     ans = nums[0] + nums[1] + nums[2]     n = len(nums)     for i in range(n - 2):         j, k = i + 1, n - 1         while j < k:             sum = nums[i] + nums[j] + nums[k]             diff = target - sum             if diff > 0:                 j += 1             elif diff < 0:                 k -= 1             else:                 ans = sum                 break             if abs(diff) < abs(target - ans):                 ans = sum     return ans   4Sum Given an array nums of n integers and an integer target, are there elements a, b, c, and d in nums such that a + b + c + d = target? Find all unique quadruplets in the array which gives the sum of target. The solution set must not contain duplicate quadruplets. Solution  vector<vector<int>> fourSum(vector<int> &nums, int target) {     vector<vector<int>> result;     int n = nums.size();     if (n < 4) return result;     sort(nums.begin(), nums.end());     for (int a = 0; a < n - 3; ++a) {         // Pruning         if (nums[a] + nums[n - 1] + nums[n - 2] + nums[n - 3] < target ||             nums[a] + nums[a + 1] + nums[a + 2] + nums[a + 3] > target ||             (a > 0 && nums[a] == nums[a - 1]))             continue;         for (int b = a + 1; b < n - 2; ++b) {             if (b > a + 1 && nums[b] == nums[b - 1])                 continue;             int c = b + 1, d = n - 1;             while (c < d) {                 int sum = nums[a] + nums[b] + nums[c] + nums[d];                 if (sum < target) {                     c++;                     while (c < d && nums[c] == nums[c - 1])                         c++;                 } else if (sum > target) {                     d--;                     while (c < d && nums[d] == nums[d + 1])                         d--;                 } else {                     result.push_back({nums[a], nums[b], nums[c++], nums[d--]});                     while (c < d && nums[c] == nums[c - 1] && nums[d] == nums[d + 1])                         c++, d--;                 }             }         }     }     return result; }   4Sum II Given four lists A, B, C, D of integer values, compute how many tuples (i, j, k, l) there are such that A[i] + B[j] + C[k] + D[l] is zero. To make problem a bit easier, all A, B, C, D have same length of N where 0 <= N <= 500. Solution  int fourSumCount(vector<int> &A, vector<int> &B, vector<int> &C, vector<int> &D) {     unordered_map<int, int> sum_freq;     int ans = 0;     for (int a : A)         for (int b : B)             sum_freq[a + b]++;     for (int c : C)         for (int d : D)             if (sum_freq.count(-(c + d)))                 ans += sum_freq[-(c + d)];     return ans; }   Remove Duplicates from Sorted Array Given a sorted array nums, remove the duplicates in-place such that each element appear only once and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Solution  int removeDuplicates(vector<int> &nums) {     if (nums.size() < 2)         return nums.size();     int j = 1;     for (int i = 1; i < nums.size(); ++i) {         if (nums[i] != nums[j - 1]) {             nums[j++] = nums[i];         }     }     return j; }   Remove Duplicates from Sorted Array II Given a sorted array nums, remove the duplicates in-place such that duplicates appeared at most twice and return the new length. Do not allocate extra space for another array, you must do this by modifying the input array in-place with O(1) extra memory. Solution At first glance, we can follow the same idea as previous problem. Compare nums[i] with the current last two elements of the new array. If either of the comparison return false, we can update the new array. In fact, we simply need to compare nums[i] == nums[j - 2]. If this returns false, we can update the new array no matter what.  If nums[i] == nums[j - 1], since we allow at most two duplicates, we can copy nums[i] to the end of the new array.   int removeDuplicates(vector<int> &nums) {     if (nums.size() < 3)         return nums.size();     int j = 2;     for (int i = 2; i < nums.size(); ++i) {         if (nums[i] != nums[j - 2]) {             nums[j++] = nums[i];         }     }     return j; }   Find Missing Positive Given an unsorted integer array, find the smallest missing positive integer. Your algorithm should run in O(n) time and uses constant extra space. Example Input: [3,4,-1,1] Output: 2  Solution  Scan through nums and swap each positive number A[i] with A[A[i]-1]. If A[A[i]-1] is again positive, swap it with A[A[A[i]-1]-1]... Do this iteratively until we meet a negative number or we have done put all the positive numbers at their correct locations. E.g. [3, 4, -1, 1] will become [1, -1, 3, 4]. Iterate integers 1 to n + 1 and check one by one if i is located at i - 1 already. If not, then i is the first missing positive integer.   def firstMissingPositive(nums: 'List[int]') -> 'int':     for i in range(len(nums)):         while nums[i] > 0 and nums[i] <= len(nums) and nums[i] != nums[nums[i] - 1]:             correct_idx = nums[i] - 1             nums[i], nums[correct_idx] = nums[correct_idx], nums[i]      for i in range(1, len(nums) + 1):         if nums[i - 1] != i:             return i     return len(nums) + 1   Insert Interval Given a set of non-overlapping intervals, insert a new interval into the intervals (merge if necessary). You may assume that the intervals were initially sorted according to their start times. Example Input: intervals = [[1,3],[6,9]], newInterval = [2,5] Output: [[1,5],[6,9]]  Solution  vector<Interval> insert(vector<Interval> &intervals, Interval newInterval) {     vector<Interval> result;     for (auto it = intervals.begin(); it != intervals.end(); it++) {         if (it->end < newInterval.start) {             result.emplace_back(*it);         } else if (it->start > newInterval.end) {             result.emplace_back(newInterval);             copy(it, intervals.end(), back_inserter(result));             return result;         } else {             newInterval.start = min(newInterval.start, it->start);             newInterval.end = max(newInterval.end, it->end);         }     }     result.push_back(newInterval);     return result; }  def insert(intervals: 'List[Interval]', newInterval: 'Interval') -> 'List[Interval]':     s, e = newInterval.start, newInterval.end     left_part = [_ for _ in intervals if _.end < s]     right_part = [_ for _ in intervals if _.start > e]     if left_part + right_part != intervals:         s = min(s, intervals[len(left_part)].start)         # a[~i] = a[len(a)-i-1], the i-th element from right to left         e = max(e, intervals[~len(right_part)].end)       return left_part + [Interval(s, e)] + right_part   Majority Element Given an array of size n, find the majority element. The majority element is the element that appears more than \u230a n/2 \u230b times. You may assume that the array is non-empty and the majority element always exist in the array. Example 1 Input: [3,2,3] Output: 3  Example 2 Input: [2,2,1,1,1,2,2] Output: 2  Solution  int majorityElement(vector<int> &nums) {     int candidate = nums[0], count = 0;     for (int num : nums) {         if (count == 0) {             candidate = num;         }         count += num == candidate ? 1 : -1;     }     return candidate; }  def majorityElement(nums: 'List[int]') -> 'int':     count = 0     for num in nums:         if count == 0:             candidate = num         count += 1 if candidate == num else -1     return candidate   Majority Element II Given an integer array of size n, find all elements that appear more than \u230a n/3 \u230b times. Note: The algorithm should run in linear time and in O(1) space. Solution  struct Candidate {     int num_, count_;      explicit Candidate(int num, int count) : num_(num), count_(count) {} };  vector<int> majorityElement(vector<int> &nums) {     vector<int> result;     if (nums.empty()) return result;     array<Candidate, 2> candidates{Candidate(0, 0), Candidate(1, 0)};     for (int num : nums) {         bool flag = false;         // If num is one of the candidates, increment its freq by 1         for (int i = 0; i < 2; ++i) {             if (candidates[i].num_ == num) {                 ++candidates[i].count_;                 flag = true;                 break;             }         }         if (flag) continue;         // If num is not one of the candidates and we are missing          // candidates, nominate it to be a new candidate         for (int i = 0; i < 2; ++i) {             if (candidates[i].count_ == 0) {                 candidates[i].count_ = 1;                 candidates[i].num_ = num;                 flag = true;                 break;             }         }         if (flag) continue;         // If num is not one of the candidates nor we are missing          // any candidates pair out current candidates by num         for (int i = 0; i < 2; ++i) {             --candidates[i].count_;         }     }     // We now have two candidates but we still need to check     // if both have votes more than n/3     for (int i = 0; i < 2; ++i) {         candidates[i].count_ = 0;     }     for (int num : nums) {         for (int i = 0; i < 2; ++i) {             if (candidates[i].num_ == num) {                 ++candidates[i].count_;                 break;             }         }     }     for (int i = 0; i < 2; ++i) {         if (candidates[i].count_ > nums.size() / 3)             result.emplace_back(candidates[i].num_);     }     return result; }   Kth Largest Element in an Array Find the kth largest element in an unsorted array. Note that it is the kth largest element in the sorted order, not the kth distinct element. Example 1: Input: [3,2,1,5,6,4] and k = 2 Output: 5  Example 2: Input: [3,2,3,1,2,4,5,5,6] and k = 4 Output: 4  Solution When nums.size() is small, sort it first and return the kth element.  def findKthLargest(nums: 'List[int]', k: 'int') -> 'int':     nums.sort(reverse=True)     return nums[k - 1] When nums.size() is large, use max heap.  import heapq  def findKthLargest(nums: 'List[int]', k: 'int') -> 'int':     nums = [-n for n in nums];     heapq.heapify(nums)     for _ in range(k):         ans = heapq.heappop(nums)     return -ans   Minimum Size Subarray Sum Given an array of n positive integers and a positive integer s, find the minimal length of a contiguous subarray of which the sum >= s. If there isn't one, return 0 instead. Example Input: s = 7, nums = [2,3,1,2,4,3] Output: 2 Explanation: the subarray [4,3] has the minimal length under the problem constraint.  Solution  int minSubArrayLen(int s, vector<int> &nums) {     int min_len = nums.size() + 1, sum = 0;     for (int i = 0, j = 0; j < nums.size(); j++) {         sum += nums[j];         while (sum >= s) {             min_len = min(min_len, j - i + 1);             sum -= nums[i++];         }     }     return min_len <= nums.size() ? min_len : 0; }   Product of Array Except Self Given an array nums of n integers where n > 1,  return an array output such that output[i] is equal to the product of all the elements of nums except nums[i]. Note: Please solve it without division and in O(n). Example: Input:  [1,2,3,4] Output: [24,12,8,6]  Solution  Iterate forwards over nums and generate output: 1,  A[0],   A[0]*A[1],  ...,    A[0]*A[1]*...*A[n-3],   A[0]*A[1]*...*A[n-2]   Iterate backwards over nums and update output: 1 * A[1]*...*A[n-1],  A[0] * A[2]*...*A[n-1],   A[0]*A[1] * A[3]*...*A[n-1],    ...,     A[0]*A[1]*...*A[n-3] * A[n-1],  A[0]*A[1]*...*A[n-2] * 1  which is the desired result.   def productExceptSelf(nums: 'List[int]') -> 'List[int]':     n = len(nums)     output = [1] * n      p = 1     for i in range(n):         output[i] *= p         p *= nums[i]      p = 1     for i in range(n - 1, -1, -1):         output[i] *= p         p *= nums[i]      return output   Missing Number Given an array containing n distinct numbers taken from 0, 1, 2, ..., n, find the one that is missing from the array. Your algorithm should run in linear runtime complexity. Could you implement it using only constant extra space complexity? Example 1: Input: [3,0,1] Output: 2  Example 2: Input: [9,6,4,2,3,5,7,0,1] Output: 8  Solution: Math The missing one is sum(0..n) - sum(nums).  def missingNumber(nums: 'List[int]') -> 'int':     n = len(nums)     return (n * (n + 1) // 2) - sum(nums) Solution: Bit Manipulation If A == B, then A ^ B == 0.  def missingNumber(nums: 'List[int]') -> 'int':     ans = len(nums)     for i in range(0, len(nums)):         ans ^= (nums[i] ^ i)     return ans   Contains Duplicate III Given an array of integers, find out whether there are two distinct indices i and j in the array such that the absolute difference between nums[i] and nums[j] is at most t and the absolute difference between i and j is at most k. Example 1: Input: nums = [1,2,3,1], k = 3, t = 0 Output: true  Example 2: Input: nums = [1,0,1,1], k = 1, t = 2 Output: true  Example 3: Input: nums = [1,5,9,1,5,9], k = 2, t = 3 Output: false  Solution: Sort Use a vector<pair<long, int>> to store (elem, index) pairs. Sort this vector. This will produce a similar structure to multimap<long, int> but we can do sliding-window technique on it using continuous indexing.  bool containsNearbyAlmostDuplicate(vector<int> &nums, int k, int t) {     vector<pair<long, int>> map;     for (int i = 0; i < nums.size(); ++i)         map.push_back({nums[i], i});     sort(map.begin(), map.end());     int j = 1;     for (int i = 0; i < map.size(); ++i) {         while (j < map.size() && abs(map[j].first - map[i].first) <= t) {             if (abs(map[j].second - map[i].second) <= k)                 return true;             j++;         }         if (j == i + 1) j++;     }     return false; }  def containsNearbyAlmostDuplicate(nums: List[int], k: int, t: int) -> bool:     map = [(e, i) for i, e in enumerate(nums)]     map.sort()     j = 1     for i in range(len(map)):         while j < len(map) and abs(map[j][0] - map[i][0]) <= t:             if abs(map[i][1] - map[j][1]) <= k:                 print(i, j)                 return True             j += 1         if j == i + 1:             j += 1     return False Solution: Ordered Set The sliding-window idea can also be implemented using set<long>, in which elements are ordered automatically.  bool containsNearbyAlmostDuplicate(vector<int> &nums, int k, int t) {     set<long> window; // set is ordered automatically     for (int i = 0; i < nums.size(); i++) {         // keep the set contains nums with |i - j| at most k         if (i > k) window.erase(nums[i - k - 1]);         // |x - nums[i]| <= t  ==> -t <= x - nums[i] <= t;         auto pos = window.lower_bound(static_cast<long>(nums[i]) - t); // x - nums[i] >= -t ==> x >= nums[i]-t         // x - nums[i] <= t ==> |x - nums[i]| <= t         if (pos != window.end() && *pos - nums[i] <= t) return true;         window.insert(nums[i]);     }     return false; }   H-Index Given an array of citations (each citation is a non-negative integer) of a researcher, write a function to compute the researcher's h-index. According to the definition of h-index on Wikipedia: \"A scientist has index h if h of his/her N papers have at least h citations each, and the other N \u2212 h papers have no more than h citations each.\" Example: Input: citations = [3,0,6,1,5] Output: 3  Explanation: [3,0,6,1,5] means the researcher has 5 papers in total and each of them had               received 3, 0, 6, 1, 5 citations respectively.               Since the researcher has 3 papers with at least 3 citations each and the remaining               two with no more than 3 citations each, her h-index is 3.  Note: If there are several possible values for h, the maximum one is taken as the h-index. Solution  If we sort the citations in decreasing order, the h-index is then the last position where the citation is greater than the position. For example, for input [3,3,3,0,6,1,5] ([6,5,3,3,3,1,0] after sorting), the last position where the citation is greater than the position is 3 (the position starts from 1). Hence the h-index is 3. Algorithm:   Counting sort: Take a cnt array of size N + 1. If a paper has a citation of c <= N, cnt[c]++; if c > N, cnt[N]++. The reason for the second if is that the h-index cannot be larger than N and so we can treat all citations larger than N the same.   We then scan from right to left, summing up cnt[i] along the way, until we reach a sum greater than or equal to the current index. Then this index is our h-index.   A simple implementation using built-in sort function can be  int hIndex(vector<int> &citations) {     sort(citations.begin(), citations.end(), greater<int>());     int h = 0;     for (int i = 0; i < citations.size(); ++i)         if (citations[i] > i) ++h;     return h; }  def hIndex(citations: List[int]) -> int:     h = 0     for i, c in enumerate(sorted(citations, reverse=True)):         if c > i:             h += 1     return h But this has a complexity of O(n logn), so is applicable if n is small. When n is large, we use counting sort.  int hIndex(vector<int> &citations) {     int n = citations.size();     vector<int> cnt(n + 1, 0);     for (int c : citations) {         if (c >= n)             cnt[n]++;         else             cnt[c]++;     }     int sum = 0;     for (int i = n; i >= 0; --i) {         sum += cnt[i];         if (sum >= i) return i;     }     return 0; }  def hIndex(self, citations: List[int]) -> int:     n = len(citations)     cnt = [0] * (n + 1);     for c in citations:         if c <= n:             cnt[c] += 1         else:             cnt[n] += 1      ans = 0     for i in range(n, -1, -1):         ans += cnt[i]         if ans >= i:             return i     return 0   Binary Search Basics  class BinSearch {     template<typename T>     static int search(vector<T> A, T K) {         int l = 0;         int u = A.size() - 1;         int m;         while (l <= u) {             m = l + ((u - l) >> 1);             if (A[m] < K) {                 l = m + 1;             } else if (A[m] == K) {                 return m;             } else {                 u = m - 1;             }         }         return -1;     } }; H-Index II Given an array of citations sorted in ascending order (each citation is a non-negative integer) of a researcher, write a function to compute the researcher's h-index. If there are several possible values for h, the maximum one is taken as the h-index. Solution The idea is to search for the first index so that citations[index] >= length(citations) - index  And return length - index as the result. The search can be done using binary search.  int hIndex(vector<int> &citations) {     int n = citations.size();     int left = 0, right = n - 1;     while (left <= right) {         int mid = left + ((right - left) >> 1);         if (citations[mid] == n - mid)             return n - mid;         else if (citations[mid] < n - mid)             left = mid + 1;         else             right = mid - 1;     }     return n - left; }  def hIndex(citations: List[int]) -> int:     n = len(citations)     left, right = 0, n - 1     while left <= right:         mid = left + ((right - left) >> 1)         if citations[mid] == n - mid:             return n - mid         elif citations[mid] < n - mid:             left = mid + 1         else:             right = mid - 1     return n - left   Linked List Binary Tree Binary Tree Inorder Traversal Given a binary tree, return the inorder traversal of its nodes' values. Example Input: [1,null,2,3]    1     \\      2     /    3  Output: [1,3,2]  Solution: Recursive  class Solution { public:     vector<int> inorderTraversal(TreeNode *root) {         inorder(root);         return result;     }  private:     vector<int> result;      void inorder(TreeNode *root) {         if (root == nullptr) return;                  inorder(root->left);         result.push_back(root->val);         inorder(root->right);     } }; Solution: Iterative  vector<int> inorderTraversal(TreeNode *root) {     vector<int> result;     stack<TreeNode *> s;      auto node = root;     while (!s.empty() || node != nullptr) {         if (node != nullptr) {             s.push(node);             node = node->left;         } else {             node = s.top();             s.pop();             result.emplace_back(node->val);             node = node->right;         }     }     return result; } Solution: Morris A binary tree is threaded by making all right child pointers that would normally be null point to the inorder successor of the node (if it exists), and all left child pointers that would normally be null point to the inorder predecessor of the node.  Inorder: ABCDEFGHI The threads we need for inorder traveral are A->B, C->D, E->F and H->I. At each subtree, we first thread p to cur (root of subtree) and next time we print out p we can use this thread to visit and print out cur. Pseudo Code 1. Initialize current as root  2. While current is not NULL    If current does not have a left child       ia) Print current\u2019s data       ib) Go to the right, i.e., current = current->right    Else       ea) Make current as right child of the rightmost node in current's left subtree       eb) Go to this left child, i.e., current = current->left  Time complexity O(n), space complexity O(1).  vector<int> inorderTraversal(TreeNode *root) {     vector<int> result;     TreeNode *cur = root, *p = nullptr;     while (cur != nullptr) {         if (cur->left == nullptr) { // cur has no left child             result.emplace_back(cur->val);             cur = cur->right;         } else { // cur has left child             // Let p point to the rightmost node of cur->left             for (p = cur->left; p->right != nullptr && p->right != cur; p = p->right);              if (p->right == nullptr) { // p has not been threaded to cur                 p->right = cur;                  cur = cur->left;             } else { // p is already threaded to cur                                 result.emplace_back(cur->val); // This line is different from preorder traversal                 p->right = nullptr; // remove thread                 cur = cur->right;             }         }     }     return result; }   Binary Tree Preorder Traversal Given a binary tree, return the preorder traversal of its nodes' values. Example Input: [1,null,2,3]    1     \\      2     /    3  Output: [1,2,3]  Solution: Recursive  class Solution { public:     vector<int> preorderTraversal(TreeNode *root) {         preorder(root);         return result;     }  private:     vector<int> result;      void preorder(TreeNode *root) {         if (root == nullptr) return;          result.push_back(root->val);         preorder(root->left);         preorder(root->right);     } }; Solution: Iterative  vector<int> preorderTraversal(TreeNode *root) {     vector<int> result;     stack<TreeNode *> s;      auto node = root;     while (!s.empty() || node != nullptr) {         if (node != nullptr) {             s.push(node);             result.emplace_back(node->val);             node = node->left;         } else {             node = s.top();             s.pop();             node = node->right;         }     }     return result; } Solution: Morris  Preorder: FBADCEGIH The threads we need for preorder traveral are A->B, C->D, E->F and H->I. The difference with inorder is that we print out cur before threading p to cur. The reason is that in preorder traversal, we need to visit the root first before traversing the left subtree.  vector<int> preorderTraversal(TreeNode *root) {     vector<int> result;     TreeNode *cur = root, *p = nullptr;     while (cur != nullptr) {         if (cur->left == nullptr) { // cur has no left child             result.emplace_back(cur->val);             cur = cur->right;         } else { // cur has left child             // Let p point to the rightmost node of cur->left             for (p = cur->left; p->right != nullptr && p->right != cur; p = p->right);              if (p->right == nullptr) { // p has not been threaded to cur                 result.emplace_back(cur->val); // This line is different from inorder traversal                 p->right = cur;                 cur = cur->left;             } else { // p is already threaded to cur                 p->right = nullptr; // remove thread                 cur = cur->right;             }         }     }     return result; }   Binary Tree Postorder Traversal Given a binary tree, return the postorder traversal of its nodes' values. Solution: Recursive  class Solution { public:     vector<int> postorderTraversal(TreeNode *root) {         preorder(root);         return result;     }  private:     vector<int> result;      void preorder(TreeNode *root) {         if (!root) return;          preorder(root->left);         preorder(root->right);         result.push_back(root->val);     } }; Solution: Iterative  vector<int> postorderTraversal(TreeNode *root) {     vector<int> result;     stack<TreeNode *> s;      auto node = root;     while (!s.empty() || node != nullptr) {         if (node != nullptr) {             s.push(node);             result.push_back(node->val);             node = node->right; // node = node->left for preorder         } else {             node = s.top();             s.pop();             node = node->left; // node = node->right for preorder         }     }     reverse(result.begin(), result.end()); // reverse in the end     return result; } Solution: Morris  void reverse(TreeNode *from, TreeNode *to) {     TreeNode *x = from, *y = from->right, *z;     if (from == to) return;     while (x != to) {         z = y->right;         y->right = x;         x = y;         y = z;     } }  template<typename func> void visit_reverse(TreeNode *from, TreeNode *to, func &visit) {     auto p = to;     reverse(from, to);      while (true) {         visit(p);         if (p == from) break;         p = p->right;     }     reverse(to, from); }  vector<int> postorderTraversal(TreeNode *root) {     vector<int> result;     TreeNode dummy(-1);     dummy.left = root;      auto visit = [&result](TreeNode *node) { result.emplace_back(node->val); };      TreeNode *cur = &dummy, *prev = nullptr, *p = nullptr;     while (cur != nullptr) {         if (cur->left == nullptr) {             prev = cur;             cur = cur->right;         } else {             for (p = cur->left; p->right != nullptr && p->right != cur; p = p->right);              if (p->right == nullptr) {                 p->right = cur;                 prev = cur;                 cur = cur->left;             } else {                 visit_reverse(cur->left, prev, visit);                 prev->right = nullptr;                 prev = cur;                 cur = cur->right;             }         }     }     return result; }   Binary Tree Level Order Traversal Given a binary tree, return the level order traversal of its nodes' values. (ie, from left to right, level by level). Example Given binary tree [3,9,20,null,null,15,7],     3    / \\   9  20     /  \\    15   7 return its level order traversal as: [   [3],   [9,20],   [15,7] ]  Solution: Recursive  class Solution { public:     vector<vector<int>> levelOrder(TreeNode *root) {         traverse(root, 0);         return result;     }  private:     vector<vector<int>> result;      void traverse(TreeNode *root, int level) {         if (root == nullptr) return;          if (level == result.size())             result.emplace_back(vector<int>{});          result[level].emplace_back(root->val);         traverse(root->left, level + 1);         traverse(root->right, level + 1);     } }; Solution: Iterative Do an preorder traversal using the interative way. Push each node and its level in the stack.  vector<vector<int>> levelOrder(TreeNode *root) {     vector<vector<int>> result;     stack<pair<TreeNode *, int>> s;     auto node = root;     int level = -1;     while (!s.empty() || node != nullptr) {         if (node != nullptr) {             s.push({node, ++level});             if (level == result.size())                 result.emplace_back(vector<int>{});             result[level].emplace_back(node->val);             node = node->left;         } else {             node = s.top().first;             level = s.top().second;             s.pop();             node = node->right;         }     }     return result; } Solution: Queue  vector<vector<int>> levelOrder(TreeNode *root) {     if (!root) return {};      vector<vector<int>> result;     queue<TreeNode *> current, next;     current.emplace(root);     while (!current.empty()) {         vector<int> level;         while (!current.empty()) {             auto node = current.front();             current.pop();             level.emplace_back(node->val);             if (node->left) next.emplace(node->left);             if (node->right) next.emplace(node->right);         }         result.emplace_back(level);         swap(next, current);     }     return result; }   Binary Tree Zigzag Level Order Traversal Given a binary tree, return the zigzag level order traversal of its nodes' values. (ie, from left to right, then right to left for the next level and alternate between). For example: Given binary tree [3,9,20,null,null,15,7],     3    / \\   9  20     /  \\    15   7  return its zigzag level order traversal as: [   [3],   [20,9],   [15,7] ]  Solution: Recursive  class Solution { public:     vector<vector<int>> zigzagLevelOrder(TreeNode *root) {         traverse(root, 0);         return result;     }  private:     vector<vector<int>> result;      void traverse(TreeNode *root, int level) {         if (root == nullptr) return;          if (level == result.size())             result.push_back({});          if (level % 2 == 0) // traverse left to right if row index is even             result[level].push_back(root->val);         else                // traverse right to left if row index is odd             result[level].insert(result[level].begin(), root->val);          traverse(root->left, level + 1);         traverse(root->right, level + 1);     } }; Solution: Iterative, Stack  vector<vector<int>> zigzagLevelOrder(TreeNode *root) {     vector<vector<int>> result;     stack<pair<TreeNode *, int>> s;     auto node = root;     int level = -1;     bool isLR = true;     while (!s.empty() || node != nullptr) {         if (node != nullptr) {             s.push({node, ++level});             if (level == result.size())                 result.push_back({});             if (level % 2 == 0)                 result[level].push_back(node->val);             else                 result[level].insert(result[level].begin(), node->val);             node = node->left;         } else {             node = s.top().first;             level = s.top().second;             s.pop();             node = node->right;         }     }     return result; }   Same Tree Given two binary trees, write a function to check if they are the same or not. Two binary trees are considered the same if they are structurally identical and the nodes have the same value. Solution: Recursive  bool isSameTree(TreeNode *p, TreeNode *q) {     if (!p && !q) return true;     if (!p || !q) return false;     return p->val == q->val            && isSameTree(p->left, q->left)            && isSameTree(p->right, q->right); } Solution: Iterative  bool isSymmetric(TreeNode *root) {     if (!root) return true;      stack<TreeNode *> s;     s.push(root->left);     s.push(root->right);      while (!s.empty()) {         auto p = s.top();         s.pop();         auto q = s.top();         s.pop();          if (!p && !q) continue;         if (!p || !q) return false;         if (p->val != q->val) return false;          s.push(p->right);         s.push(q->left);         s.push(p->left);         s.push(q->right);     }     return true; }   Construct Binary Tree from Preorder and Inorder Traversal Given preorder and inorder traversal of a tree, construct the binary tree. You may assume that duplicates do not exist in the tree. For example, given preorder = [3,9,20,15,7] inorder = [9,3,15,20,7]  Return the following binary tree:     3    / \\   9  20     /  \\    15   7  Solution Inorder: [ left subtree ] root [ right subtree ]  Preorder: root [ left subtree ] [ right subtree ]  We first find the position of root (i.e. *begin(preorder)) in inorder vector. The size of left subtree is then distance(begin(inorder), in_root_pos). Then recursively build left and right subtrees.  For left subtree, the inorder vector is inorder[0..left_size - 1] the preorder vector is preorder[1..left_size] For right subtree, the inorder vector is inorder[in_root_pos + 1..in_last] the preorder vector is preorder[left_size + 1..pre_last]   template<typename InputIterator> TreeNode *buildTree(InputIterator pre_first, InputIterator pre_last,                     InputIterator in_first, InputIterator in_last) {     if (pre_first == pre_last) return nullptr;     if (in_first == in_last) return nullptr;      auto root = new TreeNode(*pre_first);     auto in_root_pos = find(in_first, in_last, *pre_first);     auto left_size = distance(in_first, in_root_pos);      root->left = buildTree(next(pre_first), next(pre_first, left_size + 1),                            in_first, next(in_first, left_size));     root->right = buildTree(next(pre_first, left_size + 1),                             pre_last, next(in_root_pos), in_last);      return root; }  TreeNode *buildTree(vector<int> &preorder, vector<int> &inorder) {     return buildTree(begin(preorder), end(preorder), begin(inorder), end(inorder)); }   Construct Binary Tree from Inorder and Postorder Traversal Given inorder and postorder traversal of a tree, construct the binary tree. You may assume that duplicates do not exist in the tree. For example, given inorder = [9,3,15,20,7] postorder = [9,15,7,20,3]  Return the following binary tree:     3    / \\   9  20     /  \\    15   7  Solution Inorder: [ left subtree ] root [ right subtree ]  Postorder: [ left subtree ] [ right subtree ] root   template<typename InputIterator> TreeNode *buildTree(InputIterator in_first, InputIterator in_last,                     InputIterator post_first, InputIterator post_last) {     if (in_first == in_last) return nullptr;     if (post_first == post_last) return nullptr;      auto root_val = *prev(post_last);     TreeNode *root = new TreeNode(root_val);      auto in_root_pos = find(in_first, in_last, root_val);     auto left_size = distance(in_first, in_root_pos);      root->left = buildTree(in_first, in_root_pos,                            post_first, next(post_first, left_size));     root->right = buildTree(next(in_root_pos), in_last,                             next(post_first, left_size), prev(post_last));     return root; }  TreeNode *buildTree(vector<int> &inorder, vector<int> &postorder) {     return buildTree(begin(inorder), end(inorder), begin(postorder), end(postorder)); }   Binary Search Tree Validate Binary Search Tree Given a binary tree, determine if it is a valid binary search tree (BST). Assume a BST is defined as follows:  The left subtree of a node contains only nodes with keys less than the node's key. The right subtree of a node contains only nodes with keys greater than the node's key. Both the left and right subtrees must also be binary search trees.  Solution: Recursive  bool helper(TreeNode *node, TreeNode *min_node = nullptr, TreeNode *max_node = nullptr) {     if (node == nullptr) return true;      if ((min_node != nullptr && node->val <= min_node->val) ||         (max_node != nullptr && node->val >= max_node->val))         return false;      return helper(node->right, node, max_node) && helper(node->left, min_node, node); }  bool isValidBST(TreeNode *root) {     return helper(root); } Solution: Iterative Do an inorder traversal and compare node->val with pre-val along the way.  bool isValidBST(TreeNode *root) {     vector<int> result;     stack<TreeNode *> s; // nodes to be visited     TreeNode *pre = nullptr, *node = root;     while (!s.empty() || node != nullptr) {         if (node != nullptr) {             s.push(node);             node = node->left;         } else {             node = s.top();             s.pop();             if (pre != nullptr && node->val <= pre->val)                 return false;             pre = node;             node = node->right;         }     }     return true; }   Recover Binary Search Tree Two elements of a binary search tree (BST) are swapped by mistake. Recover the tree without changing its structure. Example 1: Input: [1,3,null,null,2]     1   /  3   \\    2  Output: [3,1,null,null,2]     3   /  1   \\    2  Example 2: Input: [3,1,4,null,null,2]    3  / \\ 1   4    /   2  Output: [2,1,4,null,null,3]    2  / \\ 1   4    /   3  Solution: Straightforward, O(n) space We can use a vector<TreeNode *> inorder to store the inorder-traversed nodes. This can be implemented by any one of the inorder traversal approaches (recursive, stack-based iterative or Morris). If the BST is valid, inorder should be non-decreasing. We can then forwards-iterate the vector and find the node broken1 which violates the ordering. Similarly, we can backwards-iterate the vector and find the node broken2 which violates the ordering. Swapping broken1 and broken2 yields the valid BST.  void recoverTree(TreeNode *root) {     vector<TreeNode *> inorder;     stack<TreeNode *> s;     auto cur = root, broken1 = root, broken2 = root;     while (!s.empty() || cur) {         if (cur) {             s.push(cur);             cur = cur->left;         } else {             cur = s.top();             s.pop();             inorder.emplace_back(cur);             cur = cur->right;         }     }     for (int i = 0; i < inorder.size() - 1; ++i) {         if (inorder[i]->val > inorder[i + 1]->val) {             broken1 = inorder[i];             break;         }     }     for (int i = inorder.size() - 1; i > 0; --i) {         if (inorder[i]->val < inorder[i - 1]->val) {             broken2 = inorder[i];             break;         }     }     swap(broken1->val, broken2->val); } Solution: Iterative, Stack  class Solution { public:     void recoverTree(TreeNode *root) {         stack<TreeNode *> s;         TreeNode *pre = nullptr, *cur = root;         while (!s.empty() || cur != nullptr) {             if (cur != nullptr) {                 s.push(cur);                 cur = cur->left;             } else {                 cur = s.top();                 s.pop();                 detect(pre, cur);                 pre = cur;                 cur = cur->right;             }         }         swap(broken1->val, broken2->val);     }  private:     TreeNode *broken1 = nullptr, *broken2 = nullptr;      void detect(TreeNode *prev, TreeNode *curr) {         if (prev != nullptr && prev->val > curr->val) {             if (broken1 == nullptr)                 broken1 = prev;             broken2 = curr;         }     } }; Solution: Recursive, O(1) space Actually, we don't need to record all inorder-traversed nodes. We simply need a TreeNode *pre which points to the inorder predecessor of the currently visiting node.  class Solution { public:     void recoverTree(TreeNode *root) {         inorder(root);         swap(broken1->val, broken2->val);     }  private:     TreeNode *broken1 = nullptr, *broken2 = nullptr, *pre = nullptr;      void detect(TreeNode *prev, TreeNode *curr) {         if (prev != nullptr && prev->val > curr->val) {             if (broken1 == nullptr)                 broken1 = prev;             broken2 = curr;         }     }      void inorder(TreeNode *cur) {         if (cur == nullptr) return;          inorder(cur->left);         detect(pre, cur);         pre = cur;         inorder(cur->right);     } }; Solution: Morris  class Solution { public:     void recoverTree(TreeNode *root) {         TreeNode *prev = nullptr, *cur = root, *p = nullptr;         while (cur != nullptr) {             if (cur->left == nullptr) {                 detect(prev, cur); // \"visit\"                 prev = cur;                 cur = cur->right;             } else {                 for (p = cur->left; p->right != nullptr && p->right != cur; p = p->right);                  if (p->right == nullptr) {                     p->right = cur;                     cur = cur->left;                 } else {                     detect(prev, cur); // \"visit\"                     prev = cur;                     p->right = nullptr;                     cur = cur->right;                 }             }         }         swap(broken1->val, broken2->val);     }  private:     TreeNode *broken1 = nullptr, *broken2 = nullptr;      void detect(TreeNode *prev, TreeNode *curr) {         if (prev != nullptr && prev->val > curr->val) {             if (broken1 == nullptr)                 broken1 = prev;             broken2 = curr;         }     } };   Minimum Distance Between BST Nodes Given a Binary Search Tree (BST) with the root node root, return the minimum difference between the values of any two different nodes in the tree. Example: Input: root = [4,2,6,1,3,null,null] Output: 1 Explanation: Note that root is a TreeNode object, not an array.  The given tree [4,2,6,1,3,null,null] is represented by the following diagram:            4         /   \\       2      6      / \\         1   3    while the minimum difference in this tree is 1, it occurs between node 1 and node 2, also between node 3 and node 2.  Solution: Recursive Do an inorder traversal and record predecessor node pre along the way. The minimum distance can only happen between two consecutive nodes in the traversal.  class Solution { public:     int minDiffInBST(TreeNode *root) {         return inorder(root);     }  private:     TreeNode *pre = nullptr;      int inorder(TreeNode *cur) {         if (cur == nullptr) return INT_MAX;          int min_dist = inorder(cur->left);         if (pre != nullptr)             min_dist = min(min_dist, cur->val - pre->val);         pre = cur;         return min(min_dist, inorder(cur->right));     } };   Depth First Search Generate Parentheses Given n pairs of parentheses, write a function to generate all combinations of well-formed parentheses. For example, given n = 3, a solution set is: [   \"((()))\",   \"(()())\",   \"(())()\",   \"()(())\",   \"()()()\" ]  Solution: DFS  class Solution { public:     vector<string> generateParenthesis(int n) {         helper(n, n, \"\");         return result;     }  private:     vector<string> result;      void helper(int n_left, int n_right, string valid_prefix) {         if (!n_right) {             result.emplace_back(valid_prefix);             return;         }         if (n_left > 0)              helper(n_left - 1, n_right, valid_prefix + '(');         if (n_right > n_left)              helper(n_left, n_right - 1, valid_prefix + ')');     } };  def generateParenthesis(n: 'int') -> 'List[str]':     def backtrack(s, l, r):         if l == 0 and r == 0:             ans.append(s)             return         if l > 0:             backtrack(s + '(', l - 1, r)         if r > l:             backtrack(s + ')', l, r - 1)      ans = []     backtrack(\"\", n, n)     return ans   Sudoku Solver Write a program to solve a Sudoku puzzle by filling the empty cells. A sudoku solution must satisfy all of the following rules:  Each of the digits 1-9 must occur exactly once in each row. Each of the digits 1-9 must occur exactly once in each column. Each of the the digits 1-9 must occur exactly once in each of the 9 3x3 sub-boxes of the grid. Empty cells are indicated by the character '.'.  You may assume that the given Sudoku puzzle will have a single unique solution.   Solution: Backtracking  class Solution { public:     void solveSudoku(vector<vector<char>> &board) {         if (board.size() < 9 || board[0].size() < 9)             return;         bool res = dfs(board, 0, 0);     }  private:     bool dfs(vector<vector<char>> &board, int i, int j) {         if (i == 9) return true;          int i2 = (i + (j + 1) / 9), j2 = (j + 1) % 9;         if (board[i][j] != '.') {             if (!isValid(board, i, j))                 return false;             return dfs(board, i2, j2);         } else {             for (int k = 0; k < 9; k++) {                 board[i][j] = '1' + k;                 if (isValid(board, i, j) && dfs(board, i2, j2))                     return true;             }             board[i][j] = '.';             return false;         }     }      bool isValid(vector<vector<char>> &board, int x, int y) {         int i, j;         for (i = 0; i < 9; ++i)              if (i != x && board[i][y] == board[x][y])                 return false;          for (j = 0; j < 9; ++j)              if (j != y && board[x][j] == board[x][y])                 return false;          for (i = 3 * (x / 3); i < 3 * (x / 3 + 1); ++i)              for (j = 3 * (y / 3); j < 3 * (y / 3 + 1); ++j)                  if ((i != x || j != y) && board[i][j] == board[x][y])                     return false;         return true;     } }; Solution: Backtracking with Caching  class Solution { public:     // row[i][j], column[i][j], subcube[i][j] represents repectively     // if row/column/subcube i (1..9) has number j (1..9)     // combine them into one bitset with size 9 * 9 * 3     bitset<9 * 9 * 3> flag;      void solveSudoku(vector<vector<char>> &board) {         if (board.size() < 9) return;          flag.reset();         for (uint8_t i = 0; i < 9; i++) {             for (uint8_t j = 0; j < 9; j++) {                 if (board[i][j] == '.') continue;                  auto num = static_cast<uint8_t>(board[i][j] - '1');                 auto cube = static_cast<uint8_t>(i / 3 * 3 + j / 3);                 auto row_num = static_cast<uint8_t>(i * 9 + num);                 auto col_num = static_cast<uint8_t>(j * 9 + num + 81);                 auto cb_num = static_cast<uint8_t>(cube * 9 + num + 81 * 2);                 if (flag[row_num] || flag[col_num] || flag[cb_num])                     return;                 flag.set(row_num);                 flag.set(col_num);                 flag.set(cb_num);             }         }         step(board, 0, 0);     }      bool step(vector<vector<char>> &board, uint8_t i, uint8_t j) {         if (i == 9) return true;          auto i2 = static_cast<uint8_t>(i + (j + 1) / 9);         auto j2 = static_cast<uint8_t>((j + 1) % 9);         if (board[i][j] != '.') {             if (i == 8 && j == 8)                  return true;             else                  return step(board, i2, j2);         }         auto cube = static_cast<uint8_t>(i / 3 * 3 + j / 3);         for (uint8_t k = 0; k < 9; k++) {             auto row_num = static_cast<uint8_t>(i * 9 + k);             auto col_num = static_cast<uint8_t>(j * 9 + k + 81);             auto cb_num = static_cast<uint8_t>(cube * 9 + k + 81 * 2);             if (flag[row_num] || flag[col_num] || flag[cb_num])                 continue;             flag.set(row_num);             flag.set(col_num);             flag.set(cb_num);             board[i][j] = '1' + k;              if (step(board, i2, j2))                 return true;             flag.reset(row_num);             flag.reset(col_num);             flag.reset(cb_num);             board[i][j] = '.';         }         return false;     } };   Combination Sum Given a set of candidate numbers (candidates) (without duplicates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target. The same repeated number may be chosen from candidates unlimited number of times. Note:  All numbers (including target) will be positive integers. The solution set must not contain duplicate combinations.  Example 1: Input: candidates = [2,3,6,7], target = 7, A solution set is: [   [7],   [2,2,3] ] Example 2: Input: candidates = [2,3,5], target = 8, A solution set is: [   [2,2,2,2],   [2,3,3],   [3,5] ] Solution: DFS  class Solution { public:     vector<vector<int>> combinationSum(vector<int> &candidates, int target) {         sort(candidates.begin(), candidates.end());         dfs(candidates, target, 0);         return result;     }  private:     vector<vector<int>> result;     vector<int> path;      void dfs(vector<int> &candidates, int gap, int cur) {         if (!gap) {             result.push_back(path);             return;         }                  for (int i = cur; i < candidates.size(); ++i) {             if (gap < candidates[i]) break;             path.emplace_back(candidates[i]);             // next step starts from i because duplicates are allowed             dfs(candidates, gap - candidates[i], i);             path.pop_back();         }     } };  def combinationSum(candidates: 'List[int]', target: 'int') -> 'List[List[int]]':     def dfs(gap, cur):         if gap == 0:             result.append(path[:])             return         for i in range(cur, len(candidates)):             if gap < candidates[i]:                 break             path.append(candidates[i])             # next step starts from i because duplicates are allowed             dfs(gap - candidates[i], i)             path.pop()      result = []     path = []     candidates.sort()     dfs(target, 0)     return result   Combination Sum II Given a collection of candidate numbers (candidates) and a target number (target), find all unique combinations in candidates where the candidate numbers sums to target. Each number in candidates may only be used once in the combination. Note:  All numbers (including target) will be positive integers. The solution set must not contain duplicate combinations.  Example 1: Input: candidates = [10,1,2,7,6,1,5], target = 8, A solution set is: [   [1, 7],   [1, 2, 5],   [2, 6],   [1, 1, 6] ] Example 2: Input: candidates = [2,5,2,1,2], target = 5, A solution set is: [   [1,2,2],   [5] ] Solution: DFS  class Solution { public:     vector<vector<int>> combinationSum2(vector<int> &nums, int target) {         sort(nums.begin(), nums.end());         dfs(nums, target, 0);         return this->result;     }  private:     vector<vector<int>> result;     vector<int> path;      void dfs(vector<int> &nums, int gap, int cur) {         if (!gap) {             result.push_back(path);             return;         }         for (int i = cur; i < nums.size(); ++i) {             if (gap < nums[i]) break;             path.emplace_back(nums[i]);             // next step starts from i+1 to avoid using the same number again             dfs(nums, gap - nums[i], i + 1);             path.pop_back();              // Skip duplicates             while (i < nums.size() - 1 && nums[i] == nums[i + 1])                  ++i;         }     } };  def combinationSum2(candidates: 'List[int]', target: 'int') -> 'List[List[int]]':     def dfs(gap, cur):         if gap == 0:             result.append(path[:])             return         prev = None         for i in range(cur, len(candidates)):             if gap < candidates[i]:                 break              # Skip duplicates             if prev == candidates[i]:                 continue             prev = candidates[i]              path.append(candidates[i])             # next step starts from i+1 to avoid using the same number again             dfs(gap - candidates[i], i + 1)             path.pop()      result = []     path = []     candidates.sort()     dfs(target, 0)     return result   Combination Sum III Find all possible combinations of k numbers that add up to a number n, given that only numbers from 1 to 9 can be used and each combination should be a unique set of numbers. Note:  All numbers will be positive integers. The solution set must not contain duplicate combinations.  Example 1: Input: k = 3, n = 7 Output: [[1,2,4]] Example 2: Input: k = 3, n = 9 Output: [[1,2,6], [1,3,5], [2,3,4]] Solution: DFS  class Solution { public:     vector<vector<int>> combinationSum3(int k, int n) {         dfs(k, n, 1);         return this->result;     }  private:     vector<vector<int>> result;     vector<int> path;      void dfs(int n_left, int gap, int cur) {         if (!gap && !n_left) {             result.push_back(path);             return;         }         if (n_left <= 0) return;          for (int i = cur; i <= 9; ++i) {             if (gap < i) break;             path.emplace_back(i);             dfs(n_left - 1, gap - i, i + 1);             path.pop_back();         }     } };  def combinationSum3(k: 'int', n: 'int') -> 'List[List[int]]':     def dfs(left, gap, cur):         if left == 0 and gap == 0:             result.append(path[:])             return         if left == 0:             return         for i in range(cur, 10):             if gap < i:                 break             path.append(i)             dfs(left - 1, gap - i, i + 1)             path.pop()      result, path = [], []     dfs(k, n, 1)     return result   Permutations Given a collection of distinct integers, return all possible permutations. Example: Input: [1,2,3] Output: [   [1,2,3],   [1,3,2],   [2,1,3],   [2,3,1],   [3,1,2],   [3,2,1] ]  Solution: DFS I   class Solution { public:     vector<vector<int>> permute(vector<int> &nums) {         for (int i : nums)             used[i] = false;         dfs(nums);         return result;     }  private:     vector<vector<int>> result;     vector<int> path;     unordered_map<int, bool> used;      void dfs(vector<int> &nums) {         if (path.size() == nums.size()) {             result.emplace_back(path);             return;         }         for (int i : nums) {             if (used[i]) continue;             used[i] = true;             path.emplace_back(i);             dfs(nums);             path.pop_back();             used[i] = false;         }     } };  from collections import defaultdict  def permute(nums: 'List[int]') -> 'List[List[int]]':     def dfs():         if len(path) == len(nums):             result.append(path[:])             return         for num in nums:             if used[num]:                 continue             used[num] = True             path.append(num)             dfs()             path.pop()             used[num] = False      result, path = [], []     used = defaultdict(bool)     dfs()     return result Solution: DFS II   class Solution { public:     vector<vector<int>> permute(vector<int> &nums) {         dfs(nums, 0);         return result;     }  private:     vector<vector<int>> result;      void dfs(vector<int> &nums, int start) {         if (start == nums.size()) {             result.emplace_back(nums);             return;         }          for (int i = start; i < nums.size(); ++i) {             swap(nums[i], nums[start]);             dfs(nums, start + 1);             swap(nums[start], nums[i]);         }     } };  def permute(nums: 'List[int]') -> 'List[List[int]]':     def dfs(start):         if start == len(nums):             result.append(nums[:])             return         for i in range(start, len(nums)):             nums[i], nums[start] = nums[start], nums[i]             dfs(start + 1)             nums[i], nums[start] = nums[start], nums[i]      result = []     dfs(0)     return result   Permutations II Given a collection of numbers that might contain duplicates, return all possible unique permutations. Example: Input: [1,1,2] Output: [   [1,1,2],   [1,2,1],   [2,1,1] ]  Solution: DFS  class Solution { public:     vector<vector<int>> permuteUnique(vector<int> &nums) {         int n = nums.size();         for (int i : nums)              cnt[i]++;         dfs(n, nums);         return result;     }  private:     vector<vector<int>> result;     vector<int> path;     unordered_map<int, int> cnt;      void dfs(int gap, vector<int> &nums) {         if (!gap) {             result.emplace_back(path);             return;         }         for (auto &p : cnt) {             if (p.second <= 0) continue;             p.second--;             path.emplace_back(p.first);             dfs(gap - 1, nums);             path.pop_back();             p.second++;         }     } };  from collections import Counter  def permuteUnique(nums: List[int]) -> List[List[int]]:     def dfs():         if len(path) == len(nums):             result.append(path[:])             return         for num in cnt:             if cnt[num] == 0:                 continue             cnt[num] -= 1             path.append(num)             dfs()             path.pop()             cnt[num] += 1      result, path = [], []     cnt = Counter(nums)     dfs()     return result   N-Queens The n-queens puzzle is the problem of placing n queens on an n\u00d7n chessboard such that no two queens attack each other.  Given an integer n, return all distinct solutions to the n-queens puzzle. Each solution contains a distinct board configuration of the n-queens' placement, where 'Q' and '.' both indicate a queen and an empty space respectively. Example: Input: 4 Output: [  [\".Q..\",  // Solution 1   \"...Q\",   \"Q...\",   \"..Q.\"],   [\"..Q.\",  // Solution 2   \"Q...\",   \"...Q\",   \".Q..\"] ] Explanation: There exist two distinct solutions to the 4-queens puzzle as shown above.  Solution: DFS  class Solution { public:     vector<vector<string>> solveNQueens(int n) {         col = vector<bool>(n, false);         main_diag = vector<bool>(2 * n - 1, false);         anti_diag = vector<bool>(2 * n - 1, false);          path = vector<string>(n, string(n, '.'));         dfs(0);         return result;     }  private:     vector<bool> col, main_diag, anti_diag;     vector<string> path;     vector<vector<string>> result;      void dfs(int row) {         int n = path.size();         if (row == n) {             result.emplace_back(path);             return;         }         for (int i = 0; i < n; ++i) {             bool ok = !col[i] && !main_diag[n - 1 + row - i] && !anti_diag[row + i];             if (!ok) continue;             path[row][i] = 'Q';             col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = true;             dfs(row + 1);             path[row][i] = '.';             col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = false;         }     } };  def solveNQueens(self, n: int) -> List[List[str]]:     def dfs(row):         if row == n:             result.append([\"\".join(s) for s in path])             return         for i in range(n):             if not col[i] and not main_diag[n - 1 + row - i] and not anti_diag[row + i]:                 path[row][i] = 'Q'                 col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = True                 dfs(row + 1)                 col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = False                 path[row][i] = '.'      result, path = [], [['.'] * n for _ in range(n)]     col, main_diag, anti_diag = [False] * n, [False] * (2 * n - 1), [False] * (2 * n - 1)     dfs(0)     return result   N-Queens II Follow up for N-Queens problem. Now, instead of outputting board configurations, return the total number of distinct solutions. Solution: DFS Similar as N-Queens. This time use a global count variable to keep track of the total number of distinct solutions.  class Solution { public:     int totalNQueens(int n) {         col = vector<bool>(n, false);         main_diag = vector<bool>(2 * n - 1, false);         anti_diag = vector<bool>(2 * n - 1, false);          dfs(0);         return result;     }  private:     vector<bool> col, main_diag, anti_diag;     int result = 0;      void dfs(int row) {         int n = col.size();         if (row == n) {             result += 1;             return;         }         for (int i = 0; i < n; ++i) {             if (!col[i] && !main_diag[n - 1 + row - i] && !anti_diag[row + i]) {                 col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = true;                 dfs(row + 1);                 col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = false;             }         }     } };  class Solution:         def totalNQueens(self, n: int) -> int:         def dfs(row):             if row == n:                 self.cnt += 1                 return             for i in range(n):                 if not col[i] and not main_diag[n - 1 + row - i] and not anti_diag[row + i]:                     col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = True                     dfs(row + 1)                     col[i] = main_diag[n - 1 + row - i] = anti_diag[row + i] = False          self.cnt = 0         col, main_diag, anti_diag = [False] * n, [False] * (2 * n - 1), [False] * (2 * n - 1)         dfs(0)         return self.cnt   Combinations Given two integers n and k, return all possible combinations of k numbers out of 1 ... n. Example: Input: n = 4, k = 2 Output: [   [2,4],   [3,4],   [2,3],   [1,2],   [1,3],   [1,4], ]  Solution: DFS  class Solution { public:     vector<vector<int>> combine(int n, int k) {         dfs(1, n, k);         return result;     }  private:     vector<int> path;     vector<vector<int>> result;      void dfs(int start, int n, int k) {         if (path.size() == k) {             result.emplace_back(path);             return;         }         for (int i = start; i <= n; ++i) {             path.emplace_back(i);             dfs(i + 1, n, k);             path.pop_back();         }     } };  def combine(self, n: int, k: int) -> List[List[int]]:     def dfs(start):         if len(path) == k:             result.append(path[:])             return         for i in range(start, n + 1):             path.append(i)             dfs(i + 1)             path.pop()      result, path = [], []     dfs(1)     return result   Subsets Given a set of distinct integers, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: Input: nums = [1,2,3] Output: [   [3],   [1],   [2],   [1,2,3],   [1,3],   [2,3],   [1,2],   [] ]  Solution: DFS  class Solution { public:     vector<vector<int>> subsets(vector<int> &nums) {         dfs(nums, 0);         return power_set;     }  private:     vector<vector<int>> power_set;     vector<int> subset;      void dfs(vector<int> &nums, int idx) {         power_set.push_back(subset);         for (int i = idx; i < nums.size(); i++) {             subset.push_back(nums[i]);             dfs(nums, i + 1);             subset.pop_back();         }     } };  def subsets(nums):     def dfs(idx, subset):         power_set.append(subset[:])         for i in range(idx, len(nums)):             subset.append(nums[i])             dfs(i + 1, subset)             subset.pop()      power_set = []     dfs(0, [])     return power_set Solution: Bit Manipulation When n is less than or equal to the width of an integer on the architecture (or language) we are working on, we can enumerate bit arrays by enumerating integers in [0,2^n-1] and examining the indices of bit set in these integers. These indices are determined by first isolating the lowest set bit and then getting the index by computing log2. 000 = 0  <->  {} 001 = 1  <->  {1} 010 = 2  <->  {2} 011 = 3  <->  {1, 2} 100 = 4  <->  {3} 101 = 5  <->  {1, 3} 110 = 6  <->  {2, 3} 111 = 7  <->  {1, 2, 3}   vector<vector<int>> subsets(vector<int> &nums) {     vector<vector<int>> power_set;     // i = [0..2^n - 1]     for (int i = 0; i < (1 << nums.size()); ++i) {         int bit_array = i;         vector<int> subset;         while (bit_array) {             // isolate the rightmost one and obtain its location             subset.emplace_back(nums[log2(bit_array & ~(bit_array - 1))]);             bit_array &= bit_array - 1; // unset the rightmost one         }         power_set.emplace_back(subset);     }     return power_set; }  def subsets(nums):     power_set = []     for int_for_subset in range(1 << len(nums)):         bit_array = int_for_subset         subset = []         while bit_array:             subset.append(nums[int(math.log2(bit_array & ~(bit_array - 1)))])             bit_array &= bit_array - 1         power_set.append(subset)     return power_set Solution: Iterative Take [1, 2, 3] as an example. The process of generating all the subsets is like:  Initially: [] Adding the first number to all the existed subsets: [[], [1]] Adding the second number to all the existed subsets: [[], [1], [2], [1, 2]] Adding the third number to all the existed subsets: [[], [1], [2], [1, 2], [3], [1, 3], [2, 3], [1, 2, 3]]   vector<vector<int>> subsets(vector<int> &nums) {     vector<vector<int>> power_set{{}};     for (int i = 0; i < nums.size(); ++i) {         int n = power_set.size();         for (int j = 0; j < n; ++j) {             power_set.emplace_back(power_set[j]);             power_set.back().emplace_back(nums[i]);         }     }     return power_set; }  def subsets(nums):     power_set = [[]]     for num in nums:         power_set += [subset + [num] for subset in power_set]     return power_set   Subsets II Given a collection of integers that might contain duplicates, nums, return all possible subsets (the power set). Note: The solution set must not contain duplicate subsets. Example: Input: [1,2,2] Output: [   [2],   [1],   [1,2,2],   [2,2],   [1,2],   [] ]  Solution: DFS  class Solution { public:     vector<vector<int>> subsetsWithDup(vector<int> &nums) {         sort(nums.begin(), nums.end());         dfs(nums, 0);         return power_set;     }  private:     vector<int> subset;     vector<vector<int>> power_set;      void dfs(vector<int> &nums, int idx) {         power_set.emplace_back(subset);         for (int i = idx; i < nums.size(); ++i) {             if (i != idx && nums[i] == nums[i - 1])                 continue;             subset.emplace_back(nums[i]);             dfs(nums, i + 1);             subset.pop_back();         }     } };  def subsetsWithDup(nums):     def dfs(idx, subset):         power_set.append(subset[:])         for i in range(idx, len(nums)):             if i != idx and nums[i] == nums[i - 1]:                 continue             subset.append(nums[i])             dfs(i + 1, subset)             subset.pop()      nums.sort()     power_set = []     dfs(0, [])     return power_set Solution: Iterative If we want to insert an element which is a duplicate, we can only insert it after the newly inserted elements from last step. For input [1, 2, 2], after processing 1 and 2, we get [[],[1],[2],[1,2]]  The next number is still 2. If we insert it after all current subsets, we will get duplicated subsets as following: [[], [1], [2], [1,2], [2], [1,2], [2,2], [1,2,2]]                       ----------                       duplicates   vector<vector<int>> subsetsWithDup(vector<int> &nums) {     vector<vector<int>> power_set{{}};     sort(nums.begin(), nums.end());     int length = 0, start = 0;     for (int i = 0; i < nums.size(); ++i) {         start = i > 0 && nums[i] == nums[i - 1] ? length : 0;         length = power_set.size();         for (int j = start; j < length; ++j) {             power_set.emplace_back(power_set[j]);             power_set.back().emplace_back(nums[i]);         }     }     return power_set; }  def subsetsWithDup(nums):     nums.sort()     power_set = [[]]     start, length = None, None     for i in range(len(nums)):         start = length if i > 0 and nums[i] == nums[i - 1] else 0         length = len(power_set)         power_set += [subset + [nums[i]] for subset in power_set[start:start + length]]     return power_set   Design LRU Cache Design and implement a data structure for Least Recently Used (LRU) cache. It should support the following operations: get and put.  get(key) - Get the value (will always be positive) of the key if the key exists in the cache, otherwise return -1. put(key, value) - Set or insert the value if the key is not already present. When the cache reached its capacity, it should invalidate the least recently used item before inserting a new item.  Solution   class LRUCache { public:     LRUCache(int capacity) {         this->capacity_ = capacity;     }      int get(int key) {         if (map_.find(key) == map_.end())             return -1;          // Transfer the element pointed by map_[key] from queue_ into queue_,         // inserting it at queue_.begin()         queue_.splice(queue_.begin(), queue_, map_[key]);          map_[key] = queue_.begin();         return map_[key]->value;     }      void put(int key, int value) {         if (map_.find(key) == map_.end()) { // if key is NOT in memory queue             if (queue_.size() == capacity_) {                 map_.erase(queue_.back().key);                 queue_.pop_back();             }             queue_.push_front(CacheNode(key, value));             map_[key] = queue_.begin();         } else { // if key is in memory queue             map_[key]->value = value;             queue_.splice(queue_.begin(), queue_, map_[key]);             map_[key] = queue_.begin();         }     }  private:     struct CacheNode {         int key, value;          CacheNode(int k, int v) : key(k), value(v) {}     };      list<CacheNode> queue_;     unordered_map<int, list<CacheNode>::iterator> map_;     int capacity_; };  class LRUCache:     def __init__(self, capacity: int):         self._map = OrderedDict()         self._capacity = capacity      def get(self, key: int) -> int:         if key not in self._map:             return -1         price = self._map.pop(key)         self._map[key] = price         return price      def put(self, key: int, value: int) -> None:         if key in self._map:             self._map.pop(key)         if len(self._map) == self._capacity:             self._map.popitem(last=False)         self._map[key] = value   Pandas Time Series Data Manipulation We first get the adjusted close prices (from 01/01/2000 to 12/31/2016) of Apple (AAPL), Microsoft (MSFT) and S&P 500 (^GSPC). from pandas_datareader import data import matplotlib.pyplot as plt import pandas as pd  tickers = ['AAPL', 'MSFT', '^GSPC'] start_date = '2010-01-01' end_date = '2016-12-31' panel_data = data.DataReader(tickers, 'yahoo', start_date, end_date)['Adj Close'] Reindexing and NA-Filling Get all weekdays between 01/01/2000 and 12/31/2016 and reindex adj close using all_weekdays as the new index. close = panel_data all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B') close = close.reindex(all_weekdays) close = close.fillna(method='ffill') Moving average # Get the MSFT timeseries. This now returns a Pandas Series object indexed by date. msft = close.loc[:, 'MSFT']  # Calculate the 20 and 100 days moving averages of the closing prices short_rolling_msft = msft.rolling(window=20).mean() long_rolling_msft = msft.rolling(window=100).mean()  # Plot fig, ax = plt.subplots(figsize=(6,4)) ax.plot(msft.index, msft, label='MSFT') ax.plot(short_rolling_msft.index, short_rolling_msft, label='20 days rolling') ax.plot(long_rolling_msft.index, long_rolling_msft, label='100 days rolling') ax.set_xlabel('Date') ax.set_ylabel('Adjusted closing price ($)') ax.legend() plt.savefig('msft.pdf', bbox_inches='tight')  Returns The following is taken from this great blog article. Why Returns? $$ r_i = \\frac{p_i - p_{i-1}}{p_{i-1}} $$ Benefit of using returns, versus prices, is normalization: measuring all variables in a comparable metric, thus enabling evaluation of analytic relationships amongst two or more variables despite originating from price series of unequal values. This is a requirement for many multidimensional statistical analysis and machine learning techniques. For example, interpreting an equity covariance matrix is made sane when the variables are both measured in percentage. Why Log Returns? $$ \\log(1+r_i) = \\log(p_i/p_{i-1}) $$   Log-normality:  if we assume that prices are distributed log normally (which, in practice, may or may not be true for any given price series), then $\\log(1 + r_i)$ is normally distributed.   Approximate raw-log equality: when returns are very small (common for trades with short holding durations), the following approximation ensures they are close in value to raw returns: $$ \\log(1 + r) \\approx r, \\quad r \\ll 1 $$   Time-additivity: consider an ordered sequence of $n$ trades. A statistic frequently calculated from this sequence is the compounding return, which is the running return of this sequence of trades over time: $$ (1 + r_1)(1 + r_2)  \\cdots (1 + r_n) = \\prod_i (1+r_i) $$ This formula is fairly unpleasant, as probability theory reminds us the product of normally-distributed variables is not normal. Instead, the sum of normally-distributed variables is normal (important technicality: only when all variables are uncorrelated), which is useful when we recall the following logarithmic identity: $$ \\log(1 + r_i) = \\log\\left(\\frac{p_i}{p_{i-1}}\\right) = \\log(p_i) - \\log(p_{i-1}) $$ Thus, compounding returns are normally distributed. Finally, this identity leads us to a pleasant algorithmic benefit; a simple formula for calculating compound returns: $$ \\sum_i \\log(1+r_i) = \\log(1 + r_1) + \\log(1 + r_2)  + \\cdots + \\log(1 + r_n) = \\log(p_n) - \\log(p_0) $$ Thus, the compound return over n periods is merely the difference in log between initial and final periods. In terms of algorithmic complexity, this simplification reduces O(n) multiplications to O(1) additions. This is a huge win for moderate to large n. Further, this sum is useful for cases in which returns diverge from normal, as the central limit theorem reminds us that the sample average of this sum will converge to normality (presuming finite first and second moments).   Mathematical ease: from calculus, we are reminded (ignoring the constant of integration): $$ e^x = \\int e^x dx = \\frac{d}{dx} e^x = e^x $$ This identity is tremendously useful, as much of financial mathematics is built upon continuous time stochastic processes which rely heavily upon integration and differentiation.   Numerical stability: addition of small numbers is numerically safe, while multiplying small numbers is not as it is subject to arithmetic underflow. For many interesting problems, this is a serious potential problem. To solve this, either the algorithm must be modified to be numerically robust or it can be transformed into a numerically safe summation via logs.   Pandas: returns = close.copy() returns['AAPL pct_return'] = returns['AAPL'].pct_change() returns['AAPL log_return'] = np.log(1 + returns['AAPL pct_return']) from pandas_datareader import data import matplotlib.pyplot as plt import pandas as pd import numpy as np from sklearn.preprocessing import MinMaxScaler from keras.layers import Input, Conv1D, MaxPool1D, UpSampling1D from keras import Model  start_date = '2010-01-01' end_date = '2016-12-31' encoding_dim = 3 epochs = 100 test_samples = 2000   def plot_history(history):     plt.figure(figsize=(15, 5))     ax = plt.subplot(1, 2, 1)     plt.plot(history.history[\"loss\"])     plt.title(\"Train loss\")     ax = plt.subplot(1, 2, 2)     plt.plot(history.history[\"val_loss\"])     plt.title(\"Test loss\")   tickers = ['AAPL', 'MSFT', '^GSPC'] panel_data = data.DataReader(tickers, 'yahoo', start_date, end_date)['Adj Close'] all_weekdays = pd.date_range(start=start_date, end=end_date, freq='B') panel_data = panel_data.reindex(all_weekdays) panel_data = panel_data.fillna(method='ffill')  df_ret = np.log(1 + panel_data.pct_change().dropna()) df_ret[df_ret.columns] = MinMaxScaler().fit_transform(df_ret[df_ret.columns])  timesteps = df_ret.shape[0]  inputs = Input(shape=(1, timesteps, 1)) encoded = Conv1D(16, kernel_size=3, activation='relu', padding='same')(inputs) encoded = MaxPool1D(pool_size=3, padding='same')(encoded)  decoded = Conv1D(16, kernel_size=3, activation='relu', padding='same')(encoded) decoded = MaxPool1D(pool_size=3, padding='same')(decoded) autoencoder = Model(inputs, decoded) autoencoder.compile(optimizer='adam', loss='mse')  fig, ax = plt.subplots(figsize=(20, 4)) ax.plot(df_ret['AAPL']) ax.set_xlabel('Date') ax.set_ylabel('Log Return') plt.show()   <script type=\"text/javascript\" async src=\"https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML\"> </script> ", "has_readme": true, "readme_language": "English", "repo_tags": ["algorithms", "data-structures"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f7b"}, "repo_url": "https://github.com/viraja1/crack_detection", "repo_name": "crack_detection", "repo_full_name": "viraja1/crack_detection", "repo_owner": "viraja1", "repo_desc": "Concrete Crack Detection for Structural Audit", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T18:10:24Z", "repo_watch": 5, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T12:28:56Z", "homepage": "", "size": 381242, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184412973, "is_fork": false, "readme_text": "Concrete Crack Detection For Structural Audit Getting Started   Clone Repo $ git clone https://github.com/viraja1/crack_detection.git    Change directory $ cd crack_detection    Install Requirements $ pip install -r requirements.txt    Train Model Skip this step if you want to use pretrained model Tested with python 3.7 $ python models/model.py    Run Server $ python app.py    Open http://127.0.0.1:8080 in browser and use the UI to test concrete crack detection   Try CURL requests $ curl -X POST -F file=@data/train/crack/15000_1.jpg http://127.0.0.1:8080/predict  {\"prediction\":\"crack\"}     Screenshot  Credits  Dataset - Concrete Crack Images for Classification (https://data.mendeley.com/datasets/5y9wdsg2zt/1/) Deploy Keras Model with Flask as Web App (https://github.com/mtobeiyf/keras-flask-deploy-webapp) Introduction to TensorFlow for Artificial Intelligence, Machine Learning, and Deep Learning (https://www.coursera.org/learn/introduction-tensorflow)  ", "has_readme": true, "readme_language": "English", "repo_tags": ["tensorflow", "keras", "convolutional-neural-networks", "python3", "python", "flask", "deep-learning", "tensorflow2"], "has_h5": true, "h5_files_links": ["https://github.com/viraja1/crack_detection/blob/68161efcdff18df880a134ecf468d9c28662359b/models/crack_detection.h5", "https://github.com/viraja1/crack_detection/blob/6026c6326de18cca523d0ff4a14b0d9d5cdbdb8b/models/crack_detection_mobile_net.h5"], "see_also_links": ["http://127.0.0.1:8080"], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f7c"}, "repo_url": "https://github.com/AIWintermuteAI/transfer_learning_sipeed", "repo_name": "transfer_learning_sipeed", "repo_full_name": "AIWintermuteAI/transfer_learning_sipeed", "repo_owner": "AIWintermuteAI", "repo_desc": "Image Recognition With Sipeed MaiX and Arduino IDE/Micropython", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T16:28:45Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T08:19:18Z", "homepage": "https://www.instructables.com/id/Transfer-Learning-With-Sipeed-MaiX-and-Arduino-IDE/", "size": 1843, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 1, "github_id": 184387870, "is_fork": false, "readme_text": "transfer_learning_sipeed Link to the full tutorial https://www.instructables.com/id/Transfer-Learning-With-Sipeed-MaiX-and-Arduino-IDE/ Create your own custom image classifier with transfer learning in Keras, convert the trained model to .kmodel format and run it on Sipeed board (can be any board, Bit/Dock or Go) using Micropython or Arduino IDE. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f7d"}, "repo_url": "https://github.com/vibeznstuff/my-image-classifier", "repo_name": "my-image-classifier", "repo_full_name": "vibeznstuff/my-image-classifier", "repo_owner": "vibeznstuff", "repo_desc": "Git project for a WIP image classifier with Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T19:13:19Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T18:51:20Z", "homepage": null, "size": 26650, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184465222, "is_fork": false, "readme_text": "my-image-classifier Originally inspired by the article by Adrian Rosebrock https://www.pyimagesearch.com/2018/09/10/keras-tutorial-how-to-get-started-with-keras-deep-learning-and-python/# ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f7e"}, "repo_url": "https://github.com/aspamers/autoencoder", "repo_name": "autoencoder", "repo_full_name": "aspamers/autoencoder", "repo_owner": "aspamers", "repo_desc": "A simple, easy-to-use and flexible auto-encoder neural network implementation for Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T13:04:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T09:38:21Z", "homepage": "", "size": 7, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184395825, "is_fork": false, "readme_text": "Auto-Encoder for Keras This project provides a lightweight, easy to use and flexible auto-encoder module for use with the Keras framework. Auto-encoders are used to generate embeddings that describe inter and extra class relationships. This makes auto-encoders like many other similarity learning algorithms suitable as a pre-training step for many classification problems. An example of the auto-encoder module being used to produce a noteworthy 99.84% validation performance on the MNIST dataset with no data augmentation and minimal modification from the Keras example is provided. Installation Create and activate a virtual environment for the project. $ virtualenv env $ source env/bin/activate To install the module directly from GitHub: $ pip install git+https://github.com/aspamers/autoencoder  The module will install keras and numpy but no back-end (like tensorflow). This is deliberate since it leaves the module decoupled from any back-end and gives you a chance to install whatever version you prefer. To install tensorflow: $ pip install tensorflow  Usage For detailed usage examples please refer to the examples and unit test modules. If the instructions are not sufficient feel free to make a request for improvements.  Import the module  from autoencoder import AutoEncoder  Load or generate some data.  x_train = np.random.rand(100, 3) x_test = np.random.rand(30, 3)  Design an encoder model  def create_encoder_model(input_shape):     model_input = Input(shape=input_shape)      encoder = Dense(4)(model_input)     encoder = BatchNormalization()(encoder)     encoder = Activation(activation='relu')(encoder)      return Model(model_input, encoder)  Design a decoder model      def create_decoder_model(embedding_shape):         embedding_a = Input(shape=embedding_shape)          decoder = Dense(3)(embedding_a)         decoder = BatchNormalization()(decoder)         decoder = Activation(activation='relu')(decoder)          return Model(embedding_a, decoder)  Create an instance of the AutoEncoder class  encoder_model = create_encoder_model(input_shape) decoder_model = create_decoder_model(encoder_model.output_shape) autoencoder = AutoEncoder(encoder_model, decoder_model)  Compile the model  autoencoder.compile(loss='binary_crossentropy', optimizer=keras.optimizers.adam())  Train the model  autoencoder.fit(x_train, x_train,                 validation_data=(x_test, x_test),                 epochs=epochs) Development Environment Create and activate a test virtual environment for the project. $ virtualenv env $ source env/bin/activate Install requirements $ pip install -r requirements.txt Run tests $ pytest tests/test_autoencoder.py ", "has_readme": true, "readme_language": "English", "repo_tags": ["python", "auto-encoder", "neural-network", "unsupervised-learning"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f7f"}, "repo_url": "https://github.com/manicman1999/precision-recall-keras", "repo_name": "precision-recall-keras", "repo_full_name": "manicman1999/precision-recall-keras", "repo_owner": "manicman1999", "repo_desc": "Precision and Recall GAN Evaluations using Manifolds", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T16:22:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T16:12:27Z", "homepage": null, "size": 113, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184444024, "is_fork": false, "readme_text": "Precision Recall Keras Precision and Recall GAN Evaluations using Manifolds Implementation of the metrics from this paper: \"Improved Precision and Recall Metric for Assessing Generative Models\" https://arxiv.org/pdf/1904.06991v1.pdf In Keras. *Note: This is an unofficial implementation. It is also less efficient, using CPU, and takes O(n^2) time. Feel free to contribute! Use  Place data in /data/ folder. Import get_prec_and_recall function, or add to precision-recall.py. Enter the folder names of the real and fake data respectively as parameters to the function. Optionally: Set n, meaning number of samples to use. Optionally: Set get_scores, if True this will return realism scores for fake samples. Run function.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1904.06991v1.pdf"]}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f80"}, "repo_url": "https://github.com/redhood95/deepSort", "repo_name": "deepSort", "repo_full_name": "redhood95/deepSort", "repo_owner": "redhood95", "repo_desc": "Deepsort algorithm for detecting pedestrians and vehicles", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T18:12:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T17:52:18Z", "homepage": null, "size": 181, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 184457313, "is_fork": false, "readme_text": "Introduction Thanks for these projects, this work now is support tiny_yolo v3 but only for test, if you want to train you can either train a model in darknet or in the second following works. It also can tracks many objects in coco classes, so please note to modify the classes in yolo.py. besides, you also can use camera for testing. https://github.com/nwojke/deep_sort https://github.com/qqwweee/keras-yolo3 https://github.com/Qidian213/deep_sort_yolov3 Quick Start   Download YOLOv3 or tiny_yolov3 weights from YOLO website.Then convert the Darknet YOLO model to a Keras model. Or use what i had converted https://drive.google.com/file/d/1uvXFacPnrSMw6ldWTyLLjGLETlEsUvcE/view?usp=sharing (yolo.h5 model file with tf-1.4.0) , put it into model_data folder   Run YOLO_DEEP_SORT with cmd : python demo.py    (Optional) Convert the Darknet YOLO model to a Keras model by yourself:    please download the weights at first from yolo website.  python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5  Dependencies The code is compatible with Python 2.7 and 3. The following dependencies are needed to run the tracker: NumPy sklean OpenCV Pillow  This code works on TensorFlow-1.8.0 Training the model To train the deep association metric model on your datasets you can reference to https://github.com/nwojke/cosine_metric_learning  approach which is provided as a separate repository. Be careful that the code ignores everything but person. For your task do not forget to change : [deep_sort_yolov3/yolo.py]   Lines 100 to 101 :       if predicted_class != 'person' :            continue  Result Links to the results are provided in the result.txt ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f81"}, "repo_url": "https://github.com/JayakumarMalla/2019", "repo_name": "2019", "repo_full_name": "JayakumarMalla/2019", "repo_owner": "JayakumarMalla", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-01T11:58:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T11:56:41Z", "homepage": null, "size": 42326, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184409504, "is_fork": false, "readme_text": "Python Codes in Data Science Codes in NLP, Deep Learning, Reinforcement Learning and Artificial Intelligence  Welcome to my GitHub repo.  I am a Data Scientist and I code in R, Python and Wolfram Mathematica. Here you will find some Machine Learning, Deep Learning, Natural Language Processing and Artificial Intelligence models I developed.  Outputs of the models can be seen at my portfolio:  https://drive.google.com/file/d/0B0RLknmL54khdjRQWVBKeTVxSHM/view?usp=sharing  Keras version used in models: keras==1.1.0  Autoencoder for Audio   is a model where I compressed an audio file and used Autoencoder to reconstruct the audio file, for use in phoneme classification.  Collaborative Filtering   is a Recommender System where the algorithm predicts a movie review based on genre of movie and similarity among people who watched the same movie.  Convolutional NN Lasagne   is a Convolutional Neural Network model in Lasagne to solve the MNIST task.  Ensembled Machine Learning  is a .py file where 7 Machine Learning algorithms are used in a classification task with 3 classes and all possible hyperparameters of each algorithm are adjusted. Iris dataset of scikit-learn.     GAN Generative Adversarial   are models of Generative Adversarial Neural Networks.  Hyperparameter Tuning RL   is a model where hyperparameters of Neural Networks are adjusted via Reinforcement Learning. According to a reward, hyperparameter tuning (environment) is changed through a policy (mechanization of knowledge) using the Boston Dataset. Hyperparameters tuned are: learning rate, epochs, decay, momentum, number of hidden layers and nodes and initial weights.  Keras Regularization L2   is a Neural Network model for regression made with Keras where a L2 regularization was applied to prevent overfitting.  Lasagne Neural Nets Regression   is a Neural Network model based in Theano and Lasagne, that makes a linear regression with a continuous target variable and reaches 99.4% accuracy. It uses the DadosTeseLogit.csv sample file.  Lasagne Neural Nets + Weights   is a Neural Network model based in Theano and Lasagne, where is possible to visualize weights between X1 and X2 to hidden layer. Can also be adapted to visualize weights between hidden layer and output. It uses the DadosTeseLogit.csv sample file.  Multinomial Regression   is a regression model where target variable has 3 classes.  Neural Networks for Regression   shows multiple solutions for a regression problem, solved with sklearn, Keras, Theano and Lasagne. It uses the Boston dataset sample file from sklearn and reaches more than 98% accuracy.     NLP + Naive Bayes Classifier   is a model where movie reviews were labeled as positive and negative and the algorithm then classifies a totally new set of reviews using Logistic Regression, Decision Trees and Naive Bayes, reaching an accuracy of 92%.  NLP Anger Analysis   is a Doc2Vec model associated with Word2Vec model to analyze level of anger using synonyms in consumer complaints of a U.S. retailer in Facebook posts.  NLP Consumer Complaint   is a model where Facebook posts of a U.S. computer retailer were scraped, tokenized, lemmatized and applied Word2Vec. After that, t-SNE and Latent Dirichlet Allocation were developed in order to classify the arguments and weights of each keyword used by a consumer in his complaint. The code also analyzes frequency of words in 100 posts.  NLP Convolutional Neural Network  is a Convolutional Neural Network for Text in order to classify movie reviews.  NLP Doc2Vec   is a Natural Language Procesing file where cosine similarity among phrases is measured through Doc2Vec.  NLP Document Classification   is a code for Document Classification according to Latent Dirichlet Allocation.  NLP Facebook Analysis   analyzes Facebook posts regarding Word Frequency and Topic Modelling using LDA.  NLP Facebook Scrap   is a Python code for scraping data from Facebook.  NLP - Latent Dirichlet Allocation   is a Natural Language Processing model where a Wikipedia page on Statistical Inference is classified regarding topics, using Latent Dirichlet Allocation with Gensim, NLTK, t-SNE and K-Means.  NLP Probabilistic ANN   is a Natural Language Processing model where sentences are vectorized by Gensim and a probabilistic Neural Network model is deveoped using Gensim, for sentiment analysis.  NLP Semantic Doc2Vec + Neural Network   is a model where positive and negative movie reviews were extracted and semantically classified with NLTK and BeautifulSoup, then labeled as positive or negative. Text was then used as an input for the Neural Network model training. After training, new sentences are entered in the Keras Neural Network model and then classified. It uses the zip file.  NLP Sentiment Positive   is a model that identifies website content as positive, neutral or negative using BeautifulSoup and NLTK libraries, plotting the results.  NLP Twitter Analysis ID #   is a model that extracts posts from Twitter based in ID of user or Hashtag.  NLP Twitter Scrap   is a model that scraps Twitter data and shows the cleaned text as output.  NLP Twitter Streaming   is a model of analysis of real-time data from Twitter (under development).  NLP Twitter Streaming Mood   is a model where the evolution of mood Twitter posts is measured during a period of time.  NLP Wikipedia Summarization   is a Python code that summarizes any given page in a few sentences.  NLP Word Frequency   is a model that calculates the frequency of nouns, verbs, words in Facebook posts.  Probabilistic Neural Network   is a Probabilistic Neural Network for Time Series Prediction.  REAL-TIME Twitter Analysis   is a model where Twitter streaming is extracted, words and sentences tokenized, word embeddings were created, topic modeling was made and classified using K-Means. Then, NLTK SentimentAnalyzer was used to classify each sentence of the streaming into positive, neutral or negative. Accumulated sum was used to generate the plot and the code loops each 1 second, collecting new tweets.  RESNET-2   is a Deep Residual Neural Network.  ROC Curve Multiclass   is a .py file where Naive Bayes was used to solve the IRIS Dataset task and ROC curve of different classes are plotted.  SQUEEZENET   is a simplified version of the AlexNet.  Stacked Machine Learning   is a .py notebook where t-SNE, Principal Components Analysis and Factor Analysis were applied to reduce dimensionality of data. Classification performances were measured after applying K-Means.  Support Vector Regression   is a SVM model for non linear regression in an artificial dataset.  Text-to-Speech   is a .py file where Python speaks any given text and saves it as an audio .wav file.  Time Series ARIMA   is a ARIMA model to forecast time series, with an error margin of 0.2%.  Time Series Prediction with Neural Networks - Keras   is a Neural Network model to forecast time series, using Keras with an adaptive learning rate depending upon derivative of loss.     Variational Autoencoder   is a VAE made with Keras.  Web Crawler   is a code that scraps data from different URLs of a hotel website.  t-SNE Dimensionality Reduction   is a t-SNE model for dimensionality reduction which is compared to Principal Components Analysis regarding its discriminatory power.  t-SNE PCA + Neural Networks   is a model that compares performance or Neural Networks made after t-SNE, PCA and K-Means.  t-SNE PCA LDA embeddings  is a model where t-SNE, Principal Components Analysis, Linear Discriminant Analysis and Random Forest embeddings are compared in a task to classify clusters of similar digits.          ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f82"}, "repo_url": "https://github.com/daiweihero/dl_pytorch", "repo_name": "dl_pytorch", "repo_full_name": "daiweihero/dl_pytorch", "repo_owner": "daiweihero", "repo_desc": "the book <deep learning with python> use keras, this repo rewrite code with pytorch.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T01:09:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T05:52:42Z", "homepage": null, "size": 22, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 184374950, "is_fork": false, "readme_text": "dl_pytorch the book <deep learning with python> use keras, this repo rewrite code with pytorch. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f83"}, "repo_url": "https://github.com/rpeden/cat-or-not", "repo_name": "cat-or-not", "repo_full_name": "rpeden/cat-or-not", "repo_owner": "rpeden", "repo_desc": "Is it a cat, or something else?", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T22:41:21Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-01T03:07:22Z", "homepage": null, "size": 12416, "language": "Python", "has_wiki": true, "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "open_issues_count": 0, "github_id": 184362434, "is_fork": false, "readme_text": "Cat or Not An Keras + Python image classifier that determines whether or not an image is of a cat. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/rpeden/cat-or-not/blob/428f8128e24be86e5c14d2d68f9a5192d5f5cc23/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f84"}, "repo_url": "https://github.com/roosa123/CVaPR_EEG_videos", "repo_name": "CVaPR_EEG_videos", "repo_full_name": "roosa123/CVaPR_EEG_videos", "repo_owner": "roosa123", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-03T08:36:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T17:46:14Z", "homepage": null, "size": 83, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184456544, "is_fork": false, "readme_text": "Emotion Recognition in Videos From EEG Signals Using Deep Neural Networks A project for Computer Vision and Pattern Recognition classes at Silesian University of Technology Lecturer: Krzysztof Kotowski Semester: 1st semester of the MSc studies programme Let's determine some universal rules and project requirements (listed below):  Placing the dataset in the repo directory structure:  let's place the dataset in the DEAP/ directory (already only with .gitkeep file)   IDE  the suggested IDE is PyCharm, but let's make the project able to run from any other - just make sure you have installed proper Python packages   Language & libraries  the language of the project is Python 3 (important: if you want to use TensorFlow as a backend for Keras, Python version should not be higher than 3.6!) required packages (please extend this list as you will introduce more packages):  Keras TensorFlow NumPy Matplotlib Pandas MNE SciPy   if you wish to plot the model of the network, you should install Graphviz and pydot package   GPU usage  there is a possibilty to use TensorFlow with computation acceleration on GPU. To utilise this feature, please install GPU-accelerated version of TensorFlow (TensorFlow-GPU) and additional necessary tools (CUDA Toolkit and CuDNN), listed in TensorFlow website, in section Software Requirements pay attention to the versions - TensorFlow 1.13 will work only with CUDA Toolkit 10.0 (and probably higher) make sure you've added CuDNN path to your PATH variable!    Please extend this file as any new requirements will arise! That's all, folks! :) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f85"}, "repo_url": "https://github.com/ElimuMichael/Memristor-Fault-Tolerant-API", "repo_name": "Memristor-Fault-Tolerant-API", "repo_full_name": "ElimuMichael/Memristor-Fault-Tolerant-API", "repo_owner": "ElimuMichael", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T18:12:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T19:09:51Z", "homepage": null, "size": 36267, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184467512, "is_fork": false, "readme_text": "Memristor-Fault-Tolerant-API Requirements Make sure to install the following pythn packages for successful use of the program  Keras 2 and above Tensor flow pySipleGUI Matplotlib Numpy  Clone the repository and run the memristorGUI.py file To check the requirements for the program, click on # File -> Install Dependencies To contact us for any information, go to # Help -> About ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f86"}, "repo_url": "https://github.com/ahmterdgn903/tensorflow_object_recognation", "repo_name": "tensorflow_object_recognation", "repo_full_name": "ahmterdgn903/tensorflow_object_recognation", "repo_owner": "ahmterdgn903", "repo_desc": "Real Time Object Detection with Deep Learning method of Tensorflow on CPU", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T09:26:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T02:17:31Z", "homepage": null, "size": 378929, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184358371, "is_fork": false, "readme_text": "real-time_object_detection Real Time Object Detection with Deep Learning method of Tensorflow on CPU Cloned from Tenforflow model, redesigned for CPU and improvements in working speed. Requirements  Python 3 Keras on Tensorflow OpenCV 3.4 or later min 4 GB Ram Memory Jupyter Notebook  Set-Up  Colone or download the project Go this directory (models>research>object_detection) Run the Final_directory.py file with python  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f87"}, "repo_url": "https://github.com/kumi123/nft", "repo_name": "nft", "repo_full_name": "kumi123/nft", "repo_owner": "kumi123", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-01T13:41:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T13:38:29Z", "homepage": null, "size": 508, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 184421493, "is_fork": false, "readme_text": "\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb \u57fa\u4e8e\u5434\u6069\u8fbe\u5728 Coursera \u4e0a\u300a\u6df1\u5ea6\u5b66\u4e60\u300b\u8bfe\u7a0b\u8bb2\u4e49\u5b9e\u73b0\u56fe\u50cf\u98ce\u683c\u8fc1\u79fb\u3002 \u4f9d\u8d56  NumPy Tensorflow Keras OpenCV  \u7528\u6cd5 \u4e0b\u8f7d\u9884\u8bad\u7ec3\u7684 VGG19 \u6a21\u578b\u653e\u5165 pretrained-model \u76ee\u5f55\uff0c\u7136\u540e\u6267\u884c\uff1a $ python demo.py \u5185\u5bb9\u56fe\u7247  \u98ce\u683c\u56fe\u7247  \u5408\u6210\u56fe\u7247  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://docs.scipy.org/doc/numpy-1.10.1/user/install.html"], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f88"}, "repo_url": "https://github.com/lemartinez2245/PentaLearning", "repo_name": "PentaLearning", "repo_full_name": "lemartinez2245/PentaLearning", "repo_owner": "lemartinez2245", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-01T22:53:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T22:47:27Z", "homepage": null, "size": 773, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184491442, "is_fork": false, "readme_text": "Para utilizar el proyecto necesita cargar el archivo Penta Learning.sln  en Visual Studio y correr el proyecto o abrir el archivo Penta Learning.exe en el directorio Penta Learning/bin/Debug. Dependencias de Python3:  Opencv (opencv-python) Tensorflow Keras Matplotlib Numpy  El ejecutable Penta Learning.exe llama por detr\u00e1s a segmentation.py para la segmentaci\u00f3n y a predict.py para la clasificaci\u00f3n y el uso de la red neuronal trained_neuronal_network.h5 que es la red sin feature extraction. ", "has_readme": true, "readme_language": "Spanish", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/lemartinez2245/PentaLearning/blob/56fde8984601f64b401edf5f0ece722aaec815d0/Penta%20Learning/bin/Debug/trained_neuronal_network.h5", "https://github.com/lemartinez2245/PentaLearning/blob/56fde8984601f64b401edf5f0ece722aaec815d0/Segmentation/trained_neuronal_network.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f89"}, "repo_url": "https://github.com/benlin131020/CIFAR10_TENSORFLOW_CNN", "repo_name": "CIFAR10_TENSORFLOW_CNN", "repo_full_name": "benlin131020/CIFAR10_TENSORFLOW_CNN", "repo_owner": "benlin131020", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-11T03:46:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T17:21:59Z", "homepage": null, "size": 700, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184453231, "is_fork": false, "readme_text": "CIFAR10_TENSORFLOW_CNN github https://github.com/benlin131020/CIFAR10_TENSORFLOW_CNN \u7db2\u8def\u67b6\u69cb  accuracy & loss   training data \u4e0a\u7684 accuracy \u53ca loss \u5c1a\u6709\u4e0a\u5347\u53ca\u4e0b\u964d\u7684\u7a7a\u9593\uff0c\u9700\u8981\u66f4\u591a\u7684 epoch \u8a13\u7df4\u3002 testing data \u5247\u6709 overfitting \u7684\u73fe\u8c61\uff0c\u9700\u8981\u66f4\u591a dropout \u53ca batch normalization\u3002 tf.data.Dataset \u56e0\u70ba Keras \u63d0\u4f9b\u7684 cifar10 \u6c92\u6709 batch \u7684\u529f\u80fd\uff0c\u56e0\u6b64\u5229\u7528 tf.data.Dataset \u63d0\u4f9b\u7684 batch \u529f\u80fd\u3002 \u8a73\u898b\u7a0b\u5f0f\u78bc\u5167\u7684 comment \u53ca https://www.tensorflow.org/guide/datasets \u958b\u555ftensorboard cd hw3 tensorboard --logdir logs  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f8a"}, "repo_url": "https://github.com/roshancode/RoshanStock", "repo_name": "RoshanStock", "repo_full_name": "roshancode/RoshanStock", "repo_owner": "roshancode", "repo_desc": "A Stock Recommend System Developed in Python", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T22:08:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T20:22:24Z", "homepage": "", "size": 137, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184476661, "is_fork": false, "readme_text": "RoshanStock A Stock Recommend System Developed in Python Main Requirement: Python 3.5.2 Keras 2.0.6 TensorFlow 1.2 pymongo tqdm nltk googletrans Install brew install mongodb --with-openssl brew services start mongodb mongod --dbpath (Your Porject Folder)/Data/DB When you storing stock data with mongodb mode, you may meet too many open files problem, try the following codes in command line:     sysctl -w kern.maxfiles=20480 (or whatever number you choose)     sysctl -w kern.maxfilesperproc=18000 (or whatever number you choose)     launchctl limit maxfiles 1000000 (or whatever number you choose)     brew services restart mongodb     mongodump -h localhost:27017 -d DB_STOCK -o ./  ", "has_readme": true, "readme_language": "English", "repo_tags": ["stock-market", "stock-price-prediction", "stock-prices", "python"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f8b"}, "repo_url": "https://github.com/ChenWWWeixiang/MUNIT", "repo_name": "MUNIT", "repo_full_name": "ChenWWWeixiang/MUNIT", "repo_owner": "ChenWWWeixiang", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-14T07:48:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T02:35:40Z", "homepage": null, "size": 6290, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 184359886, "is_fork": false, "readme_text": "   MUNIT: Multimodal UNsupervised Image-to-image Translation License Copyright (C) 2018 NVIDIA Corporation.  All rights reserved. Licensed under the CC BY-NC-SA 4.0 license (https://creativecommons.org/licenses/by-nc-sa/4.0/legalcode). Code usage Please check out the user manual page. Paper Xun Huang, Ming-Yu Liu, Serge Belongie, Jan Kautz, \"Multimodal Unsupervised Image-to-Image Translation\", ECCV 2018 Results Video  Edges to Shoes/handbags Translation  Animal Image Translation  Street Scene Translation  Yosemite Summer to Winter Translation (HD)  Example-guided Image Translation  Other Implementations MUNIT-Tensorflow by Junho Kim MUNIT-keras by shaoanlu Citation If you find this code useful for your research, please cite our paper: @inproceedings{huang2018munit,   title={Multimodal Unsupervised Image-to-image Translation},   author={Huang, Xun and Liu, Ming-Yu and Belongie, Serge and Kautz, Jan},   booktitle={ECCV},   year={2018} }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://jankautz.com/", "http://www.cs.cornell.edu/~xhuang/", "http://mingyuliu.net/"], "reference_list": ["https://arxiv.org/abs/1804.04732"]}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f8c"}, "repo_url": "https://github.com/alex-myzhao/dqn-autonomous", "repo_name": "dqn-autonomous", "repo_full_name": "alex-myzhao/dqn-autonomous", "repo_owner": "alex-myzhao", "repo_desc": "Course project of Optimal and Learning Control for Robotic, 19 Spring", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T03:15:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T01:02:04Z", "homepage": null, "size": 35608, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184352219, "is_fork": false, "readme_text": "DQN for Autonomous Deep Q-Learning  Unlike policy gradient methods, which attempt to learn functions which directly map an observation to an action, Q-Learning attempts to learn the estimated future return of being in a given state, and taking a specific action there.  How to use These steps are tested on macOS Mojave 10.14.4 and Miniconda 4.6.14. Install required packages To create a conda environment including required packages: make Offline training make offline Online learning  First, open the simulator term2_sim.app. Select the faster speed and lowest quality. Open \"project 4: PID controller\" in the simulator. Run the python script to connect to the server and start training.  make online Reference  Simple Reinforcement Learning with TensorFlow Human-level control through deep reinforcement learning A Walk-through of AlexNet Deep Q-Learning with Keras and Gym  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/alex-myzhao/dqn-autonomous/blob/d228d54b125f9c7283d404ec7a65f9e11cb2535f/model/autonomous-19-05-07.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/autonomous.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/offline_model.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/previous-models/autonomous-19-05-06-10-09.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/previous-models/autonomous-19-05-06-14-24.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/previous-models/autonomous-19-05-06-19-34.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/previous-models/autonomous-19-05-06-20-27.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/previous-models/autonomous-19-05-07.h5", "https://github.com/alex-myzhao/dqn-autonomous/blob/8a458da3af58f12d11230befe359e2af53767010/model/previous-models/autonomous.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f8d"}, "repo_url": "https://github.com/ctiger34/BASIC-EMOTION-DETECTION", "repo_name": "BASIC-EMOTION-DETECTION", "repo_full_name": "ctiger34/BASIC-EMOTION-DETECTION", "repo_owner": "ctiger34", "repo_desc": "BASIC REAL TIME VIDEO EMOTION DETETCTION", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T04:40:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T21:09:38Z", "homepage": "", "size": 1510, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184481984, "is_fork": false, "readme_text": "Emotion-Recognizer Basic real time video emotion face recognition python code for school project Project Name : Emotion-recognition Table of Content : 1.Description 2.Installations 3.How it works 4.Dataset 5.Credits Description: this is a school project for future class this program will be able to detect human face emotions What does Emotion Recognition mean? Emotion recognition is a technique used in software that allows a program to \"read\" the emotions on a human face using advanced image processing. Installations: -keras -imutils -cv2 -numpy how it works: The program will creat a window to display the scene capture by webcamera and another window to present the probabilities of the detected emotions.  Demo  python real_time_video.py Dataset: we have used Fer2013 dataset -fer2013 emotion classification test accuracy: 66% Credits This work is done by this guy, great work we just added small simple things. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ctiger34/BASIC-EMOTION-DETECTION/blob/b7b858e3de30ec913e569097368737de08d8f7b4/models/_mini_XCEPTION.102-0.66.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f8e"}, "repo_url": "https://github.com/DLinWL/Bi-Directional-Channel-Reciprocity", "repo_name": "Bi-Directional-Channel-Reciprocity", "repo_full_name": "DLinWL/Bi-Directional-Channel-Reciprocity", "repo_owner": "DLinWL", "repo_desc": "Python code and related materials for DualNet and U2D", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T15:58:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T05:48:46Z", "homepage": null, "size": 506, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184374629, "is_fork": false, "readme_text": "Bi-Directional-Channel-Reciprocity Python code and related materials for DualNet and U2D network. Introduction This repository contains the code and related materials for DualNet and its extension U2D network. DualNet is described in Zhenyu Liu, Lin Zhang, and Zhi Ding, \u201cExploiting Bi-Directional Channel Reciprocity in Deep Learning for Low Rate Massive MIMO CSI Feedback,\u201d IEEE Wireless Communications Letters, 2019. [Online]. Available: https://ieeexplore.ieee.org/document/8638509/. U2D network has been submitted. Requirements  Python 3.5 (or 3.6) Keras (>=2.1.1) Tensorflow (>=1.4) Numpy  Data Set The CSI data is generated using COST 2100 channel model. We will upload the data set later. You can refer the paper below and the corresponding implementations: L. Liu, J. Poutanen, F. Quitin, K. Haneda, F. Tufvesson, P. De Doncker, P. Vainikainen and C. Oestges, \u201cThe COST 2100 MIMO channel model,\u201d IEEE Wireless Commun., vol 19, issue 6, pp 92-99, Dec. 2012. [Online]. Available: https://ieeexplore.ieee.org/document/6393523/ CsiNet The implementation of CsiNet can be found in https://github.com/sydney222/Python_CsiNet. Thank authors for sharing their code. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://ieeexplore.ieee.org/document/8638509/", "https://ieeexplore.ieee.org/document/6393523/"]}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f8f"}, "repo_url": "https://github.com/H160401801/policy_textClassier", "repo_name": "policy_textClassier", "repo_full_name": "H160401801/policy_textClassier", "repo_owner": "H160401801", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T08:22:42Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T09:27:44Z", "homepage": null, "size": 5511, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184394787, "is_fork": false, "readme_text": "\u653f\u7b56\u6587\u672c\u5206\u7c7b\u5668 0.\u76ee\u5f55 bert/                   ---2.bert\u8fd0\u884c\u76ee\u5f55 chinese.../             ---bert\u6a21\u578b\u76ee\u5f55\uff0c\u7528\u6cd5\u89c1/README.MD data/                   ---\u57fa\u4e8e\u7f51\u7edc\u722c\u866b\u7684\u6570\u636e\u96c6 dataPreprocess/         ---\u6587\u672c\u9884\u5904\u7406\uff0c\u9700\u8981jieba doc/                    ---\u5b9e\u9a8c\u8fc7\u7a0b methods/                ---1.\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5 policy_textCrawer/      ---\u653f\u7b56\u6570\u636e\u722c\u866b\uff0c\u7528\u6cd5\u89c1/README.MD requirement.txt 1.\u57fa\u4e8e\u673a\u5668\u5b66\u4e60\u548c\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u7684\u6587\u672c\u5206\u7c7b\u5668 1.1\u73af\u5883\u642d\u5efa \u673a\u5668\u5b66\u4e60\uff1a sklearn \u6df1\u5ea6\u5b66\u4e60\uff1a keras == 2.1.2 tensorflow == 1.4.0 1.2\u53c2\u4e0e\u6bd4\u8f83\u7684\u673a\u5668\u5b66\u4e60\u65b9\u6cd5  CNN \u3001 CNN + word2vec LSTM \u3001 LSTM + word2vec MLP \u6734\u7d20\u8d1d\u53f6\u65af KNN SVM SVM + word2vec \u3001SVM + doc2vec  2.\u57fa\u4e8ebert\u7684\u6587\u672c\u5206\u7c7b\u5668 2.1\u73af\u5883\u642d\u5efa python == 3.7 tensorflow-gpu == 1.10 pip install -r requirements.txt 2.2\u6570\u636e\u96c6\u521b\u5efa \u5728Data_Policy_Corpus\u4e0b\u8fd0\u884c python run Create_Dataset.py \u5373\u53ef\u722c\u53d6dataset/xxx/html\u91cc\u6240\u6709\u94fe\u63a5\u7684\u6587\u672c\u4fdd\u5b58\u5230dataset/xxx/json\u91cc 2.3\u6570\u636e\u9884\u5904\u7406 \u5728Data_Policy_Corpus\u4e0b\u8fd0\u884c python run DataPreprocess.py \u5373\u53ef\u5c06dataset/xxx/json\u91cc\u6240\u6709\u6570\u636e\u4fdd\u5b58\u5230dataset/xxxData.csv\u91cc \u8fd0\u884c\u7ed3\u679c\uff1a economy\uff1a0 total raw data\uff1a91 total pre data\uff1a90 innovation\uff1a1 total raw data\uff1a98 total pre data\uff1a97 trainData\uff1a107, devData\uff1a40, testData\uff1a40 write train data\uff1a107 write dev data\uff1a40 write test data\uff1a40 2.4bert\u7528\u4e8e\u6587\u672c\u5206\u7c7b \u5728/bert\u4e0b\u8fd0\u884c bash run.sh ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f90"}, "repo_url": "https://github.com/SomayahAlbaradei/Splice_Deep", "repo_name": "Splice_Deep", "repo_full_name": "SomayahAlbaradei/Splice_Deep", "repo_owner": "SomayahAlbaradei", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-01T18:40:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T09:45:46Z", "homepage": null, "size": 71210, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184396576, "is_fork": false, "readme_text": "Splice_Deep Splice Deep: Deep Learning Model for Recognition of Splice Sites in Genomic DNA. To correctly annotate genes contained in a primary genomic DNA sequence, one also has to correctly annotate boundaries between exons and introns in case a gene contains introns. These boundaries are demarcated by the splice sites (SS), referred to as donors and acceptors. Donors and acceptor sites are characterized in almost all cases by canonical GT and AG dinucleotides in DNA. Deriving accurate computational models to predict the positions of intron/exon boundaries would be therefore useful for functional annotation of genomes. The principal challenge arises from the fact that most of the GT/AG dinucleotides that present in a DNA sequence are not SS. In this work, we propose Splice Deep, a deep learning model for recognition of SS in different organisms\u2019 genome. Our model utilizes both surrounding regions in DNA that are flanking SS candidates in order to perform binary classification. Separate DL models are applied to 5\u2019 and 3\u2019 regions as well as to the entire flanking region. The output scores from the three models are used as discriminative features for neural network (NN) model to predict weather a given SS is a true site. Requirements  The tool runs on linux machine. Anaconda Python 2.7 or later. keras.  Run Specify the organism as following: -hs: Homo sapiens. -at: Arabidopsis thaliana. -oriza: Oriza japonica. -d_mel: Drosophila melanogaster. -c_elegans: C. elegans. To run an Acceptor model:   $ python Splice_Deep_Acceptor.py     To run a Donor model:   $ python Splice_Deep_Donor.py     ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f91"}, "repo_url": "https://github.com/NVukobrat/MNIST-GANs", "repo_name": "MNIST-GANs", "repo_full_name": "NVukobrat/MNIST-GANs", "repo_owner": "NVukobrat", "repo_desc": "A simple GAN (Generative Adversarial Network) for generating MNIST digits using TensorFlow.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-19T17:25:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T13:07:37Z", "homepage": "", "size": 27413, "language": "Python", "has_wiki": true, "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "open_issues_count": 0, "github_id": 184417519, "is_fork": false, "readme_text": "MNIST-GANs A simple GAN (Generative Adversarial Network) for generating MNIST digits using TensorFlow. GANs overview Generative Adversarial Networks (GANs) belongs to the generative models. That means they are able to generate artificial content base on the arbitrary input. Generally, GANs most of the time refers to the training method, rather on the generative model. Reason for this is that GANs don't train a single network, but instead two networks simultaneously. The first network is usually called Generator, while the second Discriminator. Purpose of the Generator model is to images that look real. During training, the Generator progressively becomes better at creating images that look real. Purpose of the Discriminator model is to learn to tell real images apart from fakes. During training, the Discriminator progressively becomes better at telling fake images from real ones. The process reaches equilibrium when the Discriminator can no longer distinguish real images from fakes. This repo demonstrates how GANs work with simple generative and discriminative networks.  The MNIST dataset represents is a reference for the generative model. Repo explination  source.py - Main file for training the GANs models. ./api - Contains rest of relevant project code. ./assets/images - Contains images used in this README.md file. ./assets/checkpoint - Contains the last checkpoint produced during training the model.  Models Generator The Generator model does upsampling to produces images from random noise. It takes random noise as an input, then upsamples several times until reach desired image size (in this case 28x28x1). model = keras.Sequential([         layers.Dense(units=7 * 7 * 256, use_bias=False, input_shape=(GEN_NOISE_INPUT_SHAPE,)),         layers.BatchNormalization(),         layers.LeakyReLU(),         layers.Reshape((7, 7, 256)),          layers.Conv2DTranspose(filters=128, kernel_size=(5, 5), strides=(1, 1), padding=\"same\",                                 use_bias=False),         layers.BatchNormalization(),         layers.LeakyReLU(),          layers.Conv2DTranspose(filters=64, kernel_size=(5, 5), strides=(2, 2), padding=\"same\",                                 use_bias=False),         layers.BatchNormalization(),         layers.LeakyReLU(),          layers.Conv2DTranspose(filters=1, kernel_size=(5, 5), strides=(2, 2), padding=\"same\",                                 use_bias=False, activation=\"tanh\"),     ]) Discriminator The Discriminator is a simple CNN-based image classifier. It outputs positive values for real images, and negative values for fake images.     model = keras.Sequential([         layers.Conv2D(filters=64, kernel_size=(5, 5), strides=(2, 2), padding='same',                        input_shape=[28, 28, 1]),         layers.LeakyReLU(),         layers.Dropout(rate=0.3),          layers.Conv2D(filters=128, kernel_size=(5, 5), strides=(2, 2), padding='same'),         layers.LeakyReLU(),         layers.Dropout(rate=0.3),          layers.Flatten(),         layers.Dense(units=1),     ]) Training results Training has done on 1k epochs. Bellow are examples during this process. 1 epoch  500 epoch  1000 epoch  ", "has_readme": true, "readme_language": "English", "repo_tags": ["gans", "tensorflow", "mnist"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f92"}, "repo_url": "https://github.com/giusy123/MIDS", "repo_name": "MIDS", "repo_full_name": "giusy123/MIDS", "repo_owner": "giusy123", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-01T19:09:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T18:14:11Z", "homepage": null, "size": 1118, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184460220, "is_fork": false, "readme_text": "Machine learning for Intrusion Detection System (MIDS) Il repository contiene l'implementazione di una Deep Neural Networks (DNN) per classificare esempi di attacchi da comportamenti normali Requisiti del codice Il codice realizzato \u00e8 stato testato sulla libreria python3.6+ libs. Packages necessari sono:  Tensorflow 1.13 Keras 2.2.4 Matplotlib 3.0.3 Numpy 1.15.4 Pandas 0.24.2 Scikit-learn Seaborn 0.9.0  Data Il dataset usato per gli esperimenti \u00e8 accessibile da NSL-KDD. Il dataset originale (con classificazione a 5 classi) \u00e8 stato trasformato in un dataset binario con due classificazioni: \"attack, normal\" (_oneCls files) and then the  feature  selection  stage  is  performed  by  retain  the  10top-ranked  features  according  to  Information  Gain(IG) . Inoltre, delle 41 features originali il dataset contenuto nella cartella dataset del progetto sono state selezionate 10 features, che dventano 89 dopo le fasi di preprocessing Descrizione del codice Lo script contiene il codice utile per:  Fase di preprocessing:   Trasforma da categoriche a numeriche le categorie target del dataset: attacco=0; normale=1 One-hot encoding per trasforma le features categoriche Uso dela libreria Standard Scale   Crea un autoencoder con 60-30-10 neuroni rispettivamente per la parte di encoder e 30-60 per la parte di decoder e lo addestra sul dataset di training    Salva i pesi dell'autoencoder precedentemente appreso. I primi tre livelli della parte relativa all'encoder con i pesi fissati diventano i  primi due livelli, a cui \u00e8 aggiunto un ultimo livello con fuzione softmax di un modello che classifica attacchi da non attacchi.    Il modello \u00e8 poi usato con funzione di predizione sul testing set per valutarne l'accuratezza del modello  Come usare lo script Lo script richiede in input:  La cartella in cui si trova il dataset, usare dataset (la cartella dataset \u00e8 gi\u00e0 fornita nel repository) Il nome del dataset (sugg. KDDTrain.csv \u00e8 il dataset fornito con il codice) Il path della cartella dove salvare i plot (la cartella plot \u00e8 gi\u00e0 fornita con il repository) La percentuale in cui splittare il dataset tra training set e testing set (il codice accetta in input un valore tra 0.1 e 0.5)  ", "has_readme": true, "readme_language": "Italian", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f93"}, "repo_url": "https://github.com/zjui-machine-leaning/Deep-Leaning-Based-High-Precision-Sorting-System", "repo_name": "Deep-Leaning-Based-High-Precision-Sorting-System", "repo_full_name": "zjui-machine-leaning/Deep-Leaning-Based-High-Precision-Sorting-System", "repo_owner": "zjui-machine-leaning", "repo_desc": "Deep Leaning Based High Precision Sorting System", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-11T15:47:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T09:24:51Z", "homepage": null, "size": 115721, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 184394481, "is_fork": false, "readme_text": "Mask R-CNN for Object Detection and Segmentation This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone. See code of our project following the path: ./samples/train/  The repository includes:  Source code of Mask R-CNN built on FPN and ResNet101. Training code for MS COCO Pre-trained weights for MS COCO Jupyter notebooks to visualize the detection pipeline at every step ParallelModel class for multi-GPU training Evaluation on MS COCO metrics (AP) Example of training on your own dataset  The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here. Getting Started   demo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.   train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.   (model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.   inspect_data.ipynb. This notebook visualizes the different pre-processing steps to prepare the training data.   inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.   inspect_weights.ipynb This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.   Step by Step Detection To help with debugging and understanding the model, there are 3 notebooks (inspect_data.ipynb, inspect_model.ipynb, inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples: 1. Anchor sorting and filtering Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.  2. Bounding Box Refinement This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.  3. Mask Generation Examples of generated masks. These then get scaled and placed on the image in the right location.  4.Layer activations Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).  5. Weight Histograms Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.  6. Logging to TensorBoard TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.  6. Composing the different pieces into a final result  Training on MS COCO We're providing pre-trained weights for MS COCO to make it easier to start. You can use those weights as a starting point to train your own variation on the network. Training and evaluation code is in samples/coco/coco.py. You can import this module in Jupyter notebook (see the provided notebooks for examples) or you can run it directly from the command line as such: # Train a new model starting from pre-trained COCO weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  # Train a new model starting from ImageNet weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet  # Continue training a model that you had trained earlier python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5  # Continue training the last model you trained. This will find # the last trained weights in the model directory. python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last  You can also run the COCO evaluation code with: # Run COCO evaluation on the last trained model python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last  The training schedule, learning rate, and other parameters should be set in samples/coco/coco.py. Training on Your Own Dataset Start by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application. In summary, to train the model on your own dataset you'll need to extend two classes: Config This class contains the default configuration. Subclass it and modify the attributes you need to change. Dataset This class provides a consistent way to work with any dataset. It allows you to use new datasets for training without having to change the code of the model. It also supports loading multiple datasets at the same time, which is useful if the objects you want to detect are not all available in one dataset. See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py. Differences from the Official Paper This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.   Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.   Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation. To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, and only 0.01% differed by 10px or more.   Learning Rate: The paper uses a learning rate of 0.02, but we found that to be too high, and often causes the weights to explode, especially when using a small batch size. It might be related to differences between how Caffe and TensorFlow compute gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively. We found that smaller learning rates converge faster anyway so we go with that.   Citation Use this bibtex to cite this repository: @misc{matterport_maskrcnn_2017,   title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},   author={Waleed Abdulla},   year={2017},   publisher={Github},   journal={GitHub repository},   howpublished={\\url{https://github.com/matterport/Mask_RCNN}}, }  Contributing Contributions to this repository are welcome. Examples of things you can contribute:  Speed Improvements. Like re-writing some Python code in TensorFlow or Cython. Training on other datasets. Accuracy Improvements. Visualizations and examples.  You can also join our team and help us build even more projects like this one. Requirements Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt. MS COCO Requirements: To train or test on MS COCO, you'll also need:  pycocotools (installation instructions below) MS COCO Dataset Download the 5K minival and the 35K validation-minus-minival subsets. More details in the original Faster R-CNN implementation.  If you use Docker, the code has been verified to work on this Docker container. Installation   Install dependencies pip3 install -r requirements.txt   Clone this repository   Run setup from the repository root directory python3 setup.py install   Download pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.   (Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).  Linux: https://github.com/waleedka/coco Windows: https://github.com/philferriere/cocoapi. You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)    Projects Using this Model If you extend this model to other datasets or build projects that use it, we'd love to hear from you. 4K Video Demo by Karol Majek.  Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.  Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.  Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science Bowl Code is in the samples/nucleus directory.  Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.  Reconstructing 3D buildings from aerial LiDAR A proof of concept project by Esri, in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.  Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy A project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.   Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery Research project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.  Mask-RCNN Shiny A computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.  Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.  GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ond\u0159ej Pe\u0161ek.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cocodataset.org/#home", "http://www.mdpi.com/2072-4292/10/9/1487"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f94"}, "repo_url": "https://github.com/jchu33/Music-Recommendation-Engine", "repo_name": "Music-Recommendation-Engine", "repo_full_name": "jchu33/Music-Recommendation-Engine", "repo_owner": "jchu33", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T14:54:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T18:48:40Z", "homepage": null, "size": 2407, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184464901, "is_fork": false, "readme_text": "CPSC 490 Music Recommendation Engine Jason Chu Advisor: Scott Petersen Spring 2019 This is a music recommendation engine that uses collaborative filtering and content-based filtering to suggest similar songs and artists. How to use this engine: Here is the Google Drive folder that contains all the data that you will need for any of the future steps listed here. You may also need to pip install some dependencies such as keras, librosa, etc. If you DON'T want to start from scratch: Getting collaborative filtering ready:  Download the data from Drive: song_data.csv, collaborative_filtering_user_data, and collaborative_filtering_user_profiles from Drive folder. The first has the Last.fm user data, and the last two are python pickles stored after reading a part of the raw csv. Make sure they are in the same directory as all the other files. Download the kNN models from Drive: cf_artist_model.sav and cf_song_model.sav.  Getting content-based filtering ready:  Download 4genreweights.best.h5 from Drive, make sure it is in the same directory as all the other files.  If you DO want to start from scratch: Build the collaborative filtering component:  Download song_data.csv from the Drive folder, make sure it is in the same directory as all the other files. Run collaborative_filtering_songs.py to generate the song kNN model. Download collaborative_filtering_user_data and collaborative_filtering_artist_data from Drive, make sure they are in the same directory as all the other files. Run collaborative_filtering_artists.py to generate the artist kNN model.  Build the content-based filtering component  Download 4GenreFMA.data, 4GenreFMA.onehotlabels, 4GenreGTZAN.data, 4GenreGTZAN.onehotlabels, 4GenreGTZAN.labels from Drive folder: these are the audio features and one-hot-encoded genre classifications of the dataset. 4GenreFMA is used for training (4000 genre balanced songs) and 4GenreGTZAN is used for testing (400 genre balanced songs). Run cnn.py to generate and save the model  Either way you choose, after all that, run song_recommender.py and follow its instructions. Most importantly, you have to pass in an audio sample that is named in a certain format. It might take sometime for song_recommender to prompt you for an audio sample as it needs to load a huge spreadsheet in order to run. Description of files: song_recommender.py: the main thing; run this file to be able to actually get song recommendations from both CF and content-based components cnn.py: contains CNN code collaborative_filtering_artists.py: builds the kNN model to find similar artists collaborative_filtering_songs.py: builds the kNN model to find similar songs extract_features.py: extracts features from the audio samples process_fma.py: goes through FMA data to rename and genre tag otherwise unknown songs visual_spectrogram.py: plots a spectrogram of the features collected from a song using Librosa ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f95"}, "repo_url": "https://github.com/Ah-sonKevin/TrashCanCan-IOTSprint", "repo_name": "TrashCanCan-IOTSprint", "repo_full_name": "Ah-sonKevin/TrashCanCan-IOTSprint", "repo_owner": "Ah-sonKevin", "repo_desc": "IOT Sprint project: Trash Can Can ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T19:19:35Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-01T18:42:04Z", "homepage": null, "size": 32146, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184463958, "is_fork": false, "readme_text": "#Trash can can #Concept Recycling and sorting waste is something that is widely talked about, yet still seems to be difficult for individuals in the sense that 100% of the people we interviewed want to do it more and better. Issues include unclear directions at recycling stations, lack of resources at home and just plain laziness. We can even see people treating trash in a completely ignorant way, when it\u2019s all over streets, parks and festival areas. And when asked how people feel about this, the words hate, mad, littering fines, terrible, awful and uncivilized came up. So we built a product to make recycling more effortless, fun, and directly valuable for people. TrashCanCan is a service to be combined with the increased use of city bikes. The city bikes are already attached to stops that serve as fixed locations the users are going to visit anyway. The idea is that when a city bike stop is attached to a TrashCanCan, in order to lock the bike, a piece of trash has to be thrown in the can. The coolest part is that the trash can only has one input so the user doesn\u2019t need to think about recycling, but the can will automatically sort the trash inside itself to different containers with the help of image recognition and robotics. Here is a simple demo how it sorts pieces of trash into two categories: #What this program do The camera will scan for trash on the plateform, if detected it will rotate to put the trash in the appropriate compartment, cup will go to the right, orange, apple, spoon, bottle and other object will go left. A led will lit up when an object is detected #Installation of the dependencies sudo aptitude install -y  python3-pip gfortran python3-dev python-setuptools    python3-pyqt5 libtbb2   libqt4-test  libv4l-dev libxvidcore-dev libx264-dev libgtk-3-dev  libatlas-base-dev   libilmbase-dev libopenexr-dev libgstreamer1.0-dev libqtgui4  libatlas3-base libopencv-dev libpng12-dev    libhdf5-dev libjpeg8-dev  libjpeg-dev libtiff5-dev liblcms2-dev libwebp-dev libharfbuzz-dev libfribidi-dev tcl8.6-dev tk8.6-dev python-tk libgtk2.0-dev libavcodec-dev libavformat-dev libswscale-dev    libtbb-dev ibtiff-dev libjasper-dev libdc1394-22-dev #Installation of tensorflow python3 -m pip install --user -Uv tensorflow #Installation of the other dependencies for the image recognition python3 -m pip install --user -Uv  pillow keras h5py matplotlib opencv-python  picamera scipy numpy RPi.GPIO #Installation of the image recognition python3 -m pip install -Uv --user https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl #Installation of wiringpi to control the motor  (/!\\ don't forget the sudo) sudo python3 -m pip install --user -Uv wiringpi #Pinout (BCM numbering) Control Pin of the motor : 18 Led positive pin : 14 #How to use it To start the program type python3 main.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Ah-sonKevin/TrashCanCan-IOTSprint/blob/64b8aaf01d1a1586d95962bd10c6401105eba513/yolo-tiny.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f96"}, "repo_url": "https://github.com/lizhensong/tensorflow-deep-learning", "repo_name": "tensorflow-deep-learning", "repo_full_name": "lizhensong/tensorflow-deep-learning", "repo_owner": "lizhensong", "repo_desc": "\u4f7f\u7528tf.keras\u5b9e\u73b0deep learning\u7684\u4e00\u4e9b\u5c0f\u9879\u76ee", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-25T11:55:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T10:19:52Z", "homepage": null, "size": 3602, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184399855, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f97"}, "repo_url": "https://github.com/sohamshah/find_circle_cnn", "repo_name": "find_circle_cnn", "repo_full_name": "sohamshah/find_circle_cnn", "repo_owner": "sohamshah", "repo_desc": "Basic CNN through tf.keras for detecting circles with gaussian noise ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-01T21:52:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T21:37:42Z", "homepage": null, "size": 86805, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184485076, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/sohamshah/find_circle_cnn/blob/c9cb2905fde74408b42f67106302d0ea27f479d4/cnn_deeper.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5187f7eb8d62ad0728f98"}, "repo_url": "https://github.com/AhmadMoussa/A-guide-to-Wavenet", "repo_name": "A-guide-to-Wavenet", "repo_full_name": "AhmadMoussa/A-guide-to-Wavenet", "repo_owner": "AhmadMoussa", "repo_desc": "A Wavenet Primer. From audio preparation to audio generation.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T07:53:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-01T06:24:45Z", "homepage": "", "size": 337, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 184377455, "is_fork": false, "readme_text": "Wavenet (Work in Progress) A compendium of everything you need to know to get started with Wavenet. From turning Audio into Data, Creating the Wavenet Model, feeding and training your model on your data and ultimately generate your own sounds. (Hopefully :p) Introduction:  Wavenet was first introduced in this paper and is an essential read for the rest of this article. What makes the wavenet so powerful? As we will go through the different parts that will ultimately conglomerate this network, we will notice that a lot of ideas have been borrowed from other types of networks, and all of them are powerful mechanisms in their own right, such as convolutional layers, dilated filters, gated activation units, 1by1 convolutions for channel shrinking, skip connections and residual connections. But they also work towards fixing some of the problems that previous deep networks struggled with in the past.  Tools:  python pytorch theano  Relevant Articles:  A paper a day delays the neuron decay Korean Guy  From Audio to Data: What are Sampling Rate and Bit Depth?   To understand the underlying inner workings of the wavenet, we need to first take a closer look at the data that we are going to use. Because without Data in the first place, there would be no need for this neural network anyway. We need to train our model on audio. Easier said than done. First and foremost we need to find a way to convert from audio, that we humans perceive and \"understand\" with our ears, to a format that is machine understandable (Spoiler: Numbers!).   Sound, as we hear it in the real world, can be thought of as a continuous analog waveform (continuous vibrations in the air). Converting this analog waveform to a number representation is done by capturing it's descriptive values at successive points in time. analogically it's somewhat like capturing a video, which is ultimately just a succesion of images. Later on, we can chain these descriptive values (samples) together and to accurately recreate the original waveform. Naturally, the more \"snapshot\" we take of a given sound the better we will be able to recreate it later on with a good \"resolution\". Hence, the rate of capture is called the \"sampling Rate\". Bit Depth stands for the number of bits that are used to represent each captured sample. (talk about what bit depth does and help quantize signal-to-noise ratio (SNR))   From Audio to Time Series  Now, we can think of the audio data points that we captured as a time series, which is simply said, a bunch of data points that have some correlation and causality with each other in relation to time. But we have a little-not-so-little problem. There is a massive amount of these sample data points, as well as a gigantic dynamic range for each sound.  Why do these two factors cause us problems?  At the end of the day, we're going to want to generate some audio samples. Our nework is going to try to recreate the sample data points that we recorded and fed into our machine. Assume these audio files are Encoded in Stereo 16-bit. That means that there are 65536 values along the y-axis where data points could be located. Now we are not going to delve into madness and try to assume that our network is going to take an educated guess as to where it should place the data point at a given time step t. Luckily, there is a way to reduce this humonguous number of values to a smaller range, specifically 256. Now that's a number that I can work with!  \u03bc-law Quantization or Companding Transformation  An ingenious way to shrink our dynamic range. To understand what we are actually doing we should primarily have a look at a visual analogy.    So, what exactly are we looking at? Observe the two squares to the left side. Which one of them has more dots? Obviously, the lower one. Now observe the two squares to the right. Guess what, it's a little bit more difficult to say which one has more dots now (It's still the lower one, the lower squares have exactly 10 dots more than their upper counterparts). This is known as the Weber-Fechner law, which treats the relation between the actual change in a physical stimulus and the perceived change in the stimulus. What does this have to do with Audio? Speech, for example, has a very high dynamic range, and when we record someone speaking, we want to capture the big frequency jumps that their voice makes, and compared to these jumps, subtle variations and finer details are lost in comparision. Hence with the mu-law companding transformation we can compress a waveform and represent it with significantly less bits, without loosing any important information of the original data. And here's the formula:   Looks scary, but it's not really.  I found several implementations korean guy and lemonzi but somehow I couldn't get either to work \"right\", I'll have to look into it at some later point. A quick python implementatio of the mu-law encode, we can use numpy or torch (couldn't figure it out yet though):  # function that will create the mu-law encoding from the input waveform vector def encode_mu_law(to_encode, mu = 256):     mu = mu -1     toplog = np.log(1 + (np.abs(to_encode) * mu))     botlog = np.log(1 + mu)     sign = np.sign(to_encode)     fx =  sign * ( toplog / botlog )     return ((fx + 1) / 2 * mu + 0.5).astype(np.long) But we still have to convert to the desired range that we want to project onto, namely -256,256 Model Structure: Causal Dilated Convolutions: Convolutions:   Let's digress a bit and start easy, if you don't know what a convolution is I recommend you go for a little stroll, and read this wonderfully comprehensive beginner's guide by Adit Deshpande (this is the most comprehensive and beginner friendly read I could find)   Sometimes it is beneficial to look at the surroundings of a given spot (neuron) and focus on a smaller area, rather than the entire data given to us. We can learn a whole lot by observing the relationship between some data and it's surrounding data. In the case of neural networks that deal with images, it is not practical to use a fully connected feedforward neural network (Even though we can still learn features with this architecture). This concise area of interest that we are going to inspect in detail, is usually called a \"receptive field\". The tool (we can also refer to it as a lens) with which we inspect this receptive field is reffered to as a \"Filter\".   What does the filter look like? The filter is but a small layer of parameters, simply said, a weight matrix. Why is it called a filter? Because we are going to place this filter over our area of interest (figuratively) and pull information through it (dot products between the entries of the filter and the input at any position, or rather element wise multiplications) to learn something new about our data. After that we slide our filter to a new area of interest and repeat. Performing this sliding movement around the picture can also be reffered to as convolving, wherefrom stems the term convolutional neural network (CNN).      You must wonder: what is this filter actually doing? In theory we are taking some numbers from the input layer and multiplying them with the weights in the filter, to get some new numbers to describe what is happening in the picture with some abstraction. Ultimately, we end up with an output layer, called an activation map, or feature map. This activation map, represents what the network thinks, is in the image. If we keep repeating this process our model gains a deeper understanding of the initial input.   This was a brief introduction to convolutional layers. If you are hungry for more convolutional neural network shenanigans, I suggest you read this course by Stanford University on COnvolutional Neural Networks   And here's the code for a simple 2D convolution that detects vertical lines using keras: from numpy import asarray from keras import Sequential from keras.layers import Conv2D  # define input data data = [[0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0],   [0, 0, 0, 1, 1, 0, 0, 0]]  # Creating a numpy array from the data above data = asarray(data)  '''     Here we are converting our data to a 4Dimensional container     Think of it as an array of tensors (Tensor being a 3Dimensional Array)     Such that [number of samples, columns, rows, channels]     In this trivial case we only have one sample, and the channels are shallow ''' data = data.reshape(1, 8, 8, 1) print(data)  # Create a Sequential keras model, we'll only have one layer here model = Sequential() # https://keras.io/layers/convolutional/ # Conv2D(number of filters, tuple specifying the dimension of the convolution window, input_shape) model.add(Conv2D(1, (3,3), input_shape=(8, 8, 1)))  # Define a vertical line detector detector = [[[[0]],[[1]],[[0]]],             [[[0]],[[1]],[[0]]],             [[[0]],[[1]],[[0]]]] weights = [asarray(detector), np.asarray([0.0])] # store the weights in the model model.set_weights(weights) # confirm they were stored print(model.get_weights())  # apply filter to input data yhat = model.predict(data)  for r in range(yhat.shape[1]):  # print each column in the row  print([yhat[0,r,c,0] for c in range(yhat.shape[2])])  this tutorial helped with this example.  Dilations:  Now let's expand the concept of a filter (literally and figuratively).     What are dilated convolutions? A dilated convolution refers to a convolution with a filter that is dilated, where dilated means that the filter is enlarged by spacing out the weight values and padding them with zeroes in between. What is really happening, is that we are expanding the receptive field and increasing our coverage, we are looking at the relationship between neighbours that are a little bit more distant from each other. This is useful if some important features of our data are only definable in regions larger than what our receptive field covers. To define such a feature, normally one would have to convovle as second time, or use a dilated filter. Below is an illustrative schematic of such a filter:    Why are dilated convolutions useful?  Stacked dilated convolutions enable networks to have very large receptive fields with just a few layers, while preserving the input resolution throughout the network as well as computational efficiency.  Let's dissect this,beginning with \"resolution\". Usually, when the filter convolves over an input, we end up with an activation map that has a lesser size than what we started with. Imagine it being like a funnel being applied to each area of the input. In that sense, we are losing resolution. The more convolutional layers we have, the more our input will shrink. One approach involves repeated up-convolutions that aim to recover lost resolution while carrying over the global perspective from downsampled layers (Noh et al., 2015; Fischer et al., 2015). This leaves open the question of whether severe intermediate downsampling was truly necessary (Chen et al., 2015; Yu & Koltun, 2016).    Hence in the case of audio, preserving resolution and ordering of data is one of our main priorities.  Original Paper on Dilated Convolutions Causality:  Talk about difference between causal convolution and RNN and how causal convolution is easier to compute. Add how dilating the filter fixes the problem that the causal convolution has.  What's a Causal Convolution?   Causality in our case simply means that we are hiding all future time steps(data samples), from the filter that is currently at work. We don't want it to see yet what's coming up in the future, but we would like it to learn the relationship between the current time-step and previous ones. If there would be \"leakage\" of future content into the current time-step, then we wouldn't be teaching our network the correct content. The origin of this causal layer can be drawn back, yet again, to the pixelCNN where instead of causality they use masks, but it is basically the same.   In the PixelCNN, in simple terms, we are trying to learn what the next pixel should look like (it's RGB values), given all the pixels that have occurred previously. Think about it intuitively for a second, we've been given a half complete image that is mostly blue-ish of color, it would make sense to complete the sequence with another blue pixel, maybe a little bit brighter or less bright.   And this is the reason why the authors have labelled PixelCNN as a model that performs \"Autoregressive Density Estimation\", where density stands for a probability density function. In both Wavenet and PixelCNN we are trying to model the joint probability of a timestep as a product of conditional probabilities of all previous timesteps. In layman's terms, what's the chance we're simething is going to happen, given everything that has happened already.    Why use Causal Convolutions rather than RNNs or LSTMs   Recurrent Neural Networks are notorious for being hard to train(vanishing/exploding gradient and recurrent connections), but they have an advantage, which is a long memory retention.   Now our causal filter has to be linearly very large to achieve an effective history size, which is cumbersome to compute and introduces an overhead. But this problem can simply be solved by stacking dilated layers on top of this causal one, which allows us to exponentially increase our filter size and achieve a larger history size.   In addition, the backpropagation method is different from that used in RNNs, and therefore avoiding the vanishing/exploding gradient problem altogether.   Gated Activation Units:   The term \"Gate\" has been adopted in a number of fields, for example in music production, we use the term \"Noise-Gate\" when we refer to a device that is responsible for attenuating signals that fall below some pre determined threshhold, and simpler said, if a certain sound is not loud enough, then the listener will not be able to hear it at all.   What are activation functions and why are they useful? As it's name indicates, it can be viewed as a mechanism that decides, based on the amplitude of an incoming signal, if it should send a signal or not. Usually, the type of the problem we are dealing with, determines which type of activation functions would be best suited. Let's have a look at three different types of these functions:   I think I should write a tutorial for these activation functions separately, as it blows up the range of this article quite a bit. Sigmoid:  Historically, this has been the most widely used activation function (before it was replaced by the ReLU). Why is the sigmoid function used? And what makes it a good activation function? It has several nice properties. One of them is differentiability, this is especially important when the gradient needs to be calculated during back-propagation. It is also non-linear, this allows us to stack layers.  A stack of linear functions would be equivalent to have a single one. Therefore with this tanh: Rectified Linear Unit:    Apparently these gates yield better results than using a rectified linear unit. Pin-pointing why they do so   Using this type of gate, it allows us to control what information will be propagated throughout the remaining layers. This concept has been adopted from LSTM models where gates are employed to establish a \"long term memory\" for important information, as opposed to RNNs which struggle retaining such information. Dauphin et. al. offers a good explanation for this in section 3 of his paper on \"Language Modeling with Gated Convolutional Networks\":    LSTMs enable long-term memory via a separate cell controlled by input and forget gates. This allows information to flow unimpeded through potentially many timesteps. Without these gates, information could easily vanish through the transformations of each timestep. In contrast, convolutional networks do not suffer from the same kind of vanishing gradient and we find experimentally that they do not require forget gates. Therefore, we consider models possessing solely output gates, which allow the network to control what information should be propagated through the hierarchy of layers.  And here's the paper for reference.  Another reason why this concept was adopted might be because PixelRNN have been outperforming PixelCNN due to them having recurrent connections between one layer to all other layers in the network, whereas PixelCNN does not have this feature. This is solved by adding more layers to add depth, and since CNNs don't struggle with vanishing gradients we don't have to worry about that, whereas RNNs do. We also add a gate to mimic the gates of the inner workings of LSTMs and have more control over the flow of information through our model.  Additionally, one last point I would like to add, is that generally sigmoid and tanh are better for approximating classifier functions. Hence this might lead to faster training and convergence. Therefore, this could also be a reason, why they chose to replace the ReLU with thei custom function. 1 x 1 Convolution:   This seems like a really trivial thing, when it is applied in a 2 Dimensional context, then we would simply be multiplying each number in a matrix by another number.   But in higher dimensions this allows to do a rather non-trivial computation on the input volume. Assume we have a tensor of dimensions 6 x 6 x 32 (each channels has 32 attributes). Now, if we'd multiply this tensor by a 1 x 1 x 32 filter, we would end up with a output that has 6 x 6 x 1 dimensionality. Pretty cool, huh? With a simple 1 x 1 filter we were able to shrink the number of channels. This idea was first proposed in this paper.   This effectively allows us to shrink the number of channels to the number of filters that were applied.  Residual Blocks:  The term \"Residual Block\" refers the structure that computes the residual, which will be passed on to subsequent residual blocks. What is a residual  Implementing the network: Dependencies:  We'll need a couple of things before we get started: pip install tensorflow pip install tensorboard pip install librosa pip install numpy pip install matplotlib  Before we start:  Make sure you install librosa and tensorflow. Librosa might throw some errors, hence try debugging it first and getting it to work. we will need it to load audio samples into our model.  import librosa import numpy as np import matplotlib.pyplot as plt  def load_audio():     filename = librosa.util.example_audio_file()     audio, _ = librosa.load(filename, 11025, mono=True, duration = 0.1)     audio = audio.reshape(-1, 1)     return audio  plt.plot(load_audio()) plt.show()  Let's create a minimal audio loader, and test it by plotting a small part of the waveform. You should get something that looks like this:    Next step is quantizing this audio by passing it through the mu-law encode:  def mu_law_encode(audio, quantization_channels):     mu = tf.to_float(quantization_channels - 1)     safe_audio_abs = tf.minimum(tf.abs(audio), 1.0)     magnitude = tf.log1p(mu * safe_audio_abs) / tf.log1p(mu)     signal = tf.sign(audio) * magnitude     return tf.to_int32((signal + 1) / 2 * mu + 0.5)  quantized_audio = mu_law_encode(audio, 256)  sess = tf.InteractiveSession() print(quantized_audio.shape)  for i in range(quantized_audio.shape[0]):     print(quantized_audio[i].eval(session = sess))    Looking good so far. Notice the scale of the two graphs on their y-axes, even though it seems like the quantized graph has a much larger scale, it actually only spans 256 discrete values, whereas the prior graph has an almost continuous decimal range. The first graph has a better resolution visually, but the quantized version is good enough to work with. Also, notice the second huge dip, it is almost vertical in the quantized version as aopposed to the original. This reinforces that our encode pays more detail to the little variations more than it does for the huge frequency swings.   Now last thing before we can move one is converting this quantized audio data into one hot encodings.   def _one_hot(input_batch):     encoded = tf.one_hot(input_batch, depth = 256, dtype = tf.float32)     shape = [1, -1, 256]     encoded = tf.reshape(encoded, shape)     return encoded  one_hot = _one_hot(quantized_audio) one_hot_data = one_hot[0,0,:].eval(session=sess) print(one_hot_data)  And you should see something like this:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]   The only value that is not zero, is the 128th, which is in concordance with what we see in the graphs and when we print out the content of the arrays.  Enter Wavenet:  Our input data has to go through a causal convolution first, the purpose of it is to shrink the number of layers. What this is in fact, is a feature pooling technique as discussed previously in 1x1 convoltuion section.  import tensorflow as tf  def create_variable(name, shape):     initializer = tf.contrib.layers.xavier_initializer_conv2d()     variable = tf.Variable(initializer(shape=shape), name=name)     return variable  def _create_causal_layer(input_batch):         initial_filter_width = 32         initial_channels = 2**8         residual_channels = 16         weights_filter = create_variable('filter', [initial_filter_width, initial_channels, residual_channels])         return causal_conv(input_batch, weights_filter, 1)  # what this essentially does is reduce the number of channels and def causal_conv(value, filter_, dilation, name='causal_conv'):     filter_width = tf.shape(filter_)[0]     restored = tf.nn.conv1d(value, filter_, stride=1, padding='VALID')     tf.global_variables_initializer().run()     # Remove excess elements at the end.     out_width = tf.shape(value)[1] - (filter_width - 1) * dilation     result = tf.slice(restored, [0, 0, 0], [-1, out_width, -1])     return result We are also cutting a couple of elements at the end depending on the size of the batch that was fed to it. Structure and Helper Functions:   Let's first create all the parts that will make up the final network, wou'll need to understand variable scopes and the python with statement. Go ahead and look them up and come back:   We need afunction that creates variables according to our specified shape, and we'll have a number of different ones. So let's slap a wrapper around the tensorflow Variable() function:   import tensorflow as tf  def create_variable(name, shape):     initializer = tf.contrib.layers.xavier_initializer_conv2d()     variable = tf.Variable(initializer(shape=shape), name=name)     return variable   Next let's create the structure of the network, which is basically a large dictionary of learnable filters that we're gonna use to convolve over the input sequence, and update them during backprop. Code:  import tensorflow as tf from create_variable import create_variable  def construct_network(filter_width, quantization_channels,residual_channels,dilation_channels,skip_channels,dilations):     net = dict()      with tf.variable_scope('wavenet'):         with tf.variable_scope('causal_layer'):             layer = dict()             layer['filter'] = create_variable('filter', [filter_width, quantization_channels,residual_channels])             net['causal_layer'] = layer         net['dilated_stack'] = list()         with tf.variable_scope('dilated_stack'):             for i, dilation in enumerate(dilations):                 with tf.variable_scope('layer{}'.format(i)):                     current = dict()                     current['filter'] = create_variable('filter', [filter_width, residual_channels, dilation_channels])                     current['gate'] = create_variable('gate', [filter_width, residual_channels, dilation_channels])                     current['dense'] = create_variable('dense',[1, dilation_channels,  residual_channels])                     current['skip'] = create_variable('skip', [1, dilation_channels, skip_channels])                     net['dilated_stack'].append(current)         with tf.variable_scope('postprocessing'):             current = dict()             current['postprocess1'] = create_variable('postprocess1',[1, skip_channels, skip_channels])             current['postprocess2'] = create_variable('postprocess2',[1, skip_channels, quantization_channels])         net['postprocessing'] = current     return net   Essentially, what we're doing here, is creating a massive python dictionary that will point towards \"containers\", that describe and carry our layers. This dictionary is comprised of:  A python dictionary that contains the causal layer at the front of the network. A python list of dictionaries that represents the dilated stack, wherein each dict is representing a residual block with a filter, gate, dense and skip layer. And finally a dictionary 2 postprocessing layers at the end.    Here's a diagram to help your imagination:   Now let's define some useful operations:  causal_conv we're going to use this one a bunch. It has two modes, a simple 1D convolution that serves as a channel shrinking operation when the dilation factor is equal to 1. And another mode when the dilation factor is higher than 1. For which we'll need the other two functions, time_to_batch and batch_to_time. time_to_batch batch_to_time    def time_to_batch(value, dilation, name = None):  with tf.name_scope('time_to_batch'):   shape = tf.shape(value)   pad_elements = dilation - 1 - (shape[1] + dilation - 1) % dilation   padded = tf.pad(value, [[0, 0], [0, pad_elements], [0, 0]])   reshaped = tf.reshape(padded, [-1, dilation, shape[2]])   transposed = tf.transpose(reshaped, perm = [1, 0, 2])   return tf.reshape(transposed, [shape[0] * dilation, -1, shape[2]])    def batch_to_time(value, dilation, name = None):  with tf.name_scope('batch_to_time'):   shape = tf.shape(value)   prepared = tf.reshape(value, [dilation, -1, shape[2]])   transposed = tf.transpose(prepared, perm = [1, 0, 2])   return tf.reshape(transposed, [tf.div(shape[0], dilation), -1, shape[2]])    def causal_conv(value, filter_, dilation, name = 'causal_conv'):  with tf.name_scope(name):   filter_width = tf.shape(filter_)[0]   if dilation > 1:    transformed = time_to_batch(value, dilation)    conv = tf.nn.conv1d(transformed, filter_, stride = 1, padding = 'VALID')    restored = batch_to_time(conv, dilation)   else:    restored = tf.nn.conv1d(value, filter_, stride = 1, padding = 'VALID')        out_width = tf.shape(value)[1] - (filter_width - 1) * dilation    result = tf.slice(restored, [0,0,0], [-1, out_width, -1])    return result   The convolutions we are going to use are all causal. Hence we're creating a function that takes our input value, the filter which we are going to convolve with over the input, and the dilation parameter to specify how dilated the filter is. If the dilation argument is less or equal to 1 we just convolve normally. If it's larger than 1 we need to some processing on the input. The input is in the form of a one_hot encoding  Terms we need to understand: I found that while reading research papers, I would come across a lot of words and terms that I couldn't understand, and they were not explained as it is assumed that you have some knowledge in the field that is being discussed. But if you've just started then a lot of the terms will be a difficult to digest. There will be sections throughout this article that will breka down the important ideas.  Tractable: you will come across this term in the context of solving problems, computing/calculating things and creating models. In the context of solving a problem, when we state that a problem is \"tractable\", it means that it can be solved or that the value can be found in a reasonable amount of time. Some problems are said to be \"Intractable\". From a computational complexity stance, intractable problems are problems for which there exist no efficient algorithms to solve them. Most intractable problems have an algorithm \u2013 the same algorithm \u2013 that provides a solution, and that algorithm is the brute-force search. Latent: a variable is said to be latent, if it can't be observed directly but has to be \"inferred\" somehow Stochastic: something that was randomly determined  Recurrent Neural Networks(RNN):  Introduction to neural networks by kaparthy Wonderful Article to start with, along with great examples and code on github Code for the above mentione article How can we make neural networks more exciting? We make them accept sequences as input rather than having a fixed number of inputs.  Convolutional Network:  Convolutional Layer: a layer over which a filter is being applied Dilated Convolution: In this type of convolution, the filter expands. This is sometimes also called \"Atrous\" convolution, where \"Atrous\" comes from the french word \"\u00e0 trous\" meaning \"with holes\". Causal Concolution: this term is rather vague, but a \"causal\" convolution means that there is no information leakage from future to past. Fractional Upsampling: when the stride of the filter is less than 1 (S < 1). Then we end up a with a feature map size larger than the input layer size.  Problems with librosa:  I ran into an error using librosa, which wouldn't allow it to load audio files or read them. This github issue accurately describes this probem and offers solutions for different environments raise NoBackendError(). On windows however, this fixed it for me. Also, you'll need this for windows:  to open admin command prompt press Windows+X and select the admin prompt. If it shows powershell instead of command prompt got to settings->personalize->change to command prompt    ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "deep-learning", "python3", "wavenet"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cs231n.github.io/convolutional-networks/#conv", "http://karpathy.github.io/2015/05/21/rnn-effectiveness/", "http://digitalsoundandmusic.com/5-3-8-algorithms-for-audio-companding-and-compression/"], "reference_list": ["https://arxiv.org/abs/1609.03499", "https://arxiv.org/pdf/1511.07122.pdf", "https://arxiv.org/pdf/1803.01271.pdf", "https://arxiv.org/pdf/1612.08083.pdf", "https://arxiv.org/pdf/1312.4400.pdf", "https://arxiv.org/abs/1803.01271"]}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3c7"}, "repo_url": "https://github.com/Tony607/Keras_RK3399pro", "repo_name": "Keras_RK3399pro", "repo_full_name": "Tony607/Keras_RK3399pro", "repo_owner": "Tony607", "repo_desc": "How to run Keras model on RK3399Pro", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T02:05:54Z", "repo_watch": 4, "repo_forks": 2, "private": false, "repo_created_at": "2019-05-02T12:43:53Z", "homepage": null, "size": 576, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 184579184, "is_fork": false, "readme_text": "Run Keras/Tensorflow model on RK3399Pro Clone or download this repo git clone https://github.com/Tony607/Keras_RK3399pro  Download pre-compiled Python wheel files from my aarch64_python_packages repo and rknn_toolkit wheels from their official GitHub. Step1: Freeze Keras model and convert to RKNN model (On Linux development machine) Require Python 3.5+. Install required libraries for your development machine pip3 install -r requirements.txt The install rknn toolkit with the following command. pip3 install rknn_toolkit-0.9.9-cp36-cp36m-linux_x86_64.whl  To freeze a Keras InceptionV3 ImageNet model to a single .pb file. The frozen graph will accept inputs with shape (N, 299, 299, 3). freeze_graph.py  To convert the .pb file to .rknn file, run python3 convert_rknn.py  Step2: Make prediction (On RK3399Pro board) Setup for the first time. sudo dnf update -y sudo dnf install -y cmake gcc gcc-c++ protobuf-devel protobuf-compiler lapack-devel sudo dnf install -y python3-devel python3-opencv python3-numpy-f2py python3-h5py python3-lmdb sudo dnf install -y python3-grpcio  sudo pip3 install scipy-1.2.0-cp36-cp36m-linux_aarch64.whl sudo pip3 install onnx-1.4.1-cp36-cp36m-linux_aarch64.whl sudo pip3 install tensorflow-1.10.1-cp36-cp36m-linux_aarch64.whl sudo pip3 install rknn_toolkit-0.9.9-cp36-cp36m-linux_aarch64.whl To run inference benchmark on RK3399Pro board, in its terminal run, python3 benchmark_incption_v3.py  ", "has_readme": true, "readme_language": "English", "repo_tags": ["rk3399", "rk3399pro", "keras"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3c8"}, "repo_url": "https://github.com/plumerai/zookeeper", "repo_name": "zookeeper", "repo_full_name": "plumerai/zookeeper", "repo_owner": "plumerai", "repo_desc": "A small library for managing deep learning models, hyper parameters and datasets", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T19:06:29Z", "repo_watch": 3, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T09:32:55Z", "homepage": "", "size": 62, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 5, "github_id": 184554869, "is_fork": false, "readme_text": "Zookeeper        A small library for managing deep learning models, hyper parameters and datasets designed to make training deep learning models easy and reproducible. Getting Started Zookeeper allows you to build command line interfaces for training deep learning models with very little boiler plate using click and TensorFlow Datasets. It helps you structure your machine learning projects in a framework agnostic and effective way. Zookeeper is heavily inspired by Tensor2Tensor and Fairseq but is designed to be used as a library making it lightweight and very flexible. Currently zookeeper is limited to image classification tasks but we are working on making it useful for other tasks as well. Installation pip install zookeeper pip install colorama  # optional for colored console output Registry Zookeeper keeps track of data preprocessing, models and hyperparameters to allow you to reference them by name from the commandline. Datasets and Preprocessing TensorFlow Datasets provides many popular datasets that can be downloaded automatically. In the following we will use MNIST and define a default preprocessing for the images that scales the image to [0, 1]: import tensorflow as tf  from zookeeper import cli, build_train, HParams, registry  @registry.register_preprocess(\"mnist\") def default(image, training=False):     return tf.cast(image, dtype=tf.float32) / 255 Models Next we will register a model called cnn. We will use the Keras API for this: @registry.register_model def cnn(hp, dataset):     return tf.keras.models.Sequential(         [             tf.keras.layers.Conv2D(                 hp.filters[0],                 (3, 3),                 activation=hp.activation,                 input_shape=dataset.input_shape,             ),             tf.keras.layers.MaxPooling2D((2, 2)),             tf.keras.layers.Conv2D(hp.filters[1], (3, 3), activation=hp.activation),             tf.keras.layers.MaxPooling2D((2, 2)),             tf.keras.layers.Conv2D(hp.filters[2], (3, 3), activation=hp.activation),             tf.keras.layers.Flatten(),             tf.keras.layers.Dense(hp.filters[3], activation=hp.activation),             tf.keras.layers.Dense(dataset.num_classes, activation=\"softmax\"),         ]     ) Hyperparameters For each model we can register one or more hyperparameters sets that will be passed to the model function when called: @registry.register_hparams(cnn) class basic(HParams):     activation = \"relu\"     batch_size = 32     filters = [64, 64, 64, 64]     learning_rate = 1e-3      @property     def optimizer(self):         return tf.keras.optimizers.Adam(self.learning_rate) Training loop To train the models registered above we will need to write a custom training loop. Zookeeper will then tie everything together: @cli.command() @build_train def train(build_model, dataset, hparams, output_dir, epochs):     model = build_model(hparams, dataset)     model.compile(         optimizer=hparams.optimizer,         loss=\"categorical_crossentropy\",         metrics=[\"categorical_accuracy\", \"top_k_categorical_accuracy\"],     )      model.fit(         dataset.train_data(hparams.batch_size),         epochs=epochs,         steps_per_epoch=dataset.train_examples // hparams.batch_size,         validation_data=dataset.validation_data(hparams.batch_size),         validation_steps=dataset.validation_examples // hparams.batch_size,     ) This will register Click command called train which can be executed from the command line. Command Line Interface To make the file we just created executable we will add the following lines at the bottom: if __name__ == \"__main__\":     cli() If you want to register your models in separate files, make sure to import them before calling cli to allow zookeeper to properly register them. To install your CLI as a executable command checkout the setuptools integration of Click. Usage Zookeeper already ships with prepare, plot, and tensorboard commands, but now also includes the train command we created above: python examples/train.py --help Usage: train.py [OPTIONS] COMMAND [ARGS]...  Options:   --help  Show this message and exit.  Commands:   plot   prepare   tensorboard   train To train the model we just registered run: python examples/train.py train cnn --dataset mnist --epochs 10 --hparams-set basic --hparams batch_size=64 ", "has_readme": true, "readme_language": "English", "repo_tags": ["python", "machine-learning", "deep-learning", "command-line-interface"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://click.palletsprojects.com/en/7.x/setuptools/", "http://yann.lecun.com/exdb/mnist"], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3c9"}, "repo_url": "https://github.com/microsoft/MLOps_VideoAnomalyDetection", "repo_name": "MLOps_VideoAnomalyDetection", "repo_full_name": "microsoft/MLOps_VideoAnomalyDetection", "repo_owner": "microsoft", "repo_desc": "Operationalize a video anomaly detection model with Azure ML", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T15:23:34Z", "repo_watch": 6, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T16:45:29Z", "homepage": null, "size": 168, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184617292, "is_fork": false, "readme_text": "Video Anomaly Detection - powered by Azure MLOps  The automation of detecting anomalous events in videos is a challenging problem that currently attracts a lot of attention by researchers, but also has broad applications across industry verticals. The approach involves training deep neural networks to develop an in-depth understanding of the physical and causal rules in the observed scenes. The model effectively learns to predict future frames in the video in a self-supervised fashion. By calculating the error in this prediction, it is then possible to detect if something unusual, an anomalous event, occurred, if there is a large prediction error. The approach can be used both in a supervised and unsupervised fashion, thus enabling the detection of pre-defined anomalies, but also of anomalous events that have never occurred in the past.  Post on LinkedIn (includes video demonstration)  Learning Goals You will learn:  How to adapt an existing neural network architecture to your use-case. How to prepare video data for deep learning. How to perform hyperparameter tuning with HyperDrive to improve the performance of you model. How to deploy a deep neural network as a webservice for video processing. How to post-process the output of a Keras model for secondary tasks (here, anomaly detection) How to define a build pipeline for DevOps.  Pre-requisites Skills  Some familiarity with concepts and frameworks for neural networks:  Framework: Keras Concepts: convolutional, recurrent, and pooling layers.   Knowledge of basic data science and machine learning concepts. Here and here you'll find short introductory material. Moderate skills in coding with Python and machine learning using Python. A good place to start is here.  Software Dependencies  Various python modules. We recommend working with a conda environement (see environment.yml) - Documentation VS code https://code.visualstudio.com/ X2Go https://wiki.x2go.org/doku.php  We found that a useful development environment is to have a VM with a GPU and connect to it using X2Go. Hardware Dependencies A computer with a GPU, Standard NC6 sufficient, faster learning with NC6_v2/3 or ND6. compare VM sizes Dataset UCSD Anomaly Detection Dataset Agenda Getting Started  Data Preparation Model Development Hyperparameter Tuning Anomaly Detection Deployment  Advanced Topics  Transfer learning - How to quickly retrain the model on new data. AML Pipelines - Use AML pipelines to scale your solution. MLOps - How to quickly scale your solution with the MLOps extension for DevOps.  References / Resources   Research Article: Deep predictive coding networks for video prediction and unsupervised learning by Lotter, W., Kreiman, G. and Cox, D., 2016.  @article{lotter2016deep,  title={Deep predictive coding networks for video prediction and unsupervised learning},  author={Lotter, William and Kreiman, Gabriel and Cox, David},  journal={arXiv preprint arXiv:1605.08104},  year={2016}  }    Original Prednet implentation is on github.com. Note, that the original implementation will only work in Python 2, but not in Python 3.   Interesting blog post on Self-Supervised Video Anomaly Detection by Steve Shimozaki   ", "has_readme": true, "readme_language": "English", "repo_tags": ["mlops", "azureml"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.svcl.ucsd.edu/projects/anomaly/dataset.htm"], "reference_list": ["https://arxiv.org/abs/1605.08104"]}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3ca"}, "repo_url": "https://github.com/GKalliatakis/DisplaceNet", "repo_name": "DisplaceNet", "repo_full_name": "GKalliatakis/DisplaceNet", "repo_owner": "GKalliatakis", "repo_desc": "DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level - CVPR '19 Workshop on Computer Vision for Global Challenges", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T16:09:52Z", "repo_watch": 2, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T14:17:19Z", "homepage": "", "size": 4576, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184593971, "is_fork": false, "readme_text": "         Introduction To reduce the amount of manual labour required for human-rights-related image analysis,  we introduce DisplaceNet, a novel model which infers potential displaced people from images  by integrating the dominance level of the situation and a CNN classifier into one framework.     Grigorios Kalliatakis \u00a0\u00a0\u00a0   Shoaib Ehsan \u00a0\u00a0\u00a0   Maria Fasli \u00a0\u00a0\u00a0   Klaus McDonald-Maier \u00a0\u00a0\u00a0   To appear in 1st CVPR Workshop on  Computer Vision for Global Challenges (CV4GC) \u00a0\u00a0\u00a0   arXiv preprint  \u00a0\u00a0|\u00a0\u00a0 poster coming soon...  Dependencies  Python 2.7+ Keras 2.1.5+ TensorFlow 1.6.0+ HDF5 and h5py (required if you plan on saving/loading Keras models to disk)  Installation Before installing DisplaceNet, please install one of Keras backend engines: TensorFlow, Theano, or CNTK. We recommend the TensorFlow backend - DisplaceNet has not been tested on Theano or CNTK backend engines.  TensorFlow installation instructions. Theano installation instructions. CNTK installation instructions.  More information can be found at the official Keras installation instructions. Then, you can install DisplaceNet itself. There are two ways to install DisplaceNet: Install DisplaceNet from the GitHub source (recommended): $ git clone https://github.com/GKalliatakis/DisplaceNet.git  Alternatively: install DisplaceNet from PyPI (not tested): $ pip install DisplaceNet  Getting started Inference on new data with pretrained models To make a single image inference using DisplaceNet, run the script below. See run_DisplaceNet.py for a list of selectable parameters. $ python run_DisplaceNet.py --img_path test_image.jpg \\                             --hra_model_backend_name VGG16 \\                             --emotic_model_backend_name VGG16 \\                             --nb_of_conv_layers_to_fine_tune 1 Generate predictions on new data: DisplaceNet vs vanilla CNNs Make a single image inference using DisplaceNet and display the results against vanilla CNNs (as shown in the paper). For example to reproduce image below, run the following script. See displacenet_vs_vanilla.py for a list of selectable parameters. $ python displacenet_vs_vanilla.py --img_path test_image.jpg \\                                    --hra_model_backend_name VGG16 \\                                    --emotic_model_backend_name VGG16 \\                                    --nb_of_conv_layers_to_fine_tune 1    Training DisplaceNet's branches from scratch   If you need to, you can train displaced people branch on the HRA subset, by running the training script below. See train_emotic_unified.py for a list of selectable parameters. $ python train_hra_2class_unified.py --pre_trained_model vgg16 \\                                   --nb_of_conv_layers_to_fine_tune 1 \\                                   --nb_of_epochs 50   To train human-centric branch on the EMOTIC subset, run the training script below. See train_emotic_unified.py for a list of selectable parameters. $ python train_emotic_unified.py --body_backbone_CNN VGG16 \\                                  --image_backbone_CNN VGG16_Places365 \\                                  --modelCheckpoint_quantity val_loss \\                                  --earlyStopping_quantity val_loss \\                                  --nb_of_epochs 100 \\ Please note that for training the human-centric branch yourself, the HDF5 file containing the preprocessed images and their respective annotations is required (10.4GB).   Data of DisplaceNet     Human Rights Archive is the core set of the dataset which has been used to train DisplaceNet. The constructed dataset contains 609 images of displaced people and the same number of non displaced people counterparts for training, as well as 100 images collected from the web for testing and validation.  Train images Validation images Test images   Results (click on images to enlarge)         Performance of DisplaceNet The performance of displaced people recognition using DisplaceNet is listed below.  As comparison, we list the performance of various vanilla CNNs trained with various network backbones,  for recognising displaced people. We report comparisons in both accuracy and coverage-the proportion of a data set for which a classifier is able to produce a prediction- metrics     Citing DisplaceNet If you use our code in your research or wish to refer to the baseline results, please use the following BibTeX entry: @article{kalliatakis2019displacenet, title={DisplaceNet: Recognising Displaced People from Images by Exploiting Dominance Level}, author={Kalliatakis, Grigorios and Ehsan, Shoaib and Fasli, Maria and McDonald-Maier, Klaus D}, journal={arXiv preprint arXiv:1905.02025}, year={2019} }     This repo is under development so make sure you have starred it to receive updates. We use GitHub issues to track public bugs. Report a bug by   opening a new issue.  ", "has_readme": true, "readme_language": "English", "repo_tags": ["cnns", "image-interpetation", "computer-vision-and-human-rights", "displaced-populations", "image-analysis", "keras-implementations", "keras-tensorflow", "keras-neural-networks", "convolutional-neural-networks"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://deeplearning.net/software/theano/install.html#install", "http://docs.h5py.org/en/latest/build.html"], "reference_list": ["https://arxiv.org/pdf/1905.02025.pdf", "https://arxiv.org/pdf/1905.02025.pdf"]}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3cb"}, "repo_url": "https://github.com/karl-joan/deepjazz", "repo_name": "deepjazz", "repo_full_name": "karl-joan/deepjazz", "repo_owner": "karl-joan", "repo_desc": "Deep learning driven jazz generation using Keras & Theano!", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-12T12:58:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T14:03:34Z", "homepage": "http://deepjazz.io", "size": 12227, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 184591594, "is_fork": false, "readme_text": "Note: deepjazz has been succeeded by songbird.ai and is no longer being actively developed.   Using Keras & Theano for deep learning driven jazz generation I built deepjazz in 36 hours at a hackathon. It uses Keras & Theano, two deep learning libraries, to generate jazz music. Specifically, it builds a two-layer LSTM, learning from the given MIDI file. It uses deep learning, the AI tech that powers Google's AlphaGo and IBM's Watson, to make music -- something that's considered as deeply human.  Check out deepjazz's music on SoundCloud! Dependencies  Keras Theano (\"bleeding-edge\" version on GitHub) music21  Instructions Run on CPU with command: python generator.py [# of epochs]  Run on GPU with command: THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python generator.py [# of epochs]  Note: running Keras/Theano on GPU is formally supported for only NVIDIA cards (CUDA backend). Note: preprocess.py must be modified to work with other MIDI files (the relevant \"melody\" MIDI part needs to be selected). The ability to handle this natively is a planned feature. Author Ji-Sung Kim Princeton University, Department of Computer Science hello (at) jisungkim.com Citations This project develops a lot of preprocessing code (with permission) from Evan Chow's jazzml. Thank you Evan! Public examples from the Keras documentation were also referenced. Code License, Media Copyright Code is licensed under the Apache License 2.0 Images and other media are copyrighted (Ji-Sung Kim) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://songbird.ai", "http://web.mit.edu/music21/doc/installing/index.html", "http://keras.io/#installation", "http://deeplearning.net/software/theano/install.html#bleeding-edge-install-instructions", "http://deeplearning.net/tutorial/lstm.html"], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3cc"}, "repo_url": "https://github.com/Muj1r1n/Skimming-on-Pyspark-Dask-Scikit-learn-TensorFlow-Keras", "repo_name": "Skimming-on-Pyspark-Dask-Scikit-learn-TensorFlow-Keras", "repo_full_name": "Muj1r1n/Skimming-on-Pyspark-Dask-Scikit-learn-TensorFlow-Keras", "repo_owner": "Muj1r1n", "repo_desc": "Skimming about deep learning, machinge learning and the concept of the pipeline on Pyspark, Scikit-learn, Dask, TensorFlow and Keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T02:13:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T02:03:34Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184507808, "is_fork": false, "readme_text": "Skimming-on-Pyspark-Dask-Scikit-learn-TensorFlow-Keras Skimming about deep learning, machinge learning and the concept of the pipeline on Pyspark, Scikit-learn, Dask, TensorFlow and Keras. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3cd"}, "repo_url": "https://github.com/akeryilmaz/DeepLearningProject", "repo_name": "DeepLearningProject", "repo_full_name": "akeryilmaz/DeepLearningProject", "repo_owner": "akeryilmaz", "repo_desc": "Aalto University CS-E4890 Deep Learning Project: Landmark Classification From Images Using Convolutional Deep Neural Networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-22T10:22:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T12:41:55Z", "homepage": "", "size": 13720, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184578899, "is_fork": false, "readme_text": "DeepLearningProject Aalto University CS-E4890 Deep Learning Project: Landmark Classification From Images Using Convolutional Deep Neural Networks Data available at: https://www.kaggle.com/c/landmark-recognition-2019/data Please Note: Python files under Source/DatasetScripts are highly dependent on directory structure and even Operating System. Keras-VGG16-places365 Directory taken from: https://github.com/GKalliatakis/Keras-VGG16-places365 Source/datasetScripts/downloadResizeDelete.py taken from: https://www.kaggle.com/sermakarevich/download-resize-clean-12-hours-44gb ", "has_readme": true, "readme_language": "English", "repo_tags": ["computervision", "imageclassification", "landmark-recognition", "deeplearning", "convolutional-neural-networks"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3ce"}, "repo_url": "https://github.com/whdlgp/keras_cnn_disparity", "repo_name": "keras_cnn_disparity", "repo_full_name": "whdlgp/keras_cnn_disparity", "repo_owner": "whdlgp", "repo_desc": "A test repository for testing disparity estimation with keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T10:53:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T06:14:38Z", "homepage": null, "size": 7997, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184530463, "is_fork": false, "readme_text": "Disparity estimation with Keras Intro Much research about making depth image with stereo pair has been done. However, the traditional method of using epipolar geometry requires a lot of computation to make accurate disparity maps, and the conventional window matching method has a limitation that it does not work well when the image frequency is low. fig 1. Segmentation with color  fig 2. _Window matching result(Blue: good, Red: bad)  fig 3. Estimated disparity  Recently, there have been many attempts to solve the problem of computer vision using deep learning, and depth map estimation has also been attempted to solve using deep learning. In this simple project, we implement the CNN network using Keras and look for ways to develop it. Disparity map estimation with deep learning in stereo vision  Mendoza Guzm\u00b4an V\u00b4\u0131ctor Manuel1, Mej\u00b4\u0131a Mu\u02dcnoz Jos\u00b4e Manuel1, Moreno M\u00b4arquez Nayeli Edith1, Rodr\u00b4\u0131guez Azar Paula Ivone1, and Santiago Ram\u00b4\u0131rez Everardo1  Universidad Autnoma de Ciudad Jurez @alumnos.uacj.mx http://www.uacj.mx/Paginas/Default.aspx  Plane Implement CNN with disparity estimation  Implement proposed CNN in paper with Keras  learn and test Test set is Driving https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html  Looking for way to improvement I planed to use  Batch normalization Drop out (if need) Modify and change CNN model with reference to other articles and ideas  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/whdlgp/keras_cnn_disparity/blob/50aac618200b4aa7be98b7263e8278f772c80bb2/cnn_mixed/check_point/checkpoint.h5", "https://github.com/whdlgp/keras_cnn_disparity/blob/f221ed781a523c3dce8c233031902cac14ee8045/cnn/check_point/checkpoint.h5", "https://github.com/whdlgp/keras_cnn_disparity/blob/f221ed781a523c3dce8c233031902cac14ee8045/cnn_batchnormalize/check_point/checkpoint.h5", "https://github.com/whdlgp/keras_cnn_disparity/blob/fb7dee446bdcbaaec98a818044571741bf8058a7/cnn_mixed_2/check_point/checkpoint.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3cf"}, "repo_url": "https://github.com/wang20c/capsule", "repo_name": "capsule", "repo_full_name": "wang20c/capsule", "repo_owner": "wang20c", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T23:06:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T22:43:19Z", "homepage": null, "size": 1073, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 184663332, "is_fork": false, "readme_text": "CapsNet-Keras  A Keras implementation of CapsNet in the paper: Sara Sabour, Nicholas Frosst, Geoffrey E Hinton. Dynamic Routing Between Capsules. NIPS 2017 The current average test error = 0.34% and best test error = 0.30%. Differences with the paper:  We use the learning rate decay with decay factor = 0.9 and step = 1 epoch, while the paper did not give the detailed parameters (or they didn't use it?). We only report the test errors after 50 epochs training. In the paper, I suppose they trained for 1250 epochs according to Figure A.1? Sounds crazy, maybe I misunderstood. We use MSE (mean squared error) as the reconstruction loss and the coefficient for the loss is lam_recon=0.0005*784=0.392. This should be equivalent with using SSE (sum squared error) and lam_recon=0.0005 as in the paper.  TODO  Conduct experiments on other datasets. Explore interesting characteristics of CapsuleNet.  Contacts  Your contributions to the repo are always welcome. Open an issue or contact me with E-mail guoxifeng1990@163.com or WeChat wenlong-guo.  Usage Step 1. Install Keras>=2.0.7 with TensorFlow>=1.2 backend. pip install tensorflow-gpu pip install keras  Step 2. Clone this repository to local. git clone https://github.com/XifengGuo/CapsNet-Keras.git capsnet-keras cd capsnet-keras  Step 3. Train a CapsNet on MNIST Training with default settings: python capsulenet.py  More detailed usage run for help: python capsulenet.py -h  Step 4. Test a pre-trained CapsNet model Suppose you have trained a model using the above command, then the trained model will be saved to result/trained_model.h5. Now just launch the following command to get test results. $ python capsulenet.py -t -w result/trained_model.h5  It will output the testing accuracy and show the reconstructed images. The testing data is same as the validation data. It will be easy to test on new data, just change the code as you want. You can also just download a model I trained from https://pan.baidu.com/s/1sldqQo1 or https://drive.google.com/open?id=1A7pRxH7iWzYZekzr-O0nrwqdUUpUpkik Step 5. Train on multi gpus This requires Keras>=2.0.9. After updating Keras: python capsulenet-multi-gpu.py --gpus 2  It will automatically train on multi gpus for 50 epochs and then output the performance on test dataset. But during training, no validation accuracy is reported. Results Test Errors CapsNet classification test error on MNIST. Average and standard deviation results are reported by 3 trials. The results can be reproduced by launching the following commands. python capsulenet.py --routings 1 --lam_recon 0.0    #CapsNet-v1    python capsulenet.py --routings 1 --lam_recon 0.392  #CapsNet-v2 python capsulenet.py --routings 3 --lam_recon 0.0    #CapsNet-v3  python capsulenet.py --routings 3 --lam_recon 0.392  #CapsNet-v4     Method Routing Reconstruction MNIST (%) Paper     Baseline -- -- -- 0.39   CapsNet-v1 1 no 0.39 (0.024) 0.34 (0.032)   CapsNet-v2 1 yes 0.36 (0.009) 0.29 (0.011)   CapsNet-v3 3 no 0.40 (0.016) 0.35 (0.036)   CapsNet-v4 3 yes 0.34 (0.016) 0.25 (0.005)    Losses and accuracies:  Training Speed About 100s / epoch on a single GTX 1070 GPU. About 80s / epoch on a single GTX 1080Ti GPU. About 55s / epoch on two GTX 1080Ti GPU by using capsulenet-multi-gpu.py. Reconstruction result The result of CapsNet-v4 by launching python capsulenet.py -t -w result/trained_model.h5  Digits at top 5 rows are real images from MNIST and digits at bottom are corresponding reconstructed images.  Manipulate latent code python capsulenet.py -t --digit 5 -w result/trained_model.h5   For each digit, the ith row corresponds to the ith dimension of the capsule, and columns from left to right correspond to adding [-0.25, -0.2, -0.15, -0.1, -0.05, 0, 0.05, 0.1, 0.15, 0.2, 0.25] to the value of one dimension of the capsule. As we can see, each dimension has caught some characteristics of a digit. The same dimension of different digit capsules may represent different characteristics. This is because that different digits are reconstructed from different feature vectors (digit capsules). These vectors are mutually independent during reconstruction.           Other Implementations   PyTorch:  XifengGuo/CapsNet-Pytorch timomernick/pytorch-capsule gram-ai/capsule-networks nishnik/CapsNet-PyTorch leftthomas/CapsNet    TensorFlow:  naturomics/CapsNet-Tensorflow I referred to some functions in this repository. InnerPeace-Wu/CapsNet-tensorflow chrislybaer/capsules-tensorflow    MXNet:  AaronLeong/CapsNet_Mxnet    Chainer:  soskek/dynamic_routing_between_capsules    Matlab:  yechengxi/LightCapsNet    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1710.09829"]}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d0"}, "repo_url": "https://github.com/sridharavulapati/RubensZimbres-Repo-2017", "repo_name": "RubensZimbres-Repo-2017", "repo_full_name": "sridharavulapati/RubensZimbres-Repo-2017", "repo_owner": "sridharavulapati", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T11:43:43Z", "repo_watch": 0, "repo_forks": 2, "private": false, "repo_created_at": "2019-05-02T11:41:00Z", "homepage": null, "size": 42326, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184570734, "is_fork": false, "readme_text": "Python Codes in Data Science Codes in NLP, Deep Learning, Reinforcement Learning and Artificial Intelligence  Welcome to my GitHub repo.  I am a Data Scientist and I code in R, Python and Wolfram Mathematica. Here you will find some Machine Learning, Deep Learning, Natural Language Processing and Artificial Intelligence models I developed.  Outputs of the models can be seen at my portfolio:  https://drive.google.com/file/d/0B0RLknmL54khdjRQWVBKeTVxSHM/view?usp=sharing  Keras version used in models: keras==1.1.0  Autoencoder for Audio   is a model where I compressed an audio file and used Autoencoder to reconstruct the audio file, for use in phoneme classification.  Collaborative Filtering   is a Recommender System where the algorithm predicts a movie review based on genre of movie and similarity among people who watched the same movie.  Convolutional NN Lasagne   is a Convolutional Neural Network model in Lasagne to solve the MNIST task.  Ensembled Machine Learning  is a .py file where 7 Machine Learning algorithms are used in a classification task with 3 classes and all possible hyperparameters of each algorithm are adjusted. Iris dataset of scikit-learn.     GAN Generative Adversarial   are models of Generative Adversarial Neural Networks.  Hyperparameter Tuning RL   is a model where hyperparameters of Neural Networks are adjusted via Reinforcement Learning. According to a reward, hyperparameter tuning (environment) is changed through a policy (mechanization of knowledge) using the Boston Dataset. Hyperparameters tuned are: learning rate, epochs, decay, momentum, number of hidden layers and nodes and initial weights.  Keras Regularization L2   is a Neural Network model for regression made with Keras where a L2 regularization was applied to prevent overfitting.  Lasagne Neural Nets Regression   is a Neural Network model based in Theano and Lasagne, that makes a linear regression with a continuous target variable and reaches 99.4% accuracy. It uses the DadosTeseLogit.csv sample file.  Lasagne Neural Nets + Weights   is a Neural Network model based in Theano and Lasagne, where is possible to visualize weights between X1 and X2 to hidden layer. Can also be adapted to visualize weights between hidden layer and output. It uses the DadosTeseLogit.csv sample file.  Multinomial Regression   is a regression model where target variable has 3 classes.  Neural Networks for Regression   shows multiple solutions for a regression problem, solved with sklearn, Keras, Theano and Lasagne. It uses the Boston dataset sample file from sklearn and reaches more than 98% accuracy.     NLP + Naive Bayes Classifier   is a model where movie reviews were labeled as positive and negative and the algorithm then classifies a totally new set of reviews using Logistic Regression, Decision Trees and Naive Bayes, reaching an accuracy of 92%.  NLP Anger Analysis   is a Doc2Vec model associated with Word2Vec model to analyze level of anger using synonyms in consumer complaints of a U.S. retailer in Facebook posts.  NLP Consumer Complaint   is a model where Facebook posts of a U.S. computer retailer were scraped, tokenized, lemmatized and applied Word2Vec. After that, t-SNE and Latent Dirichlet Allocation were developed in order to classify the arguments and weights of each keyword used by a consumer in his complaint. The code also analyzes frequency of words in 100 posts.  NLP Convolutional Neural Network  is a Convolutional Neural Network for Text in order to classify movie reviews.  NLP Doc2Vec   is a Natural Language Procesing file where cosine similarity among phrases is measured through Doc2Vec.  NLP Document Classification   is a code for Document Classification according to Latent Dirichlet Allocation.  NLP Facebook Analysis   analyzes Facebook posts regarding Word Frequency and Topic Modelling using LDA.  NLP Facebook Scrap   is a Python code for scraping data from Facebook.  NLP - Latent Dirichlet Allocation   is a Natural Language Processing model where a Wikipedia page on Statistical Inference is classified regarding topics, using Latent Dirichlet Allocation with Gensim, NLTK, t-SNE and K-Means.  NLP Probabilistic ANN   is a Natural Language Processing model where sentences are vectorized by Gensim and a probabilistic Neural Network model is deveoped using Gensim, for sentiment analysis.  NLP Semantic Doc2Vec + Neural Network   is a model where positive and negative movie reviews were extracted and semantically classified with NLTK and BeautifulSoup, then labeled as positive or negative. Text was then used as an input for the Neural Network model training. After training, new sentences are entered in the Keras Neural Network model and then classified. It uses the zip file.  NLP Sentiment Positive   is a model that identifies website content as positive, neutral or negative using BeautifulSoup and NLTK libraries, plotting the results.  NLP Twitter Analysis ID #   is a model that extracts posts from Twitter based in ID of user or Hashtag.  NLP Twitter Scrap   is a model that scraps Twitter data and shows the cleaned text as output.  NLP Twitter Streaming   is a model of analysis of real-time data from Twitter (under development).  NLP Twitter Streaming Mood   is a model where the evolution of mood Twitter posts is measured during a period of time.  NLP Wikipedia Summarization   is a Python code that summarizes any given page in a few sentences.  NLP Word Frequency   is a model that calculates the frequency of nouns, verbs, words in Facebook posts.  Probabilistic Neural Network   is a Probabilistic Neural Network for Time Series Prediction.  REAL-TIME Twitter Analysis   is a model where Twitter streaming is extracted, words and sentences tokenized, word embeddings were created, topic modeling was made and classified using K-Means. Then, NLTK SentimentAnalyzer was used to classify each sentence of the streaming into positive, neutral or negative. Accumulated sum was used to generate the plot and the code loops each 1 second, collecting new tweets.  RESNET-2   is a Deep Residual Neural Network.  ROC Curve Multiclass   is a .py file where Naive Bayes was used to solve the IRIS Dataset task and ROC curve of different classes are plotted.  SQUEEZENET   is a simplified version of the AlexNet.  Stacked Machine Learning   is a .py notebook where t-SNE, Principal Components Analysis and Factor Analysis were applied to reduce dimensionality of data. Classification performances were measured after applying K-Means.  Support Vector Regression   is a SVM model for non linear regression in an artificial dataset.  Text-to-Speech   is a .py file where Python speaks any given text and saves it as an audio .wav file.  Time Series ARIMA   is a ARIMA model to forecast time series, with an error margin of 0.2%.  Time Series Prediction with Neural Networks - Keras   is a Neural Network model to forecast time series, using Keras with an adaptive learning rate depending upon derivative of loss.     Variational Autoencoder   is a VAE made with Keras.  Web Crawler   is a code that scraps data from different URLs of a hotel website.  t-SNE Dimensionality Reduction   is a t-SNE model for dimensionality reduction which is compared to Principal Components Analysis regarding its discriminatory power.  t-SNE PCA + Neural Networks   is a model that compares performance or Neural Networks made after t-SNE, PCA and K-Means.  t-SNE PCA LDA embeddings  is a model where t-SNE, Principal Components Analysis, Linear Discriminant Analysis and Random Forest embeddings are compared in a task to classify clusters of similar digits.          ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d1"}, "repo_url": "https://github.com/kdhht2334/Keras2.2_flow_from_directory", "repo_name": "Keras2.2_flow_from_directory", "repo_full_name": "kdhht2334/Keras2.2_flow_from_directory", "repo_owner": "kdhht2334", "repo_desc": "Method to extend `flow_from_directory` function of Keras library", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T05:02:16Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T07:43:02Z", "homepage": "", "size": 45, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184540611, "is_fork": false, "readme_text": "Keras (above version 2.2) Custom flow_from_directory    In this repository, we show how to extend the functionality of flow_from_directory function of Keras.  Usage +1. Add your training options to allowed_class_mode of DirectoryIterator class in directory_iterator.py. (Full path is anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras_preprocessing/image/directory_iterator.py)  Like this. class DirectoryIterator(BatchFromFilesMixin, Iterator):  allowed_class_modes = {'categorical', 'binary', 'sparse', 'input', 'input', 'colorize', 'kl_divergence_ILSVRC', 'kl_divergence_96', None}     def __init__(self,                  directory,                  image_data_generator,                  target_size=(256, 256),                  dimension_ILSVRC=512,                  dimension_96=96,                      ...  +2. Add your method to BatchFromFilesMixin class in iterator.py. (Full path is anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras_preprocessing/image/iterator.py) More specifically, you have to change _get_batches_of_transformed_samples functions to add some functionality.  Like this. def _get_batches_of_transformed_samples(self, index_array):                      ...                                 # build batch of labels     if self.class_mode == 'input':         batch_y = batch_x.copy()     elif self.class_mode in {'binary', 'sparse'}:         batch_y = np.empty(len(batch_x), dtype=self.dtype)         for i, n_observation in enumerate(index_array):             batch_y[i] = self.classes[n_observation]     elif self.class_mode == 'categorical':         batch_y = np.zeros((len(batch_x), len(self.class_indices)),                                dtype=self.dtype)         for i, n_observation in enumerate(index_array):               batch_y[i, self.classes[n_observation]] = 1.     elif self.class_mode == 'other':          batch_y = self.data[index_array]               #TODO(): Add new functionality :)     elif self.class_mode == 'colorize':         batch_x, batch_y = image_a_b_gen(batch_x)     elif self.class_mode == 'kl_divergence_96':         batch_y = np.random.normal(size=(len(batch_x), self.dimension_96))     elif self.class_mode == 'kl_divergence_ILSVRC':         batch_y = np.random.normal(size=(len(batch_x), self.dimension_ILSVRC))     else:         return batch_x     return batch_x, batch_y +3. Use flow_from_directory function in your source like this. from keras.preprocessing.image import ImageDataGenerator  # data augmentation train_datagen = ImageDataGenerator(     preprocessing_function=preprocess)  train_generator2 = train_datagen.flow_from_directory(         '%s/train/' % ds,         target_size=(224, 224),         batch_size=batch_size,         class_mode=\"colorize\")  +4. Run ex_cls.py --gpus 0   Milestone    Add our own iterator.py    Add classification task example    Add colorization task example    Add Kullback-Leibler divergence example   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d2"}, "repo_url": "https://github.com/Harphies1/Image-clasifier", "repo_name": "Image-clasifier", "repo_full_name": "Harphies1/Image-clasifier", "repo_owner": "Harphies1", "repo_desc": "A Convolutional Neural Network Model  for Image classifier using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T18:35:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T14:13:47Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184593337, "is_fork": false, "readme_text": "Image-clasifier A Convolutional Neural Network Model  for Image classifier using Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d3"}, "repo_url": "https://github.com/ZaneRammal/KerasQT", "repo_name": "KerasQT", "repo_full_name": "ZaneRammal/KerasQT", "repo_owner": "ZaneRammal", "repo_desc": "An exercise where neural network training with Keras is given a GUI designed in QT.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T17:40:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T17:24:54Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 184622585, "is_fork": false, "readme_text": "KerasQT An exercise where neural network training with Keras is given a GUI designed in QT. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d4"}, "repo_url": "https://github.com/CnBDM-Su/imdb_nlp_model", "repo_name": "imdb_nlp_model", "repo_full_name": "CnBDM-Su/imdb_nlp_model", "repo_owner": "CnBDM-Su", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T02:01:11Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T08:36:55Z", "homepage": null, "size": 18, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184547549, "is_fork": false, "readme_text": "imdb_nlp_model use scrapy from ajax crawlling review of imdb use keras to build and train a rnn model to do the sentiment analysis ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d5"}, "repo_url": "https://github.com/Hescu6/RNNlstmAttention", "repo_name": "RNNlstmAttention", "repo_full_name": "Hescu6/RNNlstmAttention", "repo_owner": "Hescu6", "repo_desc": "Recurrent NN with attention", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T22:57:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T22:49:02Z", "homepage": null, "size": 1524, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184664119, "is_fork": false, "readme_text": "RNNlstmAttention Recurrent NN with attention Encoder-Decoder Recurrent Neural Network with LSTM Attention will be used to make predictions. This model aims to evade sequential processing over time series with Attention as well as to mitigate the vanishing gradient problem seen in long range dependencies by using LSTM and a generic attention module. \u2022 Python o time, numpy, time, and pyplot libraries \u2022 Keras and TensorFlow libraries o AttentionDecoder module (Ahmed, 2017) \u2022 Jupyter Notebook o matplotlib \u2022 CSV Historical Data from Yahoo Finance) o SPY - 26 years \u2013 weekly In this project, Python is used as the programming language with the aid of numpy library to help with calculations and array manipulation, the \u2018matplotlib.pyplot\u2018 library is used to create the charts in jupyter notebook, and the Keras library with Tensorflow backend to summon the neural network. Keras library serves as an API that works with tensorflow to add the model\u2019s hidden layers with LSTM cells. Because at the time of this paper, Keras doesn\u2019t have any module for \u2018Attention\u2019, a custom attention model from a tutorial by Zafarali Ahmed (2017) is used. Steps in program  Load Data into integer list Define batch size input and output for training One-hot encode and reshape list as 3D data Build RNN with LSTM and the Attention module Train LSTM NN Predict and one-hot decode data for output Plot prediction  Parameters in Jupyter Notebook file \u2022 Get Data o Data_1: list obtained from csv file. o Data_2: same list obtained from csv file, modified for testing purposes. o Cardinality: total number of elements in the list. o Maxval_train: highest number between data_1 and data_2 list. \u2022 Set up model parameters o Steps_in: Number of elements in epoch. Also serves as window size o Steps_out: number of elements to be predicted o Real: transforms the data_list 2 by shifting data steps_out spaces to give a more real time series prediction. \u2022 Build model o Model: coder-decoder RNN LSTM with attention model instance \u2022 Train model o Start: Starting point when traversing through the data. End-start = window size o End: stop point for the data set to input into epoch. End-start = window size o Minutes: seconds taken for training converted into minutes o X and y: one hot encoded and 3-D transformed data_1 and data_2 \u2022 Predict o Traverse: number of iterations needed to fully complete the prediction series o Prediction: list where prediction is stored o X and y: one hot encoded and 3-D transformed data_1 and data_2 Functions in RNNattention.py file \u2022 load_data o Input: file name o Output: Data list, its cardinality, and highest number \u2022 encode o Input: list and its highest value element o One hot encodes data o Output: encoded array \u2022 decode o Input: list o One hot decodes data o Output: decoded list \u2022 regulate_window: o Parameters: start, end, window_step, cardinality, window size o During training and predicting, this function regulates the window steps as well as the window size of the data to be fed in the model o  Outputs the range that should be fed into the model as \u2018start\u2019 and \u2018end\u2019 \u2022 transform_data: o Parameters: steps_in, steps_out, max_val, data_1, and data_2 o Calls encode function and reshapes list into 3D o Output: encoded x and y \u2022 plot_it: o Input: predicted data and true data o Output: plotted data in graph ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d6"}, "repo_url": "https://github.com/iwatake2222/JetsonNanoTest", "repo_name": "JetsonNanoTest", "repo_full_name": "iwatake2222/JetsonNanoTest", "repo_owner": "iwatake2222", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T02:20:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T14:32:39Z", "homepage": null, "size": 21261, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184596418, "is_fork": false, "readme_text": "Test code for Jetson Nano Speed Test Inference of Mobilenet(224x224, retrieved using mmdownload)  Python, Tensorflow.keras (CPU): 162.3msec Python, Tensorflow.keras (GPU): 61.8msec C++, ncnn (CPU): 48.1msec C++, ncnn (GPU): 44.8msec  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d7"}, "repo_url": "https://github.com/dronefreak/dji-tello-object-detection-segmentation", "repo_name": "dji-tello-object-detection-segmentation", "repo_full_name": "dronefreak/dji-tello-object-detection-segmentation", "repo_owner": "dronefreak", "repo_desc": "This Git repo allows to implement the state-of-the-art MaskRCNN algorithm for instance segmentation on the video feed from DJI-Tello drone.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T12:36:15Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T13:41:31Z", "homepage": null, "size": 926, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184588049, "is_fork": false, "readme_text": "Mask-RCNN Implement Real-Time Semantic Segmentation with Mask_RCNN. Requirements  Ubuntu 18.04 Python 3.6 Tensorflow 1.9 Keras 2.1.6 OpenCV 4.0  The algorithm has been tested on the above mentioned configuration, but I'm pretty sure that other combinations would also work effectively. But please make sure that you have TF/Keras combination as mentioned above. Opencv 3.4 would suffice. This implementation would work better if you have a GPU in your system and use CUDA-accelerated learning. In a MSI laptop with 1050Ti (4 GB with 768 cuda cores), i5-8220 and 8GB RAM, the FPS obtained is 4.637. Also, in order to test it on a Tello, make sure that you have the drone turned on and connected to its WiFi network. Once you esecute this code, press TAB to take-off and BACKSPACE to land. Other manual navigation commands are given in the header of the python code. Getting Started Install Dependencies (Not Mentioned Above) $ sudo -H pip3 install -r requirements.txt $ sudo -H pip3 install git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI Using pre-trained weights for MS COCO It is included in {telloCV-masked-rcnn.py} that downloading the pre-trained weights for MS COCO. Run Demonstration $ python3 telloCV-masked-rcnn.py Alternatively, you could download the weights file from here.    ", "has_readme": true, "readme_language": "English", "repo_tags": ["dji-tello", "computer-vision", "image-segmentation", "mask-rcnn"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d8"}, "repo_url": "https://github.com/ltemihai/AI_Neural_Network_MNIST", "repo_name": "AI_Neural_Network_MNIST", "repo_full_name": "ltemihai/AI_Neural_Network_MNIST", "repo_owner": "ltemihai", "repo_desc": "Neural Network for MNIST dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T17:02:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T14:35:48Z", "homepage": "https://github.com/ltemihai/AI_Neural_Network_MNIST", "size": 2168, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184596896, "is_fork": false, "readme_text": "AI_Neural_Network_MNIST Neural Network for MNIST dataset Here you can find the dataset https://we.tl/t-H8c2KjvU2O To run, make sure you copy the dataset into the root of the project Scratch network  Network made with keras  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3d9"}, "repo_url": "https://github.com/laugh12321/GeneratePoetry-Translate", "repo_name": "GeneratePoetry-Translate", "repo_full_name": "laugh12321/GeneratePoetry-Translate", "repo_owner": "laugh12321", "repo_desc": "use RNN and LSTM generate poetry by Keras, and translate poetry. (\u7528Keras\u5b9e\u73b0RNN+LSTM\u7684\u6a21\u578b\u81ea\u52a8\u7f16\u5199\u53e4\u8bd7, \u5e76\u5c06\u751f\u6210\u7684\u53e4\u8bd7\u7ffb\u8bd1\u6210\u82f1\u6587.)", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-21T07:08:09Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T08:03:11Z", "homepage": "https://github.com/laugh12321/GeneratePoetry_Keras", "size": 5475, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184543170, "is_fork": false, "readme_text": "Generate Poetry & Translate use RNN and LSTM generate poetry by Keras, and translate poetry. \u7528Keras\u5b9e\u73b0RNN+LSTM\u7684\u6a21\u578b\u81ea\u52a8\u7f16\u5199\u53e4\u8bd7, \u5e76\u5c06\u751f\u6210\u7684\u53e4\u8bd7\u7ffb\u8bd1\u6210\u82f1\u6587. Note:  main.py \u4e2d\u5df2\u7ecf\u7701\u7565\u7684\u6a21\u578b\u7684\u6784\u5efa\u8fc7\u7a0b\uff0c\u91c7\u7528\u7684\u662f\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684\u6a21\u578b data/model_1000.h5\uff08\u8fed\u4ee3\u4e861000\u6b21\uff09, data/poetry.txt \u662f\u751f\u6210\u8bd7\u8bcd\u6240\u5fc5\u987b\u7684\u8bed\u6599\u6587\u4ef6. Usage git clone https://github.com/laugh12321/GeneratePoetry-Translate.git  python main.py --word \u98ce\u706b\u5c71\u6797 --num 7  \u98ce\u96e8\u6d1e\u5e7d\u9669\u9a7f\u96e8 \u706b\u7ef4\u5730\u4e0a\u9752\u5c71\u4f55 \u5c71\u767d\u4e91\u96e8\u4e2d\u519b\u53f8 \u6797\u96e8\u5915\u6bbf\u91cc\u96c1\u53c8 Wind and rain Tunnel, dangerous post-rain, fire on the ground where the green hills. Military commander in mountain white clouds and rain, wild goose in forest rain evening place.  Note:  you must run it with GPU!!! ", "has_readme": true, "readme_language": "English", "repo_tags": ["rnn-lstm", "keras"], "has_h5": true, "h5_files_links": ["https://github.com/laugh12321/GeneratePoetry-Translate/blob/cd9cf7a24974a98fc3c4cbc0bf56f639a61adcb1/data/model_1000.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3da"}, "repo_url": "https://github.com/ashwinvk94/reinforcement_learning_racing_quad", "repo_name": "reinforcement_learning_racing_quad", "repo_full_name": "ashwinvk94/reinforcement_learning_racing_quad", "repo_owner": "ashwinvk94", "repo_desc": "This repository is an implmentation of a reinforcement algorithm which can teach itself to pass throuhg gates. The implmentation uses python, keras and the flightgoggles simulation environment.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-14T00:13:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T15:41:38Z", "homepage": null, "size": 6963, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184607912, "is_fork": false, "readme_text": "reinforcement-_learning_racing_quad This repository is an implmentation of a reinforcement algorithm which can teach itself to pass throuhg gates. The implmentation uses python, keras and the flightgoggles simulation environment. Dependencies In order to run this project you will have to install the follwing respositories  ROS Kinetic Keras FlightGoggles  Instructions Training and testing the supervised learning neural network Note: The training data is stored as rosbag files in the training data folder directory  Run the 'extract_rosbag.py' file to extract the rosbag information and store it as a pickle file Run 'train_nn.py' file to train the supervised neural neural network. This will save the trained model to oa file. Start the flightgoggles simulator by running the 'start_nn.sh' file Test the supervised learning model bu running the 'supervised_rate_thrust_publisher.py' file.  Training and testing the supervised learning neural network  Run the 'dqn.py' file to iteratively train the supervised model using reinforcement learning Start the flightgoggles simulator by running the 'start_nn.sh' file Test the supervised learning model bu running the 'reinforcement_learning_rate_thrust_publisher.py' file.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ashwinvk94/reinforcement_learning_racing_quad/blob/e24b75126c8adb4e957d7b8bbed7ee7083ad21ca/scripts/my_model.h5", "https://github.com/ashwinvk94/reinforcement_learning_racing_quad/blob/e24b75126c8adb4e957d7b8bbed7ee7083ad21ca/scripts/working_model_roll_pitch_thrust.h5"], "see_also_links": ["http://wiki.ros.org/kinetic/Installation"], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3db"}, "repo_url": "https://github.com/Beyond-Data-llc/Deep-Learning", "repo_name": "Deep-Learning", "repo_full_name": "Beyond-Data-llc/Deep-Learning", "repo_owner": "Beyond-Data-llc", "repo_desc": "A repository for Beyond Data's deep learning projects.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T23:07:02Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T11:04:05Z", "homepage": "", "size": 36, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184566170, "is_fork": false, "readme_text": "The \"Deep-Learning\" repository, is a repository composed for Beyond Data's deep leaning projects and other AI related projects that are open sourced. The projects (including future projects) dependencies, so far are using but are not only limited to; TensorFlow, Keras, Caffe, Intel's OpenVino Toolkit, & PyTorch. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3dc"}, "repo_url": "https://github.com/shivakrishna2497/Convolutional-Neural-Network-to-classify-Handwritten-Digits", "repo_name": "Convolutional-Neural-Network-to-classify-Handwritten-Digits", "repo_full_name": "shivakrishna2497/Convolutional-Neural-Network-to-classify-Handwritten-Digits", "repo_owner": "shivakrishna2497", "repo_desc": "Convolutional Neural Network to classify hand written digits with Tensorflow\u2019s Keras API and achieved an accuracy over 98% ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T13:18:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T13:12:31Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184583554, "is_fork": false, "readme_text": "Convolutional Neural Network to classify hand written digits Convolutional Neural Network to classify hand written digits with Tensorflow\u2019s Keras API and achieved an accuracy over 98% Steps Involved : 1)Downloading the Mnist Data 2)Reshaping and Normalizing the Images 3)Building the Convolutional Neural Network 4)Compiling and Fitting the Model 5)Evaluating the Model ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3dd"}, "repo_url": "https://github.com/tianxiang84/new-vision-project", "repo_name": "new-vision-project", "repo_full_name": "tianxiang84/new-vision-project", "repo_owner": "tianxiang84", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T02:03:05Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T01:59:16Z", "homepage": null, "size": 14907, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184507414, "is_fork": false, "readme_text": "Training python train_emotion_classifier.py  Testing python run_classifier.py test.jpg  Things to try: Look at the data!  How is it distributed?  How well would a random model do? Can the model learn on a tiny subset of the data?  Better models  Better architecture  Data cleanup  Normalize  Data generation  https://keras.io/preprocessing/image/  Reduce learning rate on plateau  https://keras.io/callbacks/#reducelronplateau  Find more training data online? Anything else? ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/emotion.h5", "https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/wandb/run-20190501_205350-ob21xpxe/model-best.h5", "https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/wandb/run-20190501_205725-psqn4cxy/model-best.h5", "https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/wandb/run-20190501_211133-tk7tbul3/model-best.h5", "https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/wandb/run-20190501_211526-tfzx257h/model-best.h5", "https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/wandb/run-20190501_211925-h2ff0l20/model-best.h5", "https://github.com/tianxiang84/new-vision-project/blob/1a599677dbe9cb8172e834e6be2059b2dec5f3ec/wandb/run-20190501_212238-2jyf0w0w/model-best.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3de"}, "repo_url": "https://github.com/w5688414/keras-CORe50-benchmark", "repo_name": "keras-CORe50-benchmark", "repo_full_name": "w5688414/keras-CORe50-benchmark", "repo_owner": "w5688414", "repo_desc": "this is the reimplementation of A new Dataset and Benchmark for Continuous Object Recognition", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T09:26:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T09:08:26Z", "homepage": null, "size": 52, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184551817, "is_fork": false, "readme_text": "model  vgg16 alexnet  reuqirements  keras 2.1.4 ubuntu 16.04 tensorflow-gpu 1.4 python3  train   download the dataset   download caffe source code   git clone https://github.com/vlomonaco/core50   train   NI alexnet  python train_alexnet_NI.py -dp your_data_path   NI vgg16  python train_vgg16_NI.py -dp your_data_path   NC vgg16  python train_vgg16_NC.py -dp your_data_path   NC alexnet  python train_alexnet_NC.py -dp your_data_path   NIC alexnet  python train_alexnet_NIC.py -dp your_data_path   NIC vgg16  python train_vgg16_NIC.py -dp your_data_path  reference https://github.com/chenlongzhen/vgg_finetune ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3df"}, "repo_url": "https://github.com/robert1ridley/co-attention", "repo_name": "co-attention", "repo_full_name": "robert1ridley/co-attention", "repo_owner": "robert1ridley", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-20T03:52:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T08:54:17Z", "homepage": null, "size": 60940, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 184549973, "is_fork": false, "readme_text": "co-attention Code for BEA 13 paper \"Co-Attention Based Neural Network for Source-Dependent Essay Scoring\" Dependencies python 2 for data/preprocess_asap.py python 3 for the rest  keras 2.2.4 tensorflow 1.12.0  Cite If you use the code, please cite the following paper: @inproceedings{zhang2018co,   title={Co-Attention Based Neural Network for Source-Dependent Essay Scoring},   author={Zhang, Haoran and Litman, Diane},   booktitle={Proceedings of the Thirteenth Workshop on Innovative Use of NLP for Building Educational Applications},   pages={399--409},   year={2018} }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e0"}, "repo_url": "https://github.com/jragustin/SP-SEEN", "repo_name": "SP-SEEN", "repo_full_name": "jragustin/SP-SEEN", "repo_owner": "jragustin", "repo_desc": "SEEN: Comparative study of Steganalysis using CNN, StegDetect and Zhang Ping Steganalysis", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-25T13:56:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T21:18:20Z", "homepage": null, "size": 625465, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184652880, "is_fork": false, "readme_text": "SP-SEEN SEEN: Comparative study of Steganalysis using CNN, StegDetect and Zhang Ping Steganalysis SET-UP Download the repository git clone https://github.com/jragustin/SP-SEEN.git cd SP-SEEN  you can also download the already trained models using this link and save it to SP-SEEN/models folder https://drive.google.com/file/d/1NdvZAGCGPYcH9-1qvBXvdkGSV4oZ7uEK/view?usp=sharing Download all the required python modules using the ff commands: sudo apt install python3-pip pip3 install keras numpy tensorflow pillow tkinter matplotlib  Run and Train the Neural Network python3 network.py  you might wait for hours in order to finish the training. Run the classifier using this command python3 ui.py  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e1"}, "repo_url": "https://github.com/ashujack/Digit_recognizer", "repo_name": "Digit_recognizer", "repo_full_name": "ashujack/Digit_recognizer", "repo_owner": "ashujack", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T11:29:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T10:13:25Z", "homepage": null, "size": 4356, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184559890, "is_fork": false, "readme_text": "Hand-Written Digit Recognizer Digit recognition on hand-written digits using Convolutional Neural Network with Keras library. Achieved an accuracy of 99.50%. Setup :- Python --> Python 3.6.5 OS --> Ubuntu 19.04 / Windows 10 GPU --> Nvidia Geforce 940MX (2GB) Intel Core i5-7200 @ 2.50GHz RAM --> 4gb Using mnist_train2.py :- Model can be trained by running mnist_train2.py python mnist_train2.py Trains the model to predict hand-written digits from their images. Saves the weights in model.h5 file. Saves the model in model.json file. Using mnist_test2.py :- Model can be tested by running mnist_test2.py python mnist_test2.py Predicts hand-written digits from their images. Loads the weights from model.h5 file. Loads the model from model.json file. Displays a random image of a hand-written digit on screen. Predicts the digit and prints it on terminal. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ashujack/Digit_recognizer/blob/27d33c8fc2cbd31c3abc61a3abcf819b5e211d92/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e2"}, "repo_url": "https://github.com/marisilva1/FCC_flowers", "repo_name": "FCC_flowers", "repo_full_name": "marisilva1/FCC_flowers", "repo_owner": "marisilva1", "repo_desc": "Final project for 1-credit python course", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T09:35:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T21:21:59Z", "homepage": null, "size": 536735, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184653279, "is_fork": false, "readme_text": "FCC_flowers Final project for 1-credit python course. DOCUMENTATION My ultimate objective for this code (which still failed) was to read in large iage datasets of both daisies and dandelions, have the user input an image of the flower they wish to identify, and have the computer recognize if the plant is LESS likely to be a weed (arbitrary probability of below 75%) and should not be sprayed with weedkiller. I played with using docker to install and operate tensorflow locally, and got PRETTY DARN CLOSE (messy thought processes and other files included in \"valiant efforts.\" In the end, I used Jupyter (which I learned how to use with terminal through this FCC! Very handy!) to grab tensorflow and keras and build my own Convolutional Neural Net rather than using one of tensorflow's (at first I was hoping to use Inception). I fought with the SETUP for a lot of this for much longer than 20 hours (all somewhat last-minute too, admittedly; patly my own doing and partly a brutual dance schedule), and pretty significant syntax issues relating to the TRAINING STEP and PREDICTION() have persisted as of 2am the morning the FCC was due... NEVERTHELESS, this entire experience has been extremely educational for me - first, to understand that my proposal was overwhelmingly ambitious because of the sheer fact that I didn't understand what a neural network even was, much less how much effort it takes for amateur coders just to differentiate between two types of flowers, when google does this every millisecond with millions upon  millions of images (maybe billions?). Not to mention, neural networks can be applied to other formats than just images. This is a bit too abstract for me, and I appreciate knowing that my skill in python extends far enough that I can process logic easily and execute mathematical, graphical, and data-related funcitonalities. I will leave the computer architecture and machine learning to specialists (who hopefully I will collaborate with in the future!). In which case, my more refined understanding of tensorflow NOW will benefit me in the long run, no matter how frustrating and stressful just some SYNTAX ended up being. How to run the code (such as it is) You may need to install/upgrade  Tensorflow Keras image (pillow)  tf_files includes the images that I have used. I believe if you just download the script CNN_DaisyORWeed.py and run, you will be able to see how far I have gotten. I have also included the .ipynb file, if you have jupyter installed. Feel free to use the one image, C_tinctoria_fake_daisy.jpg, as a test image if you wish (this would work better if my code actually worked). Feel free to peruse the 'valiant effort,' and I apologize for not being able to be proactive enough to work around like-avoidable issues with Setup (unless my 12-inch MacBook is truly not cut out for this stuff, though I doubt that's the issue). Thanks for a great semester; I must stress despite my progress on this project that I have learned a lot about python, and expect for this to be a great advantage for me when it comes to doing environmental research in the future! -- Mariana Silva ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e3"}, "repo_url": "https://github.com/Derrick-Wan/CatClassifier", "repo_name": "CatClassifier", "repo_full_name": "Derrick-Wan/CatClassifier", "repo_owner": "Derrick-Wan", "repo_desc": "It's a classifier that distinguish different kinds of cats.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-18T06:50:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T03:06:23Z", "homepage": null, "size": 52, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184513730, "is_fork": false, "readme_text": "CatClassifier \u8fd9\u4e2a\u5c0f\u9879\u76ee\u4f7f\u7528Keras\u6765\u505a\u732b\u54aa\u7684\u79cd\u7c7b\u5206\u7c7b\uff0c\u540e\u53f0\u7528\u7684Tensorflow\u3002\u611f\u89c9\u732b\u54aa\u5206\u7c7b\u8fd8\u662f\u4e00\u4e2a\u5f88\u7b80\u5355\uff0c\u5f88\u9002\u5408\u6df1\u5ea6\u5b66\u4e60\u65b0\u624b\u62ff\u6765\u7ec3\u624b\u7684\u9879\u76ee\u3002 \u601d\u8def\u5c31\u662f\u4f7f\u7528\u79fb\u690d\u5b66\u4e60\uff0c\u5148\u5c06 VGG16 \u7528\u4f5c\u5377\u79ef\uff0c\u6ce8\u610f\uff1a\u6b64\u65f6\u7684\u5377\u79ef\u5c42\u9700\u8981\u628a\u4ed6\u51b0\u51bb\u8d77\u6765\uff01\u5728Flatten\u4ee5\u540e\uff0c\u5728\u653e\u5165\u5168\u8fde\u63a5\u5c42\u4e2d\u3002\u5728\u8fdb\u884c\u4e86\u4e00\u6bb5\u65f6\u95f4\u7684\u5b66\u4e60\u540e\uff0c\u81ea\u5df1\u521b\u5efa\u7684Dense\u5c42\u57fa\u672c\u4f18\u5316\u7684\u8f83\u597d\u3002\u7136\u540e\u5bf9 VGG16 \u7684\u7b2c\u4e94\u5757\u5377\u79ef\u5c42\u8fdb\u884c\u201c\u89e3\u51bb\u201d\uff0c\u5bf9\u89e3\u51bb\u7684\u7b2c\u4e94\u5757\u5377\u79ef\u5c42\u8fdb\u884c\u5fae\u8c03\uff08\u611f\u89c9\u8fd9\u4e2a\u5fae\u8c03\u5bf9\u6a21\u578b\u7684\u4f18\u5316\u7a0b\u5ea6\u8fd8\u633a\u5927\u7684\uff09\u3002\u6700\u540e\uff0c\u53ef\u4ee5\u8fdb\u884c\u901a\u8fc7test\u6587\u4ef6\u6765\u5bf9\u81ea\u5df1\u7684\u6a21\u578b\u8fdb\u884c\u68c0\u6d4b\u3002So easy\uff01 \u6211\u7684\u82f1\u6587\u6c34\u5e73\u592a\u5dee\u4e86\uff0c\u61d2\u5f97\u7528\u9f99\u9e23\u82f1\u8bed\u5199\u4e86\uff0c\u5c31\u7528\u4e2d\u6587\u5199\u597d\u4e86\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e4"}, "repo_url": "https://github.com/Nordsvich/iphoneOrNot", "repo_name": "iphoneOrNot", "repo_full_name": "Nordsvich/iphoneOrNot", "repo_owner": "Nordsvich", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-04T16:42:08Z", "repo_watch": 0, "repo_forks": 2, "private": false, "repo_created_at": "2019-05-02T14:33:35Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 1, "github_id": 184596542, "is_fork": false, "readme_text": "iphoneOrNot You should to create model to detect iphone (all versions) on a picture. If picture contains two or more iphones you should return only one probability for all picture. Picture is a typical for internet shop. Solution should contains all ML stages (you can skip collect data stage) and pretrained model in git or link to another data storage. Also you should to provide example running inference. Data: Collect data is a part of task. Restrictions:  inference work on CPU neural net frameworks - keras, pytorch python3 work without internet  Performance measure: Area under precision and recall on hidden data. Who will commit solution which better then random and don't copy  pasted get 20 scores. Another scores will depend on you rating based on hidden data. Hard deadline: June 1st Interface to detection python predict.py --model path_to_model --input path_to_input_data --output path_to_results input data - folder with images output data - csv file with two columns: image_name,iphone_probability ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e5"}, "repo_url": "https://github.com/srothst1/CNN-Search-For-Art", "repo_name": "CNN-Search-For-Art", "repo_full_name": "srothst1/CNN-Search-For-Art", "repo_owner": "srothst1", "repo_desc": "CNN-Search-For-Art", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T21:54:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T02:44:55Z", "homepage": "", "size": 946, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184511752, "is_fork": false, "readme_text": "CNN-Search-For-Art Dataset Overview: \"The dataset covers ten of the categories present in PASCAL VOC, and is split into training, validation, and test sets. Objects have a variety of sizes, poses and depictive styles, and can be partially occluded or truncated. The paintings have been obtained from Your Paintings. The annotations have been provided by the public as part of the Tagger Project as well as having been extracted from the painting titles.\" -Department of Engineering Science, University of Oxford In this project, we use convolutional networks to find features in paintings. Running our code:   Ensure that you have the following packages installed: keras, pillow, xlrd, numpy, requests, and BitesIO   Enter python Painting_ConvNet.py into the terminal.   Once the images are compiled, you will be asked if you would like to test a few data points.  Enter 0 to test data points.  Enter anything else to omit testing.   TensorFlow will begin running.  Please note it may take more than 4 hours to complete all epochs. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e6"}, "repo_url": "https://github.com/Linan2018/LSTM_myopia_prediction", "repo_name": "LSTM_myopia_prediction", "repo_full_name": "Linan2018/LSTM_myopia_prediction", "repo_owner": "Linan2018", "repo_desc": "Myopia prediction based on LSTM myopia prediction (2019 XDU Mathematical Modeling Competition)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T08:01:38Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T12:10:56Z", "homepage": null, "size": 55, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184574530, "is_fork": false, "readme_text": "LSTM_myopia_prediction Myopia prediction model based on LSTM (2019 XDU Mathematical Modeling Competition) \u57fa\u4e8eLSTM\u7684\u8fd1\u89c6\u9884\u6d4b\u6a21\u578b\uff082019\u5e74XDU\u6570\u5b66\u5efa\u6a21\u5927\u8d5b\uff09 \u4f9d\u8d56\u5e93 numpy  pandas  matplotlib  sklearn  tensorflow  keras  \u4f7f\u7528\u65b9\u6cd5 \uff08\u9700\u8981\u5b89\u88c5Git\uff0c\u4ee5Windows\u7cfb\u7edf\u4e3a\u4f8b\uff09 1.\u6253\u5f00\u7ec8\u7aef\u5e76\u521b\u5efa\u76ee\u5f55 \u4ee5Windows\u7cfb\u7edf\u4e3a\u4f8b\uff0c\u5728D\u76d8\u521b\u5efa\u6587\u4ef6\u5939\u3002 d: & md mygit & cd mygit  2.\u5c06\u9879\u76ee\u590d\u5236\u5230\u672c\u5730 git init & git clone https://github.com/Linan2018/LSTM_myopia_prediction.git  3.\u6267\u884c \u6570\u636e\u4ee5\u9879\u76ee\u4e2d\u7684dt.csv\u4e3a\u4f8b\uff0c\u5728\u76f8\u5e94\u4f4d\u7f6e\u53ef\u66ff\u6362\u4e3a\u5176\u4ed6\u8def\u5f84\u3002 cd LSTM_myopia_prediction  python main.py ./dt.csv  \u6ce8\u610f dt.csv\u4e3a\u968f\u673a\u751f\u6210\u7684\u6570\u636e\uff0c\u8bad\u7ec3\u7ed3\u679c\u6ca1\u6709\u53c2\u8003\u610f\u4e49\u3002 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e7"}, "repo_url": "https://github.com/sabirdvd/Bert_with_keras_custom_layer", "repo_name": "Bert_with_keras_custom_layer", "repo_full_name": "sabirdvd/Bert_with_keras_custom_layer", "repo_owner": "sabirdvd", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T09:21:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T08:58:11Z", "homepage": null, "size": 39, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184550472, "is_fork": false, "readme_text": "BertLayer with tf.keras for fine tuning CPU/GPU  Tensorflow 1.13.1 For binary classification task  Model  Tested on CPU 25000/25000 [==============================] - 68819s 3s/sample - loss: 0.3261 - acc: 0.8567 - val_loss: 0.2369 - val_acc: 0.9032 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e8"}, "repo_url": "https://github.com/nagrjungururaj/Prediction-of-breast-cancer-using-the-Winconsin-dataset", "repo_name": "Prediction-of-breast-cancer-using-the-Winconsin-dataset", "repo_full_name": "nagrjungururaj/Prediction-of-breast-cancer-using-the-Winconsin-dataset", "repo_owner": "nagrjungururaj", "repo_desc": "Analysis and prediction of breast cancer in patients ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T16:06:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T12:24:50Z", "homepage": null, "size": 383, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184576448, "is_fork": false, "readme_text": "Prediction-of-breast-cancer-using-the-Winconsin-dataset Analysis and prediction of type of tumour for breast cancer in patients Problem Statement : To classify the type of tumour as 'Malign' or 'Benign' for the breast cancer dataset (https://www.kaggle.com/uciml/breast-cancer-wisconsin-data). For more information on the dataset, please look at this link : https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic) Approach : There are 4 parts of solutions,  Data Analyis : Histogram, Correlation matrix of features, mean and variance are calculated Data cleaning : Find and eliminate any missing values, NaNs, Null or empty values and columns in the dataset Feature selection : Solution uses a decision trees from sklearn to rank features and drop the least 50% important features. Model for classification : The solution suggests a 5 layered feed-forward neural network for classifying the type of tumour.  Model is trained and tested for 2 cases:  Input : Cleaned data without feature selection ; Output : a single number denoting probability of 2 classes Input : Cleaned data with feature selection ; Output : a single number denoting probability of 2 classes  Results : 1st case : With feature selected data as input to FFNN  Test accuracy: 0.8571428543045407 F1-score: 0.8983050847457628  2nd case: Without feature selected data as input to FFNN  Test accuracy: 0.8809523809523809 F1-score: 0.9152542372881356  Libraries and Dependencies : numpy, sklearn, keras(with tensorflow backend), seaborn, matplotlib, pandas How to :  Download the zipped folder Extract the zipped folder to the local system Run predict_cancer.py to train the dataset with 120 epochs and test on the fly. 'trained_models' consists the trained models for both cases for 120 epochs  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3e9"}, "repo_url": "https://github.com/jflatby/AI-Machine-Learning", "repo_name": "AI-Machine-Learning", "repo_full_name": "jflatby/AI-Machine-Learning", "repo_owner": "jflatby", "repo_desc": "Using a neural network to have an AI teach itself to play a simple game using unsupervised reinforcement learning", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-02T11:50:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T10:46:15Z", "homepage": "", "size": 156, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184563901, "is_fork": false, "readme_text": "AI-Machine-Learning Using a neural network to have an AI teach itself to play a simple game using unsupervised reinforcement learning in python3.    Untrained vs. trained AI  The game is made using pygame, and the neural network is set up using keras which uses a tensorflow backend, all of which need to be installed for this program to work. The neural network consists of a single input node in the input layer, one hidden layer with 120 neurons, and an output layer with two nodes. The input is solely defined by one boolean which is True/False depending on whether the position of the food is to the right or left of the current direction vector. In other words whether the angle between those two vectors is larger or smaller than 180 degrees. Positive reward is given when food is collected, negative reward is given on death and otherwise no reward is given. In the first 30 games, a decreasing amount of randomness is applied to the decision-making process for the AI, to make sure it keeps trying new things in the beginning. Given this \"simple\" setup of one input and two output nodes, the AI learns the right way to play the game pretty quickly, depending on how long it takes to randomly get the first couple points. The negative reward on death makes it so that it usually figures out in the first 2-4 games that turning a lot lets it survive longer. After 10-20 games it usually gets the idea, and starts consistently turning towards the food. At game 30 and onwards the randomness is taken out of the process and it acts solely on its own prediction. However the game is intentionally designed so that the player will always die even if the AI constantly turns the correct way. The turning speed is set pretty low compared to the movement speed, which makes it so that the turning radius is pretty big. So when the food spawns close to the wall the player is likely to die after collecting it. If the turning speed is set really high the AI would probably be able to play the game forever. An idea moving forward is to add collision detection to the front-right and front-left of the player, giving the neural network two more booleans as input. Hopefully the AI would be able to learn to use those booleans to avoid the wall, and maybe in the end learn to pick up food that is close to the wall without dying. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3ea"}, "repo_url": "https://github.com/aryanbdps9/aml_music_generation", "repo_name": "aml_music_generation", "repo_full_name": "aryanbdps9/aml_music_generation", "repo_owner": "aryanbdps9", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-03T21:36:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T11:01:36Z", "homepage": null, "size": 9798, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184565842, "is_fork": false, "readme_text": "DeepBach This repository contains implementations of the DeepBach model described in DeepBach: a Steerable Model for Bach chorales generation Ga\u00ebtan Hadjeres, Fran\u00e7ois Pachet, Frank Nielsen ICML 2017 arXiv:1612.01010 The code uses python 3.6 together with PyTorch v1.0 and music21 libraries. For the original Keras version, please checkout the original_keras branch. Examples of music generated by DeepBach are available on this website Installation You can clone this repository, install dependencies using Anaconda and download a pretrained model together with a dataset with the following commands: git clone git@github.com:SonyCSL-Paris/DeepBach.git cd DeepBach conda env create --name deepbach_pytorch -f environment.yml bash dl_dataset_and_models.sh  This will create a conda env named deepbach_pytorch. music21 editor You might need to Open a four-part chorale. Press enter on the server address, a list of computed models should appear. Select and (re)load a model. Configure properly the music editor called by music21. On Ubuntu you can eg. use MuseScore: sudo apt install musescore python -c 'import music21; music21.environment.set(\"musicxmlPath\", \"/usr/bin/musescore\")' For usage on a headless server (no X server), just set it to a dummy command: python -c 'import music21; music21.environment.set(\"musicxmlPath\", \"/bin/true\")' Usage Usage: deepBach.py [OPTIONS]  Options:   --note_embedding_dim INTEGER    size of the note embeddings   --meta_embedding_dim INTEGER    size of the metadata embeddings   --num_layers INTEGER            number of layers of the LSTMs   --lstm_hidden_size INTEGER      hidden size of the LSTMs   --dropout_lstm FLOAT            amount of dropout between LSTM layers   --linear_hidden_size INTEGER    hidden size of the Linear layers   --batch_size INTEGER            training batch size   --num_epochs INTEGER            number of training epochs   --train                         train or retrain the specified model   --num_iterations INTEGER        number of parallel pseudo-Gibbs sampling                                   iterations   --sequence_length_ticks INTEGER                                   length of the generated chorale (in ticks)   --help                          Show this message and exit.  Examples You can generate a four-bar chorale with the pretrained model and display it in MuseScore  by simply running python deepBach.py  You can train a new model from scratch by adding the --train flag. Usage with NONOTO The command python flask_server.py  starts a Flask server listening on port 5000. You can then use NONOTO to compose with DeepBach in an interactive way. This server can also been started using Docker with: docker run -p 5000:5000 -it --rm ghadjeres/deepbach  (CPU version), with or docker run --runtime=nvidia -p 5000:5000 -it --rm ghadjeres/deepbach  (GPU version, requires nvidia-docker. Usage within MuseScore Deprecated This only works with MuseScore2. Put deepBachMuseScore.qml file in your MuseScore2/Plugins directory, and run python musescore_flask_server.py  Open MuseScore and activate deepBachMuseScore plugin using the Plugin manager. You can then click on the Compose button without any selection to create a new chorale from scratch. You can then select a region in the chorale score and click on the Compose button to regenerated this region using DeepBach. Issues Music21 editor not set music21.converter.subConverters.SubConverterException: Cannot find a valid application path for format musicxml. Specify this in your Environment by calling environment.set(None, '/path/to/application')  Either set it to MuseScore or similar (on a machine with GUI) to to a dummy command (on a server). See the installation section. Citing Please consider citing this work or emailing me if you use DeepBach in musical projects. @InProceedings{pmlr-v70-hadjeres17a,   title =   {{D}eep{B}ach: a Steerable Model for {B}ach Chorales Generation},   author =   {Ga{\\\"e}tan Hadjeres and Fran{\\c{c}}ois Pachet and Frank Nielsen},   booktitle =   {Proceedings of the 34th International Conference on Machine Learning},   pages =   {1362--1371},   year =   {2017},   editor =   {Doina Precup and Yee Whye Teh},   volume =   {70},   series =   {Proceedings of Machine Learning Research},   address =   {International Convention Centre, Sydney, Australia},   month =   {06--11 Aug},   publisher =   {PMLR},   pdf =   {http://proceedings.mlr.press/v70/hadjeres17a/hadjeres17a.pdf},   url =   {http://proceedings.mlr.press/v70/hadjeres17a.html}, }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://web.mit.edu/music21/", "http://web.mit.edu/music21/doc/moduleReference/moduleEnvironment.html", "http://proceedings.mlr.press/v70/hadjeres17a.html"], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3eb"}, "repo_url": "https://github.com/dswang2011/TokenSelection4NNs", "repo_name": "TokenSelection4NNs", "repo_full_name": "dswang2011/TokenSelection4NNs", "repo_owner": "dswang2011", "repo_desc": "Do we really need all tokens? Rethinking token selection in neuralnetwork for NLP", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T15:33:38Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-02T10:07:09Z", "homepage": null, "size": 69, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184559133, "is_fork": false, "readme_text": "TokenSelection4NNs Do we really need all tokens? Rethinking token selection in neuralnetwork for NLP preparetion Dependent environment  Python3.0+ pip install stanfordcorenlp, keras, numpy, pickle, argparse, nltk, etc. you need to download english \"stopword\" for nltk if you did not. you need to download stanford CoreNLP, You can find in: https://stanfordnlp.github.io/CoreNLP/download.html the version we used is: \"stanford-corenlp-full-2018-10-05\", put the file path in configuration file (refer to configuration section). stanford-corenlp  curl http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip  and copy the folder to somewhere and set the corenlp_root parameter with the downloaded path. running steps Step1: Prepare the data (generate the token selection files).  Python File: \"token_selection.py\". Add your data into path as follows: prepared/your_dataset_name/your_file_name => e.g. prepared/IMBD/train.csv ; prepared/IMBD/test.csv ; etc.. Change the parameter in main , part of the code is shown below:   nlp = StanfordCoreNLP(params.corenlp_root) ## below is where you can set your dataset and file_name token_select.token_selection_preparation(nlp = nlp, dataset=\"IMDB\",file_name=\"train.csv\") token_select.token_selection_preparation(nlp = nlp, dataset=\"IMDB\",file_name=\"test.csv\") nlp.close() # Do not forget to close! The backend server will consume a lot memery.  Step2: run the neural model.  Python File: \"main.py\". you need to change the parameters in the function of train_model(), especially for the line below:   # strategy can be: fulltext, stopword, random, POS, dependency, entity train = token_select.get_train(dataset=\"IMDB\",file_name=\"train.csv\",stragety=\"stopword\",POS_category=\"Noun\")   where you need to specify the dataset and file_name. if strategy=\"POS\", then POS_category works, possible value: \"Noun\", \"Verb\", \"Adjective\", \"Noun_Verb\", \"Noun_Adjective\", \"Verb_Adjective\", \"Noun_Verb_Adjective\". if strategy=\"fulltext\", \"stopword\", \"entity\", or \"triple\", then it works independently, other parameters won't affect. if strategy=\"random\", then selected_ratio can work if set, possible values: 0.9,0.8,0.7,0.6,0.5 if stragety=\"dependency\", then cut can work if set, possible values: 1,2,3  configuration  Configuration File:\"config/config.ini\" Basically, you just need to put the standford CoreNLP file path for \"corenlp_root\"; And Glove embedding files for \"GLOVE_DIR\"  Our config.ini looks like below: [COMMON] MAX_SEQUENCE_LENGTH = 150 MAX_SEQUENCE_LENGTH_contatenate = 150  MAX_NB_WORDS = 20000    EMBEDDING_DIM = 100 VALIDATION_SPLIT = 0.1 batch_size = 64 epoch_num = 100 dropout_rate = 0.2 hidden_unit_num = 100 hidden_unit_num_second = 100 cell_type = gru contatenate = 1 lr= 0.001 corenlp_root=/home/dongsheng/data/resources/stanford-corenlp-full-2018-10-05 GLOVE_DIR = /home/dongsheng/data/resources/glove dataset_dir = input/dataset model= bilstm2  data load in other models ** python file: data_reader.py ** MR, IMDB : function -> load_classification_data(file_path,hasHead=0) ; return texts, labels ** factcheck: function -> load_pair_data(file_path,hasHead=0); return texts1,texts2,labels (claims, support_docs, labels) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3ec"}, "repo_url": "https://github.com/anonymousscientist/anonymous_code_copy", "repo_name": "anonymous_code_copy", "repo_full_name": "anonymousscientist/anonymous_code_copy", "repo_owner": "anonymousscientist", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T12:08:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T12:07:26Z", "homepage": null, "size": 50860, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184574095, "is_fork": false, "readme_text": "Siamese Mask R-CNN This is an anonymous copy of the implementation of Siamese Mask R-CNN for One-Shot Instance Segmentation. It is only intended to be used to keep anonymity of the authors during the review process. Please don't use this code for any projects as it will be deleted upon paper acceptance. The official version of the code can be found on github. This implementation is based on the Mask R-CNN implementation by Matterport.    The repository includes:   Source code of Siamese Mask R-CNN  Training code for MS COCO  Evaluation on MS COCO metrics (AP)  Training and evaluation of one-shot splits of MS COCO  Training code to reproduce the results from the paper  Pre-trained weights for ImageNet  Pre-trained weights for all models from the paper  Code to evaluate all models from the paper  Code to generate result figures  One-Shot Instance Segmentation One-shot instance segmentation can be summed up as: Given a query image and a reference image showing an object of a novel category, we seek to detect and segment all instances of the corresponding category (in the image above \u2018person\u2019 on the left, \u2018car\u2019 on the right). Note that no ground truth annotations of reference categories are used during training. This type of visual search task creates new challenges for computer vision algorithms, as methods from metric and few-shot learning have to be incorporated into the notoriously hard tasks ofobject identification and segmentation. Siamese Mask R-CNN extends Mask R-CNN - a state-of-the-art object detection and segmentation system - with a Siamese backbone and a matching procedure to perform this type of visual search. Installation  Clone this repository Prepare COCO dataset as described below Run the install_requirements.ipynb notebook to install all relevant dependencies.  Requirements Linux, Python 3.4+, Tensorflow, Keras 2.1.6, cython, scikit_image 0.13.1, h5py, imgaug and opencv_python Prepare COCO dataset The model requires MS COCO and the CocoAPI to be added to /data. cd data git clone https://github.com/cocodataset/cocoapi.git  It is recommended to symlink the dataset root of MS COCO. ln -s $PATH_TO_COCO$/coco coco  If unsure follow the instructions of the Matterport Mask R-CNN implementation. Get pretrained weights Get the pretrained weights from the release menu and save them to /checkpoints. Training To train a small version of siamese mask r-cnn on MS COCO simply follow the instructions in the training.ipynb notebook. This model runs on a single GPU with 12GB memory. To train the models reported in the paper run the notebooks provided in experiments. Those models need 4 GPUs with 12GB memory each. Evaluation To evaluate and visualize a models results run the evaluation.ipynb notebook. Make sure to use the same config as used for training the model. To evaluate the models reported in the paper run the evaluation notebook provided in experiments. Each model will be evaluated 5 times to compensate for the stochastic effects introduced by randomly choosing the reference instances. The final result is the mean of those five runs. Model description Siamese Mask R-CNN is designed as a minimal variation of Mask R-CNN which can perform the visual search task described above. For more details please read the paper.    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://cocodataset.org/#home"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3ed"}, "repo_url": "https://github.com/Leezhen2014/SSD.Pytorch1.0", "repo_name": "SSD.Pytorch1.0", "repo_full_name": "Leezhen2014/SSD.Pytorch1.0", "repo_owner": "Leezhen2014", "repo_desc": "SSD impliments use Pytroch1.0", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-14T03:36:25Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T16:18:03Z", "homepage": null, "size": 78308, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 184613313, "is_fork": false, "readme_text": "SSD.Pytorch1.0 Note: we clone reposiable from ssd.pytorch , and do some modify : TODO  Still to come:   Support for the MS COCO dataset  Support pytroch1.0  save training some data: loss ,ac  Do evaluate  Support for SSD512 training and testing  Support for training on custom datasets c    SSD: Single Shot MultiBox Object Detector, in PyTorch A PyTorch implementation of Single Shot MultiBox Detector from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found here. Installation  Install PyTorch by selecting your environment on the website and running the appropriate command. Clone this repository.  Note: We currently only support Python 3+.   Then download the dataset by following the instructions below. Note: For training, we currently support VOC and COCO, and aim to add ImageNet support soon.  Datasets To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit torch.utils.data.Dataset, making them fully compatible with the torchvision.datasets API. COCO Microsoft COCO: Common Objects in Context Download COCO 2014 # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/COCO2014.sh VOC Dataset PASCAL VOC: Visual Object Classes Download VOC2007 trainval & test # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2007.sh # <directory> Download VOC2012 trainval # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2012.sh # <directory> Training SSD  First download the fc-reduced VGG-16 PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth By default, we assume you have downloaded the file in the ssd.pytorch/weights dir:  mkdir weights cd weights wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth  To train SSD using the train script simply specify the parameters listed in train.py as a flag or manually change them.  python train.py  Note:  For training, an NVIDIA GPU is strongly recommended for speed. For instructions on Visdom usage/installation, see the Installation section. You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see train.py for options)    Authors  Max deGroot Ellis Brown  Note: Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible. References  Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" ECCV2016. Original Implementation (CAFFE) A huge thank you to Alex Koltun and his team at Webyclip for their help in finishing the data augmentation portion. A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):  Chainer, Keras, MXNet, Tensorflow    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://mscoco.org/", "http://host.robots.ox.ac.uk/pascal/VOC/", "http://pytorch.org/", "http://www.image-net.org/", "http://www.webyclip.com", "http://pytorch.org/docs/torchvision/datasets.html", "http://github.com/ellisbrown"], "reference_list": ["http://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1409.1556"]}, {"_id": {"$oid": "5cf518917eb8d6614c2ad3ee"}, "repo_url": "https://github.com/jiajunhua/wizyoung-YOLOv3_TensorFlow", "repo_name": "wizyoung-YOLOv3_TensorFlow", "repo_full_name": "jiajunhua/wizyoung-YOLOv3_TensorFlow", "repo_owner": "jiajunhua", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-02T05:48:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-02T05:48:21Z", "homepage": null, "size": 2559, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184527851, "is_fork": false, "readme_text": "YOLOv3_TensorFlow 1. Introduction This is my implementation of YOLOv3 in pure TensorFlow. It contains the full pipeline of training and evaluation on your own dataset. The key features of this repo are:  Efficient tf.data pipeline Weights converter (converting pretrained darknet weights on COCO dataset to TensorFlow checkpoint.) Extremely fast GPU non maximum supression. Full training pipeline. Kmeans algorithm to select prior anchor boxes.  Multi-GPU training with sync batch norm. (on working)  2. Requirements  tensorflow >= 1.8.0 (lower versions may work too) opencv-python  3. Weights convertion The pretrained darknet weights file can be downloaded here. Place this weights file under directory ./data/darknet_weights/ and then run: python convert_weight.py Then the converted TensorFlow checkpoint file will be saved to ./data/darknet_weights/ directory. You can also download the converted TensorFlow checkpoint file by me via [Google Drive link] or [Github Release]and then place it to the same directory. 4. Running demos There are some demo images and videos under the ./data/demo_data/. You can run the demo by: Single image test demo: python test_single_image.py ./data/demo_data/messi.jpg Video test demo: python video_test.py ./data/demo_data/video.mp4 Some results:    Compare the kite detection results with TensorFlow's offical API result here. (The kite detection result is under input image resolution 1344x896) 5. Inference speed How fast is the inference speed? With images scaled to 416*416:    Backbone GPU Time(ms)     Darknet-53 (paper) Titan X 29   Darknet-53 (my impl.) Titan XP ~23    why is it so fast? Check the ImageNet classification result comparision from the paper:  6. Model architecture For better understanding of the model architecture, you can refer to the following picture. With great thanks to Levio for your excellent work!  7. Training 7.1 Data preparation (1) annotation file Generate train.txt/val.txt/test.txt files under ./data/my_data/ directory. One line for one image, in the format like image_absolute_path box_1 box_2 ... box_n. Box_format: label_index x_min y_min x_max y_max.(The origin of coordinates is at the left top corner.) For example: xxx/xxx/1.jpg 0 453 369 473 391 1 588 245 608 268 xxx/xxx/2.jpg 1 466 403 485 422 2 793 300 809 320 ...  NOTE: You should leave a blank line at the end of each txt file. (2)  class_names file: Generate the data.names file under ./data/my_data/ directory. Each line represents a class name. For example: bird person bike ...  The COCO dataset class names file is placed at ./data/coco.names. (3) prior anchor file: Using the kmeans algorithm to get the prior anchors: python get_kmeans.py  Then you will get 9 anchors and the average IOU. Save the anchors to a txt file. The COCO dataset anchors offered by YOLO v3 author is placed at ./data/yolo_anchors.txt, you can use that one too. NOTE: The yolo anchors should be scaled to the rescaled new image size. Suppose your image size is [W, H], and the image will be rescale to 416*416 as input, for each generated anchor [anchor_w, anchor_h], you should apply the transformation anchor_w = anchor_w / W * 416, anchor_h = anchor_g / H * 416. 7.2 Training Using train.py. The parameters are as following: $ python train.py -h usage: train.py [-h] [--train_file TRAIN_FILE] [--val_file VAL_FILE]                 [--restore_path RESTORE_PATH]                  [--save_dir SAVE_DIR]                 [--log_dir LOG_DIR]                  [--progress_log_path PROGRESS_LOG_PATH]                 [--anchor_path ANCHOR_PATH]                 [--class_name_path CLASS_NAME_PATH] [--batch_size BATCH_SIZE]                 [--img_size [IMG_SIZE [IMG_SIZE ...]]]                 [--total_epoches TOTAL_EPOCHES]                 [--train_evaluation_freq TRAIN_EVALUATION_FREQ]                 [--val_evaluation_freq VAL_EVALUATION_FREQ]                 [--save_freq SAVE_FREQ] [--num_threads NUM_THREADS]                 [--prefetech_buffer PREFETECH_BUFFER]                 [--optimizer_name OPTIMIZER_NAME]                 [--save_optimizer SAVE_OPTIMIZER]                 [--learning_rate_init LEARNING_RATE_INIT] [--lr_type LR_TYPE]                 [--lr_decay_freq LR_DECAY_FREQ]                 [--lr_decay_factor LR_DECAY_FACTOR]                 [--lr_lower_bound LR_LOWER_BOUND]                 [--restore_part [RESTORE_PART [RESTORE_PART ...]]]                 [--update_part [UPDATE_PART [UPDATE_PART ...]]]                 [--update_part [UPDATE_PART [UPDATE_PART ...]]]                 [--use_warm_up USE_WARM_UP] [--warm_up_lr WARM_UP_LR]                 [--warm_up_epoch WARM_UP_EPOCH] Check the train.py for more details. You should set the parameters yourself. Some training tricks in my experiment: (1) Apply the two-stage training strategy: First stage: Restore darknet53_body part weights from COCO checkpoints, train the yolov3_head with big learning rate like 1e-3 until the loss reaches to a low level, like less than 1. Second stage: Restore the weights from the first stage, then train the whole model with small learning rate like 1e-4 or smaller. At this stage remember to restore the optimizer parameters if you use optimizers like adam. (2) Quick train: If you want to obtain good results in a short time like in 10 minutes. You can use the coco names but substitute several with real class names in your dataset. In this way you restore the whole pretrained COCO model and get a 80 class classification model, but you only care the class names from your dataset. 8. Evaluation Using eval.py to evaluate the validation or test dataset. The parameters are as following: $ python eval.py -h usage: eval.py [-h] [--eval_file EVAL_FILE] [--restore_path RESTORE_PATH]                [--anchor_path ANCHOR_PATH]                 [--class_name_path CLASS_NAME_PATH]                [--batch_size BATCH_SIZE]                [--img_size [IMG_SIZE [IMG_SIZE ...]]]                [--num_threads NUM_THREADS]                [--prefetech_buffer PREFETECH_BUFFER] Check the eval.py for more details. You should set the parameters yourself. You will get the loss, recall and precision metrics results, like: recall: 0.927, precision: 0.945 total_loss: 0.210, loss_xy: 0.010, loss_wh: 0.025, loss_conf: 0.125, loss_class: 0.050 9. Other tricks There are many skills you can try during training: (1) Data augmentation: You can implement your data augmentation like color jittering under data_augmentation method in ./utils/data_utils.py. (2) Mixed up and label smoothing like what Gluon-CV does. (3) Normalizations like L2 norm. (4) Mutil-scale training: You can change the input image scales (i.e. different input resolutions) periodically like the author does in the original paper.  Credits: I refer to many fantastic repos during the implementation: https://github.com/YunYang1994/tensorflow-yolov3 https://github.com/qqwweee/keras-yolo3 https://github.com/eriklindernoren/PyTorch-YOLOv3 https://github.com/pjreddie/darknet ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef614e"}, "repo_url": "https://github.com/akshaybahadur21/keras-secure-image", "repo_name": "keras-secure-image", "repo_full_name": "akshaybahadur21/keras-secure-image", "repo_owner": "akshaybahadur21", "repo_desc": "Add-on library for Keras to train on encrypted images for humans", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T12:28:14Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T04:10:42Z", "homepage": "", "size": 70, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184697055, "is_fork": false, "readme_text": "Keras Secure Image   This is an add-on library for Keras. The main functionality is to encrypt the images in the dataset so they are secure. You can write your own generator and call the decrypt functionality at runtime. The decrypted images are not stored, they are stored in tuple by the generator ensuring the safety of your images. Link to the colab notebook Installation pip install keras_secure_image Usage Encrypting the images from keras_secure_image import encrypt_directory encrypt_directory(src_dir=\"/path/to/src\",                                    dest_dir=\"/path/to/dest\", image_x=100, image_y=100,                                    password=\"<PASSWORD>)                Original Image  Encrypted Image  Training on encrypted images from keras_secure_image import decrypt_img def generator_from_encrypted_data(path_to_features, labels, batch_size):  batch_features = np.zeros((batch_size, 64, 64, 3))  batch_labels = np.zeros((batch_size,1))  while True:    for i in range(batch_size):      # choose random index in path_to_features      index= random.choice(len(path_to_features),1)      img = decrypt_img(path_to_img=path_to_features[index], password=\"<PASSWORD>\", image_x=100, image_y=100)      batch_features[i] = img      batch_labels[i] = labels[index]    yield batch_features, batch_labels     Note : Check the line img = decrypt_data(path_to_img=path_to_features[index], password=\"<PASSWORD>\", image_x=100, image_y=100) This decrypt_data function takes the path to the image and decrypts it. Make sure that the <PASSWORD> is the same for encryption. Calling the fit_generator in Keras model.fit_generator(generator_from_encrypted_data(path_to_features,labels, 32),                     samples_per_epoch=20, nb_epoch=10,                     validation_data=generator_from_encrypted_data(features,labels, 16),                     validation_steps=5, callbacks=callbacks_list, shuffle=True,use_multiprocessing=True) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef614f"}, "repo_url": "https://github.com/barathv98/Bharatanatyam-Mudras-Classification", "repo_name": "Bharatanatyam-Mudras-Classification", "repo_full_name": "barathv98/Bharatanatyam-Mudras-Classification", "repo_owner": "barathv98", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-03T15:41:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T15:29:21Z", "homepage": null, "size": 7199, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184779311, "is_fork": false, "readme_text": "Keras Image Classification  CNN for binary class image classification using Keras   Download the code and dataset and run the load.py first and then predict.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6150"}, "repo_url": "https://github.com/sooraj-sudhakar/Diabetes_prediction", "repo_name": "Diabetes_prediction", "repo_full_name": "sooraj-sudhakar/Diabetes_prediction", "repo_owner": "sooraj-sudhakar", "repo_desc": "Diabetes prediction using neural network created using keras with tensorflow backend", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-10T09:20:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T10:43:01Z", "homepage": null, "size": 78, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184739068, "is_fork": false, "readme_text": "Diabetes prediction Diabetes prediction using neural network created using keras with tensorflow backend. Dependancy pip install h5py==2.8.0   pip install Keras==2.2.0   pip install Keras-Applications==1.0.2   pip install Keras-Preprocessing==1.0.1   pip install numpy==1.14.5   pip install PyYAML==3.12   pip install scikit-learn==0.19.1   pip install scipy==1.1.0   pip install six==1.11.0   pip install sklearn==0.0 ` Dataset This work is used to predict the diabetes in a patient. The dataset used here is the Pima Indians diabetes database. The dataset consists of 768 entries having 9 features. The entires correspond to the test on each patient. The 9 features are :  Pregnancies - Number of times pregnant GlucosePlasma - glucose concentration a 2 hours in an oral glucose tolerance test BloodPressure - Diastolic blood pressure (mm Hg) SkinThickness - Triceps skin fold thickness (mm) Insulin - 2-Hour serum insulin (mu U/ml) BMI - Body mass index (weight in kg/(height in m)^2) DiabetesPedigreeFunction - Diabetes pedigree function Age - Age (years) Outcome - Class variable (0 or 1) 268 of 768 are 1, the others are 0  Sample    Pregnancies GlucosePlasma BloodPressure SkinThickness Insulin BMI DiabetesPedigreeFunction Age Outcome     6 148 72 35 0 33.6 0.627 50 1   1 85 66 29 0 26.6 0.351 31 0    Working There is a main training.py file which contains the ANN defined using the keras. The input Pima Indians diabetes csv file is splitted into train & test. The trained model is saved as model.h5. This saved model will be then used for the single as well as the bulk prediction programs.   ", "has_readme": true, "readme_language": "English", "repo_tags": ["python", "keras", "neural-network", "prediction"], "has_h5": true, "h5_files_links": ["https://github.com/sooraj-sudhakar/Diabetes_prediction/blob/64f3500c63ef2ae691922fd42eeb41a564594966/weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6151"}, "repo_url": "https://github.com/dzglearning/MnistExercise", "repo_name": "MnistExercise", "repo_full_name": "dzglearning/MnistExercise", "repo_owner": "dzglearning", "repo_desc": "keras\u548ctensorflow \u6784\u5efa\u7b80\u5355\u5168\u8fde\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0Mnist\u624b\u5199\u4f53\u8bc6\u522b", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-04T09:40:46Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-03T06:14:22Z", "homepage": "", "size": 30958, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184707254, "is_fork": false, "readme_text": "\u7b80\u5355\u4f7f\u7528 keras \u548c tensorflow \u6784\u5efa\u5168\u94fe\u8fde\u63a5\u795e\u7ecf\u7f51\u7edc\uff0c\u5b9e\u73b0\u624b\u5199\u4f53\u8bc6\u522b keras \u6570\u636e\u5185\u5bb9  \u6570\u636e\u5206\u5e03  \u6d4b\u8bd5\u60c5\u51b5  tensorflow  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/dzglearning/MnistExercise/blob/dabe6d4340bec60f92c5281d8dca235232f61400/keras/model/keras_mnist.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6152"}, "repo_url": "https://github.com/GregFrench/keras-practice", "repo_name": "keras-practice", "repo_full_name": "GregFrench/keras-practice", "repo_owner": "GregFrench", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-21T02:32:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T02:02:24Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184685754, "is_fork": false, "readme_text": "Keras Practice These code snippets are just example programs written in order to help me learn and better understand Keras. Feel free to use any of the code as you wish. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6153"}, "repo_url": "https://github.com/zb1439/few-shot-segm", "repo_name": "few-shot-segm", "repo_full_name": "zb1439/few-shot-segm", "repo_owner": "zb1439", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-05T16:15:49Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-03T14:54:41Z", "homepage": null, "size": 31, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184773743, "is_fork": false, "readme_text": "Implementation of few shot semantic segmentation in Keras This repository includes several few-shot semantic segmentation frameworks in Keras and try to reproduce the results and test for the stablity of their convergence. Here are the models and frameworks that we will try to include:  OSLSM co-FCN SG-one PrototypeNet  TODO list:  finish implementation try to add the weight hashing layer  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://bmvc2018.org/contents/papers/0255.pdf"], "reference_list": ["https://arxiv.org/abs/1709.03410.pdf", "https://arxiv.org/abs/1806.07373.pdf", "https://arxiv.org/pdf/1810.09091.pdf"]}, {"_id": {"$oid": "5cf518937eb8d66358ef6154"}, "repo_url": "https://github.com/yangboz/TransferLearning4Dentist", "repo_name": "TransferLearning4Dentist", "repo_full_name": "yangboz/TransferLearning4Dentist", "repo_owner": "yangboz", "repo_desc": "Transfer Learning for Dentist diagnosis aid , an intelligent healthcare application for train/validate/test/predict.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T14:11:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T03:35:35Z", "homepage": "", "size": 598576, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184694007, "is_fork": false, "readme_text": "I think AI is akin to building a rocket ship.   You need a huge engine and a lot of fuel. If you have a large engine and a tiny amount of fuel, you won\u2019t make it to orbit.  If you have a tiny engine and a ton of fuel, you can\u2019t even lift off.  To build a rocket you need a huge engine and a lot of fuel.  The analogy to deep learning is that the rocket engine is the deep learning models and the fuel  is the huge amounts of data we can feed to these algorithms.\u200a\u2014\u200aAndrew Ng  TransferLearning4Dentist Transfer Learning for Dentist diagnosis aid , an intelligent healthcare application for train/test/predict.  Overview (VGG16) model Transfer Learning, + Keras ImageDataGenerator, fine-tune on VGG16, base on MobileNet with ImageNet weights, for prediction and evaluate the final score. Conceptual Framework & Details VGG16  VGG16+  References https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab http://cs231n.github.io/transfer-learning/ https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/ https://medium.com/datadriveninvestor/keras-imagedatagenerator-methods-an-easy-guide-550ecd3c0a92 ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "transfer-learning", "dataset", "keras", "python", "bottleneck-features", "fine-tuning", "image-classification", "image-recognition"], "has_h5": true, "h5_files_links": ["https://github.com/yangboz/TransferLearning4Dentist/blob/6d035b9595467a790fee686e2191fd875f5d8bc5/model.h5", "https://github.com/yangboz/TransferLearning4Dentist/blob/4de745ce6b302094406cb49b6a57c73116b12713/model_v1_den.h5", "https://github.com/yangboz/TransferLearning4Dentist/blob/ef40320f312e5d7652bd9bea06e6e8b36901bee8/model_v2_den.h5"], "see_also_links": ["http://cs231n.github.io/transfer-learning/"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6155"}, "repo_url": "https://github.com/Darkovian/keras-sentiment-analysis", "repo_name": "keras-sentiment-analysis", "repo_full_name": "Darkovian/keras-sentiment-analysis", "repo_owner": "Darkovian", "repo_desc": "A simple python implementation of sentiment analysis using Keras and TensorFlow.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T01:59:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T01:18:15Z", "homepage": null, "size": 9204, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184681713, "is_fork": false, "readme_text": "keras-sentiment-analysis A simple python implementation of sentiment analysis using Keras and TensorFlow. A quick (and very basic) introduction: Sentiment analysis is essentially taking data in natural language format (tweets, product reviews, conversations, etc) and generating a sentiment for that data. For the purposes of this project the categories are very basic: positive or negative. This project leverages machine learning by training a model using Keras and TensorFlow that is able to categorize a snippet of natural language (such as a sentence, tweet, etc) as having a positive sentiment or a negative sentiment. Example: The sentence \"I really hate Monday\" has a negative sentiment while the sentence \"I'm feeling the love right now!\" has a positive sentiment. Requirements: In this project we use the Twitter Sentiment Analysis Training Corpus Dataset from http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip. Make sure that the csv is saved as ./s-a-d.csv (in the same directory as sent-a.py and sent-predict.py) The required modules for this project are numpy, keras, and tensorflow. To install them you can run: sudo pip3 install numpy keras tensorflow Instructions:  If you want to generate the model on your local machine, run sent-a.py to generate the dictionary and the model. Be warned that this could take a decent amount of time- it took me an hour to train the model on a Google Cloud Compute VM with 6 cores and 25GB RAM. Otherwise (if you don't want to train the model yourself) download dictionary.json, 1000w-model.json, and 1000w-model.h5 and save them in the same folder as sent-predict.py. You can now run sent-predict.py and input test tweets/sentences  From this point you could easily change sent-predict to read in a file of sentences (a Twitter scrape, product or company reviews, etc) and iterate through them, predicting the sentiment for each and returning the overall ratio of positive/negative sentiments, etc. I did this, however I have chosen to move on to a different method which should allow much higher accuracy and so I am not uploading that part as it is not as well implemented as I'd like. The new project will have this capability. This model attains ~76% accuracy. Not bad for the relatively simple approach and the low word count (this model only uses the most popular 1000 words from the Twitter Sentiment Analysis Training Corpus Dataset). The logical evolution from here would be to train a model using word embeddings (such as GloVe or Google's Word2Vec). I will have such a repository up soon. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Darkovian/keras-sentiment-analysis/blob/ee7ce218ebec9fff745a21c4ef51ff5d28de9be2/1000w-model.h5"], "see_also_links": ["http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6156"}, "repo_url": "https://github.com/eddardd/Automatic-Data-Augmentation", "repo_name": "Automatic-Data-Augmentation", "repo_full_name": "eddardd/Automatic-Data-Augmentation", "repo_owner": "eddardd", "repo_desc": "Reproduction of Autoaugmentation paper for Image Denoising", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T14:27:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T22:43:32Z", "homepage": null, "size": 875, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184828602, "is_fork": false, "readme_text": "InnovR 2018/2019. This repository contains the code for the InnovR project \"On Automating Data Augmentation for Deep Image Denoising\" done in the 2018/2019 academic year. Table of contents Data Module Provides a common interface for dataset objects, which provide data to Deep Learning Keras-based models. Transformations Module Uses Imgaug to implement policy image transformations. Contain definitions about operations, subpolicies and policy objects. Models module Implementation of a Denoising Autoencoder in Keras. Neural network architecture:  Controller module Contains the code for the Controller Network used to predict Optimal Augmentation policies. Controller architecture:  Tests Codes to generate paper figures, perform experiments and train controller network. ", "has_readme": true, "readme_language": "English", "repo_tags": ["image-denoising", "denoising-autoencoders", "autoaugment", "reinforcement-learning", "proximal-policy-optimization"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6157"}, "repo_url": "https://github.com/AtlasCoCo/Keras_YOLO3", "repo_name": "Keras_YOLO3", "repo_full_name": "AtlasCoCo/Keras_YOLO3", "repo_owner": "AtlasCoCo", "repo_desc": "A Keras implementation of YOLOv3 (Tensorflow backend)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T07:56:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T07:49:45Z", "homepage": null, "size": 125, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184717842, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6158"}, "repo_url": "https://github.com/eugene123tw/keras_yolo3", "repo_name": "keras_yolo3", "repo_full_name": "eugene123tw/keras_yolo3", "repo_owner": "eugene123tw", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T16:47:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T13:52:29Z", "homepage": null, "size": 142, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184763689, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6159"}, "repo_url": "https://github.com/ssingh121210/Gender-Prediction-Deep-Learning", "repo_name": "Gender-Prediction-Deep-Learning", "repo_full_name": "ssingh121210/Gender-Prediction-Deep-Learning", "repo_owner": "ssingh121210", "repo_desc": "The projects works on Gender Prediction as a classification problem. The output layer in the gender prediction network is of type softmax with 2 nodes indicating the two classes \u201cMale\u201d and \u201cFemale\u201d.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T08:30:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T08:09:55Z", "homepage": null, "size": 8431, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184720393, "is_fork": false, "readme_text": "Gender-Prediction-Deep-Learning The projects works on Gender Prediction as a classification problem. The output layer in the gender prediction network is of type softmax with 2 nodes indicating the two classes \u201cMale\u201d and \u201cFemale\u201d. The network uses 3 convolutional layers, 2 fully connected layers and a final output node. The details of the layers are given below. Conv1 : The first convolutional layer has 32 nodes of kernel with relu activation function. Conv2 : The second conv layer has 32 nodes with kernel with relu activation function and one output layer with 1 node and activation function sigmoid. Libraries Required:  keras numpy h5py(To save the model) Tenser flow at the backend of keras. Theno  The dataset is available at: https://www.kaggle.com/playlist/men-women-classification/version/3# ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ssingh121210/Gender-Prediction-Deep-Learning/blob/87bafec0e2dbe1c689d02eaa9d2e6c5b5a14710c/ann_final.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef615a"}, "repo_url": "https://github.com/zhaipro/keras-wdsr", "repo_name": "keras-wdsr", "repo_full_name": "zhaipro/keras-wdsr", "repo_owner": "zhaipro", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T03:32:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T11:21:48Z", "homepage": null, "size": 7, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 184743463, "is_fork": false, "readme_text": "keras-wdsr ", "has_readme": true, "readme_language": "Malay (macrolanguage)", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef615b"}, "repo_url": "https://github.com/zack7891/Convolution-Neural-Network-using-Keras", "repo_name": "Convolution-Neural-Network-using-Keras", "repo_full_name": "zack7891/Convolution-Neural-Network-using-Keras", "repo_owner": "zack7891", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-03T03:17:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T03:15:29Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184692247, "is_fork": false, "readme_text": "Convolution-Neural-Network-using-Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef615c"}, "repo_url": "https://github.com/chenmoshushi/nram", "repo_name": "nram", "repo_full_name": "chenmoshushi/nram", "repo_owner": "chenmoshushi", "repo_desc": "Keras (novice) implementation and modification of Neural Random Access Machine", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T14:26:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T03:30:22Z", "homepage": null, "size": 18, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184693558, "is_fork": false, "readme_text": "nram Keras (novice) implementation and modification of Neural Random-Access Machine The paper Neural Random-Access Machine ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1511.06392"]}, {"_id": {"$oid": "5cf518937eb8d66358ef615d"}, "repo_url": "https://github.com/abogacki/car-recognition-binary-classification", "repo_name": "car-recognition-binary-classification", "repo_full_name": "abogacki/car-recognition-binary-classification", "repo_owner": "abogacki", "repo_desc": "Binary classification for car recognition", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-18T08:28:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T08:52:31Z", "homepage": null, "size": 105299, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184725632, "is_fork": false, "readme_text": "Car damage recognition repository This is gate one - verify wether car has been damaged Binary classification problem. Tech stack: Frontend Keras API Backend PlaidML, TensorFlow ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef615e"}, "repo_url": "https://github.com/mewstopher/cacti", "repo_name": "cacti", "repo_full_name": "mewstopher/cacti", "repo_owner": "mewstopher", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-17T00:11:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T05:30:16Z", "homepage": null, "size": 238, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184703448, "is_fork": false, "readme_text": "README Babas First CNN This is a practice problem using CNNs on a kaggle competition. The only goal here is to gain practice coding nueral networks, and a better understanding of how to effectively code them. Project status  CNN used to identify whether or not cactus in picture. First model (tools.py) was an attempt to code a VGG from scratch. Some thing is wrong with code, resulting in test/train accuracies of .99,1.0. Dense_net.py uses a fully connected 2 layer network. This one work, but accuracy is not that great. To compare with a pretrained model, VGG16 is imported and used in run_keras.py Results give test/train ~ .95, .90 respectively. Training model (VGG) from scrach works, but pretained model is (obviously) a much better option for accuracy. Training last 2 layers of pretrained VGG results in .98 accuracy on actual test data. While this is not great for the competiton results, it is good enough to move on. List of files:  keras_model: codes up a VGG from scratch. Dense_net: a fully connected 2 layer neural net imports: contains all imported modules used in model scripts run_dense/keras: script for running dense and VGG(from scratch) run VGG: runs pre-trained VGG tf_VGG: VGG16 coded from scatch using tensorflow  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef615f"}, "repo_url": "https://github.com/liwenran/EnContact", "repo_name": "EnContact", "repo_full_name": "liwenran/EnContact", "repo_owner": "liwenran", "repo_desc": "EnContact: predicting enhancer-enhancer contacts using sequence-based deep learning model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-12T22:21:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T22:33:23Z", "homepage": null, "size": 31368, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184827840, "is_fork": false, "readme_text": "EnContact EnContact: predicting enhancer-enhancer contacts using sequence-based deep learning model Model training We collected chromatin contact matrix of seven cell lines (i.e. GM, K562, HCASMC, MyLa, Na\u00efve, Th17, and Treg) from HiChIP data of Mumbach et al, 2017. We implemented the EnContact model using Keras 1.2.0 on a Linux server. All experiments were carried out with 4 Nvidia K80 GPUs which significantly accelerated the training process than CPUs. Application: predict enhancer-enhancer interactions from HiChIP data We apply the trained EnContact model to infer contacts between enhancers in situations where one or both interaction regions contain multiple regulatory elements. In this way, we predict enhancer-enhancer interactions from bin-level interactions. For each cell line, the predicted interactions are saved in 'predictions_cell.txt' files (e.g. EnContact/E-E prediction/predictions_GM.txt). Requirements  hickle numpy=1.13.3 Theano=0.8.0 keras=1.2.0 pandas=0.20.1 biopython=1.70 Scikit-learn=0.18.2  Installation Download EnContact by git clone https://github.com/liwenran/EnContact License This project is licensed under the MIT License - see the LICENSE.md file for details ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6160"}, "repo_url": "https://github.com/Helli5AISquad/Music-Generator", "repo_name": "Music-Generator", "repo_full_name": "Helli5AISquad/Music-Generator", "repo_owner": "Helli5AISquad", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-19T13:54:07Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-03T15:29:46Z", "homepage": null, "size": 2585, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184779365, "is_fork": false, "readme_text": "Music-Generator [1]: Replace all paths in \"main.py\" and \"train.py\" with your paths [2]: Install \"Keras\", \"Music21\" [3]: Run main.py for create music, run train.py to train a model with your own dataset To run train.py: py train.py [Dataset Name] [Number Of Epochs] To run main.py: py main.py [Name Of Music] [Style(Dataset Name)] [Number Of Notes] Programmers: Soheil Mohammadkhani(https://github.com/SoheilMohammadkhani) Mohammadmahdi Doolabi Arshia Samizad Sorena Khiabani Special Thanks To Mr.Hassibi ", "has_readme": true, "readme_language": "English", "repo_tags": ["python", "machine-learning", "lstm", "lstm-neural-network", "keras"], "has_h5": true, "h5_files_links": ["https://github.com/Helli5AISquad/Music-Generator/blob/cbd58e242d824d82be0d6502826f7ce8cb13ac5a/Weights/Chopin2.h5", "https://github.com/Helli5AISquad/Music-Generator/blob/cbd58e242d824d82be0d6502826f7ce8cb13ac5a/Weights/Persian.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6161"}, "repo_url": "https://github.com/zh-yao/Adaptive-Traffic-Signal-Control-Using-Reinforcement-Learning", "repo_name": "Adaptive-Traffic-Signal-Control-Using-Reinforcement-Learning", "repo_full_name": "zh-yao/Adaptive-Traffic-Signal-Control-Using-Reinforcement-Learning", "repo_owner": "zh-yao", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-03T04:22:51Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T04:22:37Z", "homepage": null, "size": 882, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184698020, "is_fork": false, "readme_text": "=========================================================================== ADAPTIVE TRAFFIC CONTROL WITH DEEP REINFORCEMENT LEARNING ===========================================================================  Introduction  The rapid increase in automobiles on roadways has naturally lead to traffic congestions all over the world, forcing drivers to sit idly in their cars wasting time and needlessly consuming fuel. Ride sharing and infrastructural improvements can help mitigate this but one of the key components to handling traffic congestion is traffic light timing. Traffic light control policies are often not optimized, leading to cars waiting pointlessly for nonexistent traffic to pass on the crossing road. We feel that traffic light control policy can be greatly improved by implementing machine learning concepts. Our project focuses on implementing a learning algorithm that will allow traffic control devices to study traffic patterns/behaviors for a given intersection and optimize traffic flow by altering stoplight timing. We do this with a Q-Learning technique, similarly seen in previous works such as Gao et. al. and Genders et. al, where  an intersection is knowledgeable of the presence of vehicles and their speed as they approach the intersection. From this information, the intersection is able to learn a set of state and action policies that allow traffic lights to make optimized decisions based on their current state. Our work seeks to alleviate traffic congestion on roads across the world by making intersections more aware of traffic presence and giving them the ability to take appropriate action to optimize traffic flow and minimize waiting time.   Setup  This project was developed using python 3. Install from python.org .  Prerequisite Packages : 1) Tensorflow (pip install tensorflow) or (pip install tensorflow-gpu) 2) Keras (pip install keras)  The simulatons were done on SUMO Traffic Simulator. It is available at following location :- http://sumo.dlr.de/wiki/Downloads  Please refer to installation instructions of sumo from the website.    Running  Run file - traffic_light_control.py             ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/zh-yao/Adaptive-Traffic-Signal-Control-Using-Reinforcement-Learning/blob/41151092c8a893e162eb61ede83efdb98e4f3d8d/Models/reinf_traf_control.h5"], "see_also_links": ["http://sumo.dlr.de/wiki/Downloads"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6162"}, "repo_url": "https://github.com/ankit--agrawal/cnn_architectures", "repo_name": "cnn_architectures", "repo_full_name": "ankit--agrawal/cnn_architectures", "repo_owner": "ankit--agrawal", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-03T16:59:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T16:50:56Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184790296, "is_fork": false, "readme_text": "CNN are widely used for computer vision tasks. In this project, I implemented popular CNN architectures (that ship with keras) on ImageNet dataset. ImageNet dataset is a project aimed at labeling (manually) and categorizing images into almost 22,000 separate categories for computer vision research. In this implementation, we are considering a subset of dataset i.e. 1,000 separate categories. Models used:  VGG16 VGG19 ResNet50 InceptionV3 Xception MobileNet DenseNet121 NASNetMobile Xception  How to run the code: python main.py --image <path/to/image> --model <model_name> How to interpret the results:  The image will be correctly preprocessed based on the model selected (including image scaling). If you are running any of these models for the 1st time, you will automatically download the weights for the appropriate model during the 1st run (introduces  time overhead only for the 1st run). The final result is top 5 predictions with confidence (probability) in Descending order.  The documentation of all these architectures is available at https://keras.io/applications/ Resources that explain these model architectures: https://cv-tricks.com/cnn/understand-resnet-alexnet-vgg-inception/ What is next? I want to implement these models with 'GlobalAveragePooling' layer to reduce the number of trainable parameters. Recent articles that I came across claim that we do not need to add Dropout, Flatten layers to CNN models anymore. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6163"}, "repo_url": "https://github.com/zlijingtao/PartialFreezeGAN", "repo_name": "PartialFreezeGAN", "repo_full_name": "zlijingtao/PartialFreezeGAN", "repo_owner": "zlijingtao", "repo_desc": "A partial-freeze technique towards fast DCGAN training (in Keras)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T20:59:56Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T18:13:06Z", "homepage": "", "size": 24383, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 184800698, "is_fork": false, "readme_text": "PartialFreezeGAN A partial-freeze technique towards fast DCGAN training (in Python, using keras framework). MMD Metric A metric to evaluate GAN, implemented using Gaussian kernel. Reference Borji, Ali. \"Pros and cons of gan evaluation measures.\" Computer Vision and Image Understanding 179 (2019): 41-65. FreezeGAN A technique to accelerate fast GAN convergence. Learnable parameters of tail of the discriminator and head of the generator which take the most part of overall trainable parameter is freezed, which is called partial-freeze.  After a certain fraction of time (1 - $\\alpha$) training on the full-potential model, we then apply the partial-freeze for the rest of the time, to continuely train on a less potential model. This technique shows 100% less training time to achieve the same level of convergence based on MMD metric. And it also shows the potential to avoid overfitting which needs further investigation.  The comparison of our proposed technique with the baseline (4,000 training steps with learning rate 0.0002, freeze ratio is set to 0.7 in FreezeGAN) shows a much better convergence.  Without the partial-freeze technique, we need train excessive 8,000 steps to achieve same level of convergence.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/zlijingtao/PartialFreezeGAN/blob/0bf8143d314cfd4b9d8afd98a8b772ad6ea285c2/FreezeGheadDtail70_4000/G_model.h5", "https://github.com/zlijingtao/PartialFreezeGAN/blob/0bf8143d314cfd4b9d8afd98a8b772ad6ea285c2/NonFreeze_4000/G_model.h5", "https://github.com/zlijingtao/PartialFreezeGAN/blob/0bf8143d314cfd4b9d8afd98a8b772ad6ea285c2/NonFreeze_8000/G_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6164"}, "repo_url": "https://github.com/Samuelpaz95/Identificacion-Facial", "repo_name": "Identificacion-Facial", "repo_full_name": "Samuelpaz95/Identificacion-Facial", "repo_owner": "Samuelpaz95", "repo_desc": "Un programa para Identificar personas por sus rostros.", "description_language": "Spanish", "repo_ext_links": null, "repo_last_mod": "2019-05-08T01:38:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T01:56:39Z", "homepage": null, "size": 52145, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184685223, "is_fork": false, "readme_text": "Identificacion-Facial Esta implementaci\u00f3n es una version simplificada de este codigo: https://github.com/Skuldur/facenet-face-recognition Posee la misma estructura del modelo de red neuronal que empleo Skulur pero con funciones de uso mas simplificados para un mejor entendimiento del codigo. Requisitos Antes de poder ejecutar el codigo es necesario tener los siguientes requisitos:  Tener instalado python 3.7 tensorflow, un framework de machine Learning.                                        https://www.tensorflow.org/ keras, un framework de modelos de redes neuronales.                                  https://keras.io/ opencv-python, una libreria para el procesamiento de imagenes.                       https://opencv.org/ numpy, un libreria de calculos matematicos.                                          https://www.numpy.org/ matplotlib, una libreria para realizar graficas \u00fatil para el estudio de datos.       https://matplotlib.org/  Recomiendo utilisar anaconda:                                                            https://www.anaconda.com/  Anaconda es un distribuci\u00f3n libre y abierta\u200b de los lenguajes Python y R, utilizada en ciencia de datos, y aprendizaje autom\u00e1tico y ya contiene varias librerias como las que mencione antes.  Uso Se a agregado la funcion register_camera() Abra una terminal e ingrese al interprete de python3    from camara import *       register_camera('su nombre', ip=None)    si tiene un camara inalambrica donde ip, por ejemplo \"http://127.0.0.1:6000\" Abrira una ventana mostrando la imagen de su camara, pulsa \"ESC\" para cerrar y registrar su foto.    camara()    ejecute la camara igualmente s\u00ed tiene una camara inalambrica ingrese el ip como parametro. Esto podria tardar unos minutos debido a que debe cargar los datos del modelo. o tambien puede crear un archivo python3 y escribir el mismo codigo de la terminal. Para cerrar la ventada de la camara presione 'ESC' ", "has_readme": true, "readme_language": "Spanish", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Samuelpaz95/Identificacion-Facial/blob/e57ebaf8cd0688cbeaedb41c4bc9528e835b908f/my_model.h5"], "see_also_links": ["http://127.0.0.1:6000"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6165"}, "repo_url": "https://github.com/Faranio/Unconstrained_ALPR", "repo_name": "Unconstrained_ALPR", "repo_full_name": "Faranio/Unconstrained_ALPR", "repo_owner": "Faranio", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-06-02T10:27:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T10:24:12Z", "homepage": null, "size": 40074, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 184736998, "is_fork": false, "readme_text": "ALPR in Unscontrained Scenarios Introduction This repository contains the author's implementation of ECCV 2018 paper \"License Plate Detection and Recognition in Unconstrained Scenarios\".  Paper webpage: http://sergiomsilva.com/pubs/alpr-unconstrained/  If you use results produced by our code in any publication, please cite our paper: @INPROCEEDINGS{silva2018a,   author={S. M. Silva and C. R. Jung},    booktitle={2018 European Conference on Computer Vision (ECCV)},    title={License Plate Detection and Recognition in Unconstrained Scenarios},    year={2018},    pages={580-596},    doi={10.1007/978-3-030-01258-8_36},    month={Sep},}  Requirements In order to easily run the code, you must have installed the Keras framework with TensorFlow backend. The Darknet framework is self-contained in the \"darknet\" folder and must be compiled before running the tests. To build Darknet just type \"make\" in \"darknet\" folder: $ cd darknet && make  The current version was tested in an Ubuntu 16.04 machine, with Keras 2.2.4, TensorFlow 1.5.0, OpenCV 2.4.9, NumPy 1.14 and Python 2.7. Download Models After building the Darknet framework, you must execute the \"get-networks.sh\" script. This will download all the trained models: $ bash get-networks.sh  Running a simple test Use the script \"run.sh\" to run our ALPR approach. It requires 3 arguments:  Input directory (-i): should contain at least 1 image in JPG or PNG format; Output directory (-o): during the recognition process, many temporary files will be generated inside this directory and erased in the end. The remaining files will be related to the automatic annotated image; CSV file (-c): specify an output CSV file.  $ bash get-networks.sh && bash run.sh -i samples/test -o /tmp/output -c /tmp/output/results.csv  Training the LP detector To train the LP detector network from scratch, or fine-tuning it for new samples, you can use the train-detector.py script. In folder samples/train-detector there are 3 annotated samples which are used just for demonstration purposes. To correctly reproduce our experiments, this folder must be filled with all the annotations provided in the training set, and their respective images transferred from the original datasets. The following command can be used to train the network from scratch considering the data inside the train-detector folder: $ mkdir models $ python create-model.py eccv models/eccv-model-scracth $ python train-detector.py --model models/eccv-model-scracth --name my-trained-model --train-dir samples/train-detector --output-dir models/my-trained-model/ -op Adam -lr .001 -its 300000 -bs 64  For fine-tunning, use your model with --model option. A word on GPU and CPU We know that not everyone has an NVIDIA card available, and sometimes it is cumbersome to properly configure CUDA. Thus, we opted to set the Darknet makefile to use CPU as default instead of GPU to favor an easy execution for most people instead of a fast performance. Therefore, the vehicle detection and OCR will be pretty slow. If you want to accelerate them, please edit the Darknet makefile variables to use GPU. In this repository The main.py file is responsible for running the Unconstrained ALPR on videos. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Faranio/Unconstrained_ALPR/blob/d549afd1b9faf878eb14f7cfb8d734f1512fe41e/data/lp-detector/wpod-net_update1.h5"], "see_also_links": ["http://sergiomsilva.com/pubs/alpr-unconstrained/"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6166"}, "repo_url": "https://github.com/xiaomuc/poemGenerator", "repo_name": "poemGenerator", "repo_full_name": "xiaomuc/poemGenerator", "repo_owner": "xiaomuc", "repo_desc": "Poem generator using LSTM in Python", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T14:28:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T10:34:10Z", "homepage": null, "size": 600, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184738102, "is_fork": false, "readme_text": "poemGenerator   \u3084\u308d\u3046\u3068\u3057\u305f\u3053\u3068 AI\u306e\u6587\u66f8\u81ea\u52d5\u751f\u6210\u3092\u4f7f\u3063\u3066\u6587\u66f8\u3092\u751f\u6210\u3059\u308b\u3053\u3068\u3001 \u666e\u901a\u306b\u6587\u66f8\u751f\u6210\u3082\u9762\u767d\u304f\u306a\u3044\u3057\u3001\u5909\u306a\u6587\u66f8\u306b\u306a\u308b\u3053\u3068\u304c\u4e88\u60f3\u3055\u308c\u305f\u305f\u3081\u3001 \u30dd\u30a8\u30e0\u306e\u4e16\u754c\u306a\u3089\u591a\u5c11\u4e0d\u81ea\u7136\u3067\u3082\u884c\u3051\u308b\u3093\u3058\u3083\u306a\u3044\u304b\u3068\u601d\u3063\u305f   \u6280\u8853\u7684\u306a\u8a71 LSTM(Long short term memory)\u3063\u3066\u3044\u3046\u30bf\u30a4\u30d7\u306e\u30cb\u30e5\u30fc\u30ed\u3092\u3064\u304b\u3063\u3066\u307f\u305f \u6642\u7cfb\u5217\u306b\u5f37\u3044\u3089\u3057\u3044\u3002\u3068\u8a00\u3063\u3066\u3082\u5b9f\u88c5\u306fKeras(Tensorflow)\u306a\u306e\u3067\u4e2d\u8eab\u307e\u3067\u306f\u3088\u304f\u308f\u304b\u3089\u306a\u3044\u3002   \u53c2\u8003\u306b\u3057\u305f\u306e\u306f\u3053\u3053 https://qiita.com/odrum428/items/8864a5476027b910b9b5#_reference-7380c30cdb49b634522b   \u5b66\u7fd2\u304c\u9045\u3044\u306e\u3067Google Colab\u3067\u3084\u3063\u3066\u307f\u305f\u3089\u7d50\u69cb\u65e9\u3044 https://colab.research.google.com/drive/1tToJwMKF9ojm_1jepujn3Are21n3oVow#scrollTo=m599Jk8pJzjT   ", "has_readme": true, "readme_language": "Japanese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6167"}, "repo_url": "https://github.com/chriszhangwj/fyp-advgan-tf", "repo_name": "fyp-advgan-tf", "repo_full_name": "chriszhangwj/fyp-advgan-tf", "repo_owner": "chriszhangwj", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-04T18:28:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T20:49:30Z", "homepage": null, "size": 7548, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184818575, "is_fork": false, "readme_text": "AdvGAN-tf Tensorflow implementation of Generating Adversarial Examples with Adversarial Networks USAGE Create a ./weights directory as well as subdirectories ./weights/generator, ./weights/discriminator, ./weights/target_model to contain the saved weights for each model. First run: python target_models.py  which will extract the MNIST dataset using the Keras API and train a simple CNN model that will serve as the 'target model' for the generator to trick. Next, run: python AdvGAN.py  This script will first train the generator. You can specifiy whether or not you want it to be targeted. A different generator will be trained for each target. You will want to tweak the weight paths for each target (or I will update that soon). Once the training process is complete, the function attack will be called. This function will load the weights from the generator and run an attack on the test set. It will also print out a before and after picture of two images from the last batch. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6168"}, "repo_url": "https://github.com/cad0p/maskrcnn-modanet", "repo_name": "maskrcnn-modanet", "repo_full_name": "cad0p/maskrcnn-modanet", "repo_owner": "cad0p", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T15:52:26Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-03T15:34:28Z", "homepage": null, "size": 78, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 1, "github_id": 184780048, "is_fork": false, "readme_text": "A MaskRCNN Keras implementation with Modanet annotations on the Paperdoll dataset MaskRCNN ModaNet My bachelor's thesis project.  Getting Started This project is written in Python 3, so it works in all major OSes. Keep in mind to use pip or pip3 depending on your settings. Run pip install maskrcnn-modanet --user Now that you've installed it, run maskrcnn-modanet datasets download the/folder/you/want/to/put/data/in Prerequisites Install Matlab Files used  teslasheet on my Drive - created with SourceFiles taken from Tesla Motors Club Forum  Built With  Sublime Text - The text editor used Matlab - To develop initially GitHub Desktop - To manage developement  Contributing The following is a copy of PurpleBooth  Please read CONTRIBUTING.md for details on our code of conduct, and the process for submitting pull requests to us.  Versioning We use SemVer for versioning. For the versions available, see the tags on this repository. Authors  Pier Carlo Cadoppi - Initial work  See also the list of contributors who participated in this project. License This project is licensed under the MIT License - see the LICENSE.md file for details Acknowledgments  Hat tip to anyone whose code was used   Billie Thompson - README Template - PurpleBooth   Inspiration etc lol  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://semver.org/"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6169"}, "repo_url": "https://github.com/ankit--agrawal/predict_car_speed", "repo_name": "predict_car_speed", "repo_full_name": "ankit--agrawal/predict_car_speed", "repo_owner": "ankit--agrawal", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-19T05:45:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T22:59:30Z", "homepage": null, "size": 34394, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184829776, "is_fork": false, "readme_text": "Welcome to the comma.ai 2017 Programming Challenge! The goal is to predict the speed of a car from a video. The input files can be found at http://commachallenge.s3-us-west-2.amazonaws.com/speed_challenge_2017.tar train.mp4 is a video of driving containing 20400 frames. Video is shot at 20 fps. train.txt contains the speed of the car at each frame, one speed on each line. test.mp4 is a different driving video containing 10798 frames. Video is shot at 20 fps. The deliverable is test.txt The evaluation is done on test.txt using mean squared error. <10 is good. <5 is better. <3 is heart. Approach 1: I first used openCV to extract the frames per second from the video and then train a 4 layer Convolutional Neural Network in Keras with Batch Normalization, 2D convolution with strides and callbacks with 3 dense layers of fully connected network at the end. Total trainable parameters approx 9 Million Current best performance of the model: MSE = 1.7 Conclusion:  training error < 1.7 while the validation error = 0.8 #Fixed the overfitting issue. Submitted results ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ankit--agrawal/predict_car_speed/blob/2d56e6c3681ce6dbb5bb62d3fd2b44524aa8c973/speed_car.h5"], "see_also_links": ["http://commachallenge.s3-us-west-2.amazonaws.com/speed_challenge_2017.tar"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef616a"}, "repo_url": "https://github.com/Sidharth1998/ChatBot", "repo_name": "ChatBot", "repo_full_name": "Sidharth1998/ChatBot", "repo_owner": "Sidharth1998", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-03T13:49:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T13:33:35Z", "homepage": null, "size": 11179, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184760772, "is_fork": false, "readme_text": "Message Replier Giving suggestions for a message is same as creating a chatbot which will help in replying a message automatically. A human has to reply to a lot of e-mails everyday. This task takes a lot of valuable time from the user. So to curb this problem, this project was made to automate the task of replying to a given message. This project was made in python and Deep Learning techniques were used. The technique that I used in this project is Seq2Seq modelling using RNN(Recurrent Neural Networks). A sequence to sequence model aims to map a fixed length input with a fixed length output where the length of the input and output may differ.The model consists of 3 parts: encoder, intermediate (encoder) vector and decoder. Encoder encodes the input sequence. Encoder vector is the final hidden state produced from the encoder part of the model. Decoder decodes the predicted output sequence by the neural network. Dataset Used The data used to train the model is gunthercox. This data was used because there were different domains such as AI, sports, movies etc conversations in this dataset. The results are not great because the dataset is too small. Tools and Technologies Used The complete code is written in python 3. The deep learning framework that is used is Keras because of its simplicity. Numpy is used to create matrices of data. The website was made with the help of Flask because of its simplicity. HTML and CSS were used for front-end. Downloading the dependencies from requirements file pip install -r requirements.txt or  sudo pip install -r requirements.txt To run the website python main.py The website will run on server \"127.0.0.1:5000\". To retrain the model python word_seq2seq_train.py  Note: The model is trained and all the weights and id2word etc files are stored in models folder. Sample image  Thank You ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Sidharth1998/ChatBot/blob/1afc5821e101d1b53b39d86968dad963774b06b3/models/gunthercox/word-weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef616b"}, "repo_url": "https://github.com/suryotriatmojo/Ujian_Balik_Elemen_List", "repo_name": "Ujian_Balik_Elemen_List", "repo_full_name": "suryotriatmojo/Ujian_Balik_Elemen_List", "repo_owner": "suryotriatmojo", "repo_desc": "[Purwadhika] Ujian Python Fundamental JCDS Batch 4", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T05:49:52Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T07:10:44Z", "homepage": "", "size": 123, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184713243, "is_fork": false, "readme_text": "Soal Ujian Python Data Science Fundamental   Soal 2 - Membalik Posisi Elemen List Buatlah sebuah return function dengan 1 parameter yang dapat membalik urutan elemen dari suatu list. Misal terdapat suatu list: [1,2,3,4,5] maka function yang Anda buat dapat membalik urutan elemen list menjadi: [5,4,3,2,1]. Namun Anda dilarang keras untuk menggunakan cara-cara berikut: \u2b50 Cara 1. menggunakan reverse( ) method pada list a = [1, 2, 3, 4] a.reverse() print(a)  // hasil = [4, 3, 2, 1] \u2b50 Cara 2. menggunakan list slicing syntax ( [ : : -1] ) b = [5, 6, 7, 8] print(b[::-1])  // hasil = [8, 7, 6, 5] \u2b50 Cara 3: menggunakan reversed( ) function c = [9, 10, 11, 12] print(list(reversed(c)))  // hasil = [12, 11, 10, 9]    Case Flow: Saat dieksekusi, program akan mencetak nilai return function, yakni membalik posisi elemen dari list yang dimasukkan sebagai nilai parameter function, misal: print(balikPosisi([1, 2, 3, 4, 5, 6, 7, 8, 9])) print(balikPosisi(['A', 'B', 'C', 'D', 'E', 'F', 'G'])) print(balikPosisi(['Messi', 'Suarez', 'Coutinho', 'Dembele', 'Rakitic']))   Output yang diharapkan saat file diekseskusi via terminal: [9, 8, 7, 6, 5, 4, 3, 2, 1] ['G', 'F', 'E', 'D', 'C', 'B', 'A'] ['Rakitic', 'Dembele', 'Coutinho', 'Suarez', 'Messi']   Contoh screenshot:    Catatan: \u2705 Buatlah sebuah return function dengan 1 parameter: balikPosisi(x) \u274c Dilarang menggunakan reverse( ) list method \u274c Dilarang menggunakan list slicing syntax [ : : -1] \u274c Dilarang menggunakan reversed( ) function \u2705 Commit & push source code jawaban soal ini ke Github Anda, buatlah repo dengan nama Ujian_Balik_Elemen_List, kemudian lampirkan url link repo Github Anda via email ke lintang@purwadhika.com!  #HappyCoding \u263a\ufe0f Lintang Wisesa \ud83d\udc8c lintangwisesa@ymail.com Facebook | Twitter | Google+ | Youtube |  GitHub | Hackster ", "has_readme": true, "readme_language": "Indonesian", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef616c"}, "repo_url": "https://github.com/james94/P4-Behavioral-Cloning-CarND", "repo_name": "P4-Behavioral-Cloning-CarND", "repo_full_name": "james94/P4-Behavioral-Cloning-CarND", "repo_owner": "james94", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T00:10:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T23:36:08Z", "homepage": null, "size": 265608, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184832197, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/james94/P4-Behavioral-Cloning-CarND/blob/26e6ca482faaaf5fe97cd95c2ad4bfa24e14c9bb/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef616d"}, "repo_url": "https://github.com/Raysuner/Library-Subscribe", "repo_name": "Library-Subscribe", "repo_full_name": "Raysuner/Library-Subscribe", "repo_owner": "Raysuner", "repo_desc": "\u5173\u4e8e\u5b66\u6821\u56fe\u4e66\u9986\u81ea\u52a8\u9884\u7ea6\u7a0b\u5e8f\u7684\u5b9e\u73b0", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-25T12:58:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T09:30:49Z", "homepage": null, "size": 21951, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184730492, "is_fork": false, "readme_text": "\u56fe\u4e66\u9986\u81ea\u52a8\u9884\u7ea6\u7684\u5b9e\u73b0 \u57fa\u672c\u539f\u7406 \u901a\u8fc7selenium\u5de5\u5177\u767b\u5f55\u56fe\u4e66\u9986\u9884\u7ea6\u7f51\u5740\uff0c\u83b7\u53d6\u4e00\u5b9a\u91cf\u7684\u9a8c\u8bc1\u7801\u56fe\u7247\uff0c\u7136\u540e\u5bf9\u56fe\u7247\u8fdb\u884c\u7070\u5ea6\u3001\u4e8c\u503c\u5316\u3001\u5206\u5272\u3001\u8c03\u6574\u5927\u5c0f\u6700\u7ec8\u79f0\u4e3a28x28\u7684\u5355\u5b57\u7b26\u56fe\u7247\u7684\u8bad\u7ec3\u6837\u672c\uff0c\u7136\u540e\u8fdb\u884c\u5f52\u4e00\u5316\u548conehot\u7f16\u7801\u6279\u91cf\u8f93\u5165\u56fe\u7247\u548c\u6807\u7b7e\u8fdb\u5165\u795e\u7ecf\u7f51\u7edc\uff0c\u8bad\u7ec3\u597d\u6a21\u578b\uff0c\u968f\u540e\u5373\u53ef\u5229\u7528\u6a21\u578b\u8bc6\u522b\u9a8c\u8bc1\u7801  \u524d\u671f\u5de5\u4f5c   \u6b65\u9aa4\u4e00\uff1a\u73af\u5883\u914d\u7f6e \u5b89\u88c5keras\u3001tensorflow\u6846\u67b6\uff0cPython3\u73af\u5883\u5b89\u88c5   \u6b65\u9aa4\u4e8c\uff1a\u751f\u6210\u9a8c\u8bc1\u7801   python3 captcha_gen.py    \u6b65\u9aa4\u4e09\uff1a\u6279\u91cf\u5904\u7406\u9a8c\u8bc1\u7801   python3 captcha_process.py    \u6b65\u9aa4\u56db\uff1a\u7edf\u4e00\u56fe\u7247\u7684\u5927\u5c0f   python3 captcha_resize.py    \u6b65\u9aa4\u4e94\uff1a\u8bad\u7ec3\u6a21\u578b   python3 captcha_train.py    \u6b65\u9aa4\u516d\uff1a\u8bc6\u522b\u9a8c\u8bc1\u7801   python3 captcha_predict.py    \u6700\u7ec8\u5b9e\u73b0 \u6700\u7ec8\u7528\u4e8e\u56fe\u4e66\u9986\u9884\u7ea6\u5904\u7406\u7684\u7a0b\u5e8f\u5c31\u662f\u4e3b\u7a0b\u5e8f\uff1amain.py python3 main.py  \u7531\u4e8e\u65f6\u95f4\u7d27\u8feb\uff0c\u7a0b\u5e8f\u53ea\u5b9e\u73b0\u4e86\u6240\u9700\u4e3b\u8981\u529f\u80fd\uff0c\u8fd8\u6709\u5f88\u591a\u7ec6\u8282\u6709\u5f85\u5b8c\u5584\uff0c\u4ee5\u540e\u6709\u65f6\u95f4\u518d\u6162\u6162\u5b8c\u5584 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef616e"}, "repo_url": "https://github.com/Zenodia/MelanomaWebApp", "repo_name": "MelanomaWebApp", "repo_full_name": "Zenodia/MelanomaWebApp", "repo_owner": "Zenodia", "repo_desc": "WebApp to serve the melanoma model trained on keras (with tf backend)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-14T06:45:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T10:55:09Z", "homepage": null, "size": 225, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184740498, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef616f"}, "repo_url": "https://github.com/tapioho/oneshot-learning-environmental-audio", "repo_name": "oneshot-learning-environmental-audio", "repo_full_name": "tapioho/oneshot-learning-environmental-audio", "repo_owner": "tapioho", "repo_desc": "Oneshot Learning with Siamese Networks for Environmental Audio", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T11:38:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T15:56:26Z", "homepage": null, "size": 239, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184783217, "is_fork": false, "readme_text": "Oneshot Learning with Siamese Networks for Environmental Audio The purpose of this reposirory is to provide a working example of a oneshot learning implementation utilizing siamese networks for environmental audio classification. This code was done as part of a BSc thesis at Tampere University. Table of contents  Dependencies Dataset Model How to use  Dependencies   Keras Hyperopt LibROSA scikit-learn Matplotlib  Dataset  For this example script, the ESC-50 dataset is used. The dataset is available here: ESC-50 Clone the repository to the root of this repository, or alternative change the data path variable in main.py or parameter_optimization.py Model  The model consists of two convolutional input networks, followed by a merging layer and a final output layer. The input networks share the same architecture and weights in order to act as identical encoding layers for both inputs. This also means that the weights are updated simultaneously for both networks during training. The basic idea of the model is illustrated below.  How to use  Download ESC-50 and move it to the root of this repository (or change the data path variable in main.py or parameter_optimization.py) Train and evaluate a siamese network  Run main.py. The script should start by setting up the environment, reading audio files and calculating a mel-log scaled spectrogram for each audio sample. The spectra are then saved for future, i.e. the next time the script is run the spectra are not required to be calculated. Next, the script will start the training procedure. If a previously saved model is found, the training is continued from that. By default 40 classes are used for training, 5 for validation and 5 for evaluation. Change the split size at the start of the script to experiment with different splits. Batch size is a limiting factor here (change if needed), since a single training sample consists of two spectra. Negative-to-positive ratio refers to the ratio between different pairs and similar pairs, since the number of negative pairs can be made significantly higher than positive pairs. The actual training is done inside a for-loop, where a model is trained for a single epoch and validated with the one-shot task. Early stopping and best model checkpoint are also implemented here. Finally, the results for the evaluation are calculated, visualized and saved to results folder.  Hyperparameter optimization Currently the default parameters for a siamese network instance are set to previously optimized parameters. If additional optimization is desired (e.g. for new parameters), parameter_optimization.py can be executed.  Run parameter_optimization.py. The script should start fitting a model to parameters chosen from the parameter space defined at the top of the file. After training, the model is evaluated using validation data. The one-shot classification score is used as the optimized objective. The next parameters will be chosen based on previous iterations. Additionally, the current state of the optimizer is saved. If a saved state is found at the start of a new iteration, it is loaded and continued from. The best parameters so far are saved after each iteration. The optimization can be stopped at any point, and the current best parameters can be loaded from checkpoint folder.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6170"}, "repo_url": "https://github.com/A-Jacobson/shiba", "repo_name": "shiba", "repo_full_name": "A-Jacobson/shiba", "repo_owner": "A-Jacobson", "repo_desc": "A simple, flexible, pytorch training loop.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T06:38:48Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T23:41:52Z", "homepage": "", "size": 4915, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 184832591, "is_fork": false, "readme_text": "shiba A simple, flexible, pytorch trainer. Lighter (just a trainer) and lower level than fastai, higher level than ignite. There are many like it, but I've tried them all and wasn't satisfied. So, I'm shamelessly stealing the best (my opinion) parts of each. Features  callbacks/fit api (keras/sklearn) learning rate finder (fastai) one_cycle (fastai) mixed precision training (apex) process_functions/process_function zoo (ignite/me) output_transforms for metrics (ignite) tensorboard prediction vis functions (me)  Install pip install -U git+https://github.com/A-Jacobson/shiba.git Train resnet18 on CIFAR10 with tensorboard logging, Checkpointing, and a customer Metric. from torch import nn from torchvision.datasets import CIFAR10 from torchvision.transforms import ToTensor from torchvision.models import resnet18  from shiba import Trainer from shiba.callbacks import TensorBoard, Save, Metric from shiba.vis import vis_classify from shiba.metrics import categorical_accuracy  train_dataset = CIFAR10('data', train=True, download=True, transform=ToTensor()) val_dataset = CIFAR10('data', train=False, transform=ToTensor())  model = resnet18() model.fc = nn.Linear(512, 10) criterion = nn.CrossEntropyLoss()       trainer = Trainer(model, criterion)  trainer.find_lr(train_dataset) # prints lr finder graph  callbacks = [TensorBoard(log_dir='runs/shiba-test-cifar', vis_function=vis_classify),              Metric(name='accuracy', score_func=categorical_accuracy),              Save('weights/cifar', monitor='val_loss')]  trainer.fit_one_cycle(train_dataset, val_dataset, epochs=10, max_lr=1e-3, callbacks=callbacks) Write your own training steps and validation steps. shiba comes with sensible default steps that can be easily overridden by passing your own train_step and/or val_step functions to the constructor. def default_train_step(trainer, batch):     inputs, targets = batch     inputs = inputs.to(trainer.device, non_blocking=True)     targets = targets.to(trainer.device, non_blocking=True)     outputs = trainer.model(inputs)     loss = trainer.criterion(outputs, targets)     return dict(loss=loss,                 inputs=inputs,                 outputs=outputs,                 targets=targets)                  def rnn_step(trainer, batch):     \"\"\"An Example RNN step, output is saved to trainer.out\"\"\"     hidden = repackage_hidden(trainer.out['hidden'])     inputs, targets = batch  # inputs.shape : (seq_len, batch_size)     outputs, hidden = trainer.model(inputs, hidden)     seq_len, batch_size, vocab_size = outputs.shape     loss = trainer.criterion(outputs.view(-1, vocab_size), targets.view(-1))      return dict(loss=loss,                 inputs=inputs,                 outputs=outputs,                 hidden=hidden,                 targets=targets)   trainer = Trainer(model, criterion, train_step=rnn_step) Use Callbacks to easily add support for logging, Progress bars, metrics, and learning rate schedulers. class ProgressBar(Callback):     def __init__(self):         self.train_pbar = None         self.epoch_pbar = None      def on_train_begin(self, trainer):         self.train_pbar = tqdm(total=trainer.epochs, unit='epochs')      def on_epoch_begin(self, trainer):         self.epoch_pbar = tqdm(total=trainer.num_batches, unit='b')      def on_epoch_end(self, trainer):         self.train_pbar.update()         self.epoch_pbar.close()      def on_batch_end(self, trainer):         self.epoch_pbar.update()         self.epoch_pbar.set_postfix(trainer.metrics)      def on_eval_end(self, trainer):         self.epoch_pbar.set_postfix(trainer.metrics)      def on_train_end(self, trainer):         self.train_pbar.close()  ", "has_readme": true, "readme_language": "English", "repo_tags": ["pytorch", "deep-learning", "nlp", "computer-vision"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518937eb8d66358ef6171"}, "repo_url": "https://github.com/PtitDoudoux/BobRossIA", "repo_name": "BobRossIA", "repo_full_name": "PtitDoudoux/BobRossIA", "repo_owner": "PtitDoudoux", "repo_desc": "A command line tool that transfer the style of an image to another with ML.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-03T11:50:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T09:56:59Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184733811, "is_fork": false, "readme_text": "Bob Ross IA Project 5IBD ESGI 2018-2019 This project aim to paint an image in the style of another like the artist would have done it. Aka transfer the style from an image to another. This project is based on the paper A Neural Algorithm of Artistic Style Also we used the code from :  Neural Style Transfer: Creating Art with Deep Learning using tf.keras and eager execution Neural Style Transfer with Eager Execution  How to use it Command line help usage: bob_ross.py [-h] [-n NUM_ITERATIONS] [--content_weight CONTENT_WEIGHT]                    [--style_weight STYLE_WEIGHT]                    {DenseNet121,VGG16,VGG19,Xception} source_image style_image                    target  Transfer the style from an image to another  positional arguments:   {DenseNet121,VGG16,VGG19,Xception}                         The pre-trained model to use   source_image          The pathname source image to apply the style on   style_image           The pathname of the style image to use   target                The pathname where to store the new image  optional arguments:   -h, --help            show this help message and exit   -n NUM_ITERATIONS, --num_iterations NUM_ITERATIONS                         The number of iterations to apply the transfer   --content_weight CONTENT_WEIGHT                         The weight for the content loss   --style_weight STYLE_WEIGHT                         The weight for the style loss  Example bob_ross.py my_img.jpg my_style.jpg my_transformatted_img.jpg  TODO  Handle errors Test the project  Licence ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1508.06576"]}, {"_id": {"$oid": "5cf518937eb8d66358ef6172"}, "repo_url": "https://github.com/xiechen0692/YOLO-Pytorch-DOAI2019", "repo_name": "YOLO-Pytorch-DOAI2019", "repo_full_name": "xiechen0692/YOLO-Pytorch-DOAI2019", "repo_owner": "xiechen0692", "repo_desc": "Dataset from DOAI2019", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T13:13:13Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-03T03:09:39Z", "homepage": null, "size": 1267, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 184691745, "is_fork": false, "readme_text": "HW2 \u2015 Object Detection In this assignment, you are given a dataset of aerial images. Your task is to detect and classify the objects present in the images by determining their bounding boxes.  For more details, please click this link to view the slides of HW2. Usage To start working on this assignment, you should clone this repository into your local machine by using the following command. git clone https://github.com/dlcv-spring-2019/hw2-<username>.git  Note that you should replace <username> with your own GitHub username. Dataset In the starter code of this repository, we have provided a shell script for downloading and extracting the dataset for this assignment. For Linux users, simply use the following command. bash ./get_dataset.sh  The shell script will automatically download the dataset and store the data in a folder called hw2_train_val. Note that this command by default only works on Linux. If you are using other operating systems, you should download the dataset from this link and unzip the compressed file manually.  \u26a0\ufe0f IMPORTANT NOTE \u26a0\ufe0f You should keep a copy of the dataset only in your local machine. DO NOT upload the dataset to this remote repository. If you extract the dataset manually, be sure to put them in a folder called hw2_train_val under the root directory of your local repository so that it will be included in the default .gitignore file.  Evaluation To evaluate your model, you can run the provided evaluation script provided in the starter code by using the following command. python3 hw2_evaluation_task.py <PredictionDir> <AnnotationDir>   <PredictionDir> should be the directory to output your prediction files (e.g. hw2_train_val/val1500/labelTxt_hbb_pred/) <AnnotationDir> should be the directory of ground truth (e.g. hw2_train_val/val1500/labelTxt_hbb/)  Note that your predicted label file should have the same filename as that of its corresponding ground truth label file (both of extension .txt). Visualization To visualization the ground truth or predicted bounding boxes in an image, you can run the provided visualization script provided in the starter code by using the following command. python3 visualize_bbox.py <image.jpg> <label.txt>  Submission Rules Deadline 108/04/17 (Wed.) 01:00 AM Late Submission Policy You have a three-day delay quota for the whole semester. Once you have exceeded your quota, the credit of any late submission will be deducted by 30% each day. Note that while it is possible to continue your work in this repository after the deadline, we will by default grade your last commit before the deadline specified above. If you wish to use your quota or submit an earlier version of your repository, please contact the TAs and let them know which commit to grade. Academic Honesty  Taking any unfair advantages over other class members (or letting anyone do so) is strictly prohibited. Violating university policy would result in an F grade for this course (NOT negotiable). If you refer to some parts of the public code, you are required to specify the references in your report (e.g. URL to GitHub repositories). You are encouraged to discuss homework assignments with your fellow class members, but you must complete the assignment by yourself. TAs will compare the similarity of everyone\u2019s submission. Any form of cheating or plagiarism will not be tolerated and will also result in an F grade for students with such misconduct.  Submission Format Aside from your own Python scripts and model files, you should make sure that your submission includes at least the following files in the root directory of this repository:  hw2_<StudentID>.pdf The report of your homework assignment. Refer to the \"Grading Policy\" section in the slides for what you should include in the report. Note that you should replace <StudentID> with your student ID, NOT your GitHub username. hw2.sh The shell script file for running your YoloV1-vgg16bn model. hw2_best.sh The shell script file for running your improved model.  We will run your code in the following manner: bash ./hw2.sh $1 $2 bash ./hw2_best.sh $1 $2  where $1 is the testing images directory (e.g. test/images), and $2 is the output prediction directory (e.g. test/labelTxt_hbb_pred/ ). Packages Below is a list of packages you are allowed to import in this assignment:  python: 3.5+ tensorflow: 1.13 keras: 2.2+ torch: 1.0 h5py: 2.9.0 numpy: 1.16.2 pandas: 0.24.0 torchvision, cv2, matplotlib, skimage, Pillow, scipy The Python Standard Library  Note that using packages with different versions will very likely lead to compatibility issues, so make sure that you install the correct version if one is specified above. E-mail or ask the TAs first if you want to import other packages. Remarks  If your model is larger than GitHub\u2019s maximum capacity (100MB), you can upload your model to another cloud service (e.g. Dropbox). However, your shell script files should be able to download the model automatically. For a tutorial on how to do this using Dropbox, please click this link. DO NOT hard code any path in your file or script, and the execution time of your testing code should not exceed an allowed maximum of 10 minutes. If we fail to run your code due to not following the submission rules, you will receive 0 credit for this assignment.  Q&A If you have any problems related to HW2, you may  Use TA hours (please check course website for time/location) Contact TAs by e-mail (ntudlcvta2019@gmail.com) Post your question in the comment section of this post  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://vllab.ee.ntu.edu.tw/dlcv.html", "http://www.numpy.org/"], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e2"}, "repo_url": "https://github.com/CyberZHG/keras-ordered-neurons", "repo_name": "keras-ordered-neurons", "repo_full_name": "CyberZHG/keras-ordered-neurons", "repo_owner": "CyberZHG", "repo_desc": "Ordered Neurons LSTM", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-03T12:13:24Z", "repo_watch": 10, "repo_forks": 6, "private": false, "repo_created_at": "2019-05-06T06:49:25Z", "homepage": "https://pypi.org/project/keras-ordered-neurons/", "size": 56, "language": "Python", "has_wiki": false, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185139554, "is_fork": false, "readme_text": "Keras Ordered Neurons LSTM           [\u4e2d\u6587|English] Unofficial implementation of ON-LSTM. Install pip install keras-ordered-neurons Usage Basic Same as LSTM except that an extra argument chunk_size should be given: from keras.models import Sequential from keras.layers import Embedding, Bidirectional, Dense from keras_ordered_neurons import ONLSTM  model = Sequential() model.add(Embedding(input_shape=(None,), input_dim=10, output_dim=100)) model.add(Bidirectional(ONLSTM(units=50, chunk_size=5))) model.add(Dense(units=2, activation='softmax')) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') model.summary() DropConnect Set recurrent_dropconnect to a non-zero value to enable drop-connect for recurrent weights: from keras_ordered_neurons import ONLSTM  ONLSTM(units=50, chunk_size=5, recurrent_dropconnect=0.2) Expected Split Points Set return_splits to True if you want to know the expected split points of master forget gate and master input gate. from keras.models import Model from keras.layers import Input, Embedding from keras_ordered_neurons import ONLSTM  inputs = Input(shape=(None,)) embed = Embedding(input_dim=10, output_dim=100)(inputs) outputs, splits = ONLSTM(units=50, chunk_size=5, return_sequences=True, return_splits=True)(embed) model = Model(inputs=inputs, outputs=splits) model.compile(optimizer='adam', loss='mse') model.summary(line_length=120) tf.keras Add TF_KERAS=1 to environment variables if you are using tensorflow.python.keras. ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "recurrent-neural-networks", "on-lstm"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e3"}, "repo_url": "https://github.com/mingsjtu/Human-Action-Recognition-from-Skeleton-Data", "repo_name": "Human-Action-Recognition-from-Skeleton-Data", "repo_full_name": "mingsjtu/Human-Action-Recognition-from-Skeleton-Data", "repo_owner": "mingsjtu", "repo_desc": "A Simple But High-accuracy LSTM for human Action Recognition ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-17T08:29:15Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T01:22:17Z", "homepage": null, "size": 98, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185102653, "is_fork": false, "readme_text": "Human-Action-Recognition-from-Skeleton-Data A Simple But High-accuracy LSTM for human Action Recognition Code structure   matlab_m/: transform the given dataset(NTU RGB+D) to your need , from \".skeleton\" to \".mat\"  demo.m: an example for you to transform the dataset using the given functions .You shall verify the \"fileFolder\",\"dirOutput\",and \"savepath\" classfile.m: divide the \"*.mat\" file according to their class. read_skeleton_file.m: a function to read the skeleton files (given by NTU RGB dataset) savetomat.m: a function to save the skeleton data from skeleton files to mat files show_skeleton_on_depthmaps.m: a function to show the skeleton information on the depthmaps(thanks to the NTU RGB+D dataset) show_skeleton_on_IR_frames.m: a function to show the skeleton information on the IR frames(thanks to the NTU RGB+D dataset) show_skeleton_on_RGB_frames.m: a function to show the skeleton information on the RGB frames(thanks to the NTU RGB+D dataset)    lstm_py/: the train and test python file using tensorflow lib.  main.py: the train python file using tensorflow. evaluate.py: the test file to evaluate your model perfermance. mtop.py: transform the skeleton files form \".mat\" to \".npy\" for python files . Also, you may use it for seperate train and test set . model_lstm/: well-trained model of lstm .    keras: the train and test python file using keras lib.   main.py: an example for you to transform the dataset using the given functions .You shall verify the \"train_file\", and \"test_file\" Requirements     code only tested on linux system (ubuntu 16.04)   Python 3 (Anaconda 3.6.3 specifically) with numpy and matplotlib   Tensorflow   keras   matlab model structure  To prepare using the given data by NTU RGB+D Using matlab (from \".skeleton\" to \".mat\") In file demo.m fileFolder=['D:\\research\\ntuRGB\\ske_f\\',num2str(t),'\\'];%using your own dataset path savepath=['D:\\research\\ntuRGB\\mat_f\\',num2str(t),'\\'];%using your own save path In file classfile.m SOURCE_PATH_t =[ 'D:\\research\\ntuRGB\\mat_f\\',num2str(i),'\\'];%using your own \"*.mat\" files path   DST_PATH_t1 = [ 'D:\\research\\ntuRGB\\mat_f\\',num2str(i),'\\test'];%using your own wanted test set saved path DST_PATH_t2 = [ 'D:\\research\\ntuRGB\\mat_f\\',num2str(i),'\\train'];%using your own wanted train set saved path matlab demo.m matlab classfile.m Using python (from \".mat\" to \".npy\") In file mtop.py trainpath='./CS/train/'#verify your train data files forder here (\"*.mat\" file) testpath='./CS/test/'#verify your test data files forder here (\"*.mat\" file) Using Tensorflow To train In file main.py train_file='CV_20/train' #verify your train data files forder here  test_file='CV_20/test' #verify your train data files forder here  model_file=\"model/my-model.meta\"#verify your train model data file model_path=\"model/\"#verify your train model data folder   python lstm_py/main.py   you will get your own model saved in the \"model/\" To test In file evaluate.py train_file='CV_20/train' #verify your train data files forder here  test_file='CV_20/test' #verify your train data files forder here  model_file=\"model/my-model.meta\"#verify your train model data file model_path=\"model/\"#verify your train model data folder   python lstm_py/evaluate.py Using keras To train and test In file main.py train_file='CV_20/train' #verify your train data files forder here  test_file='CV_20/test' #verify your train data files forder here  model_file=\"model/my-model.meta\"#verify your train model data file model_path=\"model/\"#verify your train model data folder python keras/main.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e4"}, "repo_url": "https://github.com/apllolulu/keras-applications", "repo_name": "keras-applications", "repo_full_name": "apllolulu/keras-applications", "repo_owner": "apllolulu", "repo_desc": "keras-applications", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T09:04:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T08:58:34Z", "homepage": null, "size": 290, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185160497, "is_fork": false, "readme_text": "Keras Applications  Keras Applications is the applications module of the Keras deep learning library. It provides model definitions and pre-trained weights for a number of popular archictures, such as VGG16, ResNet50, Xception, MobileNet, and more. Read the documentation at: https://keras.io/applications/ Keras Applications may be imported directly from an up-to-date installation of Keras: from keras import applications  Keras Applications is compatible with Python 2.7-3.6 and is distributed under the MIT license. Performance  The top-k errors were obtained using Keras Applications with the TensorFlow backend on the 2012 ILSVRC ImageNet validation set and may slightly differ from the original ones. The input size used was 224x224 for all models except NASNetLarge (331x331), InceptionV3 (299x299), InceptionResNetV2 (299x299), and Xception (299x299).  Top-1: single center crop, top-1 error Top-5: single center crop, top-5 error 10-5: ten crops (1 center + 4 corners and those mirrored ones), top-5 error Size: rounded the number of parameters when include_top=True Stem: rounded the number of parameters when include_top=False        Top-1 Top-5 10-5 Size Stem References     VGG16 28.732 9.950 8.834 138.4M 14.7M [paper] [tf-models]   VGG19 28.744 10.012 8.774 143.7M 20.0M [paper] [tf-models]   ResNet50 25.072 7.940 6.828 25.6M 23.6M [paper] [tf-models] [torch] [caffe]   ResNet101 23.580 7.214 6.092 44.7M 42.7M [paper] [tf-models] [torch] [caffe]   ResNet152 23.396 6.882 5.908 60.4M 58.4M [paper] [tf-models] [torch] [caffe]   ResNet50V2 24.040 6.966 5.896 25.6M 23.6M [paper] [tf-models] [torch]   ResNet101V2 22.766 6.184 5.158 44.7M 42.6M [paper] [tf-models] [torch]   ResNet152V2 21.968 5.838 4.900 60.4M 58.3M [paper] [tf-models] [torch]   ResNeXt50 22.260 6.190 5.410 25.1M 23.0M [paper] [torch]   ResNeXt101 21.270 5.706 4.842 44.3M 42.3M [paper] [torch]   InceptionV3 22.102 6.280 5.038 23.9M 21.8M [paper] [tf-models]   InceptionResNetV2 19.744 4.748 3.962 55.9M 54.3M [paper] [tf-models]   Xception 20.994 5.548 4.738 22.9M 20.9M [paper]   MobileNet(alpha=0.25) 48.418 24.208 21.196 0.5M 0.2M [paper] [tf-models]   MobileNet(alpha=0.50) 35.708 14.376 12.180 1.3M 0.8M [paper] [tf-models]   MobileNet(alpha=0.75) 31.588 11.758 9.878 2.6M 1.8M [paper] [tf-models]   MobileNet(alpha=1.0) 29.576 10.496 8.774 4.3M 3.2M [paper] [tf-models]   MobileNetV2(alpha=0.35) 39.914 17.568 15.422 1.7M 0.4M [paper] [tf-models]   MobileNetV2(alpha=0.50) 34.806 13.938 11.976 2.0M 0.7M [paper] [tf-models]   MobileNetV2(alpha=0.75) 30.468 10.824 9.188 2.7M 1.4M [paper] [tf-models]   MobileNetV2(alpha=1.0) 28.664 9.858 8.322 3.5M 2.3M [paper] [tf-models]   MobileNetV2(alpha=1.3) 25.320 7.878 6.728 5.4M 3.8M [paper] [tf-models]   MobileNetV2(alpha=1.4) 24.770 7.578 6.518 6.2M 4.4M [paper] [tf-models]   DenseNet121 25.028 7.742 6.522 8.1M 7.0M [paper] [torch]   DenseNet169 23.824 6.824 5.860 14.3M 12.6M [paper] [torch]   DenseNet201 22.680 6.380 5.466 20.2M 18.3M [paper] [torch]   NASNetLarge 17.502 3.996 3.412 93.5M 84.9M [paper] [tf-models]   NASNetMobile 25.634 8.146 6.758 7.7M 4.3M [paper] [tf-models]    Reference implementations from the community Object detection and segmentation  SSD by @rykov8 [paper] YOLOv2 by @allanzelener [paper] YOLOv3 by @qqwweee [paper] Mask RCNN by @matterport [paper] U-Net by @zhixuhao [paper] RetinaNet by @fizyr [paper]  Sequence learning  Seq2Seq by @farizrahman4u WaveNet by @basveeling [paper]  Reinforcement learning  keras-rl by @keras-rl RocAlphaGo by @Rochester-NRT [paper]  Generative adversarial networks  Keras-GAN by @eriklindernoren  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1409.1556", "https://arxiv.org/abs/1409.1556", "https://arxiv.org/abs/1512.03385", "https://arxiv.org/abs/1512.03385", "https://arxiv.org/abs/1512.03385", "https://arxiv.org/abs/1603.05027", "https://arxiv.org/abs/1603.05027", "https://arxiv.org/abs/1603.05027", "https://arxiv.org/abs/1611.05431", "https://arxiv.org/abs/1611.05431", "https://arxiv.org/abs/1512.00567", "https://arxiv.org/abs/1602.07261", "https://arxiv.org/abs/1610.02357", "https://arxiv.org/abs/1704.04861", "https://arxiv.org/abs/1704.04861", "https://arxiv.org/abs/1704.04861", "https://arxiv.org/abs/1704.04861", "https://arxiv.org/abs/1801.04381", "https://arxiv.org/abs/1801.04381", "https://arxiv.org/abs/1801.04381", "https://arxiv.org/abs/1801.04381", "https://arxiv.org/abs/1801.04381", "https://arxiv.org/abs/1801.04381", "https://arxiv.org/abs/1608.06993", "https://arxiv.org/abs/1608.06993", "https://arxiv.org/abs/1608.06993", "https://arxiv.org/abs/1707.07012", "https://arxiv.org/abs/1707.07012", "https://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1612.08242", "https://arxiv.org/abs/1703.06870", "https://arxiv.org/abs/1505.04597", "https://arxiv.org/abs/1708.02002", "https://arxiv.org/abs/1609.03499"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e5"}, "repo_url": "https://github.com/siked/keras-FCN_image", "repo_name": "keras-FCN_image", "repo_full_name": "siked/keras-FCN_image", "repo_owner": "siked", "repo_desc": "keras-FCN_image", "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T13:20:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T12:20:27Z", "homepage": null, "size": 95646, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185191084, "is_fork": false, "readme_text": "Image Segmentation Keras : Implementation of Segnet, FCN, UNet, PSPNet and other models in Keras. Implementation of various Deep Image Segmentation models in keras.    Our Other Repositories  Attention based Language Translation in Keras Ladder Network in Keras  model achives 98% test accuracy on MNIST with just 100 labeled examples  Models Following models are supported:    model_name Base Model Segmentation Model     fcn_8 Vanilla CNN FCN8   fcn_32 Vanilla CNN FCN8   fcn_8_vgg VGG 16 FCN8   fcn_32_vgg VGG 16 FCN32   fcn_8_resnet50 Resnet-50 FCN32   fcn_32_resnet50 Resnet-50 FCN32   fcn_8_mobilenet MobileNet FCN32   fcn_32_mobilenet MobileNet FCN32   pspnet Vanilla CNN PSPNet   vgg_pspnet VGG 16 PSPNet   resnet50_pspnet Resnet-50 PSPNet   unet_mini Vanilla Mini CNN U-Net   unet Vanilla CNN U-Net   vgg_unet VGG 16 U-Net   resnet50_unet Resnet-50 U-Net   mobilenet_unet MobileNet U-Net   segnet Vanilla CNN Segnet   vgg_segnet VGG 16 Segnet   resnet50_segnet Resnet-50 Segnet   mobilenet_segnet MobileNet Segnet    Getting Started Prerequisites  Keras 2.0 opencv for python Theano / Tensorflow / CNTK  sudo apt-get install python-opencv sudo pip install --upgrade keras Installing Install the module git clone https://github.com/divamgupta/image-segmentation-keras python setup.py install pip install will be available soon! Pre-trained models: import keras_segmentation  model = keras_segmentation.pretrained.resnet_pspnet_VOC12_v0_1() # load the pretrained model  out = model.predict_segmentation(     inp=\"voc_prepped/images_prepped_test/2007_000738.jpg\",     out_fname=out_vgg_1.png )  Preparing the data for training You need to make two folders  Images Folder - For all the training images Annotations Folder - For the corresponding ground truth segmentation images  The filenames of the annotation images should be same as the filenames of the RGB images. The size of the annotation image for the corresponding RGB image should be same. For each pixel in the RGB image, the class label of that pixel in the annotation image would be the value of the blue pixel. Example code to generate annotation images : import cv2 import numpy as np  ann_img = np.zeros((30,30,3)).astype('uint8') ann_img[ 3 , 4 ] = 1 # this would set the label of pixel 3,4 as 1  cv2.imwrite( \"ann_1.png\" ,ann_img ) Only use bmp or png format for the annotation images. Download the sample prepared dataset Download and extract the following: https://drive.google.com/file/d/0B0d9ZiqAgFkiOHR1NTJhWVJMNEU/view?usp=sharing You will get a folder named dataset1/ Using the python module You can import keras_segmentation in  your python script and use the API import keras_segmentation  model = keras_segmentation.models.unet.vgg_unet(n_classes=51 ,  input_height=416, input_width=608  )  model.train(      train_images =  \"dataset1/images_prepped_train/\",     train_annotations = \"dataset1/annotations_prepped_train/\",     checkpoints_path = \"/tmp/vgg_unet_1\" , epochs=5 )  out = model.predict_segmentation(     inp=1.png,     out_fname=\"/tmp/out.png\" )   import matplotlib.pyplot as plt plt.imshow(out)  Usage via command line You can also use the tool just using command line Visualizing the prepared data You can also visualize your prepared annotations for verification of the prepared data. python -m keras_segmentation verify_dataset \\  --images_path=\"dataset1/images_prepped_train/\" \\  --segs_path=\"dataset1/annotations_prepped_train/\"  \\  --n_classes=50 python -m keras_segmentation visualize_dataset \\  --images_path=\"dataset1/images_prepped_train/\" \\  --segs_path=\"dataset1/annotations_prepped_train/\"  \\  --n_classes=50 Training the Model To train the model run the following command: python -m keras_segmentation train \\  --checkpoints_path=\"path_to_checkpoints\" \\  --train_images=\"dataset1/images_prepped_train/\" \\  --train_annotations=\"dataset1/annotations_prepped_train/\" \\  --val_images=\"dataset1/images_prepped_test/\" \\  --val_annotations=\"dataset1/annotations_prepped_test/\" \\  --n_classes=50 \\  --input_height=320 \\  --input_width=640 \\  --model_name=\"vgg_unet\" Choose model_name from the table above Getting the predictions To get the predictions of a trained model python -m keras_segmentation predict \\  --checkpoints_path=\"path_to_checkpoints\" \\  --input_path=\"dataset1/images_prepped_test/\" \\  --output_path=\"path_to_predictions\"  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e6"}, "repo_url": "https://github.com/landerlini/FastQuantileLayer", "repo_name": "FastQuantileLayer", "repo_full_name": "landerlini/FastQuantileLayer", "repo_owner": "landerlini", "repo_desc": "Keras layer to compute the Quantile transform and its inverse", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T17:25:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T16:33:23Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185234014, "is_fork": false, "readme_text": "FastQuantileLayer FastQuantileLayer is a Layer for Keras implementing the QuantileTransform similarly to scikit-learn QuantileTransformer. A similar implementation, more precise but not bound to Keras, can be found here: https://github.com/yandexdataschool/QuantileTransformerTF/blob/master/README.md The purpose of this package is:  remove all dependencies on scikit-learn obtain an evaluation of the direct and inverse transform as fast as possible (trading some precision for performance) obtain a TensorFlow graph runnable in a Sequential model in Keras  The package is composed of two classes:  FixedBinInterpolator: intended to interpolate a point-defined function y = f(x) with equidistant x samples (x-grid) FastQuantileLayer: intended to compute the transform to preprocess the input data into a uniform- or normal-distributed variable.  Example outside Keras   ## Creates the training dataset    dataset = np.random.uniform ( 0., 1., 1000 )     ## Train the QuantileTransformer    transformer = FastQuantileLayer (output_distribution='normal')   transformer . fit ( dataset )     ## Gets a new dataset with the same distribution as the training dataset   test_dataset = tf.constant(np.random.uniform ( 0., 1., 100000 ))    ## Transform the variable into a Gaussian-distributed variable t    t = transformer . transform ( test_dataset )       [...]     ## Appiles the inverted transform to the Gaussian distributed variable t     bkwd = transformer . transform ( t, inverse=True )     ## bkwd differs from test_dataset only for computational errors    ## (order 1e-5) that can be reduced tuning the arguments of QuantileTransformer  Example within Keras   ## Creates the training dataset    dataset = np.random.uniform ( 0., 1., 1000 )     model = tf.keras.models.Sequential()   model.add ( FastQuantileLayer ( output_distribution = 'normal' ).fit ( dataset ) )   model.add ( Dense ( 10, activation = 'tanh' ) )   model.add ( Dense ( 1, activation = 'sigmoid' ) )       ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e7"}, "repo_url": "https://github.com/Heisenberg0391/Keras_text_recognition_ocr", "repo_name": "Keras_text_recognition_ocr", "repo_full_name": "Heisenberg0391/Keras_text_recognition_ocr", "repo_owner": "Heisenberg0391", "repo_desc": "\u57fa\u4e8eKeras\u7684\u6587\u672c\u56fe\u50cf\u8bc6\u522b\u7a0b\u5e8f", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-17T08:13:18Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T07:54:34Z", "homepage": null, "size": 54, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185149614, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e8"}, "repo_url": "https://github.com/LegolasShaw/YOLO3_Keras", "repo_name": "YOLO3_Keras", "repo_full_name": "LegolasShaw/YOLO3_Keras", "repo_owner": "LegolasShaw", "repo_desc": "Keras\u5b9e\u73b0\u5f00\u6e90YOLO3", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-20T09:10:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T07:31:13Z", "homepage": null, "size": 39, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185145919, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851e9"}, "repo_url": "https://github.com/eyny/deep-learning-gaming", "repo_name": "deep-learning-gaming", "repo_full_name": "eyny/deep-learning-gaming", "repo_owner": "eyny", "repo_desc": "A deep learning project to play Super Mario Bros. using Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T03:57:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T21:40:07Z", "homepage": null, "size": 8605, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185275752, "is_fork": false, "readme_text": "deep-learning-gaming This is the graduation project I prepared in school to get my B.Sc. degree in computer engineering. It learns how to play Super Mario Bros. game using deep learning which can be considered a subset of machine learning. It uses Keras for its deep learning library with TensorFlow backend. Gym Super Mario environment bundle is used for OpenAI Gym. In first stage, the best model from three different models for this problem is founded. In second stage, the model is trained on the first level of Super Mario Bros for a long duration. At the last stage, training process is stopped and the trained model is used on a different level. It is investigated if there is a significant improvement compared to a random model. Requirements  Keras 2.1.5 FCEUX 2.2.2 Gym Super Mario 0.0.7 OpenAI Gym 0.92  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/eyny/deep-learning-gaming/blob/227b6c9a97a34fe3a5925952c193b7cca1ff3068/model/weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851ea"}, "repo_url": "https://github.com/MelannieTorres-academico/ALS_image_classificator", "repo_name": "ALS_image_classificator", "repo_full_name": "MelannieTorres-academico/ALS_image_classificator", "repo_owner": "MelannieTorres-academico", "repo_desc": "Using ML and a Kaggle dataset identifies the ALS letter in the given image", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T04:25:19Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T22:17:28Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185279517, "is_fork": false, "readme_text": "ALS_image_classificator Using ML and a Kaggle dataset identifies the ALS letter in the given image Requirements  Keras Tensorflow  Dataset The dataset couldn't be uploaded to github due to memory limitations, but the original dataset is here and a smaller version ready for Keras is found here. How to run To train run python file_name.py, be sure to check the dataset path. To test send the model name without the extension: python run_tests.py model5 About the process If you want to know more about how this project was done, you can find the report here. Models The models were too heavy to be uploaded here, so you can find them here. If you train the model a new model will be generated. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851eb"}, "repo_url": "https://github.com/Leerw/test", "repo_name": "test", "repo_full_name": "Leerw/test", "repo_owner": "Leerw", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-12T02:35:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T02:30:35Z", "homepage": null, "size": 4605, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185110267, "is_fork": false, "readme_text": "DeepST DeepST: A Deep Learning Toolbox for Spatio-Temporal Data Tested on Windows Server 2012 R2. Installation DeepST uses the following dependencies:  Keras and its dependencies are required to use DeepST. Please read Keras Configuration for the configuration setting. Theano or TensorFlow, but Theano is recommended. numpy and scipy HDF5 and h5py pandas CUDA 7.5 or latest version. And cuDNN is highly recommended.  To install DeepST, cd to the DeepST folder and run the install command: python setup.py install  To install the development version: python setup.py develop  Data path The default DATAPATH variable is DATAPATH=[path_to_DeepST]/data. You may set your DATAPATH variable using # Windows set DATAPATH=[path_to_your_data]  # Linux export DATAPATH=[path_to_your_data]  License DeepST is released under the MIT License (refer to the LICENSE file for details). ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/data/TaxiBJ/BJ13_M32x32_T30_InOut.h5", "https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/data/TaxiBJ/BJ14_M32x32_T30_InOut.h5", "https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/data/TaxiBJ/BJ15_M32x32_T30_InOut.h5", "https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/data/TaxiBJ/BJ16_M32x32_T30_InOut.h5", "https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/data/TaxiBJ/BJ_Meteorology.h5", "https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/scripts/papers/AAAI17/TaxiBJ/MODEL/c3.p1.t1.resunit2.lr0.0002.best.h5", "https://github.com/Leerw/test/blob/60233b1acc5fe9118bbbb47582730102f6970199/scripts/papers/AAAI17/TaxiBJ/MODEL/c3.p1.t1.resunit2.lr0.0002.h5", "https://github.com/Leerw/test/blob/9ebd1abefb2e8198c0e6d0bf728d3454f012903c/data/TaxiNYC/NYC15_M10x20_T30_InOut.h5", "https://github.com/Leerw/test/blob/dc7530857756ebae077b3559ed8c8fb570976e4f/data/TaxiNYC/NYC_Meteorology.h5", "https://github.com/Leerw/test/blob/dc7530857756ebae077b3559ed8c8fb570976e4f/scripts/papers/AAAI17/TaxiNYC/MODEL/c3.p1.t1.resunit2.lr0.0002.best.h5"], "see_also_links": ["http://pandas.pydata.org/", "http://www.h5py.org/", "http://deeplearning.net/software/theano/install.html#install"], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851ec"}, "repo_url": "https://github.com/opalr/opal", "repo_name": "opal", "repo_full_name": "opalr/opal", "repo_owner": "opalr", "repo_desc": "Rocket League Bot", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T23:22:36Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T02:46:26Z", "homepage": null, "size": 1325, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185112210, "is_fork": false, "readme_text": "opal Rocket League Bot with keras ", "has_readme": true, "readme_language": "English", "repo_tags": ["rocket-league"], "has_h5": true, "h5_files_links": ["https://github.com/opalr/opal/blob/124ef154dd5c76714fa8ca73e75799090539f412/RLBotPythonExample/model.h5", "https://github.com/opalr/opal/blob/124ef154dd5c76714fa8ca73e75799090539f412/RLBotPythonExample/python_example/model.h5", "https://github.com/opalr/opal/blob/124ef154dd5c76714fa8ca73e75799090539f412/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851ed"}, "repo_url": "https://github.com/Anshul-Gupta24/Proxy-NCA", "repo_name": "Proxy-NCA", "repo_full_name": "Anshul-Gupta24/Proxy-NCA", "repo_owner": "Anshul-Gupta24", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T10:50:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T08:40:28Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185157258, "is_fork": false, "readme_text": "Proxy-NCA Keras implementation of the paper, \"No Fuss Distance Metric Learning using Proxies\" by Movshovitz-Attias et al., 2017. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851ee"}, "repo_url": "https://github.com/DanSeb1295/shopeeMLChallenge", "repo_name": "shopeeMLChallenge", "repo_full_name": "DanSeb1295/shopeeMLChallenge", "repo_owner": "DanSeb1295", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T08:18:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T08:04:27Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185151244, "is_fork": false, "readme_text": "shopeeMLChallenge Source code used for Transfer Learning on a Keras model used on Shopee's Image Classification Task, trained on an AWS EC2 instance. This model achieved a 83.7% precision on 18 of Shopee's product categories. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851ef"}, "repo_url": "https://github.com/bubblewu/lstm-demo", "repo_name": "lstm-demo", "repo_full_name": "bubblewu/lstm-demo", "repo_owner": "bubblewu", "repo_desc": "\u57fa\u4e8eTensorFlow\u548cKeras\u6846\u67b6\u7684LSTM\uff08\u957f\u77ed\u65f6\u8bb0\u5fc6\u7f51\u7edc\uff09\u5b9e\u73b0Demo", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-07T01:48:40Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T09:31:03Z", "homepage": null, "size": 30476, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185166123, "is_fork": false, "readme_text": "\u4ecb\u7ecd \u57fa\u4e8eTensorFlow\u548cKeras\u6846\u67b6\u7684LSTM\uff08\u957f\u77ed\u65f6\u8bb0\u5fc6\u7f51\u7edc\uff09\u5b9e\u73b0Demo TensorFlow \u5b9e\u4f8b\uff1a 4\u4f4d\uff08\u6570\u5b57\u548c\u5b57\u6bcd\u7ec4\u5408\uff09\u9a8c\u8bc1\u7801\u8bc6\u522b ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f0"}, "repo_url": "https://github.com/yeLer/keras_mnist", "repo_name": "keras_mnist", "repo_full_name": "yeLer/keras_mnist", "repo_owner": "yeLer", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T12:36:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T12:33:28Z", "homepage": null, "size": 143, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185193089, "is_fork": false, "readme_text": "keras on mnist \u4e00. \u7b80\u4ecb  \u672c\u4ee3\u7801\u6f14\u793a\u4e86keras\u4f7f\u7528vgg16\u7f51\u7edc\u6a21\u578b\u5728mnist\u6570\u636e\u96c6\u4e0a\u7684\u8bad\u7ec3\u53ca\u6d4b\u8bd5\u8fc7\u7a0b\uff0c\u5728\u9879\u76ee\u5f53\u4e2d\u6570\u636e\u96c6\u56fe\u7247\u91c7\u7528\u4e8610000\u5f20\uff0c\u6d4b\u8bd5\u96c6\u56fe\u7247\u4f7f\u7528\u4e862000\u5f20\uff0c\u51c6\u786e\u7387\u8fbe\u523099%\u3002   \u76f4\u63a5\u5c0610000\u5f20\u8bad\u7ec3\u56fe\u7247\u52a0\u5165\u53c2\u52a0\u8bad\u7ec3\uff0c2000\u5f20\u56fe\u7247\u52a0\u5165\u6d4b\u8bd5  \u4e8c. \u4ee3\u7801\u8be6\u89e3 1. input_data.py  \u8f93\u5165\u53c2\u6570\u5305\u62ec\u56fe\u7247\u8def\u5f84\u53ca\u9700\u8981\u56fe\u7247\u6587\u4ef6\u4e2a\u6570\uff0c\u8fd4\u56de\u5bf9\u5e94\u7684\u56fe\u7247\u6587\u4ef6\u5217\u8868\u548c\u6807\u7b7e\uff0c\u5168\u90e8\u4e00\u8d77\u8fd4\u56de\uff08\u8fd9\u6837\u5e94\u8be5\u6bd4\u8f83\u6d88\u8017\u5185\u5b58\uff09  2.cnn_mnist.py  \u5177\u4f53\u5305\u62ec\u6a21\u578b\u5b9a\u4e49\uff0c\u6570\u636e\u52a0\u8f7d\u4ee5\u540e\u7684\u4e8c\u6b21\u5904\u7406\uff0c\u8bad\u7ec3\u53ca\u6d4b\u8bd5\u7684\u8fc7\u7a0b  3. \u8bad\u7ec3  \u76f8\u5173\u53c2\u6570\u53ef\u5728args\u91cc\u9762\u8bbe\u7f6e\uff0cMNIST\u6570\u636e\u96c6\u662f\u56fe\u7247\u683c\u5f0f training/ 0/ 1/ 2/ ... testing/ 0/ ...  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f1"}, "repo_url": "https://github.com/forgi86/gym-control", "repo_name": "gym-control", "repo_full_name": "forgi86/gym-control", "repo_owner": "forgi86", "repo_desc": "A collection of control theory enviroments compatible with the gym toolkit", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T14:41:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T11:09:06Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185180592, "is_fork": false, "readme_text": "This repository contains a PIP package which contains a collection of control theory environments compatible with the OpenAI gym toolkit. Installation Install the OpenAI gym. Then install this package via pip install -e .  Usage import gym import gym_control  env = gym.make(\"CartPoleControl-v0\")  Inspired from examples found at https://github.com/matthiasplappert/keras-rl/tree/master/examples The Environments CartPoleControl A cart-pole system. The control objective is to stabilize the pendulum in the upright vertical position (around $phi=0$).  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f2"}, "repo_url": "https://github.com/mizancse7462/headline_Generation", "repo_name": "headline_Generation", "repo_full_name": "mizancse7462/headline_Generation", "repo_owner": "mizancse7462", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T17:20:12Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T15:32:03Z", "homepage": null, "size": 771, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185224185, "is_fork": false, "readme_text": "headline_Generation How to run Software:  Install python: Official website Install anaconda: Official website Install Keras Install Tensorflow Neural networks are computations heavy, GPU configuration is recommended.  Data We have used All the news dataset from kaggle Extract only title and content from the dataset. Later, we tokenized the title,content and saved it as a pickle file. We divided our data set into train, validation and test sets respectively. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f3"}, "repo_url": "https://github.com/bombark/sound_recognition", "repo_name": "sound_recognition", "repo_full_name": "bombark/sound_recognition", "repo_owner": "bombark", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T09:13:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T11:01:29Z", "homepage": null, "size": 1033, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185179518, "is_fork": false, "readme_text": "sound_recognition Dependencies pip3 install tensorflow keras numpy sklearn argparse librosa Install the dataset mv tram_demo.tar.gz ./project/ cd ./project/ tar xvf tram_demo.tar.gz Execute cd ./project/ ./program ./test_files/tram-2018-11-30-15-30-17.wav [-c cnn|nn|knn|svm ] Obs: The CNN classifier is ever trained and loaded automatically, but the others classifiers needed to be trained. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f4"}, "repo_url": "https://github.com/SrujanaN/IMDb_Reuters", "repo_name": "IMDb_Reuters", "repo_full_name": "SrujanaN/IMDb_Reuters", "repo_owner": "SrujanaN", "repo_desc": "Accuracy calculation for 2 datasets provided by keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T20:29:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T20:28:43Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185267000, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f5"}, "repo_url": "https://github.com/AVajpayeeJr/transfer-snli-keras", "repo_name": "transfer-snli-keras", "repo_full_name": "AVajpayeeJr/transfer-snli-keras", "repo_owner": "AVajpayeeJr", "repo_desc": "Reimplementation of Conneau et. al. 2017 in Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T15:47:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T15:33:07Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185224382, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f6"}, "repo_url": "https://github.com/s123600g/Keras_FaceRecognition_Demo", "repo_name": "Keras_FaceRecognition_Demo", "repo_full_name": "s123600g/Keras_FaceRecognition_Demo", "repo_owner": "s123600g", "repo_desc": "\u4f7f\u7528keras\u5efa\u7f6eCNN\u795e\u7d93\u7db2\u7d61\u6a21\u578b\u904b\u7528Tensorflow\u4e4b\u81c9\u90e8\u8fa8\u8b58\u5be6\u4f8b", "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T04:15:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T13:55:16Z", "homepage": null, "size": 2617, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185207152, "is_fork": false, "readme_text": " \u4f7f\u7528Keras\u5efa\u7f6eCNN\u795e\u7d93\u7db2\u7d61\u6a21\u578b\u642d\u914d\u4e4b\u81c9\u90e8\u8fa8\u8b58\u5be6\u4f8b \u672c\u5c08\u6848\u53ea\u662f\u57fa\u790e\u5be6\u6230\u61c9\u7528\u7b46\u8a18\uff0c\u91dd\u5c0d\u6a21\u578b\u53c3\u6578\u8a2d\u5b9a\u914d\u7f6e\u4e26\u6c92\u6709\u7279\u5225\u6df1\u5165\u8ffd\u7a76 \u3002 \u904b\u7528Keras\u64cd\u4f5cTensorflow\u3002 \u57f7\u884c\u74b0\u5883\u9700\u6c42\u8207\u5b89\u88dd\u9806\u5e8f\uff1a  \u4f7f\u7528Nvidia-GPU Visual Studio 2015 Community Nvidia CUDA 9.0 Nvidia cudnn-9.0 Python 3.6 Tensorflow-GPU Anaconda Visual Studio Code \u2192 \u53ef\u4ee5\u4e0d\u5b89\u88dd\uff0c\u53ea\u7528\u4f86\u7de8\u8f2f\u4fee\u6539\u7a0b\u5f0f DroidCam \u2192 \u5c07\u624b\u6a5f\u93e1\u982d\u7576\u4f5cWebCam\u4e32\u6d41\u4e4bAPP  Visual Studio 2015 Community \u5b98\u7db2\u4e0b\u8f09\u8cc7\u6e90\uff1a Visual Studio 2015 Community Install Visual C++ Build Tools 2015 Nvidia CUDA 9.0 \u5b98\u7db2\u4e0b\u8f09\u8cc7\u6e90\uff1a CUDA Toolkit 9.0 Downloads Nvidia cudnn-9.0 \u5b98\u7db2\u4e0b\u8f09\u8cc7\u6e90\uff1a cuDNN Download \u95dc\u65bcTensorflow-GPU\u5b89\u88dd\u65b9\u6cd5\uff0c\u53ef\u53c3\u8003\u5b98\u7db2\u8cc7\u6e90\uff1a Build from source on Windows Anaconda \u5b98\u7db2\u4e0b\u8f09\u8cc7\u6e90\uff1a Anaconda Download \u5b89\u88ddCUDA\u8207cudnn 9.0 \u6ce8\u610f\u4e8b\u9805  \u91dd\u5c0dCUDA\u5b89\u88dd\u524d\uff0c\u9700\u8981\u5148\u5b89\u88dd\u597d Visual Studio 2015 Community\u3002 \u5b89\u88ddCUDA\u5b8c\u7562\u5f8c\uff0c\u9700\u8981\u5c07cudnn-9.0\u8cc7\u6599\u593e\u5167\u6240\u6709\u6a94\u6848\u76ee\u9304\u8907\u88fd\u5230 C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0 \u5e95\u4e0b\u3002 \u5728\u7cfb\u7d71\u74b0\u5883\u8b8a\u6578\u8def\u5f91\u9700\u8981\u52a0\u4e0a C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v9.0\\bin  \u4f7f\u7528Python Package\uff1a  tensorflow-gpu==1.11 keras sklearn matplotlib numpy opencv-python dlib  \u5b89\u88dd\u65b9\u6cd51\uff1a\u4f7f\u7528\u8b80\u53d6\u6e05\u55ae\u6587\u4ef6\u5b89\u88dd  pip install -r requirements.txt   \u5982\u679c\u9078\u64c7\u6b64\u64cd\u4f5c\u9700\u8981\u518d\u7368\u81ea\u5b89\u88dddlib\u5957\u4ef6\uff0c \u6b64\u70ba\u81c9\u90e8\u5075\u6e2c\u9700\u8981\u7528\u5230\u5957\u4ef6\u5eab\u3002   pip install dlib/dlib-19.8.1-cp36-cp36m-win_amd64.whl  \u5b89\u88dd\u65b9\u6cd52\uff1a\u4f7f\u7528pip\u6307\u4ee4\u7368\u7acb\u5b89\u88dd  keras    pip install keras   sklearn    pip install sklearn   matplotlib   pip install matplotlib   numpy   pip install numpy   opencv-python   pip install opencv-python   dlib(\u6b64\u70ba\u81c9\u90e8\u5075\u6e2c\u9700\u8981\u7528\u5230\u5957\u4ef6\u5eab)    pip install dlib/dlib-19.8.1-cp36-cp36m-win_amd64.whl  \u672c\u7a0b\u5f0f\u5c08\u6848\u7d50\u69cb\uff1a  FaceID.py --> \u6574\u9ad4\u8a13\u7df4\u6d41\u7a0b\u63a7\u5236 loadImg.py --> \u8b80\u53d6\u5716\u7247\u8f49\u63db\u6210\u6578\u64da dataset_process.py --> \u5c07\u8cc7\u6599\u8f49\u6210\u8a13\u7df4\u3001\u9a57\u8b49\u3001\u6e2c\u8a66\u8cc7\u6599\u96c6 model.py --> \u5efa\u7acb CNN \u6a21\u578b\u6846\u67b6 history_plot.py --> \u5c07Loss\u3001Accuracy\u8a13\u7df4\u904e\u7a0b\u8f38\u51fa\u6210\u4e00\u5f35\u5716 model_logouput.py --> \u6a21\u578b\u53c3\u6578\u8f38\u51fa prediction_WebCam.py --> \u57f7\u884c\u81c9\u90e8\u5716\u7247\u9810\u6e2c\u7a0b\u5e8f\uff0c\u5716\u7247\u4f86\u6e90\u4f7f\u7528\u624b\u6a5f\u76f8\u6a5f\u4f5c\u70baWebCam prediction.py --> \u57f7\u884c\u81c9\u90e8\u5716\u7247\u9810\u6e2c\u7a0b\u5e8f face_capture.py --> \u904b\u7528WebCam\u9032\u884c\u81c9\u90e8\u64f7\u53d6\uff0c\u7522\u751f\u81c9\u90e8\u5716\u50cf\u8cc7\u6599\u96c6  Tensorflow \u6a21\u578b\u8a13\u7df4\u53c3\u6578\u8a2d\u5b9a\uff1a  batch_size \uff1a23 epochs \uff1a84 ImageDataGenerator Adam ( learn_rate : 0.00015 ) steps_per_epoch\uff1a18(432/23) \u2192 steps_per_epoch = (len(train_images) / batch_size)  \u5716\u50cf\u8cc7\u6599\u96c6\u64cd\u4f5c\u8a2d\u7f6e\uff1a \u5716\u50cf\u8cc7\u6599\u96c6\u5206\u5225\u5982\u4e0b\uff1a  \u8a13\u7df4\u5716\u50cf\u8cc7\u6599\u96c6\u653e\u7f6e\u76ee\u9304\uff1aFaceImg/ \u9810\u6e2c\u5716\u50cf\u8cc7\u6599\u96c6\u76ee\u9304\uff1apredictFaceimg/  \u4e0a\u9762\u5169\u8005\u8cc7\u6599\u593e\u5167\u90e8\u90fd\u6703\u6709\u4ee5\u2019face_\u6578\u5b57\u2019\u70ba\u547d\u540d\u8cc7\u6599\u593e\uff0c\u53ef\u4ee5\u7528\u4f86\u6307\u5b9a\u4f7f\u7528\u4e0d\u540c\u81c9\u90e8\u5716\u50cf\u8cc7\u6599\u96c6\u5340\u584a\uff0c\u81c9\u90e8\u5716\u50cf\u8981\u653e\u7f6e\u5728\u4ee5\u2019face_\u6578\u5b57\u2019\u70ba\u547d\u540d\u8cc7\u6599\u593e\u5167\u5c0d\u61c9\u7684\u8b58\u5225\u540d\u7a31\u8cc7\u6599\u593e\u4e4b\u4e2d\u3002 \u5728 FaceID.py\u3001prediction.py\u3001prediction_WebCam.py\u3001face_capture.py\u5167\uff0c\u6703\u6709\u4e00\u500b\u8b8a\u6578\u547d\u540d\u70ba'face_class'\uff0c\u6b64\u70ba\u64cd\u4f5c\u8981\u4f7f\u7528\u54ea\u4e00\u500b\u81c9\u90e8\u5716\u50cf\u8cc7\u6599\u96c6\u5340\u584a\u3002 \u4ee5 face_class = '4' \u70ba\u4f8b\uff0c\u6b64\u70ba\u4ee3\u8868\u8981\u64cd\u4f5c\u4f7f\u7528\u5167\u90e8\u4ee5'face_4'\u70ba\u547d\u540d\u81c9\u90e8\u5716\u50cf\u8cc7\u6599\u96c6\u5340\u584a\u8cc7\u6599\u76ee\u9304\uff0c\u5167\u90e8\u6709\u516d\u4f4d\u4e0d\u540c\u8b58\u5225\u540d\u7a31\u8cc7\u6599\u593e\uff0c\u653e\u7f6e\u5404\u81ea\u5c0d\u61c9\u81c9\u90e8\u5716\u50cf\u3002 \u5716\u50cf\u8cc7\u6599\u96c6\u5206\u5225\u5982\u4e0b\uff1a   \u8a13\u7df4\u5716\u50cf\u8cc7\u6599\u96c6\u653e\u7f6e\u76ee\u9304\uff1aFaceImg/face_4 \u9810\u6e2c\u5716\u50cf\u8cc7\u6599\u96c6\u76ee\u9304\uff1apredictFaceimg/face_4   face_capture.py \u6703\u81ea\u52d5\u8b58\u5225\u662f\u6709\u5b58\u5728'face_\u6578\u5b57'\u70ba\u547d\u540d\uff0c\u81c9\u90e8\u5716\u50cf\u8cc7\u6599\u96c6\u5340\u584a\u8cc7\u6599\u76ee\u9304\uff0c\u5982\u679c\u4e0d\u5b58\u5728\u5c31\u6703\u81ea\u52d5\u5efa\u7acb\u3002 WebCam\u8a2d\u7f6e\uff1a \u4ee5\u624b\u6a5f\u93e1\u982d\u4f5c\u70ba\u4e32\u6d41WebCam\u5a92\u9ad4\uff0c\u4f7f\u7528 DroidCam Wireless Webcam \u6b64\u624bAPP\u4f5c\u70baWebCam\u4e32\u6d41\u63a7\u5236\u3002 DroidCam Wireless Webcam \u9700\u6ce8\u610f!!\u624b\u6a5f\u8ddf\u96fb\u8166\u9700\u8981\u5728\u540c\u4e00\u500b\u7db2\u8def\u5167\uff0c\u4e5f\u5c31\u662f\u5728\u76f8\u540c\u5340\u57df\u7db2\u6bb5\u5167 \u5982\u679c\u57f7\u884cprediction_WebCam.py \u8207 face_capture.py \u5169\u8005\u7a0b\u5f0f\uff0c\u9700\u8981\u5148\u8a2d\u7f6e\u5167\u90e8camera\u9023\u7d50WebCam URL\u8b8a\u6578\uff0c\u8a9e\u6cd5\u683c\u5f0f\u5982\u4e0b\uff1a  http://\u624b\u6a5f\u5340\u57df\u7db2\u8defIP:4747/videostream.cgi?.mjpg  Anaconda \u57f7\u884c\u7a0b\u5f0f\u64cd\u4f5c\uff1a \u958b\u555f Anaconda Prompt \u958b\u555f Anaconda Prompt (Windows)\uff0c\u4e00\u958b\u59cb\u6253\u958b\u6703\u662f\u5728(base)\u9810\u8a2d\u865b\u64ec\u74b0\u5883\u5e95\u4e0b\uff0c\u5982\u679c\u8981\u5efa\u7acb\u4e00\u500b\u865b\u64ec\u74b0\u5883\uff0c\u9700\u8981\u5148\u96e2\u958b(base)\u9810\u8a2d\u865b\u64ec\u74b0\u5883\u5e95\u4e0b\u3002 \u5efa\u7acb\u865b\u64ec\u74b0\u5883 \u5efa\u7acb\u865b\u64ec\u74b0\u5883\u6307\u4ee4\uff1aconda create --name [env_name] [python_env]  \u5047\u8a2d\u8981\u5efa\u7acb\u4e00\u500b\u865b\u64ec\u74b0\u5883\u540d\u7a31\u70baKerasFaceID\uff0c\u4e26\u4e14python\u74b0\u5883\u70ba3.6  conda create --name KerasFaceID python=3.6  \u9032\u5165\u865b\u64ec\u74b0\u5883 \u9032\u5165\u865b\u64ec\u74b0\u5883\u6307\u4ee4\uff1aactivate [env_name]  \u5047\u8a2d\u8981\u9032\u5165\u4e00\u500b\u865b\u64ec\u74b0\u5883\u540d\u7a31\u70baKerasFaceID   activate KerasFaceID  \u96e2\u958b\u865b\u64ec\u74b0\u5883 \u96e2\u958b\u865b\u64ec\u74b0\u5883\u6307\u4ee4\uff1aconda deactivate  [env_name]  \u5047\u8a2d\u8981\u96e2\u958b\u4e00\u500b\u865b\u64ec\u74b0\u5883\u540d\u7a31\u70baKerasFaceID   conda deactivate  \u57f7\u884c\u7a0b\u5f0f\u5c08\u6848\u5728\u865b\u64ec\u74b0\u5883\u5e95\u4e0b \u6a21\u578b\u8a13\u7df4\u7a0b\u5f0f\uff1aFaceID.py   python FaceID.py  \u81c9\u90e8\u8b58\u5225\u7a0b\u5f0f\uff1aprediction.py \u6216 prediction_WebCam.py (\u4f7f\u7528WebCam)  python prediction.py  python prediction_WebCam.py  \u57f7\u884c\u81c9\u90e8\u64f7\u53d6\u7a0b\u5f0f\uff1aface_capture.py(\u4f7f\u7528WebCam)  python face_capture.py  ", "has_readme": true, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://%E6%89%8B%E6%A9%9F%E5%8D%80%E5%9F%9F%E7%B6%B2%E8%B7%AFIP:4747/videostream.cgi?.mjpg"], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f7"}, "repo_url": "https://github.com/Milogav/tensorflow-networks", "repo_name": "tensorflow-networks", "repo_full_name": "Milogav/tensorflow-networks", "repo_owner": "Milogav", "repo_desc": "Tensorflow implementation of some popular deep network architectures", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T19:36:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T19:34:14Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185259607, "is_fork": false, "readme_text": "tensorflow-networks Tensorflow implementation of some popular deep network architectures TODO: adapt the code using keras.layers as tf.layers is now deprecated. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f8"}, "repo_url": "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-", "repo_name": "-Real-World-Python-Deep-Learning-Projects-v-", "repo_full_name": "PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-", "repo_owner": "PacktPublishing", "repo_desc": " Real-World Python Deep Learning Projects [Video] by Packt Publishing", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T09:31:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T06:54:13Z", "homepage": null, "size": 145089, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185140247, "is_fork": false, "readme_text": "Real-World Python Deep Learning Projects [Video] This is the code repository for Real-World Python Deep Learning Projects [Video], published by Packt. It contains all the supporting project files necessary to work through the video course from start to finish. About the Video Course Deep Learning allows you to solve problems where traditional Machine Learning methods might perform poorly: detecting and extracting objects from images, extracting meaning from text, and predicting outcomes based on complex dependencies, to name a few. In this course you will learn how to use Deep Learning in practice by going through real-world examples. You will start of by creating neural networks to predict the demand for airline travel in the future. Then, you'll run through a scenario where you have to identify negative tweets for a celebrity by using Convolutional Neural Networks (CNN's). Next you will create a neural network which will be able to identify smiles in your camera app. Finally, the last project will help you forecast a company's stock prices for the next day using Deep Learning. By the end of this course, you will have a solid understanding of Deep Learning and the ability to build your own Deep Learning models. The code bundle for this video course is available at - https://github.com/PacktPublishing/Real-World-Python-Deep-Learning-Projects What You Will Learn   Effectively pre-process data (structured or unstructured) before doing any analysis on the dataset.\u00a0  Retrieving data from different data sources (CSV, JSON, Excel, PDF) and parse them in Python to give them a meaningful shape.  Learn about the amazing data storage places in an industry which are being highly optimized.  Perform statistical analysis using in-built Python libraries.  Hacks, tips, and techniques that will be invaluable throughout your Data Science career.  Instructions and Navigation Assumed Knowledge To fully benefit from the coverage included in this course, you will need: This course is intended for developers, analysts, and data scientists who have a basic machine learning knowledge and want to now explore the possibilities of Deep Learning. Python programming knowledge is essential. Technical Requirements This course has the following software requirements: Minimum Hardware Requirements For successful completion of this course, students will require the computer systems with at least the following: OS: Windows 10 Processor: Intel core i5 Memory: 4 GB Storage: 256 GB Recommended Hardware Requirements For an optimal experience with hands-on labs and other practical activities, we recommend the following configuration: OS: Windows 10 Processor: Intel core i5 Memory: 4 GB Storage: 256 GB Software Requirements Python 3.6 (https://www.python.org/downloads/) Anaconda for Python 3.6 version (https://www.anaconda.com/download/) Tensorflow (https://www.tensorflow.org/install/install_windows) Scikit-learn Keras Python package: keras (installed from command prompt using the following commands: \u201cconda install -c conda-forge keras ) Related Products   Real World Projects in Python 3.x [Video]   Exploratory Data Analysis with Pandas and Python 3.x [Video]   Data Wrangling with Python 3.x [Video]   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-1-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-1-16.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-1-2.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-1-3.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-1-4.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-10-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-100-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-16-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-2-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-2-2.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-2-32.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-25-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-3-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-32-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-4-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-5-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-6-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/05967664b72596814e6377cdb138b25ea7403dbe/Section%202%20Code/source/models/model-default-8-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-10-31.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-4-31.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-5-31.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-5-32.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-6-31.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-6-32.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/c444044689571c1964b04ccc682adf77eaaa70da/Section%203%20Code/source/models/model-default-7-31.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/44a1f94bc8a9a11184d4a2ff8a67e1af2e050fa2/Section%205%20Code/source/models/model-default-1-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/44a1f94bc8a9a11184d4a2ff8a67e1af2e050fa2/Section%205%20Code/source/models/model-default-10-1.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/44a1f94bc8a9a11184d4a2ff8a67e1af2e050fa2/Section%205%20Code/source/models/model-default-10-10.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/44a1f94bc8a9a11184d4a2ff8a67e1af2e050fa2/Section%205%20Code/source/models/model-default-10-32.h5", "https://github.com/PacktPublishing/-Real-World-Python-Deep-Learning-Projects-v-/blob/44a1f94bc8a9a11184d4a2ff8a67e1af2e050fa2/Section%205%20Code/source/models/model-default-10-64.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851f9"}, "repo_url": "https://github.com/yxiao1996/CS542SmartGrasp", "repo_name": "CS542SmartGrasp", "repo_full_name": "yxiao1996/CS542SmartGrasp", "repo_owner": "yxiao1996", "repo_desc": "Spring 2019 CS542 machine learning course project at Boston University", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-11T22:52:10Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-06T23:14:26Z", "homepage": null, "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185284802, "is_fork": false, "readme_text": "CS542 SmartGrasp Spring 2019 CS542 machine learning course project at Boston University Preprocessing the dataset Before you start, you need to get the dataset from https://www.kaggle.com/ugocupcic/grasping-dataset. Split a small chunk from the original dataset  splitData.py  We have a little Python script in this repository which can be used to split a chunk of data from the original dataset. It is called 'splitData.py'. The number of datapoints(feature, label) is controlled by the number of iterations on line 3. This script will generate a .mat file which can be read by MATLAB. Further processing includes extracting suitable features and labels for training and testing is done in MATLAB. Please see the next section. Generate training data  preprocess.m  We use a script in MATLAB to extract the label and features from raw data. To run this script, please put the small chunk of raw data generate in the previous step into your MATLAB search path and modify the name of the dataset on line 3 and line 4 to match the name of the smaller chunk. Training prediction model We use two different classification methods in this project. Please make sure you have scikit-learn and Keras installed on your machine.  svm.py  You could run this script to train a SVM prediction model. Before you run the script, please make sure you have preprocessed dataset. The file name of dataset could be changed in line 46. Also, you could change the kernel function and other parameters of SVM in line 27.  nn.py  You could run this script to train a neural network. You could change the parameter of neural network in line 72. The model is saved as json file, and the weights of neural network is saved as h5 file.  experiment.m  The experiment.m is a MATLAB script we use at the beginning of the project. We use this script to test our thoughts. It is not a part of our final results. Test the prediction model in Robot Simulator Introduction: This test code is developed based on the docker environment from Shadow Robot Company. We adapt the environment to be runnable with latest version of Keras and put our neural network model in it. Test code using ROS for our own model is developed based on previous work found in the docker image. Prerequisite: please make sure you have docker installed on your machine. Steps to run the test:   make sure you have docker installed on your machine.   pull and run the docker image from docker hub. docker run -it --name sgs -p 8080:8080 -p 8888:8888 -p 8181:8181 -p 7681:7681 yxiao1996/cs542smartgrasp   check the jupyter notebook client at localhost:8888 and Gazebo simulator at localhost:8080. you should be able to see a simulated environment with a table, a red ball and a robotic arm in the simulator.   run the grasp command in jupyter notebook. please find the \"Smart Grasping Sandbox.ipynb\". by running the fist three blocks, you should be able to observe that the robotic hand would move towards the red ball and try to grasp it.   run the prediction notebook in jupyter notebook. please find the \"DisplayGraspQualiyuNeuralNet.ipynb\", the displaying block is the final block in this notebook. to reproduce the result, please first send command to the robotic arm for grasping in \"Smart Grasping Sandbox.ipynb\" and then run all blocks in \"DisplayGraspQualiyuNeuralNet.ipynb\". you should be able to observe the jump of prediction as we shown in presentation.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851fa"}, "repo_url": "https://github.com/luozhouyang/transformer", "repo_name": "transformer", "repo_full_name": "luozhouyang/transformer", "repo_owner": "luozhouyang", "repo_desc": "Transformer implementation in tensorflow 2.x", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T11:52:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T11:39:29Z", "homepage": null, "size": 14, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185184913, "is_fork": false, "readme_text": "transformer Attention Is All You Need Transformer implementation in tensorflow 2.x(with tf.keras). Jupyter notebook with conda conda create -n transformer python=3.6 source activate transformer conda install nb_conda pip install tensorflow==2.0.0a ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1706.03762"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851fb"}, "repo_url": "https://github.com/Lintianqianjin/Sales-forecast-LSTM", "repo_name": "Sales-forecast-LSTM", "repo_full_name": "Lintianqianjin/Sales-forecast-LSTM", "repo_owner": "Lintianqianjin", "repo_desc": "\u4f7f\u7528LSTM\u9884\u6d4b\u5546\u54c1\u9500\u91cf\uff0c\u8003\u8651\u9500\u91cf\u6fc0\u589e\u70b9\u5f71\u54cd", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-15T08:44:22Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T06:10:34Z", "homepage": null, "size": 8208, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185134103, "is_fork": false, "readme_text": "Sales-forecast-LSTM,based on Keras \u4f7f\u7528LSTM\u9884\u6d4b\u5546\u54c1\u9500\u91cf\uff0c\u8003\u8651\u9500\u91cf\u6fc0\u589e\u70b9\u5f71\u54cd \u6570\u636e\u6765\u6e90 2019\u5e74\u534e\u4e2d\u6570\u6a21\u5927\u8d5b \u6570\u636e\u9884\u5904\u7406 \u5173\u952e\u95ee\u9898\u5728\u4e8e\u8ba9\u7f51\u7edc\u80fd\u5b66\u4e60\u5230\u9500\u91cf\u6fc0\u589e\u70b9\uff0c\u5982\u53cc\u5341\u4e00\uff0c\u8fd9\u6837\u7684\u4fe1\u606f\uff1b \u7ecf\u8fc7\u6d4b\u8bd5\uff0c\u6700\u7ec8\u53d1\u73b0\u6548\u679c\u6700\u597d\u7684\u5c31\u662f\u5148\u4ece\u6570\u636e\u4e2d\u8bc6\u522b\u8fd9\u6837\u7684\u6fc0\u589e\u65e5\uff0c\u7136\u540eone-hot\u7f16\u7801\u6dfb\u52a0\u5230\u8f93\u5165\u6570\u636e\u4e2d\u3002 \u5176\u5b83\u7684\u5904\u7406\u5305\u62ec\u5f52\u4e00\u5316\u4ec0\u4e48\u7684\uff0c\u8ba9\u6570\u636e\u7684\u7edd\u5bf9\u6570\u503c\u4e0d\u8981\u592a\u5927\u5c31\u884c\uff0c\u5229\u4e8e\u8bad\u7ec3\u3002 \u7f51\u7edc\u7ed3\u6784 \u6bd4\u8f83\u7b80\u5355\u7684\u4e00\u4e2a\u5c1d\u8bd5\uff0c\u4f7f\u7528\u4e09\u5c42LSTM\uff0c\u6fc0\u6d3b\u51fd\u6570\u7528tanh,\u6bcf\u4e00\u5c42\u63a5Dropout\uff0c\u6700\u540e\u63a5Dense\u3002  \u6548\u679c\u793a\u4f8b \u67d0\u6b21\u9884\u6d4b\u7684\u7ed3\u679c\u3002\u8bad\u7ec3\u96c6\u662f14\u4e2a\u5546\u54c1\uff0c\u9a8c\u8bc1\u96c6\u662f2\u4e2a\u5546\u54c1\uff0c\u628a\u8868\u73b0\u6700\u597d\u7684\u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\u3002 \u4ee5\u4e0b\u662f\u5728\u6d4b\u8bd5\u96c61\u4e2a\u5546\u54c1\u4e0a\u7684\u9884\u6d4b\u7ed3\u679c\uff0c\u53ef\u4ee5\u8bf4\u6cdb\u5316\u80fd\u529b\u662f\u633a\u4e0d\u9519\u7684\u3002\u9500\u91cf\u7684\u7edd\u5bf9\u8bef\u5dee\u5927\u7ea6\u662f9\uff0c\u5747\u65b9\u6839\u8bef\u5dee\u5927\u6982\u662f50\u3002  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": ["sales-forecasting", "sales-forecast", "lstm-neural-networks"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851fc"}, "repo_url": "https://github.com/bogdanvaduva9/medical_DL", "repo_name": "medical_DL", "repo_full_name": "bogdanvaduva9/medical_DL", "repo_owner": "bogdanvaduva9", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-19T09:55:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T19:57:08Z", "homepage": null, "size": 1302944, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185262655, "is_fork": false, "readme_text": "3rd machine learning homework License  This repository is under the MIT standard license  Purpose  Design an algorithm that predicts whether the skeletonal radiography of a pacient is normal or abnormal Use deep learning and convolutional neural networks to do this  Dataset  The dataset is composed of images containing x-rays of specific bones in the body There are 2 classes: normal and abnormal  Architecture  Using a convolutional neural network Using keras as a high-level API  Training  Training was done on the train dataset No augmentation was used The split was 90/10 (90% used for training/ 10% used for validation)  Notes  There were 3 variations of CNN used The first one was the original one from the paper The second one was with more layers than the original one The third one was with more features on each layer than the original one Each of these variations have 8 models trained, on a specific number of epochs The models were trained on 1, 2, 5, 10, 20, 30, 40 and 50 epochs  Results  The results can be found in the graphs folder The graphs folder contains two other folders: acc and loss The acc folder contains another two folders: all_images and patients The all_images folder contains the graphs for the testing results on the train and the test dataset for all the variations of CNN used, on each image individually The patients folder contains the graphs for the testing results ont he train and test dataset for all the variations of CNN used, but this time it is done by classifying each patient, not each image (in case of equal number of votes for the patient, a random choice is done) The loss folder contains another three folders: more_features, more_layers and originals Each of these folders contains graphs for the loss during training for each model (created with a specific variation of CNN) ~ in total, 8 graphs for each folder  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/10morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/10morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/10original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/1morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/1morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/1original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/20morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/20morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/20original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/2modified.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/2morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/2morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/2original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/30morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/30morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/30original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/40morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/40morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/40original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/50morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/50morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/50original.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/5morefeatures.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/5morelayers.h5", "https://github.com/bogdanvaduva9/medical_DL/blob/ae7fbe8c56249701611dccfbf977ad98b82989dc/models/5original.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851fd"}, "repo_url": "https://github.com/citmca/Autonomous-Identification-of-fruit-stock-using-DeepLearning", "repo_name": "Autonomous-Identification-of-fruit-stock-using-DeepLearning", "repo_full_name": "citmca/Autonomous-Identification-of-fruit-stock-using-DeepLearning", "repo_owner": "citmca", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-06T13:54:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T13:09:03Z", "homepage": null, "size": 1288, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185199013, "is_fork": false, "readme_text": "Autonomous-Identification-of-fruit-stock-using-DeepLearning This project aims to automate identifying the fruits available in a personal storage (inside a refrigerator). This is to enable autonomous ordering/ replenishing of the fruits as required for the user. Identifying/Recognizing a fruit is challenging digital image processing problem due to the ambiguous nature of the colour, shape and texture of a fruit. Discriminating one fruit from another is a challenging task, especially as a fruit changes shape, colour based on its ripening. Additional complexities are due to factor of obfuscation, lighting and shadow. This project approaches the problem of identifying fruits from camera image by using Deep Learning Techniques. Deep Learning allows multiple layers of artificial neural network to be trained appropriately. A dataset of images of nineteen types of fruits used to train and create the Deep Learning model. The Deep Learning system were developed on TensorFlow (backend) using KERAS (API) with PyCharm IDE. This project also compares three approaches to building the Deep Learning system. The first approach uses Convolutional Neural Network, here the convolutional layer of the Neural Network was developed along with pooling layer, dense layers. Next a Transfer Learning approach was done with the VGG16 model was used on top of which four layers were added. The third approach in Fully Convolutional Neural Network. This do not have flatten or dense layer in the architecture. Result of these three approaches are presented. The initial scope of the project was to identify the fruits and find the number of fruits of each type. Identification of fruits is completed and presented in the report. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/citmca/Autonomous-Identification-of-fruit-stock-using-DeepLearning/blob/4ffb66e86e095b4b1decfd9be7cdd02211a5472b/Models/fcn1weights00000100.h5", "https://github.com/citmca/Autonomous-Identification-of-fruit-stock-using-DeepLearning/blob/4ffb66e86e095b4b1decfd9be7cdd02211a5472b/Models/weights00006000.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851fe"}, "repo_url": "https://github.com/aryanmisra/NeuraScale", "repo_name": "NeuraScale", "repo_full_name": "aryanmisra/NeuraScale", "repo_owner": "aryanmisra", "repo_desc": "Super Resolution Generative Adversarial Network (SRGAN)  using Tensorflow 2.0", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T17:57:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T23:11:07Z", "homepage": "http://aryanmisra.com/neurascale", "size": 31957, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 185284514, "is_fork": false, "readme_text": "NeuraScale Video for Powered by TF2.0 challenge. Website What it does NeuraScale, the fancy name we gave our project, is basically a Super Resolution Generative Adversarial Network (SRGAN) with the purpose of upscaling image resolutions by a factor of two using deep learning. This way, a picture which initially appears pixellated and/or blurry can be modified so that the features are quite more distinguishable. The model is trained on the COCO unlabeled2017 dataset. Download here. Requirements  Tensorflow 2.0 Scipy, Numpy PIL Matplotlib MS COCO unlabeled2017 Dataset (for training)  Directory Tree Structure /datasets   /unlabeled2017 /Neurascale   /images     /unlabeled2017   /main     /temp     /tests     data_loader.py     preprocessing.py     srgan.py   /saves  Usage To train model (which we highly reccomend doing some more): python srgan.py  To run the model on an image: python srgan.py -p image.jpg  Thats it! Images Original:  SuperResolution:  Original:  SuperResolution:  How we built it We used TensorFlow 2.0 as the API for creating and training our SRGAN. The model was built with Keras and trained on the MS COCO Dataset. Numpy, Matplotlib, and several other libraries were used as well to allow for proper image preprocessing, as different image sizes need to be modified in order to be properly evaluated by the network. Challenges we ran into Since most neural networks require a fixed input/output size, figuring out the image preprocessing was a difficult part of our project, as we ran into many bugs and sighed in frustration multiple times. We found a way to split an image into several regular pieces to be fed into the network and then stitch the output together to end up with a proper, upscaled image. What's next for NeuraScale Our next steps for NeuraScale include smoothing out some of the rough edges in our code, converting the model to use it in a web app with TF.js or as a native app with TF Lite, and possibly retraining the model to output with colour enhancements. We also plan to heavily improve our model performance, as hardware limitation have resulted in poor convergence. Meta Aryan Misra Alex Yu Distributed under the GNU General Public License v3.0. See license for more information. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/aryanmisra/NeuraScale/blob/a289bd9b8ac6103acfb0343a6ba70a65d37cea5a/saves/6850/model.h5", "https://github.com/aryanmisra/NeuraScale/blob/a289bd9b8ac6103acfb0343a6ba70a65d37cea5a/saves/6900/model.h5"], "see_also_links": ["http://aryanmisra.com", "http://cocodataset.org/#download"], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b851ff"}, "repo_url": "https://github.com/18483666678/Mask_RCNN", "repo_name": "Mask_RCNN", "repo_full_name": "18483666678/Mask_RCNN", "repo_owner": "18483666678", "repo_desc": "\u8bad\u7ec3\u6570\u636e\u65b0\u5efatrain_data\uff0c\u91cc\u9762\u5305\u542b\u56db\u4e2a\u6587\u4ef6\u5939", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-06T08:46:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T08:32:28Z", "homepage": "", "size": 73713, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185155903, "is_fork": false, "readme_text": "Mask R-CNN for Object Detection and Segmentation This is an implementation of Mask R-CNN on Python 3, Keras, and TensorFlow. The model generates bounding boxes and segmentation masks for each instance of an object in the image. It's based on Feature Pyramid Network (FPN) and a ResNet101 backbone.  The repository includes:  Source code of Mask R-CNN built on FPN and ResNet101. Training code for MS COCO Pre-trained weights for MS COCO Jupyter notebooks to visualize the detection pipeline at every step ParallelModel class for multi-GPU training Evaluation on MS COCO metrics (AP) Example of training on your own dataset  The code is documented and designed to be easy to extend. If you use it in your research, please consider citing this repository (bibtex below). If you work on 3D vision, you might find our recently released Matterport3D dataset useful as well. This dataset was created from 3D-reconstructed spaces captured by our customers who agreed to make them publicly available for academic use. You can see more examples here. Getting Started   demo.ipynb Is the easiest way to start. It shows an example of using a model pre-trained on MS COCO to segment objects in your own images. It includes code to run object detection and instance segmentation on arbitrary images.   train_shapes.ipynb shows how to train Mask R-CNN on your own dataset. This notebook introduces a toy dataset (Shapes) to demonstrate training on a new dataset.   (model.py, utils.py, config.py): These files contain the main Mask RCNN implementation.   inspect_data.ipynb. This notebook visualizes the different pre-processing steps to prepare the training data.   inspect_model.ipynb This notebook goes in depth into the steps performed to detect and segment objects. It provides visualizations of every step of the pipeline.   inspect_weights.ipynb This notebooks inspects the weights of a trained model and looks for anomalies and odd patterns.   Step by Step Detection To help with debugging and understanding the model, there are 3 notebooks (inspect_data.ipynb, inspect_model.ipynb, inspect_weights.ipynb) that provide a lot of visualizations and allow running the model step by step to inspect the output at each point. Here are a few examples: 1. Anchor sorting and filtering Visualizes every step of the first stage Region Proposal Network and displays positive and negative anchors along with anchor box refinement.  2. Bounding Box Refinement This is an example of final detection boxes (dotted lines) and the refinement applied to them (solid lines) in the second stage.  3. Mask Generation Examples of generated masks. These then get scaled and placed on the image in the right location.  4.Layer activations Often it's useful to inspect the activations at different layers to look for signs of trouble (all zeros or random noise).  5. Weight Histograms Another useful debugging tool is to inspect the weight histograms. These are included in the inspect_weights.ipynb notebook.  6. Logging to TensorBoard TensorBoard is another great debugging and visualization tool. The model is configured to log losses and save weights at the end of every epoch.  6. Composing the different pieces into a final result  Training on MS COCO We're providing pre-trained weights for MS COCO to make it easier to start. You can use those weights as a starting point to train your own variation on the network. Training and evaluation code is in samples/coco/coco.py. You can import this module in Jupyter notebook (see the provided notebooks for examples) or you can run it directly from the command line as such: # Train a new model starting from pre-trained COCO weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=coco  # Train a new model starting from ImageNet weights python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=imagenet  # Continue training a model that you had trained earlier python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=/path/to/weights.h5  # Continue training the last model you trained. This will find # the last trained weights in the model directory. python3 samples/coco/coco.py train --dataset=/path/to/coco/ --model=last  You can also run the COCO evaluation code with: # Run COCO evaluation on the last trained model python3 samples/coco/coco.py evaluate --dataset=/path/to/coco/ --model=last  The training schedule, learning rate, and other parameters should be set in samples/coco/coco.py. Training on Your Own Dataset Start by reading this blog post about the balloon color splash sample. It covers the process starting from annotating images to training to using the results in a sample application. In summary, to train the model on your own dataset you'll need to extend two classes: Config This class contains the default configuration. Subclass it and modify the attributes you need to change. Dataset This class provides a consistent way to work with any dataset. It allows you to use new datasets for training without having to change the code of the model. It also supports loading multiple datasets at the same time, which is useful if the objects you want to detect are not all available in one dataset. See examples in samples/shapes/train_shapes.ipynb, samples/coco/coco.py, samples/balloon/balloon.py, and samples/nucleus/nucleus.py. Differences from the Official Paper This implementation follows the Mask RCNN paper for the most part, but there are a few cases where we deviated in favor of code simplicity and generalization. These are some of the differences we're aware of. If you encounter other differences, please do let us know.   Image Resizing: To support training multiple images per batch we resize all images to the same size. For example, 1024x1024px on MS COCO. We preserve the aspect ratio, so if an image is not square we pad it with zeros. In the paper the resizing is done such that the smallest side is 800px and the largest is trimmed at 1000px.   Bounding Boxes: Some datasets provide bounding boxes and some provide masks only. To support training on multiple datasets we opted to ignore the bounding boxes that come with the dataset and generate them on the fly instead. We pick the smallest box that encapsulates all the pixels of the mask as the bounding box. This simplifies the implementation and also makes it easy to apply image augmentations that would otherwise be harder to apply to bounding boxes, such as image rotation. To validate this approach, we compared our computed bounding boxes to those provided by the COCO dataset. We found that ~2% of bounding boxes differed by 1px or more, ~0.05% differed by 5px or more, and only 0.01% differed by 10px or more.   Learning Rate: The paper uses a learning rate of 0.02, but we found that to be too high, and often causes the weights to explode, especially when using a small batch size. It might be related to differences between how Caffe and TensorFlow compute gradients (sum vs mean across batches and GPUs). Or, maybe the official model uses gradient clipping to avoid this issue. We do use gradient clipping, but don't set it too aggressively. We found that smaller learning rates converge faster anyway so we go with that.   Citation Use this bibtex to cite this repository: @misc{matterport_maskrcnn_2017,   title={Mask R-CNN for object detection and instance segmentation on Keras and TensorFlow},   author={Waleed Abdulla},   year={2017},   publisher={Github},   journal={GitHub repository},   howpublished={\\url{https://github.com/matterport/Mask_RCNN}}, }  Contributing Contributions to this repository are welcome. Examples of things you can contribute:  Speed Improvements. Like re-writing some Python code in TensorFlow or Cython. Training on other datasets. Accuracy Improvements. Visualizations and examples.  You can also join our team and help us build even more projects like this one. Requirements Python 3.4, TensorFlow 1.3, Keras 2.0.8 and other common packages listed in requirements.txt. MS COCO Requirements: To train or test on MS COCO, you'll also need:  pycocotools (installation instructions below) MS COCO Dataset Download the 5K minival and the 35K validation-minus-minival subsets. More details in the original Faster R-CNN implementation.  If you use Docker, the code has been verified to work on this Docker container. Installation   Clone this repository   Install dependencies pip3 install -r requirements.txt   Run setup from the repository root directory python3 setup.py install   Download pre-trained COCO weights (mask_rcnn_coco.h5) from the releases page.   (Optional) To train or test on MS COCO install pycocotools from one of these repos. They are forks of the original pycocotools with fixes for Python3 and Windows (the official repo doesn't seem to be active anymore).  Linux: https://github.com/waleedka/coco Windows: https://github.com/philferriere/cocoapi. You must have the Visual C++ 2015 build tools on your path (see the repo for additional details)    Projects Using this Model If you extend this model to other datasets or build projects that use it, we'd love to hear from you. 4K Video Demo by Karol Majek.  Images to OSM: Improve OpenStreetMap by adding baseball, soccer, tennis, football, and basketball fields.  Splash of Color. A blog post explaining how to train this model from scratch and use it to implement a color splash effect.  Segmenting Nuclei in Microscopy Images. Built for the 2018 Data Science Bowl Code is in the samples/nucleus directory.  Detection and Segmentation for Surgery Robots by the NUS Control & Mechatronics Lab.  Reconstructing 3D buildings from aerial LiDAR A proof of concept project by Esri, in collaboration with Nvidia and Miami-Dade County. Along with a great write up and code by Dmitry Kudinov, Daniel Hedges, and Omar Maher.  Usiigaci: Label-free Cell Tracking in Phase Contrast Microscopy A project from Japan to automatically track cells in a microfluidics platform. Paper is pending, but the source code is released.   Characterization of Arctic Ice-Wedge Polygons in Very High Spatial Resolution Aerial Imagery Research project to understand the complex processes between degradations in the Arctic and climate change. By Weixing Zhang, Chandi Witharana, Anna Liljedahl, and Mikhail Kanevskiy.  Mask-RCNN Shiny A computer vision class project by HU Shiyu to apply the color pop effect on people with beautiful results.  Mapping Challenge: Convert satellite imagery to maps for use by humanitarian organisations.  GRASS GIS Addon to generate vector masks from geospatial imagery. Based on a Master's thesis by Ond\u0159ej Pe\u0161ek.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.mdpi.com/2072-4292/10/9/1487", "http://cocodataset.org/#home"], "reference_list": ["https://arxiv.org/abs/1703.06870"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85200"}, "repo_url": "https://github.com/jackd/weighpoint", "repo_name": "weighpoint", "repo_full_name": "jackd/weighpoint", "repo_owner": "jackd", "repo_desc": "Weighted Point Cloud Convolutions in tensorflow", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-10T12:17:59Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-06T12:56:16Z", "homepage": null, "size": 121, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185196848, "is_fork": false, "readme_text": "WeighPoint: Weighted Point Cloud Convolutions Point cloud convolutions in deep learning have seen a lot of interest lately. Broadly speaking, these involve grouping points according to local proximity using some data structure like a KDTree. While results on classification and segmentation tasks are promising, most publicly available implementations suffer from a number of factors including:  k-nearest-neighbors search to ensure a fixed number of neighbors, rather than a fixed neighborhood size (as is suggested should be the case for convolutions in integral form); discontinuity in space: k-nearest neighbors is discontinous as the kth and k+1th neighbors switch order; and custom kernels which require additional setup and maintenance.  To address these, we implement:  neighborhoods defined by ball-searches, implemented using tf.RaggedTensors; a weighted convolution operation that ensures the integrated function trails to zero at the ball-search radius and is invariant to point-density; and a meta-network building process that allows per-layer preprocessing operations to be built in conjunction with the learnable operations before being split into separate preprocessing and learned tf.keras.Models.  The resulting architecture makes efficient use of CPUs for preprocessing without the need for custom kernels. As a bonus, the radii over which the ball searches occur can also be learned. The project is under heavy development. Currently we are able to achieve competitive results on modelnet40 classification task (~90%) and are working towards semantic segmentation and point cloud generation implementations. Usage git clone https://github.com/jackd/weighpoint.git cd weighpoint pip install -r requirements.txt pip install -e . cd config/cls python ../../weighpoint/bin/train.py --gin_file=uup-exp-geo1-ctg-l2-lrd2b The first time running this will require the download and preprocessing of the modelnet dataset. This will take quite some time (20min-ish, depending on computer/internet connection). The progress bar may appear frozen at times, but this is due to issues partially reading tar files. It should resolve itself eventually. Training progress can be observed using tensorboard: cd ~/weighpoint/cls tensorboard --logdir=./ The first epoch or two will normally have terrible evaluation loss/accuracy. This is due to fast training in the early stages outpacing the batch-normalization statistics, resulting in out-of-sync offset/scalings in evaluation mode. Theory See this paper for a basic overview of the theory associated with the operations. Python/Tensorflow Compatibility While we provide no guarantees, best efforts have been made to make this code compatible with python 2/3 and tensorflow versions >=1.13 and 2.0.0-alpha. Please note weighpoint.tf_compat includes some very dirty monkey-patching to make everything feel closer to 2.0. Importing any weighpoint subpackages will result in changes to the tf namespace and sub-name spaces which may affect external code. Neighborhood Implementations We use KDTree.query_pairs (along with random sampling and/or masking) to calculate neighborhoods with a variable number of neighbors. While no tensorflow implementation exists, we find performance is acceptable using tf.function during preprocessing (i.e. inside a tf.data.Dataset.map). We store the calculated neighborhoods in tf.RaggedTensors where possible and make extensive use of tf.ragged operations. Data Pipeline The data pipeline developed in this project is critical to the timely training of the networks without introducing custom operations. It is made up of:  tensorflow_dataset base implementations that manage downloading and serialization of the raw point cloud data; weighpoint/data/augmentation for model-independent preprocessing; weighpoint/meta for tools to write per-layer preprocessing operations (like KDTrees) along with learned components. The result is a learnable tf.keras.Model along with a model-dependent preprocessing and batching functions with a tf.data.Dataset pipeline.  Classification TODO Segmentation Work ongoing. Point Cloud Generation Work ongoing Issues  Saving in tf 2.0  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85201"}, "repo_url": "https://github.com/somil55/MC-GAN-", "repo_name": "MC-GAN-", "repo_full_name": "somil55/MC-GAN-", "repo_owner": "somil55", "repo_desc": "Multi-Content GAN for Few-Shot Font Style Transfer", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-06T04:56:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T01:56:37Z", "homepage": "", "size": 394, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185106308, "is_fork": false, "readme_text": "Objective Given a few colored and stylized texts of English alphabet generate remaining unobserved glyphs. Overview We implement MC-GAN architecture defined in Multi-Content GAN for Few-Shot Font Style Transfer to transfer glyph structure and style. Dataset The dataset(named Capitals_colorGrad64 ) consists of stylized color images with 26 English alphabets. Each image is of size 64x64. An example image is given below    Architecture We divide the network into two stages each specialized for a particular task. The first network, glphyNet models the overall glyph shape and structure. The second network, ornaNet models the appearance with color and texture, enabling the transfer of fine decorative elements. Each of these networks follow a conditional GAN architecture. Generator is basically an encoder decoder model and ensures that input and output are of same dimension. Descriminator consist of a local and global discriminator where weights of the local discriminator is shared with the latter. Receptive field of global discriminator is whole image.          Fig: glyphNet (Left) and ornaNet(Right)    Training GlyphNet is trained on the Capitals_colorGrad64 dataset.  All know and unknown alphabets are tiled across channels resulting in the input of size 64x64x26. Each unknown alphabet channel is set to one. An additional L1 loss is applied between the output of the generator and ground truth. ornaNet is trained following leave one out approach. For a known set of alphabet, we create a batch consisting of all know letters except one letter. We pass this batch through the glyphNet and obtain the corresponding letter for the left out alphabet. To obtain unknown alphabets we pass all the know alphabets through glyphNet. This is then taken as ornaNet conditional input and steps same as GlyphNet training are followed.    Baseline We implemented a baseline using image-to-image translation network for this task. In this baseline approach, we consider channel-wise letters in input and output stacks with dimensions B \u00d7 78 \u00d7 64 \u00d7 64, where B stands for training batch size and 78 corresponds to the 26 RGB channels. The input stack is given with \"observed\" color letters while all letters are generated in the output stack. We train this network on oCapitals_colorGrad64 where we have applied randomly chosen color gradients on each grayscale font. Feeding in a random subset of RGB letters of an arbitrary font into this model during test time, it is expected to generate stylistically similar 26 letters. The figure below gives an output obtained using baseline. The first row prints the conditional input, second-row prints fake image output and third-row prints ground truth.         Fig: Baseline  Result The following images display the outputs from glyphNet. The first row prints the conditional input, second-row prints fake image output by generator and the third-row prints ground truth. We observe that glyphNet can correctly generalize the output shape given very few glyphs of the typeface.          Fig: GlyphNet output    The next set of images display the outputs from ornaNet. The first row prints the conditional input to glyphNet generator, second-row prints conditional input to ornaNet generator, third-row prints ornaNet prediction and fourth-row prints the ground truth. ornaNet can generalize with the image gradients patterns and color.          Fig: ornaNet output    Requirements The following packages are a minimum requirement to run this project numpy==1.16.2 matplotlib==3.0.2 tensorflow-gpu==1.12.0 Keras==2.2.4  Instruction to run code  Download dataset with ./dataset/download_font_dataset.sh Capitals64 Run corresponding model in src directory by python src/<fileName>.py  ", "has_readme": true, "readme_language": "English", "repo_tags": ["gan", "style-transfer"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1712.00516"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85202"}, "repo_url": "https://github.com/christopherpryer/fyords", "repo_name": "fyords", "repo_full_name": "christopherpryer/fyords", "repo_owner": "christopherpryer", "repo_desc": "A python library for modeling operations research, data science, and financial engineering problems.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T21:18:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T13:23:14Z", "homepage": "", "size": 139, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185201412, "is_fork": false, "readme_text": " fyords A library for operations research, data science, and financial engineering. Info The purpose of developing this library is for personal learning and professional development. Subjects:  Open-source software development. Data Science. Operations Research. Financial Engineering. Visualizations in Python or JavaScript. Comprehensive self-education of tools such as NumPy, Pandas, D3.js, Matplotlib, IPython and jupyter, scikit-learn and SciPy, git, Google OR Tools (ortools), Pyomo, Supply Chain Guru, Keras and/or Hadoop, AWS, GCP.  The Library The code provides everything from low level optimization engines to higher level helper name-spaces. Cluster (engine) cluster is aimed at identifying groups in data. See clustering. Current Scope:  Greenfield Analysis - a facility location and operation problem.  Base (TBD) Grouped modules for abstracted fyords functionality. Constraint Solver (engine) constraint_solver is aimed at providing an optimization engine for constraint programming. Current Scope:  Routing with demand, capacity, time-windows, resource limitations and variety, pickups and deliveries, penalties and dropping demand, ambiguous origins and destinations, and more. More detail coming soon. Employee Scheduling.  helpers/Describe (helper) Grouped modules for descriptive statistics and general analysis of data. Current Scope:  Logistics-based opportunities  Graph Solver (engine) This solver is limited to solutions to graph theory problems. Current Scope:  Network Flow for supply chain design.  Learn (engine) learn is aimed at leveraging predictive or machine learning techniques. Current Scope:  Neural networks for demand forecasting. Regression for demand forecasting.  Linear Solver (engine) The linear_solver employs solutions to linear programming problems. helpers/Preprocess (helper) Name-space for preprocessing data. Quant (TBD) quant is aimed at providing financial engineering modules. quant is not limited to specifically financial applications of mathematics, but also involves subjects pertaining to the responsibilities of a quantitative analyst. There is a lot of overlap between OR and quantitative finance. Risk management is a good example of this. Various modeling methodologies in quantitative finance share similarities with both operations research and data science. For this reason quant is integrated with this library. Simulate (engine) simulate is an engine dedicated to various simulation suites. The word simulation is used loosely here and does not limit the engine to literal classifications of simulation. Current Scope:  Genetic Algorithm. Stochastic Simulation.  helpers Name-space for organized modules containing generally useful functions and objects. Current Scope:  Pandas-dataframe-based wrangling  helpers/Visualize (TBD) visualize is aimed at providing helpful data visualization suites. helpers/Wrangle Name-space for data wrangling wrappers and helpers. Collaborating Since this is a learning project, the code will be developed with the intention of solving a real-world problem pertaining to the developer. This will impact the evolution of this library's design. An example of this taylored design would be the module in the constraint_solver name-space. This name-space is intended to house a constraint programming engine with problem-agnostic design. Early on a routing.py module will house specified modeling such as DedicatedFleet and AmbiguousFleet. It is required for this evolution of design strategy to be followed if you would like to collaborate. Each problem-set must be developed in its own feature branch. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://supplychaindetective.com/2017/08/12/network-strategy-part-1-greenfield-analysis/"], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85203"}, "repo_url": "https://github.com/sthanhng/CNN-Visualization", "repo_name": "CNN-Visualization", "repo_full_name": "sthanhng/CNN-Visualization", "repo_owner": "sthanhng", "repo_desc": "This repo discovers how to develop simple visualizations for filters and feature maps in a Convolutional Neural Network", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T09:21:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T09:52:57Z", "homepage": "", "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185169672, "is_fork": false, "readme_text": "Convolutional Neural Network Visualization Overview  Visualizing Convolutional Layers Pre-trained VGG Model How to visualize filters How to visualize feature maps  Visualizing Convolutional Layers Convolutional neural networks are designed to work with image data, and their structure and function suggest that should be less inscrutable than other types of neural networks. Both filters and feature maps can be visualized. Pre-trained VGG Model We can load and summarize the VGG16 model with just a few lines of code: # Import the VGG16 model from keras.applications.vgg16 import VGG16  # Load the model model = VGG16() # Summarize the model model.summary() How to visualize filters The first step is to review the filters in the model, to see what we have to work with. Layer (type)                 Output Shape              Param # ================================================================= input_1 (InputLayer)         (None, 224, 224, 3)       0 _________________________________________________________________ block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792 _________________________________________________________________ block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928 _________________________________________________________________ block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0 _________________________________________________________________ block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856 _________________________________________________________________ block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584 _________________________________________________________________ block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0 _________________________________________________________________ block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168 _________________________________________________________________ block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080 _________________________________________________________________ block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080 _________________________________________________________________ block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0 _________________________________________________________________ block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160 _________________________________________________________________ block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808 _________________________________________________________________ block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808 _________________________________________________________________ block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0 _________________________________________________________________ block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808 _________________________________________________________________ block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808 _________________________________________________________________ block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808 _________________________________________________________________ block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0 _________________________________________________________________ flatten (Flatten)            (None, 25088)             0 _________________________________________________________________ fc1 (Dense)                  (None, 4096)              102764544 _________________________________________________________________ fc2 (Dense)                  (None, 4096)              16781312 _________________________________________________________________ predictions (Dense)          (None, 1000)              4097000 ================================================================= Total params: 138,357,544 Trainable params: 138,357,544 Non-trainable params: 0 _________________________________________________________________  The model summary printed above summarizes the output shape of each layer, e.g. the shape of the resulting feature maps. It does not five any idea of the shape of the filters (weights) in the network, only the total number of weights per layer. We can access all of the layers of the model via the model.layers property. Each layer has a layer.name property, where the convolutional layers have a naming convolution like block#_conv#, where the '#' is an integer. Each convolutional layer has two sets of weights:  One is the block of filters, and The other is the block of bias values.  These are accessible via the layer.get_weights() function. We can retrieve these weights and then summarize their shape. # Summarize filters in each convolutional layer from keras.applications.vgg16 import VGG16  # Load the model model = VGG16() # Summarize filter shapes for layer in model.layers:  # Check for convolutional layer  if 'conv' not in layer.name:   continue  # Get filter weights  filters, biases = layer.get_weights()  print(layer.name, filters.shape) A list of layer details block1_conv1 (3, 3, 3, 64) block1_conv2 (3, 3, 64, 64) block2_conv1 (3, 3, 64, 128) block2_conv2 (3, 3, 128, 128) block3_conv1 (3, 3, 128, 256) block3_conv2 (3, 3, 256, 256) block3_conv3 (3, 3, 256, 256) block4_conv1 (3, 3, 256, 512) block4_conv2 (3, 3, 512, 512) block4_conv3 (3, 3, 512, 512) block5_conv1 (3, 3, 512, 512) block5_conv2 (3, 3, 512, 512) block5_conv3 (3, 3, 512, 512)  We can retrieve the filters from the first layer: filters, biases = model.layers[1].get_weights() We can normalize their values to the range 0-1 to make them easy to visualize. f_min, f_max = filters.min(), filters.max() filters = (filters - f_min) / (f_max - f_min) Now we can enumerate the first six filters out of the 64 in the block and plot each of the three channels of each filter. from matplotlib import pyplot  # Plot first few filters n_filters, ix = 6, 1 for i in range(n_filters):  # Get the filter  f = filters[:, :, :, i]  # Plot each channel separately  for j in range(3):   # Specify subplot and turn of axis   ax = pyplot.subplot(n_filters, 3, ix)   ax.set_xticks([])   ax.set_yticks([])   # Plot filter channel in grayscale   pyplot.imshow(f[:, :, j], cmap='gray')   ix += 1 # Show the figure pyplot.show() How to visualize feature maps The activation maps (feature maps) capture the result of applying the filters to input, such as the input image or another feature map. The idea of visualizing a feature map for a specific input image would be to understand what features of the input are detected or preserved in the feature maps. The expectation would be that the feature maps close to the input detect small or fine-grained detail, whereas feature maps close to the output of the model capture more general features. The example below will enumerate all layers in the model and print the output size or feature map size for each convolutional layer as well as the layer index in the model. # Summarize feature map size for each conv layer from keras.applications.vgg16 import VGG16  # Load the model model = VGG16() # Summarize feature map shapes for i in range(len(model.layers)):  layer = model.layers[i]  # Check for convolutional layer  if 'conv' not in layer.name:   continue  # Summarize output shape  print(i, layer.name, layer.output.shape) Output: 1 block1_conv1 (?, 224, 224, 64) 2 block1_conv2 (?, 224, 224, 64) 4 block2_conv1 (?, 112, 112, 128) 5 block2_conv2 (?, 112, 112, 128) 7 block3_conv1 (?, 56, 56, 256) 8 block3_conv2 (?, 56, 56, 256) 9 block3_conv3 (?, 56, 56, 256) 11 block4_conv1 (?, 28, 28, 512) 12 block4_conv2 (?, 28, 28, 512) 13 block4_conv3 (?, 28, 28, 512) 15 block5_conv1 (?, 14, 14, 512) 16 block5_conv2 (?, 14, 14, 512) 17 block5_conv3 (?, 14, 14, 512)  We can use this information and design a new model that is subset of the layers in the full VGG16 model. For example, after loading the VGG model, we can define a new model that outputs a feature map from the first convolutional layer (index 1) as follows. # Redefine model to output right after the first hidden layer model = Model(inputs=model.inputs, outputs=model.layers[1].output) After defining the model, we need to load the bird image with the size expected by the model, in this case, 224\u00d7224. # Load the image with the required shape img = load_img('test_img.jpg', target_size=(224, 224))  # The image PIL object needs to be converted to a NumPy array of pixel data # and expanded from a 3D array to a 4D array with the dimensions of # [samples, rows, cols, channels], where we only have one sample.  # Convert the image to an array img = img_to_array(img)  # Expand dimensions so that it represents a single 'sample' img = expand_dims(img, axis=0)  # Prepare the image (e.g. scale pixel values for the vgg) img = preprocess_input(img) We are now ready to get the feature map. We can do this easy by calling the model.predict() function and passing in the prepared single image. # Get feature maps for the first hidden layer feature_maps = model.predict(img) We can plot all 64 two-dimensional images as an 8\u00d78 square of images. from matplotlib import pyplot  # Plot all 64 maps in an 8x8 squares square = 8 ix = 1 for _ in range(square):  for _ in range(square):   # Specify subplot and turn of axis   ax = pyplot.subplot(square, square, ix)   ax.set_xticks([])   ax.set_yticks([])   # Plot filter channel in grayscale   pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')   ix += 1 # how the figure pyplot.show() Running the example first summarizes the new, smaller model that takes an image and outputs a feature map. _________________________________________________________________ Layer (type)                 Output Shape              Param # ================================================================= input_1 (InputLayer)         (None, 224, 224, 3)       0 _________________________________________________________________ block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792 ================================================================= Total params: 1,792 Trainable params: 1,792 Non-trainable params: 0 _________________________________________________________________  Reference  How to Visualize Filters and Feature Maps in Convolutional Neural Networks  ", "has_readme": true, "readme_language": "English", "repo_tags": ["convolutional-neural-networks", "cnn", "vgg16", "cnn-visualization", "feature-map", "filters"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85204"}, "repo_url": "https://github.com/wtc-micky/capsule", "repo_name": "capsule", "repo_full_name": "wtc-micky/capsule", "repo_owner": "wtc-micky", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T15:46:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T14:26:46Z", "homepage": null, "size": 4248, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185212842, "is_fork": false, "readme_text": "MATRIX CAPSULES EM-Tensorflow   A Tensorflow implementation of CapsNet based on paper Matrix Capsules with EM Routing  Status:  With configuration A=32, B=8, C=16, D=16, batch_size=128, the code can work on a Tesla P40 GPU at a speed of 8s/iteration. The definitions of A-D can be referred to the paper. With configuration A=B=C=D=32, batch_size=64, the code can work on a Tesla P40 GPU at a speed of 25s/iteration. More optimization on implementation structure is required. Some modification and optimization is implemented to prompt the numerical stability of GMM. Specific explanations can be found in the code. With configuration A=32, B=4, D=4, D=4, batch_size=128, each iteration of training takes around 0.6s on a Tesla P40 GPU.    Current Results on smallNORB:    Configuration: A=32, B=8, C=16, D=16, batch_size=50, iteration number of EM routing: 2, with Coordinate Addition, spread loss, batch normalization   Training loss. Variation of loss is suppressed by batch normalization. However there still exists a gap between our best results and the reported results in the original paper.    Test accuracy(current best result is 91.8%)     Ablation Study on smallNORB:   Configuration: A=32, B=8, C=16, D=16, batch_size=32, iteration number of EM routing: 2, with Coordinate Addition, spread loss, test accuracy is 79.8%.   Current Results on MNIST:    Configuration: A=32, B=8, C=16, D=16, batch_size=50, iteration number of EM routing: 2, with Coordinate Addition, spread loss, batch normalization, reconstruction loss.   Training loss.    Test accuracy(current best result is 99.3%, only 10% samples are used in test)     Ablation Study on MNIST:   Configuration: A=32, B=4, C=4, D=4, batch_size=128, iteration number of EM routing: 2, no Coordinate Addition, cross entropy loss, test accuracy is 96.4%. Configuration: A=32, B=4, C=4, D=4, batch_size=128, iteration number of EM routing: 2, with Coordinate Addition, cross entropy loss, test accuracy is 96.8%. Configuration: A=32, B=8, C=16, D=16, batch_size=32, iteration number of EM routing: 2, with Coordinate Addition, spread loss 99.1%.   To Do List:  Experiments on smallNORB as in paper is about to be casted.   Any questions and comments to the code and the original algorithms are welcomed!!! My email: zhangsuofei at njupt.edu.cn Requirements  Python >= 3.4 Numpy Tensorflow >= 1.2.0 Keras  pip install -r requirement.txt Usage Step 1. Clone this repository with git. $ git clone https://github.com/www0wwwjs1/Matrix-Capsules-EM-Tensorflow.git $ cd Matrix-Capsules-EM-Tensorflow  Step 2. Download the MNIST dataset, mv and extract it into data/mnist directory.(Be careful the backslash appeared around the curly braces when you copy the wget  command to your terminal, remove it) $ mkdir -p data/mnist $ wget -c -P data/mnist http://yann.lecun.com/exdb/mnist/{train-images-idx3-ubyte.gz,train-labels-idx1-ubyte.gz,t10k-images-idx3-ubyte.gz,t10k-labels-idx1-ubyte.gz} $ gunzip data/mnist/*.gz  To install smallNORB, follow instructions in ./data/README.md Step 3. Start the training(MNIST): $ python3 train.py \"mnist\"  Step 4. Download the Fashion MNIST dataset, mv and extract it into data/fashion_mnist directory.(Be careful the backslash appeared around the curly braces when you copy the wget  command to your terminal, remove it) $ mkdir -p data/fashion_mnist $ wget -c -P data/fashion_mnist http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/{train-images-idx3-ubyte.gz,train-labels-idx1-ubyte.gz,t10k-images-idx3-ubyte.gz,t10k-labels-idx1-ubyte.gz} $ gunzip data/fashion_mnist/*.gz  Start the training(smallNORB): $ python3 train.py \"smallNORB\"  Start the training(CNN baseline): $ python3 train_baseline.py \"smallNORB\"  Step 4. View the status of training: $ tensorboard --logdir=./logdir/{model_name}/{dataset_name}/train_log/  Open the url tensorboard has shown. Step 5. Start the test on MNIST: $ python3 eval.py \"mnist\" \"caps\"  Start the test on smallNORB: $ python3 eval.py \"smallNORB\" \"caps\"  Step 6. View the status of test: $ tensorboard --logdir=./test_logdir/{model_name}/{dataset_name}/  Open the url tensorboard has shown. Reference  naturomics/CapsNet-Tensorflow: the implementation of Hinton's paper Dynamic Routing Between Capsules  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://yann.lecun.com/exdb/mnist/"], "reference_list": ["https://arxiv.org/abs/1710.09829"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85205"}, "repo_url": "https://github.com/vliu15/3d-brain-tumor-segmentation", "repo_name": "3d-brain-tumor-segmentation", "repo_full_name": "vliu15/3d-brain-tumor-segmentation", "repo_owner": "vliu15", "repo_desc": "Volumetric MRI brain tumor segmentation using autoencoder regularization", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-03T06:32:12Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T09:05:09Z", "homepage": null, "size": 20221, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185161620, "is_fork": false, "readme_text": "Volumetric Brain Tumor Segmentation We seek to build off of the model that won the BraTS 2018 Segmentation Challenge. The top model was created by the NVDLMED team under Andriy Myronenko, who is the first author of the 3D MRI brain tumor segmentation using autoencoder regularization paper that we use for reference. Model We adopt the encoder-decoder convolutional architecture described in the above paper with a variational autoencoder branch for regularizing the encoder. There are several architectural changes that we have made based on common practices combined with the incompleteness of the paper in some areas. We have  Added squeeze-excitation layers (SENet) in the ResNet blocks, as they have been shown to improve performance. Reordered all convolutional layers to consist of Conv3D, GroupNorm, ReLU, except for all pointwise and output layers. Replaced strided convolutions in downsampling with max pooling. Use He normal initialization for all layer kernels except those with sigmoid activations, which we use Glorot normal initialization for. Add per-epoch learning rate linear warmup from a base learning rate of 1e-6 for the first 10 epochs.   Use the --use_se flag to add squeeze-excitation layers to the model during training.   Use the --norm [group, batch, layer] flag to specify the normalization mechanism. Defaults to group for group normalization.   Use the --downsamp [max, avg, conv] flag to specify the downsampling method. Defaults to max for max pooling.   Use the --upsamp [linear, conv] flag to specify the upsampling method. Defaults to linear for linear upsampling.  Usage Dependencies are only supported for Python3 and can be found in requirements.txt. We use numpy==1.15 for the bulk of preprocessing and tensorflow==2.0.0-alpha0 for the model architecture, utilizing tf.keras.Model and tf.keras.Layer subclassing. BraTS Data The BraTS 2017/2018 dataset is not publicly available, so we cannot provide download scripts for those. Once downloaded, run preprocessing on the original data format, which should look something like this: BraTS17TrainingData/*/*/*[t1,t1ce,t2,flair,seg].nii.gz  Preprocessing For each example, there are 4 modalities and 1 label, each of shape 240 x 240 x 155. We slightly adjust the preprocessing steps described in the paper to match our use case with Stanford Medicine:  Concatenate the t1ce and flair modalities along the channel dimension. Compute per-channel image-wise mean and std and normalize per channel for the training set. With probability 0.75, flip and add each image to the dataset. Randomly crop 3 crops of size 144 x 144 x 128 from each image. Serialize to tf.TFRecord format for convenience in training.  Note that the preprocessed data is memory-intensive. The training and validation .tfrecords files are around 70GB and 20GB, respectively. See scripts/preprocess.sh for a detailed example of how to run preprocessing. python preprocess.py --brats_folder data/BraTS17TrainingData --create_val   All command-line arguments can be found in utils/arg_parser.py.   There are 285 training examples in the BraTS 2017/2018 training sets, but since we do not have access to the validation set, we opt for a 10:1 split and end up with 260 and 25 training and validation examples, respectively. To create this split, run with the --create_val flag.  Training As per the paper, we adopt all hyperparameters used in training (as default arguments). We will provide our training logs and graphs here shortly. See scripts/train.sh for a detailed example of how to run training. The size of the model can be adjusted in utils/constants.py. python3\\ train.py --train_loc data/train.image_wise.tfrecords --val_loc data/val.image_wise.tfrecords   Use the --gpu flag to run on GPU.  Testing: Generating Segmentation Masks The testing script test.py run inference on unlabeled data provided as input by generating sample labels on each crop of the image, then stitching them together to form the complete mask over the whole image. See scripts/test.sh for a detailed example of how to run testing. python test.py --test_folder /path/to/test/data --prepro_file data/image_mean_std.npy --chkpt_file chkpt.hdf5  You must specify the same model parameters as the ones use in training for the trained weights to be successfully loaded. Results We run training on a V100 32GB GPU. Each epoch consist of around 1400 examples after data augmentation and takes around 75 minutes to run. ", "has_readme": true, "readme_language": "English", "repo_tags": ["image-segmentation", "tensorflow", "variational-autoencoder", "brain-tumor-segmentation", "encoder-decoder", "convolutional-neural-network", "cnn", "keras", "keras-tensorflow", "unet", "dice-loss"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1810.11654.pdf", "https://arxiv.org/pdf/1810.11654.pdf"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85206"}, "repo_url": "https://github.com/KevinZhaoZL/SSD300_pytorch", "repo_name": "SSD300_pytorch", "repo_full_name": "KevinZhaoZL/SSD300_pytorch", "repo_owner": "KevinZhaoZL", "repo_desc": "\u4f7f\u7528\u76ee\u6807\u68c0\u6d4b\u6846\u67b6SSD300\u8bad\u7ec3\u81ea\u5df1\u7684\u6570\u636e", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-06T05:48:13Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-06T03:28:20Z", "homepage": "", "size": 2725, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185117442, "is_fork": false, "readme_text": "\u4f7f\u7528SSD300\u8bad\u7ec3\u81ea\u5df1\u7684\u6570\u636e\uff0c\u5c06\u76ae\u5f71\u4efb\u52a1\u7684\u56db\u80a2\uff0c\u5934\u90e8\uff0c\u8eab\u4f53\u4e3b\u5e72\u68c0\u6d4b\u51fa\u6765 \u539f\u59cb\u4ee3\u7801\u94fe\u63a5 https://github.com/amdegroot/ssd.pytorch \u6570\u636e\u96c6 \u8bad\u7ec3\u597d\u7684SSD\u6a21\u578b \u63d0\u53d6\u7801\uff1aqaxd vgg16\u9884\u8bad\u7ec3\u6a21\u578b \u63d0\u53d6\u7801\uff1airms \u8bad\u7ec3\u635f\u5931  \u6d4b\u8bd5\u7ed3\u679c     SSD: Single Shot MultiBox Object Detector, in PyTorch A PyTorch implementation of Single Shot MultiBox Detector from the 2016 paper by Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang, and Alexander C. Berg.  The official and original Caffe code can be found here.  Table of Contents  Installation Datasets Train Evaluate Performance Demos Future Work Reference  \u00a0 \u00a0 \u00a0 \u00a0 Installation  Install PyTorch by selecting your environment on the website and running the appropriate command. Clone this repository.  Note: We currently only support Python 3+.   Then download the dataset by following the instructions below. We now support Visdom for real-time loss visualization during training!  To use Visdom in the browser:  # First install Python server and client pip install visdom # Start the server (probably in a screen or tmux) python -m visdom.server  Then (during training) navigate to http://localhost:8097/ (see the Train section below for training details).   Note: For training, we currently support VOC and COCO, and aim to add ImageNet support soon.  Datasets To make things easy, we provide bash scripts to handle the dataset downloads and setup for you.  We also provide simple dataset loaders that inherit torch.utils.data.Dataset, making them fully compatible with the torchvision.datasets API. COCO Microsoft COCO: Common Objects in Context Download COCO 2014 # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/COCO2014.sh VOC Dataset PASCAL VOC: Visual Object Classes Download VOC2007 trainval & test # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2007.sh # <directory> Download VOC2012 trainval # specify a directory for dataset to be downloaded into, else default is ~/data/ sh data/scripts/VOC2012.sh # <directory> Training SSD  First download the fc-reduced VGG-16 PyTorch base network weights at:              https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth By default, we assume you have downloaded the file in the ssd.pytorch/weights dir:  mkdir weights cd weights wget https://s3.amazonaws.com/amdegroot-models/vgg16_reducedfc.pth  To train SSD using the train script simply specify the parameters listed in train.py as a flag or manually change them.  python train.py  Note:  For training, an NVIDIA GPU is strongly recommended for speed. For instructions on Visdom usage/installation, see the Installation section. You can pick-up training from a checkpoint by specifying the path as one of the training parameters (again, see train.py for options)    Evaluation To evaluate a trained network: python eval.py You can specify the parameters listed in the eval.py file by flagging them or manually changing them.  Performance VOC2007 Test mAP    Original Converted weiliu89 weights From scratch w/o data aug From scratch w/ data aug     77.2 % 77.26 % 58.12% 77.43 %    FPS GTX 1060: ~45.45 FPS Demos Use a pre-trained SSD network for detection Download a pre-trained network  We are trying to provide PyTorch state_dicts (dict of weight tensors) of the latest SSD model definitions trained on different datasets. Currently, we provide the following PyTorch models:  SSD300 trained on VOC0712 (newest PyTorch weights)  https://s3.amazonaws.com/amdegroot-models/ssd300_mAP_77.43_v2.pth   SSD300 trained on VOC0712 (original Caffe weights)  https://s3.amazonaws.com/amdegroot-models/ssd_300_VOC0712.pth     Our goal is to reproduce this table from the original paper    Try the demo notebook  Make sure you have jupyter notebook installed. Two alternatives for installing jupyter notebook:   If you installed PyTorch with conda (recommended), then you should already have it.  (Just  navigate to the ssd.pytorch cloned repo and run): jupyter notebook   If using pip:     # make sure pip is upgraded pip3 install --upgrade pip # install jupyter notebook pip install jupyter # Run this inside ssd.pytorch jupyter notebook  Now navigate to demo/demo.ipynb at http://localhost:8888 (by default) and have at it!  Try the webcam demo  Works on CPU (may have to tweak cv2.waitkey for optimal fps) or on an NVIDIA GPU This demo currently requires opencv2+ w/ python bindings and an onboard webcam  You can change the default webcam in demo/live.py   Install the imutils package to leverage multi-threading on CPU:  pip install imutils   Running python -m demo.live opens the webcam and begins detecting!  TODO We have accumulated the following to-do list, which we hope to complete in the near future  Still to come:   Support for the MS COCO dataset  Support for SSD512 training and testing  Support for training on custom datasets    Authors  Max deGroot Ellis Brown  Note: Unfortunately, this is just a hobby of ours and not a full-time job, so we'll do our best to keep things up to date, but no guarantees.  That being said, thanks to everyone for your continued help and feedback as it is really appreciated. We will try to address everything as soon as possible. References  Wei Liu, et al. \"SSD: Single Shot MultiBox Detector.\" ECCV2016. Original Implementation (CAFFE) A huge thank you to Alex Koltun and his team at Webyclip for their help in finishing the data augmentation portion. A list of other great SSD ports that were sources of inspiration (especially the Chainer repo):  Chainer, Keras, MXNet, Tensorflow    ", "has_readme": true, "readme_language": "English", "repo_tags": ["ssd", "pytorch", "object-detection"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://jupyter.readthedocs.io/en/latest/install.html", "http://pytorch.org/", "http://localhost:8097/", "http://mscoco.org/", "http://www.image-net.org/", "http://host.robots.ox.ac.uk/pascal/VOC/", "http://github.com/ellisbrown", "http://localhost:8888", "http://pytorch.org/docs/torchvision/datasets.html", "http://www.webyclip.com"], "reference_list": ["http://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1409.1556", "http://arxiv.org/abs/1512.02325"]}, {"_id": {"$oid": "5cf5189b7eb8d662f8b85207"}, "repo_url": "https://github.com/julianalverio/pytorch_nasnet", "repo_name": "pytorch_nasnet", "repo_full_name": "julianalverio/pytorch_nasnet", "repo_owner": "julianalverio", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T12:33:05Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-06T15:07:22Z", "homepage": null, "size": 578, "language": "Python", "has_wiki": true, "license": {"key": "bsd-3-clause", "name": "BSD 3-Clause \"New\" or \"Revised\" License", "spdx_id": "BSD-3-Clause", "url": "https://api.github.com/licenses/bsd-3-clause", "node_id": "MDc6TGljZW5zZTU="}, "open_issues_count": 0, "github_id": 185219994, "is_fork": false, "readme_text": "Pretrained models for Pytorch (Work in progress) The goal of this repo is:  to help to reproduce research papers results (transfer learning setups for instance), to access pretrained ConvNets with a unique interface/API inspired by torchvision.   News:  27/10/2018: Fix compatibility issues, Add tests, Add travis 04/06/2018: PolyNet and PNASNet-5-Large thanks to Alex Parinov 16/04/2018: SE-ResNet* and SE-ResNeXt* thanks to Alex Parinov 09/04/2018: SENet154 thanks to Alex Parinov 22/03/2018: CaffeResNet101 (good for localization with FasterRCNN) 21/03/2018: NASNet Mobile thanks to Veronika Yurchuk and Anastasiia 25/01/2018: DualPathNetworks thanks to Ross Wightman, Xception thanks to T Standley, improved TransformImage API 13/01/2018: pip install pretrainedmodels, pretrainedmodels.model_names, pretrainedmodels.pretrained_settings 12/01/2018: python setup.py install 08/12/2017: update data url (/!\\ git pull is needed) 30/11/2017: improve API (model.features(input), model.logits(features), model.forward(input), model.last_linear) 16/11/2017: nasnet-a-large pretrained model ported by T. Durand and R. Cadene 22/07/2017: torchvision pretrained models 22/07/2017: momentum in inceptionv4 and inceptionresnetv2 to 0.1 17/07/2017: model.input_range attribut 17/07/2017: BNInception pretrained on Imagenet  Summary  Installation Quick examples Few use cases  Compute imagenet logits Compute imagenet validation metrics   Evaluation on ImageNet  Accuracy on valset Reproducing results   Documentation  Available models  AlexNet BNInception CaffeResNet101 DenseNet121 DenseNet161 DenseNet169 DenseNet201 DenseNet201 DualPathNet68 DualPathNet92 DualPathNet98 DualPathNet107 DualPathNet113 FBResNet152 InceptionResNetV2 InceptionV3 InceptionV4 NASNet-A-Large NASNet-A-Mobile PNASNet-5-Large PolyNet ResNeXt101_32x4d ResNeXt101_64x4d ResNet101 ResNet152 ResNet18 ResNet34 ResNet50 SENet154 SE-ResNet50 SE-ResNet101 SE-ResNet152 SE-ResNeXt50_32x4d SE-ResNeXt101_32x4d SqueezeNet1_0 SqueezeNet1_1 VGG11 VGG13 VGG16 VGG19 VGG11_BN VGG13_BN VGG16_BN VGG19_BN Xception   Model API  model.input_size model.input_space model.input_range model.mean model.std model.features model.logits model.forward     Reproducing porting  ResNet* ResNeXt* Inception*    Installation  python3 with anaconda pytorch with/out CUDA  Install from pip  pip install pretrainedmodels  Install from repo  git clone https://github.com/Cadene/pretrained-models.pytorch.git cd pretrained-models.pytorch python setup.py install  Quick examples  To import pretrainedmodels:  import pretrainedmodels  To print the available pretrained models:  print(pretrainedmodels.model_names) > ['fbresnet152', 'bninception', 'resnext101_32x4d', 'resnext101_64x4d', 'inceptionv4', 'inceptionresnetv2', 'alexnet', 'densenet121', 'densenet169', 'densenet201', 'densenet161', 'resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152', 'inceptionv3', 'squeezenet1_0', 'squeezenet1_1', 'vgg11', 'vgg11_bn', 'vgg13', 'vgg13_bn', 'vgg16', 'vgg16_bn', 'vgg19_bn', 'vgg19', 'nasnetalarge', 'nasnetamobile', 'cafferesnet101', 'senet154',  'se_resnet50', 'se_resnet101', 'se_resnet152', 'se_resnext50_32x4d', 'se_resnext101_32x4d', 'cafferesnet101', 'polynet', 'pnasnet5large']  To print the available pretrained settings for a chosen model:  print(pretrainedmodels.pretrained_settings['nasnetalarge']) > {'imagenet': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth', 'input_space': 'RGB', 'input_size': [3, 331, 331], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1000}, 'imagenet+background': {'url': 'http://data.lip6.fr/cadene/pretrainedmodels/nasnetalarge-a1897284.pth', 'input_space': 'RGB', 'input_size': [3, 331, 331], 'input_range': [0, 1], 'mean': [0.5, 0.5, 0.5], 'std': [0.5, 0.5, 0.5], 'num_classes': 1001}}  To load a pretrained models from imagenet:  model_name = 'nasnetalarge' # could be fbresnet152 or inceptionresnetv2 model = pretrainedmodels.__dict__[model_name](num_classes=1000, pretrained='imagenet') model.eval() Note: By default, models will be downloaded to your $HOME/.torch folder. You can modify this behavior using the $TORCH_MODEL_ZOO variable as follow: export TORCH_MODEL_ZOO=\"/local/pretrainedmodels  To load an image and do a complete forward pass:  import torch import pretrainedmodels.utils as utils  load_img = utils.LoadImage()  # transformations depending on the model #\u00a0rescale, center crop, normalize, and others (ex: ToBGR, ToRange255) tf_img = utils.TransformImage(model)   path_img = 'data/cat.jpg'  input_img = load_img(path_img) input_tensor = tf_img(input_img)         # 3x400x225 -> 3x299x299 size may differ input_tensor = input_tensor.unsqueeze(0) # 3x299x299 -> 1x3x299x299 input = torch.autograd.Variable(input_tensor,     requires_grad=False)  output_logits = model(input) # 1x1000  To extract features (beware this API is not available for all networks):  output_features = model.features(input) # 1x14x14x2048 size may differ output_logits = model.logits(output_features) # 1x1000 Few use cases Compute imagenet logits  See examples/imagenet_logits.py to compute logits of classes appearance over a single image with a pretrained model on imagenet.  $ python examples/imagenet_logits.py -h > nasnetalarge, resnet152, inceptionresnetv2, inceptionv4, ...  $ python examples/imagenet_logits.py -a nasnetalarge --path_img data/cat.jpg > 'nasnetalarge': data/cat.jpg' is a 'tiger cat'   Compute imagenet evaluation metrics  See examples/imagenet_eval.py to evaluate pretrained models on imagenet valset.  $ python examples/imagenet_eval.py /local/common-data/imagenet_2012/images -a nasnetalarge -b 20 -e > * Acc@1 92.693, Acc@5 96.13  Evaluation on imagenet Accuracy on validation set (single model) Results were obtained using (center cropped) images of the same size than during the training process.    Model Version Acc@1 Acc@5     PNASNet-5-Large Tensorflow 82.858 96.182   PNASNet-5-Large Our porting 82.736 95.992   NASNet-A-Large Tensorflow 82.693 96.163   NASNet-A-Large Our porting 82.566 96.086   SENet154 Caffe 81.32 95.53   SENet154 Our porting 81.304 95.498   PolyNet Caffe 81.29 95.75   PolyNet Our porting 81.002 95.624   InceptionResNetV2 Tensorflow 80.4 95.3   InceptionV4 Tensorflow 80.2 95.3   SE-ResNeXt101_32x4d Our porting 80.236 95.028   SE-ResNeXt101_32x4d Caffe 80.19 95.04   InceptionResNetV2 Our porting 80.170 95.234   InceptionV4 Our porting 80.062 94.926   DualPathNet107_5k Our porting 79.746 94.684   ResNeXt101_64x4d Torch7 79.6 94.7   DualPathNet131 Our porting 79.432 94.574   DualPathNet92_5k Our porting 79.400 94.620   DualPathNet98 Our porting 79.224 94.488   SE-ResNeXt50_32x4d Our porting 79.076 94.434   SE-ResNeXt50_32x4d Caffe 79.03 94.46   Xception Keras 79.000 94.500   ResNeXt101_64x4d Our porting 78.956 94.252   Xception Our porting 78.888 94.292   ResNeXt101_32x4d Torch7 78.8 94.4   SE-ResNet152 Caffe 78.66 94.46   SE-ResNet152 Our porting 78.658 94.374   ResNet152 Pytorch 78.428 94.110   SE-ResNet101 Our porting 78.396 94.258   SE-ResNet101 Caffe 78.25 94.28   ResNeXt101_32x4d Our porting 78.188 93.886   FBResNet152 Torch7 77.84 93.84   SE-ResNet50 Caffe 77.63 93.64   SE-ResNet50 Our porting 77.636 93.752   DenseNet161 Pytorch 77.560 93.798   ResNet101 Pytorch 77.438 93.672   FBResNet152 Our porting 77.386 93.594   InceptionV3 Pytorch 77.294 93.454   DenseNet201 Pytorch 77.152 93.548   DualPathNet68b_5k Our porting 77.034 93.590   CaffeResnet101 Caffe 76.400 92.900   CaffeResnet101 Our porting 76.200 92.766   DenseNet169 Pytorch 76.026 92.992   ResNet50 Pytorch 76.002 92.980   DualPathNet68 Our porting 75.868 92.774   DenseNet121 Pytorch 74.646 92.136   VGG19_BN Pytorch 74.266 92.066   NASNet-A-Mobile Tensorflow 74.0 91.6   NASNet-A-Mobile Our porting 74.080 91.740   ResNet34 Pytorch 73.554 91.456   BNInception Our porting 73.524 91.562   VGG16_BN Pytorch 73.518 91.608   VGG19 Pytorch 72.080 90.822   VGG16 Pytorch 71.636 90.354   VGG13_BN Pytorch 71.508 90.494   VGG11_BN Pytorch 70.452 89.818   ResNet18 Pytorch 70.142 89.274   VGG13 Pytorch 69.662 89.264   VGG11 Pytorch 68.970 88.746   SqueezeNet1_1 Pytorch 58.250 80.800   SqueezeNet1_0 Pytorch 58.108 80.428   Alexnet Pytorch 56.432 79.194    Notes:  the Pytorch version of ResNet152 is not a porting of the Torch7 but has been retrained by facebook. For the PolyNet evaluation each image was resized to 378x378 without preserving the aspect ratio and then the central 331\u00d7331 patch from the resulting image was used.  Beware, the accuracy reported here is not always representative of the transferable capacity of the network on other tasks and datasets. You must try them all! :P Reproducing results Please see Compute imagenet validation metrics Documentation Available models NASNet* Source: TensorFlow Slim repo  nasnetalarge(num_classes=1000, pretrained='imagenet') nasnetalarge(num_classes=1001, pretrained='imagenet+background') nasnetamobile(num_classes=1000, pretrained='imagenet')  FaceBook ResNet* Source: Torch7 repo of FaceBook There are a bit different from the ResNet* of torchvision. ResNet152 is currently the only one available.  fbresnet152(num_classes=1000, pretrained='imagenet')  Caffe ResNet* Source: Caffe repo of KaimingHe  cafferesnet101(num_classes=1000, pretrained='imagenet')  Inception* Source: TensorFlow Slim repo and Pytorch/Vision repo for inceptionv3  inceptionresnetv2(num_classes=1000, pretrained='imagenet') inceptionresnetv2(num_classes=1001, pretrained='imagenet+background') inceptionv4(num_classes=1000, pretrained='imagenet') inceptionv4(num_classes=1001, pretrained='imagenet+background') inceptionv3(num_classes=1000, pretrained='imagenet')  BNInception Source: Trained with Caffe by Xiong Yuanjun  bninception(num_classes=1000, pretrained='imagenet')  ResNeXt* Source: ResNeXt repo of FaceBook  resnext101_32x4d(num_classes=1000, pretrained='imagenet') resnext101_62x4d(num_classes=1000, pretrained='imagenet')  DualPathNetworks Source: MXNET repo of Chen Yunpeng The porting has been made possible by Ross Wightman in his PyTorch repo. As you can see here DualPathNetworks allows you to try different scales. The default one in this repo is 0.875 meaning that the original input size is 256 before croping to 224.  dpn68(num_classes=1000, pretrained='imagenet') dpn98(num_classes=1000, pretrained='imagenet') dpn131(num_classes=1000, pretrained='imagenet') dpn68b(num_classes=1000, pretrained='imagenet+5k') dpn92(num_classes=1000, pretrained='imagenet+5k') dpn107(num_classes=1000, pretrained='imagenet+5k')  'imagenet+5k' means that the network has been pretrained on imagenet5k before being finetuned on imagenet1k. Xception Source: Keras repo The porting has been made possible by T Standley.  xception(num_classes=1000, pretrained='imagenet')  SENet* Source: Caffe repo of Jie Hu  senet154(num_classes=1000, pretrained='imagenet') se_resnet50(num_classes=1000, pretrained='imagenet') se_resnet101(num_classes=1000, pretrained='imagenet') se_resnet152(num_classes=1000, pretrained='imagenet') se_resnext50_32x4d(num_classes=1000, pretrained='imagenet') se_resnext101_32x4d(num_classes=1000, pretrained='imagenet')  PNASNet* Source: TensorFlow Slim repo  pnasnet5large(num_classes=1000, pretrained='imagenet') pnasnet5large(num_classes=1001, pretrained='imagenet+background')  PolyNet Source: Caffe repo of the CUHK Multimedia Lab  polynet(num_classes=1000, pretrained='imagenet')  TorchVision Source: Pytorch/Vision repo (inceptionv3 included in Inception*)  resnet18(num_classes=1000, pretrained='imagenet') resnet34(num_classes=1000, pretrained='imagenet') resnet50(num_classes=1000, pretrained='imagenet') resnet101(num_classes=1000, pretrained='imagenet') resnet152(num_classes=1000, pretrained='imagenet') densenet121(num_classes=1000, pretrained='imagenet') densenet161(num_classes=1000, pretrained='imagenet') densenet169(num_classes=1000, pretrained='imagenet') densenet201(num_classes=1000, pretrained='imagenet') squeezenet1_0(num_classes=1000, pretrained='imagenet') squeezenet1_1(num_classes=1000, pretrained='imagenet') alexnet(num_classes=1000, pretrained='imagenet') vgg11(num_classes=1000, pretrained='imagenet') vgg13(num_classes=1000, pretrained='imagenet') vgg16(num_classes=1000, pretrained='imagenet') vgg19(num_classes=1000, pretrained='imagenet') vgg11_bn(num_classes=1000, pretrained='imagenet') vgg13_bn(num_classes=1000, pretrained='imagenet') vgg16_bn(num_classes=1000, pretrained='imagenet') vgg19_bn(num_classes=1000, pretrained='imagenet')  Model API Once a pretrained model has been loaded, you can use it that way. Important note: All image must be loaded using PIL which scales the pixel values between 0 and 1. model.input_size Attribut of type list composed of 3 numbers:  number of color channels, height of the input image, width of the input image.  Example:  [3, 299, 299] for inception* networks, [3, 224, 224] for resnet* networks.  model.input_space Attribut of type str representating the color space of the image. Can be RGB or BGR. model.input_range Attribut of type list composed of 2 numbers:  min pixel value, max pixel value.  Example:  [0, 1] for resnet* and inception* networks, [0, 255] for bninception network.  model.mean Attribut of type list composed of 3 numbers which are used to normalize the input image (substract \"color-channel-wise\"). Example:  [0.5, 0.5, 0.5] for inception* networks, [0.485, 0.456, 0.406] for resnet* networks.  model.std Attribut of type list composed of 3 numbers which are used to normalize the input image (divide \"color-channel-wise\"). Example:  [0.5, 0.5, 0.5] for inception* networks, [0.229, 0.224, 0.225] for resnet* networks.  model.features /!\\ work in progress (may not be available) Method which is used to extract the features from the image. Example when the model is loaded using fbresnet152: print(input_224.size())            # (1,3,224,224) output = model.features(input_224)  print(output.size())               # (1,2048,1,1)  # print(input_448.size())          # (1,3,448,448) output = model.features(input_448) # print(output.size())             # (1,2048,7,7) model.logits /!\\ work in progress (may not be available) Method which is used to classify the features from the image. Example when the model is loaded using fbresnet152: output = model.features(input_224)  print(output.size())               # (1,2048, 1, 1) output = model.logits(output) print(output.size())               # (1,1000) model.forward Method used to call model.features and model.logits. It can be overwritten as desired. Note: A good practice is to use model.__call__ as your function of choice to forward an input to your model. See the example bellow. # Without model.__call__ output = model.forward(input_224) print(output.size())      # (1,1000)  # With model.__call__ output = model(input_224) print(output.size())      # (1,1000) model.last_linear Attribut of type nn.Linear. This module is the last one to be called during the forward pass.  Can be replaced by an adapted nn.Linear for fine tuning. Can be replaced by pretrained.utils.Identity for features extraction.  Example when the model is loaded using fbresnet152: print(input_224.size())            # (1,3,224,224) output = model.features(input_224)  print(output.size())               # (1,2048,1,1) output = model.logits(output) print(output.size())               # (1,1000)  # fine tuning dim_feats = model.last_linear.in_features # =2048 nb_classes = 4 model.last_linear = nn.Linear(dim_feats, nb_classes) output = model(input_224) print(output.size())               # (1,4)  # features extraction model.last_linear = pretrained.utils.Identity() output = model(input_224) print(output.size())               # (1,2048) Reproducing Hand porting of ResNet152 th pretrainedmodels/fbresnet/resnet152_dump.lua python pretrainedmodels/fbresnet/resnet152_load.py  Automatic porting of ResNeXt https://github.com/clcarwin/convert_torch_to_pytorch Hand porting of NASNet, InceptionV4 and InceptionResNetV2 https://github.com/Cadene/tensorflow-model-zoo.torch Acknowledgement Thanks to the deep learning community and especially to the contributers of the pytorch ecosystem. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://yjxiong.me", "http://pytorch.org", "http://rwightman.com"], "reference_list": ["https://arxiv.org/abs/1712.00559"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbbf"}, "repo_url": "https://github.com/CyberZHG/tf-keras-kervolution-2d", "repo_name": "tf-keras-kervolution-2d", "repo_full_name": "CyberZHG/tf-keras-kervolution-2d", "repo_owner": "CyberZHG", "repo_desc": "Kervolutional neural networks", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-03T12:38:26Z", "repo_watch": 7, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-08T04:47:16Z", "homepage": null, "size": 15, "language": "Python", "has_wiki": false, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185530020, "is_fork": false, "readme_text": "Tf-Keras Kervolution 2D    Unofficial implementation of Kervolutional Neural Networks. Install python setup.py install Usage Basic from tensorflow.python.keras.models import Sequential from tensorflow.python.keras.layers import Flatten, Dense from tf_keras_kervolution_2d import KernelConv2D, PolynomialKernel   model = Sequential() model.add(KernelConv2D(     input_shape=(3, 5, 5),     filters=4,     kernel_size=3,     kernel_function=PolynomialKernel(p=3, trainable_c=True), )) model.add(Flatten()) model.add(Dense(units=2, activation='softmax')) model.compile(optimizer='adam', loss='sparse_categorical_crossentropy') model.summary() Kernels from tf_keras_kervolution_2d import LinearKernel      # Equivalent to normal convolution from tf_keras_kervolution_2d import L1Kernel          # Manhattan distance from tf_keras_kervolution_2d import L2Kernel          # Euclidean distance from tf_keras_kervolution_2d import PolynomialKernel  # Polynomial from tf_keras_kervolution_2d import GaussianKernel    # Gaussin / RBF ", "has_readme": true, "readme_language": "English", "repo_tags": ["convolutional-neural-networks"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1904.03955.pdf"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc0"}, "repo_url": "https://github.com/liuyaaaaang/Dv_Net", "repo_name": "Dv_Net", "repo_full_name": "liuyaaaaang/Dv_Net", "repo_owner": "liuyaaaaang", "repo_desc": "A fusion network of VGG16 and DenseNet", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T08:11:45Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T02:19:48Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185511788, "is_fork": false, "readme_text": "Dv_Net A fusion network of VGG16 and DenseNet This is a simple convolutional neural network based on DenseNet and VGG16. requirement python 3.5  keras 2.1.1  tensorflow 1.10.0  numpy 1.12.0 usage Only use : python Main.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc1"}, "repo_url": "https://github.com/double-y/deep_learning_docker_cluster", "repo_name": "deep_learning_docker_cluster", "repo_full_name": "double-y/deep_learning_docker_cluster", "repo_owner": "double-y", "repo_desc": "docker templates for deep learning project", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T02:28:39Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T09:04:19Z", "homepage": "", "size": 2676, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185570629, "is_fork": false, "readme_text": "This is docker template for deep learning project System Requirements  Install docker-compose Install nvidia-docker (only for gpu usage)  Containers flask_keras_2_2_4_cpu flask api container for Keras 2.2.4 $ docker-compose up flask_keras_2_2_4_cpu  In different shell window, $ curl -F \"img=@./data/sample/gu/gu1.jpg\" http://localhost:5000/images {\"result\":1.0,\"success\":true}  jupyter_keras_2_2_4_cpu jupyter container for Keras 2.2.4 $ docker-compose up jupyter_keras_2_2_4_cpu  Access http://localhost:8888/ See keras_2_2_4_cpu_example.ipynb to see an example jupyter_pytorch_1_0_0_gpu jupyter container for PyTorch 1.0.0 $ docker-compose up jupyter_pytorch_1_0_0_gpu  Access http://localhost:28888/ See pytorch_1_0_0_gpu_example.ipynb to see an example train_keras_2_2_4_cpu training container example for Keras 2.2.4 $ docker-compose up train_keras_2_2_4_cpu  train_keras_2_2_4_gpu training gpu container example for Keras 2.2.4 $ docker-compose up train_keras_2_2_4_gpu  nginx nginx container to deploy application to web server $ docker-compose up nginx  $ curl -F \"img=@./data/sample/gu/gu1.jpg\" http://localhost/images {\"result\":1.0,\"success\":true}  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://localhost:28888/", "http://localhost:8888/"], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc2"}, "repo_url": "https://github.com/hanneko/keras_unet", "repo_name": "keras_unet", "repo_full_name": "hanneko/keras_unet", "repo_owner": "hanneko", "repo_desc": "U-Net for Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T18:32:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T11:52:07Z", "homepage": "", "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185595995, "is_fork": false, "readme_text": "U-Net for Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc3"}, "repo_url": "https://github.com/alanjia163/Keras", "repo_name": "Keras", "repo_full_name": "alanjia163/Keras", "repo_owner": "alanjia163", "repo_desc": "Keras\u57fa\u7840", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-24T15:22:21Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T11:30:50Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185592983, "is_fork": false, "readme_text": "Keras Keras\u57fa\u7840 ", "has_readme": true, "readme_language": "Malay (macrolanguage)", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc4"}, "repo_url": "https://github.com/voldelord/starGAN-keras", "repo_name": "starGAN-keras", "repo_full_name": "voldelord/starGAN-keras", "repo_owner": "voldelord", "repo_desc": "Creating keras implementation of starGan", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-03T12:02:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T21:20:07Z", "homepage": null, "size": 167, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185680903, "is_fork": false, "readme_text": "starGAN-keras Creating keras implementation of starGan ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc5"}, "repo_url": "https://github.com/jjpakingan/KerasSeriesData", "repo_name": "KerasSeriesData", "repo_full_name": "jjpakingan/KerasSeriesData", "repo_owner": "jjpakingan", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T22:56:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T21:11:16Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185679930, "is_fork": false, "readme_text": "Keras Series Data For Machine Learning Data Evaluation using Keras and Tensorflow on the back-end. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc6"}, "repo_url": "https://github.com/braquino/search_by_image", "repo_name": "search_by_image", "repo_full_name": "braquino/search_by_image", "repo_owner": "braquino", "repo_desc": "This is under contruction", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T00:12:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T23:59:12Z", "homepage": null, "size": 6923, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185695401, "is_fork": false, "readme_text": "SSD: Single-Shot MultiBox Detector implementation in Keras  Contents  Overview Performance Examples Dependencies How to use it Download the convolutionalized VGG-16 weights Download the original trained model weights How to fine-tune one of the trained models on your own dataset ToDo Important notes Terminology  Overview This is a Keras port of the SSD model architecture introduced by Wei Liu et al. in the paper SSD: Single Shot MultiBox Detector. Ports of the trained weights of all the original models are provided below. This implementation is accurate, meaning that both the ported weights and models trained from scratch produce the same mAP values as the respective models of the original Caffe implementation (see performance section below). The main goal of this project is to create an SSD implementation that is well documented for those who are interested in a low-level understanding of the model. The provided tutorials, documentation and detailed comments hopefully make it a bit easier to dig into the code and adapt or build upon the model than with most other implementations out there (Keras or otherwise) that provide little to no documentation and comments. The repository currently provides the following network architectures:  SSD300: keras_ssd300.py SSD512: keras_ssd512.py SSD7: keras_ssd7.py - a smaller 7-layer version that can be trained from scratch relatively quickly even on a mid-tier GPU, yet is capable enough for less complex object detection tasks and testing. You're obviously not going to get state-of-the-art results with that one, but it's fast.  If you would like to use one of the provided trained models for transfer learning (i.e. fine-tune one of the trained models on your own dataset), there is a Jupyter notebook tutorial that helps you sub-sample the trained weights so that they are compatible with your dataset, see further below. If you would like to build an SSD with your own base network architecture, you can use keras_ssd7.py as a template, it provides documentation and comments to help you. Performance Here are the mAP evaluation results of the ported weights and below that the evaluation results of a model trained from scratch using this implementation. All models were evaluated using the official Pascal VOC test server (for 2012 test) or the official Pascal VOC Matlab evaluation script (for 2007 test). In all cases the results match (or slightly surpass) those of the original Caffe models. Download links to all ported weights are available further below.    Mean Average Precision   evaluated on VOC2007 test VOC2012 test   trained onIoU rule 07+120.5 07+12+COCO0.5 07++12+COCO0.5   SSD300 77.5 81.2 79.4   SSD512 79.8 83.2 82.3   Training an SSD300 from scratch to convergence on Pascal VOC 2007 trainval and 2012 trainval produces the same mAP on Pascal VOC 2007 test as the original Caffe SSD300 \"07+12\" model. You can find a summary of the training here.    Mean Average Precision    Original Caffe Model Ported Weights Trained from Scratch   SSD300 \"07+12\" 0.772 0.775 0.771   The models achieve the following average number of frames per second (FPS) on Pascal VOC on an NVIDIA GeForce GTX 1070 mobile (i.e. the laptop version) and cuDNN v6. There are two things to note here. First, note that the benchmark prediction speeds of the original Caffe implementation were achieved using a TitanX GPU and cuDNN v4. Second, the paper says they measured the prediction speed at batch size 8, which I think isn't a meaningful way of measuring the speed. The whole point of measuring the speed of a detection model is to know how many individual sequential images the model can process per second, therefore measuring the prediction speed on batches of images and then deducing the time spent on each individual image in the batch defeats the purpose. For the sake of comparability, below you find the prediction speed for the original Caffe SSD implementation and the prediction speed for this implementation under the same conditions, i.e. at batch size 8. In addition you find the prediction speed for this implementation at batch size 1, which in my opinion is the more meaningful number.    Frames per Second    Original Caffe Implementation This Implementation   Batch Size 8 8 1   SSD300 46 49 39   SSD512 19 25 20   SSD7  216 127   Examples Below are some prediction examples of the fully trained original SSD300 \"07+12\" model (i.e. trained on Pascal VOC2007 trainval and VOC2012 trainval). The predictions were made on Pascal VOC2007 test.                   Here are some prediction examples of an SSD7 (i.e. the small 7-layer version) partially trained on two road traffic datasets released by Udacity with roughly 20,000 images in total and 5 object categories (more info in ssd7_training.ipynb). The predictions you see below were made after 10,000 training steps at batch size 32. Admittedly, cars are comparatively easy objects to detect and I picked a few of the better examples, but it is nonetheless remarkable what such a small model can do after only 10,000 training iterations.                   Dependencies  Python 3.x Numpy TensorFlow 1.x Keras 2.x OpenCV Beautiful Soup 4.x  The Theano and CNTK backends are currently not supported. Python 2 compatibility: This implementation seems to work with Python 2.7, but I don't provide any support for it. It's 2018 and nobody should be using Python 2 anymore. How to use it This repository provides Jupyter notebook tutorials that explain training, inference and evaluation, and there are a bunch of explanations in the subsequent sections that complement the notebooks. How to use a trained model for inference:  ssd300_inference.ipynb ssd512_inference.ipynb  How to train a model:  ssd300_training.ipynb ssd7_training.ipynb  How to use one of the provided trained models for transfer learning on your own dataset:  Read below  How to evaluate a trained model:  In general: ssd300_evaluation.ipynb On MS COCO: ssd300_evaluation_COCO.ipynb  How to use the data generator:  The data generator used here has its own repository with a detailed tutorial here  Training details The general training setup is layed out and explained in ssd7_training.ipynb and in ssd300_training.ipynb. The setup and explanations are similar in both notebooks for the most part, so it doesn't matter which one you look at to understand the general training setup, but the parameters in ssd300_training.ipynb are preset to copy the setup of the original Caffe implementation for training on Pascal VOC, while the parameters in ssd7_training.ipynb are preset to train on the Udacity traffic datasets. To train the original SSD300 model on Pascal VOC:  Download the datasets:  wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtrainval_06-Nov-2007.tar wget http://host.robots.ox.ac.uk/pascal/VOC/voc2007/VOCtest_06-Nov-2007.tar  Download the weights for the convolutionalized VGG-16 or for one of the trained original models provided below. Set the file paths for the datasets and model weights accordingly in ssd300_training.ipynb and execute the cells.  The procedure for training SSD512 is the same of course. It is imperative that you load the pre-trained VGG-16 weights when attempting to train an SSD300 or SSD512 from scratch, otherwise the training will probably fail. Here is a summary of a full training of the SSD300 \"07+12\" model for comparison with your own training:  SSD300 Pascal VOC \"07+12\" training summary  Encoding and decoding boxes The ssd_encoder_decoder sub-package contains all functions and classes related to encoding and decoding boxes. Encoding boxes means converting ground truth labels into the target format that the loss function needs during training. It is this encoding process in which the matching of ground truth boxes to anchor boxes (the paper calls them default boxes and in the original C++ code they are called priors - all the same thing) happens. Decoding boxes means converting raw model output back to the input label format, which entails various conversion and filtering processes such as non-maximum suppression (NMS). In order to train the model, you need to create an instance of SSDInputEncoder that needs to be passed to the data generator. The data generator does the rest, so you don't usually need to call any of SSDInputEncoder's methods manually. Models can be created in 'training' or 'inference' mode. In 'training' mode, the model outputs the raw prediction tensor that still needs to be post-processed with coordinate conversion, confidence thresholding, non-maximum suppression, etc. The functions decode_detections() and decode_detections_fast() are responsible for that. The former follows the original Caffe implementation, which entails performing NMS per object class, while the latter performs NMS globally across all object classes and is thus more efficient, but also behaves slightly differently. Read the documentation for details about both functions. If a model is created in 'inference' mode, its last layer is the DecodeDetections layer, which performs all the post-processing that decode_detections() does, but in TensorFlow. That means the output of the model is already the post-processed output. In order to be trainable, a model must be created in 'training' mode. The trained weights can then later be loaded into a model that was created in 'inference' mode. A note on the anchor box offset coordinates used internally by the model: This may or may not be obvious to you, but it is important to understand that it is not possible for the model to predict absolute coordinates for the predicted bounding boxes. In order to be able to predict absolute box coordinates, the convolutional layers responsible for localization would need to produce different output values for the same object instance at different locations within the input image. This isn't possible of course: For a given input to the filter of a convolutional layer, the filter will produce the same output regardless of the spatial position within the image because of the shared weights. This is the reason why the model predicts offsets to anchor boxes instead of absolute coordinates, and why during training, absolute ground truth coordinates are converted to anchor box offsets in the encoding process. The fact that the model predicts offsets to anchor box coordinates is in turn the reason why the model contains anchor box layers that do nothing but output the anchor box coordinates so that the model's output tensor can include those. If the model's output tensor did not contain the anchor box coordinates, the information to convert the predicted offsets back to absolute coordinates would be missing in the model output. Using a different base network architecture If you want to build a different base network architecture, you could use keras_ssd7.py as a template. It provides documentation and comments to help you turn it into a different base network. Put together the base network you want and add a predictor layer on top of each network layer from which you would like to make predictions. Create two predictor heads for each, one for localization, one for classification. Create an anchor box layer for each predictor layer and set the respective localization head's output as the input for the anchor box layer. The structure of all tensor reshaping and concatenation operations remains the same, you just have to make sure to include all of your predictor and anchor box layers of course. Download the convolutionalized VGG-16 weights In order to train an SSD300 or SSD512 from scratch, download the weights of the fully convolutionalized VGG-16 model trained to convergence on ImageNet classification here: VGG_ILSVRC_16_layers_fc_reduced.h5. As with all other weights files below, this is a direct port of the corresponding .caffemodel file that is provided in the repository of the original Caffe implementation. Download the original trained model weights Here are the ported weights for all the original trained models. The filenames correspond to their respective .caffemodel counterparts. The asterisks and footnotes refer to those in the README of the original Caffe implementation.   PASCAL VOC models:  07+12: SSD300*, SSD512* 07++12: SSD300*, SSD512* COCO[1]: SSD300*, SSD512* 07+12+COCO: SSD300*, SSD512* 07++12+COCO: SSD300*, SSD512*    COCO models:  trainval35k: SSD300*, SSD512*    ILSVRC models:  trainval1: SSD300*, SSD500    How to fine-tune one of the trained models on your own dataset If you want to fine-tune one of the provided trained models on your own dataset, chances are your dataset doesn't have the same number of classes as the trained model. The following tutorial explains how to deal with this problem: weight_sampling_tutorial.ipynb ToDo The following things are on the to-do list, ranked by priority. Contributions are welcome, but please read the contributing guidelines.  Add model definitions and trained weights for SSDs based on other base networks such as MobileNet, InceptionResNetV2, or DenseNet. Add support for the Theano and CNTK backends. Requires porting the custom layers and the loss function from TensorFlow to the abstract Keras backend.  Currently in the works:  A new Focal Loss loss function.  Important notes  All trained models that were trained on MS COCO use the smaller anchor box scaling factors provided in all of the Jupyter notebooks. In particular, note that the '07+12+COCO' and '07++12+COCO' models use the smaller scaling factors.  Terminology  \"Anchor boxes\": The paper calls them \"default boxes\", in the original C++ code they are called \"prior boxes\" or \"priors\", and the Faster R-CNN paper calls them \"anchor boxes\". All terms mean the same thing, but I slightly prefer the name \"anchor boxes\" because I find it to be the most descriptive of these names. I call them \"prior boxes\" or \"priors\" in keras_ssd300.py and keras_ssd512.py to stay consistent with the original Caffe implementation, but everywhere else I use the name \"anchor boxes\" or \"anchors\". \"Labels\": For the purpose of this project, datasets consist of \"images\" and \"labels\". Everything that belongs to the annotations of a given image is the \"labels\" of that image: Not just object category labels, but also bounding box coordinates. \"Labels\" is just shorter than \"annotations\". I also use the terms \"labels\" and \"targets\" more or less interchangeably throughout the documentation, although \"targets\" means labels specifically in the context of training. \"Predictor layer\": The \"predictor layers\" or \"predictors\" are all the last convolution layers of the network, i.e. all convolution layers that do not feed into any subsequent convolution layers.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1512.02325", "https://arxiv.org/abs/1708.02002"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc7"}, "repo_url": "https://github.com/yyysjz1997/yolov3-myself", "repo_name": "yolov3-myself", "repo_full_name": "yyysjz1997/yolov3-myself", "repo_owner": "yyysjz1997", "repo_desc": "yolov3 myself", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T08:20:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T06:19:29Z", "homepage": null, "size": 3551, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185542966, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc8"}, "repo_url": "https://github.com/Super-Jolly/ModelImplement", "repo_name": "ModelImplement", "repo_full_name": "Super-Jolly/ModelImplement", "repo_owner": "Super-Jolly", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-14T08:12:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T09:53:40Z", "homepage": null, "size": 25108, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185579020, "is_fork": false, "readme_text": "ModelImplement We train the two-stage models separately and save the best models. #Experiment enviroment: python\u2014\u20143.6 Keras\u2014\u20142.2.0 Tensorflow\u2014\u20141.8.0 keras-self-attention\u2014\u20140.31.0 scikit-learn\u2014\u20140.20.1 In this project we provided the code for the above two stages of stage1.py and stage2.py. User can use useTwoStageModel.py to automatic extract DDIs from xml format corpora. The test instances are finally classified using the saved two model through the architecture of Fig 2. Since the model contains multiple layer, it generally need some time to train. If the users have no time or GPU to train model, the saved model in the model_saved can be loaded to test. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Super-Jolly/ModelImplement/blob/928ae6c1a0a3a797a62f84719bd0c273bec7b42f/model_saved/stage1ModelWeight.hdf5", "https://github.com/Super-Jolly/ModelImplement/blob/928ae6c1a0a3a797a62f84719bd0c273bec7b42f/model_saved/stage2ModelWeight.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbc9"}, "repo_url": "https://github.com/hanneko/keras_alocc", "repo_name": "keras_alocc", "repo_full_name": "hanneko/keras_alocc", "repo_owner": "hanneko", "repo_desc": "Adversarially Learned One-Class Classifier for Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T14:30:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T11:18:10Z", "homepage": "", "size": 12, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185591280, "is_fork": false, "readme_text": "Adversarially Learned One-Class Classifier for Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbca"}, "repo_url": "https://github.com/hiroshijsc/keras-image-recognition", "repo_name": "keras-image-recognition", "repo_full_name": "hiroshijsc/keras-image-recognition", "repo_owner": "hiroshijsc", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-11T04:33:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T15:38:47Z", "homepage": null, "size": 33594, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185634989, "is_fork": false, "readme_text": "keras-image-recognition ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbcb"}, "repo_url": "https://github.com/awakedev/handwritten-digits-classifier", "repo_name": "handwritten-digits-classifier", "repo_full_name": "awakedev/handwritten-digits-classifier", "repo_owner": "awakedev", "repo_desc": "a classifier for handwritten digits that boasts over 99% accuracy on the famous MNIST dataset.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T15:25:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T15:25:38Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185632737, "is_fork": false, "readme_text": "keras-tutorial ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbcc"}, "repo_url": "https://github.com/sunghlim/DNN-fading-channel", "repo_name": "DNN-fading-channel", "repo_full_name": "sunghlim/DNN-fading-channel", "repo_owner": "sunghlim", "repo_desc": "Autoencoder PHY for fading channel", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T00:51:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T00:47:34Z", "homepage": null, "size": 20313, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185499652, "is_fork": false, "readme_text": "DNN-fading-channel Autoencoder PHY for fading channel Needs Tensorflow, Matlibplot, Keras, numpy ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbcd"}, "repo_url": "https://github.com/himeno/tutorial_tensorflow_text_classification", "repo_name": "tutorial_tensorflow_text_classification", "repo_full_name": "himeno/tutorial_tensorflow_text_classification", "repo_owner": "himeno", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-16T13:49:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T12:00:08Z", "homepage": null, "size": 2896, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185597263, "is_fork": false, "readme_text": "TensorFlow\u30c1\u30e5\u30fc\u30c8\u30ea\u30a2\u30eb \u6620\u753b\u30ec\u30d3\u30e5\u30fc\u306e\u30c6\u30ad\u30b9\u30c8\u5206\u985e https://www.tensorflow.org/tutorials/keras/basic_text_classification?hl=ja ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbce"}, "repo_url": "https://github.com/sunlanchang/YOLOv3-keras", "repo_name": "YOLOv3-keras", "repo_full_name": "sunlanchang/YOLOv3-keras", "repo_owner": "sunlanchang", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-12T03:55:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T11:49:48Z", "homepage": null, "size": 2448, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185595662, "is_fork": false, "readme_text": "yolov3-keras-master \u4f7f\u7528yolov3-keras\u8bad\u7ec3voc\u98ce\u683c\u7684\u81ea\u5df1\u6570\u636e\u96c6 \u73af\u5883\u8981\u6c42\uff1a  pip/conda install tensorflow/tensroflow-gpu==1.10.0    pip/conda install keras    pip/conda install opencv-python  1\u3001\u5c06\u6240\u6709\u7684\u6807\u7b7e\u6570\u636e\u5168\u90e8\u653e\u5728Annotations\u4e0b\uff0c\u56fe\u7247\u6570\u636e\u96c6\u653e\u5728JPEGImages\u4e0b\u3002 2\u3001\u8fd0\u884ctest.py\u811a\u672c\u751f\u6210ImageSets\u6587\u4ef6\u4e0bMain\u91cc\u9762\u56db\u4e2atxt\u6587\u4ef6\u3002 3\u3001\u4fee\u6539voc_annotation.py\u7b2c7\u884c\u91cc\u9762\u5185\u5bb9\u4e3a\u81ea\u5df1\u7684\u7c7b\u522b\u6807\u7b7e\u540c\u65f6\u5728model_data\u91cc\u9762\u4fee\u6539voc_classes\u5185\u5bb9\u4e3a\u81ea\u5df1\u7c7b\u522b\u6807\u7b7e\uff0c \uff08\u8be5\u8fc7\u7a0b\u8981\u4fdd\u8bc1\u4e24\u4e2a\u6807\u7b7e\u7684\u987a\u5e8f\u548c\u7a7a\u683c\u4e0d\u7136\u5bfc\u81f4inference\u8fc7\u7a0b\u6ca1\u6cd5\u6b63\u5e38\u8fd0\u884c\uff09\u8fd0\u884cvoc_annotation.py\u751f\u62102007_train.txt/2007_val.txt/2007_test.txt 4\u3001\u8fd0\u884ckmeans\u811a\u672c\u751f\u6210\u81ea\u5df1\u6570\u636e\u7684\u9884\u8bbeanchor\uff0c\u6839\u76ee\u6807\u4e0b\u751f\u6210\u81ea\u5df1\u6570\u636e\u7684yolo_anchorers\u5c06\u8be5\u6587\u4ef6\u653e\u5230model_data\u76ee\u5f55\u4e0b\u53d6\u4ee3\u539f\u6709\u7684\u3002 5\u3001\u8fd0\u884ctrain.py\u4e3a\u8fc1\u79fb\u5b66\u4e60\u8bad\u7ec3\u81ea\u5df1\u6570\u636e\uff0c\u8fd0\u884ctrain_my.py\u4e3a\u521d\u59cb\u5316\u8bad\u7ec3\u81ea\u5df1\u7684\u6570\u636e\u3002\uff08\u5982\u679c\u8fc1\u79fb\u5b66\u4e60\u5728train.py\u91cc\u976232\u884c\u52a0\u8f7d\u9884\u8bad\u7ec3\u6743\u91cd\u8fd9\u91cc\u6211\u5199\u597d\u4e0d\u8981\u4e71\u6539\uff09 6\u3001cfg\u548ctrain/train\u2014my\u91cc\u9762\u7684\u4fee\u6539\uff1a cfg\u91cc\u9762\u4fee\u6539\u5185\u5bb9 2/3\u884c\u4fee\u6539\uff1a\u542b\u4e49\u4e3abatch\u53c2\u6570\u9664\u4ee5subdivisions\u7b49\u4e8etrain\u811a\u672c\u91cc\u9762batchsize\uff08\u8fd9\u662f\u4e00\u4e2a\u975e\u5e38\u7ecf\u5178\u7684\u8bad\u7ec3\u6d4b\u8bd5\uff09 \u7c7b\u522bclasses\uff1a607/693/783/\u884c\u4e3a\u7c7b\u522b\u6570\u76ee filters = 3*\uff085+classses\uff09 7\u3001\u6d4b\u8bd5\u4fee\u6539yolo.py\u7684216/217\u884c\u6267\u884c\u811a\u672c\u3002 8\u3001\u76f8\u5173\u53c2\u6570\u89e3\u91ca: https://github.com/Eric3911/Dakrnet-YOLOv3/blob/master/%E4%BC%98%E5%8C%96%E8%AE%AD%E7%BB%83%E5%8F%82%E6%95%B0%E8%A7%A3%E9%87%8A \u6a21\u578b\u7ed3\u6784\u56fe     \u6211\u4eec\u8bba\u6587\u8dd1\u7684\u7ed3\u679c   \u5176\u4ed6\u76f8\u5173\u6559\u7a0b https://blog.csdn.net/maweifei/article/details/81204702 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbcf"}, "repo_url": "https://github.com/roshnpl/NPL_Project", "repo_name": "NPL_Project", "repo_full_name": "roshnpl/NPL_Project", "repo_owner": "roshnpl", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T19:19:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T18:22:36Z", "homepage": null, "size": 168, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185658676, "is_fork": false, "readme_text": "NPI This project uses the NPI architecture to perform three tasks:  Multiplication Bucket Sort Selection Sort  Dependencies: Python 3 Keras Tensorflow scikit-learn numpy Npi.py contains the code for NPI core Authors(List by name) Bharat Matai - Bucket sort Performing Bucket Sort which will use bubble sort to sort the different buckets in and then collabrate all the buckets to form the results in the TestModel.py file the buckets are created and then sorted using the NPI MOdel using bubble sort, bubble sort code we try to establish from the paper but are facing errors in giving accurate results and with Keras Implementation. *Readme file inside folder Roshni - Multipication multiplication.py file is used which will perform the multipication between two numbers in the range of 1 to 1000 and trying to establish it with the left pointer Khyati - Selection sort Performed Selection sort by training the NPI model on a dataset with arrays sorted from 0 to 1000. *Readme file inside folder ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd0"}, "repo_url": "https://github.com/GauranshMathur/AI-SignLanguage", "repo_name": "AI-SignLanguage", "repo_full_name": "GauranshMathur/AI-SignLanguage", "repo_owner": "GauranshMathur", "repo_desc": "Application that converts ASL into english using tensorflow keras and computer vision", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T09:22:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T09:07:37Z", "homepage": null, "size": 6210, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185571210, "is_fork": false, "readme_text": "AI-SignLanguage Application that converts ASL into english using tensorflow keras and computer vision Using the demo files. To use the demo files you need to have python installed with keras and tensorflow packages. Most of it all can be downloaded through Anaconda. To run the program you can either open it in spyder and run it directly from there or go to compand promt and type python demo.py  This is for the static letters python dynamic-cam.py  This is for the letters J and Z Systems used The model used was a pre-created model found online. But the filters were changed as to fit the image sizing. You can look at the model under the folder pyimagesearch. Datasets The datasets created used 2 different filters to be trained to get the model to comprehend all 24 static letters in the alphabet. Because a lot of the alphabets are quite similar to each other. The two types of filters used were:  A background masking filter A hand counturing filter  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd1"}, "repo_url": "https://github.com/idiap/attention-sampling", "repo_name": "attention-sampling", "repo_full_name": "idiap/attention-sampling", "repo_owner": "idiap", "repo_desc": "This Python package enables the training and inference of deep learning models for very large data, such as megapixel images, using attention-sampling", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T05:37:42Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T08:15:27Z", "homepage": "", "size": 37, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185562524, "is_fork": false, "readme_text": "Attention Sampling This repository provides a python library to accelerate the training and inference of neural networks on large data. This code is the reference implementation of the methods described in our ICML 2019 publication \"Processing Megapixel Images with Deep Attention-Sampling Models\". We plan to update and support this code so stay tuned for more documentation and simpler installation via pip.  Installation For now one has to install the package in dev mode using pip and then build the tensorflow extensions manually using the cmake.  Usage A good example of using the library can be seen in scripts/mnist.py. A small outline is given below: # Keras imports  from ats.core import attention_sampling from ats.utils.layers import SampleSoftmax from ats.utils.regularizers import multinomial_entropy  # Create our two inputs. # Note that x_low could also be an input if we have access to a precomputed # downsampled image. x_high = Input(shape=(H, W, C)) x_low = AveragePooling2D(pool_size=(10,))(x_high)  # Create our attention model attention = Sequential([     ...     Conv2D(1),     SampleSoftmax(squeeze_channels=True) ])  # Create our feature extractor per patch, we assume that it returns a # vector per patch. feature = Sequential([     ...     GlobalAveragePooling2D(),     L2Normalize() ])  features, attention, patches = attention_sampling(     attention,     feature,     patch_size=(32, 32),     n_patches=10,     attention_regularizer=multinomial_entropy(0.01) )([x_low, x_high])  y = Dense(output_size, activation=\"softmax\")(features)  model = Model(inputs=x_high, outputs=y) You can also run tests on the MNIST artificial task with the following code: $ # First we need to create the dataset, an easy one $ ./scripts/make_mnist.py /path/to/datasetdir --width 500 --height 500 --no_noise --scale 0.2 $ # or a much harder one $ ./scripts/make_mnist.py /path/to/datasetdir --width 1500 --height 1500 --scale 0.12 $ $ # Now we can train a model with attention sampling $ ./scripts/mnist.py /path/to/datasetdir /path/to/outputdir \\ >       --lr 0.001 --optimizer adam \\ >       --n_patches 10 --patch_size 32x32 \\ >       --epochs 200 --batch_size 128  Research If you found this work influential or helpful in your research in any way, we would appreciate if you cited us. @inproceedings{katharopoulos2019ats,     title={Processing Megapixel Images with Deep Attention-Sampling Models},     author={Katharopoulos, A. and Fleuret, F.},     booktitle={Proceedings of the International Conference on Machine Learning (ICML)},     year={2019} }  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1905.03711"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd2"}, "repo_url": "https://github.com/AliGangeh/Deep_Neural_Net", "repo_name": "Deep_Neural_Net", "repo_full_name": "AliGangeh/Deep_Neural_Net", "repo_owner": "AliGangeh", "repo_desc": "Creating a basic deep neural network to better understand their functionality", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T05:12:49Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T02:31:28Z", "homepage": null, "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185513483, "is_fork": false, "readme_text": "Deep_Neural_Net I am creating a basic deep neural network to better understand their functionality. This will be done through the keras library. I made use of a single layer dense network with 2 input nodes and 1 output nodes. Cross entropy is used to find error, and I made use of the sigmoid activation function, and the adam optimizer. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd3"}, "repo_url": "https://github.com/ace3600/TirexAI", "repo_name": "TirexAI", "repo_full_name": "ace3600/TirexAI", "repo_owner": "ace3600", "repo_desc": "Neural Network play T-Rex", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T17:05:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T15:57:32Z", "homepage": null, "size": 6441, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185638199, "is_fork": false, "readme_text": "TirexAI Neural Network play T-Rex To run this programme without error you need Python 3.6 and libraries pywin32, numpy, opencv, tensorflow, keras you can install them by running these commands on cmd or terminal pip install pywin32 pip install numpy pip install opencv-python pip install tensorflow pip install keras dowload this game https://github.com/wayou/t-rex-runner to launch the AI you need to open tirex game put the window of chrome to the left most and start the game and click on another window so the game will be paused then launch test.py and wait for the script to detect the game after the game is detected just click on the chrome window and let the AI play this AI is trained on this game https://github.com/wayou/t-rex-runner you can dowload it and use it if you use another game you can face issus where the AI can't play well you can always retrain your model by running the script extract_info.py don't forget to pause the game until the script detect your game and delete all data in newData folder then play for while like 15min then run the script balance_data.py to balance data then run train.py and wait for training to end then you can play the game the probleme with this scripts is detecting the game to capture the video, the game can be anywhere in the screen you can always set your settings to capture the screen and set your gameover detector and retrain the model if you set your own settings to capture the screen try to not capture t-rex capture what's in front of him and don't capture the highest flying bird this is a Video shows how things works https://www.youtube.com/watch?v=iKmU1rZ8M0w ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ace3600/TirexAI/blob/e15c580eb10e092bd0fe99312cc69c14ec8cebc0/tirexu.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd4"}, "repo_url": "https://github.com/xwang186/NPL_pn", "repo_name": "NPL_pn", "repo_full_name": "xwang186/NPL_pn", "repo_owner": "xwang186", "repo_desc": "Some modifications on Pointer-network solving TSP", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T20:04:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T18:40:02Z", "homepage": "", "size": 1454, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185661064, "is_fork": false, "readme_text": "NPL_pn How To run the project Two folders are in this project. The one name without \"modi\" is the original pointer network implementation. The one with modi is the archtecture I designed. To run my project or the original one, use python<3 keras=1.2.2 sudo pip install keras=1.2.2 (sudo if in docker) python run.py Description Pointer-network is proven to have a good performance solving TSP(Trave Salesman Problem). I believe that the solution of a TSP problem should not be influenced by the order it feeds in. It should depend more on the location relative. Suppose that we happen to have the distribution of the points before finding the path, we want to use the cluster distribution to help us better solve TSP. In this task, I ran a preprocess, which is generating a cluster information using K-means. After that, I tried to plug in the cluster information into the original data and train the pointer network again. Conclusion Seems that the new network uses less time to reach a reasonable training result. The validation shows that the result performs a little bit better than before. However, the network changes to larger and the number of to-be-trained parameters increases, so I think the result is not convincing enough. Training details Structure: dec_seq = K.repeat(h, input_shape[1]) Eij = time_distributed_dense(en_seq, self.W1, output_dim=1) Dij = time_distributed_dense(dec_seq, self.W2, output_dim=1) U = self.vt * tanh(Eij + Dij) U = K.squeeze(U, 2) Parameter: ncoder = LSTM(output_dim = hidden_size, return_sequences = True, name=\"encoder\")(main_input) decoder = PointerLSTM(hidden_size, output_dim=hidden_size, name=\"decoder\")(encoder) model = Model(input=main_input, output=decoder) model.compile(optimizer='adadelta', loss='categorical_crossentropy', metrics=['accuracy']) model.fit(X, YY, nb_epoch=nb_epochs, batch_size=64,callbacks=[LearningRateScheduler(scheduler),]) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/xwang186/NPL_pn/blob/40b647978969ea3ece99c6b5fedbe39cd8ef85a4/pointer-networks/model_weight_100.hdf5", "https://github.com/xwang186/NPL_pn/blob/40b647978969ea3ece99c6b5fedbe39cd8ef85a4/pointer-networks_modi/model_weight_100.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd5"}, "repo_url": "https://github.com/zahrabashir98/Classify-Farsi-Digits", "repo_name": "Classify-Farsi-Digits", "repo_full_name": "zahrabashir98/Classify-Farsi-Digits", "repo_owner": "zahrabashir98", "repo_desc": "I want to classify Farsi digits by using the Hoda Farsi Digit Dataset", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T19:36:34Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T12:19:30Z", "homepage": null, "size": 202, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185600248, "is_fork": false, "readme_text": "Classify-Farsi-Digits I want to classify Farsi digits by using the Hoda Farsi Digit Dataset. I have two files:  usage_example.py: which contains some examoles of using HodaFarsiDataSet MLP.py: which contains implementing a MLP and training it on this data set.It can finaly train of Farsi numbers and recognize the input picture as what it is with 97/8 percent of accuracy.  My results according to different evaluation metrics are attached. Dependencies:  Keras HodaDatasetReader  DataSet refrence:HodaFarsiDataSet ", "has_readme": true, "readme_language": "English", "repo_tags": ["mlp-network", "hoda-dataset", "keras"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd6"}, "repo_url": "https://github.com/jabosso/GS", "repo_name": "GS", "repo_full_name": "jabosso/GS", "repo_owner": "jabosso", "repo_desc": "Soccer Reporter", "description_language": "Danish", "repo_ext_links": null, "repo_last_mod": "2019-05-18T08:05:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T12:49:46Z", "homepage": "", "size": 582, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185605054, "is_fork": false, "readme_text": "Soccer Reporter  Soccer Reporter is a project in progress to built a Soccer reporter based on AI. Tech  Python Keras  And  there is a public repository on GitHUb with the following files : First step of our original pipeline is the extraction of frames form videos by reading the annotation of SoccerNet dataset. We can check the code of this step in Frame_Extractor.py  [Utility.py]  TO DO LIST   create artificial annotation  extract frames from videos  split dataset in train, validation and test   preprocessing data  train nets   evaluate nets  electors system  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd7"}, "repo_url": "https://github.com/moelgendy/mobileNetV2", "repo_name": "mobileNetV2", "repo_full_name": "moelgendy/mobileNetV2", "repo_owner": "moelgendy", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T21:49:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T21:46:50Z", "homepage": "", "size": 1086, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185683661, "is_fork": false, "readme_text": "MobileNet v2 A Python 3 and Keras 2 implementation of MobileNet V2 and provide train method. According to the paper: Inverted Residuals and Linear Bottlenecks Mobile Networks for Classification, Detection and Segmentation. Requirement  OpenCV 3.4 Python 3.5 Tensorflow-gpu 1.2.0 Keras 2.1.3  MobileNet v2 and inverted residual block architectures MobileNet v2: Each line describes a sequence of 1 or more identical (modulo stride) layers, repeated n times. All layers in the same sequence have the same number c of output channels. The first layer of each sequence has a stride s and all others use stride 1. All spatial convolutions use 3 X 3 kernels. The expansion factor t is always applied to the input size.  Bottleneck Architectures:  Train the model The recommended size of the image in the paper is 224 * 224. The data\\convert.py file provide a demo of resize cifar-100 dataset to this size. The dataset folder structure is as follows: | - data/  | - train/     | - class 0/    | - image.jpg     ....   | - class 1/     ....   | - class n/  | - validation/     | - class 0/   | - class 1/     ....   | - class n/  Run command below to train the model: python train.py --classes num_classes --batch batch_size --epochs epochs --size image_size  The .h5 weight file was saved at model folder. If you want to do fine tune the trained model, you can run the following command. However, it should be noted that the size of the input image should be consistent with the original model. python train.py --classes num_classes --batch batch_size --epochs epochs --size image_size --weights weights_path --tclasses pre_classes  Parameter explanation  --classes, The number of classes of dataset. --size,    The image size of train sample. --batch,   The number of train samples per batch. --epochs,  The number of train iterations. --weights, Fine tune with other weights. --tclasses, The number of classes of pre-trained model.  Experiment Due to the limited computational resources, we used cifar-100 dataset to test the model. device: Tesla K80 dataset: cifar-100 optimizer: Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)   batch_szie: 128   These are the details for the CIFAR-100 experiment. Although the network did not completely converge, still achieved good accuracy.    Metrics Loss Top-1 Accuracy Top-5 Accuracy     cifar-100 0.195 94.42% 99.82%     Reference @article{MobileNetv2,     title={Inverted Residuals and Linear Bottlenecks Mobile Networks for Classification, Detection and Segmentatio},     author={Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen},   journal={arXiv preprint arXiv:1801.04381},   year={2018} }  Copyright See LICENSE for details. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1801.04381"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd8"}, "repo_url": "https://github.com/tirakitsu/Word-Prediction", "repo_name": "Word-Prediction", "repo_full_name": "tirakitsu/Word-Prediction", "repo_owner": "tirakitsu", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T23:18:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T23:01:31Z", "homepage": null, "size": 156, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185690504, "is_fork": false, "readme_text": "Word-Prediction Word Prediction Program created with keras and numpy, after initialization, it will tell the user to input the name of the corpus (the one provided is humpty dumpty, though it can be edited to be something else). It will then ask the user to input a word model, or if it needs to train a new one. There is one provided, in the file, however the option to create a new one was provied in case there was an issue with the output. Then it asks the user for an input, which has to be something included in the corpus file, which can be edited. It next asks the user to enter the numbre of words to predict, though the lower the number, the more accurate it tends to be. After inputting the number of words to predict, it will output its prediction. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/tirakitsu/Word-Prediction/blob/23c94ae78e70d158dea58f8e30ab7e7fab53e773/model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbd9"}, "repo_url": "https://github.com/amarsdd/R-NET-Behavioral-Navigation", "repo_name": "R-NET-Behavioral-Navigation", "repo_full_name": "amarsdd/R-NET-Behavioral-Navigation", "repo_owner": "amarsdd", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T05:49:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T04:02:50Z", "homepage": null, "size": 8837, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185525570, "is_fork": false, "readme_text": "Finding a High-Level Plan for Behavioral Robot Navigation with R-NET In this work, we utlize a modified R-NET with gated attention and self-matching attention to develop an understanding of the behavioral navigational graph to enable the pointer network to produce a sequence of behaviors representing the path for robot navigation. The source code for data preparation is adapted from this tensorflow implementation of Translating Navigation Instructions in Natural Language to a High-Level Plan for Behavioral Robot Navigation. The source code for R-NET is adapted from this Keras implementation of R-NET: MACHINE READING COMPREHENSION WITH SELF-MATCHING NETWORKS  Getting Started The work is trained and tested on the behavioral navigation graph dataset from Stanford. In case natural language directions are used, GLOVE embeddings is also required. You can download the 100d (glove.6B.100d.txt) embedding from the their website or download it here and place it in the 'data' folder. All the other necessary dataset parts are available in \"data\" folder. Please clone the entire project. git clone https://github.com/amarsdd/R-NET-Behavioral-Navigation.git  Prerequisites The code has only been tested on python 2.7.16 with the following requirements matplotlib==2.2.4 six==1.12.0 tensorflow_gpu==1.13.1 pandas==0.24.2 nltk==3.4 tqdm==4.31.1 Keras==2.0.6 numpy==1.16.2 tensorflow==1.13.1  Training Run the following to train the model with default configurations: python train.py  The following are the set of arguments that can be provided: ('--with_instruction', default=False, help='Use instruction or not', type=int) ('--hdim', default=100, help='Number of units in BiRNN', type=int) ('--nlayers', default=3, help='Number of layers in BiRNN', type=int) ('--batch_size', default=128, help='Batch size', type=int) ('--nb_epochs', default=50, help='Number of Epochs', type=int) ('--optimizer', default='adam', help='Optimizer', type=str) ('--lr', default=None, help='Learning rate', type=float) ('--dropout', default='0.0', help='Dropout', type=str) ('--name', default='Rnet_navigation', help='Model dump name prefix', type=str)  ('--data_dir', default='data', help='Data directory', type=str) ('--exp_dir', default='experiments', help='Experiment results directory (Model checkpoint and Tensorboard logs)', type=str)   Testing To do evaluation we require the trained model which can be downloaded here and placed in the root folder for the project. These trained models are for configurations of hdim=[50, 100], nlayers=[1, 3] and models=[model_0 (without directions), model_1 (with directions)]. Run the following to evaluate the model with default configurations on both test sets: python evaluate.py  Trained models for various configurations are in 'trained_models' folder. Given the following arguments the trained model is selected. The following are the set of arguments that can be provided: ('--with_instruction', default=False, help='Use instruction or not', type=int) ('--hdim', default=100, help='Number of units in BiRNN', type=int) ('--nlayers', default=3, help='Number of layers in BiRNN', type=int) ('--batch_size', default=128, help='Batch size', type=int) ('--nb_epochs', default=50, help='Number of Epochs', type=int) ('--optimizer', default='adam', help='Optimizer', type=str) ('--lr', default=None, help='Learning rate', type=float) ('--dropout', default='0.0', help='Dropout', type=str) ('--name', default='Rnet_navigation', help='Model dump name prefix', type=str)  ('--data_dir', default='data', help='Data directory', type=str) ('--exp_dir', default='experiments', help='Experiment results directory (Model checkpoint and Tensorboard logs)', type=str) ('--model_dir', default='trained_models', help='Trained Model directory', type=str)  Authors  Amar Shrestha amshrest@syr.edu  License This project is licensed under the MIT License - see the LICENSE.md file for details Acknowledgments  Hat tip to anyone whose code was used  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://follow-nav-directions.stanford.edu/"], "reference_list": ["https://arxiv.org/abs/1810.00663"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbda"}, "repo_url": "https://github.com/hk57816631/fuzzy_unet", "repo_name": "fuzzy_unet", "repo_full_name": "hk57816631/fuzzy_unet", "repo_owner": "hk57816631", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-13T01:02:49Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T05:25:02Z", "homepage": null, "size": 38, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185533824, "is_fork": false, "readme_text": "fuzzy_unet Note: my codes based on python 3.5, keras 2.1.2, tensorflow 1.9.0, Cuda 9.0, cudnn 7.0.5. Windows and Linux and Max OS are all supported and tested.  Download the dataset. Save the dataset in ./BUS/data2/original/ (original image) and ./BUS/data2/GT/ (ground truth) For training, just run the train.py. In console, move in to the root file of my code (where the train.py is located). Type python -M unet to train the original unet. Type python -M fuzzyunet to train the fuzzy unet. For testing, there are two ways. First one is testing a single image. Using test.py and giving img_path, and label_path it can output segmentation result of one image. For testing a set of samples, test_path.py is used. img_path, label_path and the txt file containing name list are provided and then the set of samples are segmented and save in result_path. Wavelet.py is used to do wavelet transform. Norm.py is used to do histogram normalization. Showmed.py is used to show the intermediate results of fuzzy layer. The weights of the network are saved in \u201cmodel name\u201d + _model_weight.h5  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbdb"}, "repo_url": "https://github.com/kkhuiaa/msbg6000g_pj", "repo_name": "msbg6000g_pj", "repo_full_name": "kkhuiaa/msbg6000g_pj", "repo_owner": "kkhuiaa", "repo_desc": "Transform a clothing from model to another person", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T15:56:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T15:07:52Z", "homepage": "", "size": 75966, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185629484, "is_fork": false, "readme_text": "MSBD6000G-Spring2019 group project Group project for MSBD5002 Getting Started Important: it is assumed that the pwd is in root directory of this project (if not, please use cd command to change the driectory) Prerequisites The script is written in python 3 with required packages: numpy==1.15.4 scipy==1.2.0 pandas==0.24.2 opencv==3.4.4 keras==2.2.4 tensorflow==1.13.1 matplotlib==3.0.4  Installing os, time, pickle, warnings should be installed already when installing python. For installing specific version of package, you can install it as follow: (through pip) pip install -Iv pandas==0.23.4  Run the script To run step1: pip3 step1/fashion.py  To run step2: python3 step2/cagan.py  If you do not have the data in step2/input, please run python3 step2/data.py  For step2, please make sure you have GPU environment to build the model. Coding style tests The scipt follows pip8 standard python notation. Acknowledgments Model has been considered and modified from anish9 and shaoanlu. Authors  Hui Kwat Kong - 20123133 Song Yangyang - 20534320  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/kkhuiaa/msbg6000g_pj/blob/3c7e00217f87d3b2bd1c2f0a983527bcc972a503/model/netG1556592005.524499_2000.h5", "https://github.com/kkhuiaa/msbg6000g_pj/blob/3c7e00217f87d3b2bd1c2f0a983527bcc972a503/step1/model/binaryfashion.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbdc"}, "repo_url": "https://github.com/YousefAlKafif/ANNmodel", "repo_name": "ANNmodel", "repo_full_name": "YousefAlKafif/ANNmodel", "repo_owner": "YousefAlKafif", "repo_desc": "An ANN (Artificial Neural Network) model I created which can predict whether a customer will leave a bank or not, utilizing a customers history. #Built using Keras that runs on TensorFlow framework.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-17T03:05:08Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T22:03:39Z", "homepage": "", "size": 373, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185685281, "is_fork": false, "readme_text": "ANNmodel This is an Artificial Neural Network Deep-Learning model I created to predict whether a customer will leave the bank using a customers geographical location, credit score, gender, age, tenure, balance, # of products purchased, credit card holder or not, active member or not, and estimated salary. I used a dataset that contained information on 10,000 mock customers which also included whether or not the customer had left the bank - which was used for validation. This was done using Keras' API that ran on TensorFlow. ANN Evaluation:- I managed to train the ANN to a final accuracy of 86.61%, I used the K-fold technique (using sklearn's cross_val_score() module) to evaluate the mean accuracy and variance of my model utilizing 10 folds. The mean accuracy is 85.8% and the variance is ~1%, indicating a fairly low bias. Methods I used:- Pre-processing the data :  I had to encode the categorical data (alphabetical categories i.e. words) into numerical values. Creating 'Dummy Variables' with the OneHotEncoder module. I used the StandardScaler() module to scale my test and training set.  Improving the ANN :  Parameter-tuning using the GridSearchCV() module to test different combinations of parameters, which gave me feedback on what parameters would be optimal. The 'Dropout' method in my ANNs hidden layers to prevent over-fitting. Saving my ANNs loss history for records purposes.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/YousefAlKafif/ANNmodel/blob/bafbf077922cc6dd27ff01f30c67875ac059552a/CustomerChurnANN.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbdd"}, "repo_url": "https://github.com/Lkgsr/League-of-Legends-Predict-Lane", "repo_name": "League-of-Legends-Predict-Lane", "repo_full_name": "Lkgsr/League-of-Legends-Predict-Lane", "repo_owner": "Lkgsr", "repo_desc": "A simple CNN in Keras that predicts the Lanes for Champions from League of Legends by there ChampionId and SummonerSpellId's", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-12T16:23:54Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T18:57:55Z", "homepage": null, "size": 2141, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185663534, "is_fork": false, "readme_text": "Predicting the lanes for League of Legends Participants with a CNN Setup Download the Git repository and install the requirements pip install -r requirements.txt How to use Training the CNN I really recommend the use tensorflow-gpu for the training or it will take some time to train it To run the training open the train.py file in you favorite python editor. You may change the name: from tools.train import CnnTrain cnn = CnnTrain(data_name='data') cnn.run('test') You can also look up the Comment i tried to explain everything If you want to see the graphs from you model while its training you can run in the console. When you are in the League-of-Legends-Predict-Lane's folder (--logdir=\"Path to logs\") tensorboard --logdir=logs Predicting Lanes There is all ready a pre trained model in the models folder which you can use. To predict something you import the predict function from the predict.py file and run it with a Summoner Name. The Summoner have to be in a game to call the spectator api. from tools.predict import Lane lane = Lane(\"RGAPI-XXX-XXX\", model_name='test4_conv1D_153_one_dense_153_1557335309_1557335316') prediction = lane.predict('SaItySurprise') prediction = lane.predict(4023710639) print(prediction) {  'blue':     {     'Lux': {'Bottom': 92.0, 'Middle': 7.4, 'Top': 0.73, 'Jungle': 0.34},      'Ashe': {'Bottom': 92.0, 'Middle': 7.2, 'Top': 0.71, 'Jungle': 0.34},      'Zed': {'Middle': 90.0, 'Top': 7.0, 'Bottom': 2.7, 'Jungle': 0.72},      'Urgot': {'Top': 83.0, 'Jungle': 8.6, 'Middle': 6.4, 'Bottom': 1.5},      'Evelynn': {'Jungle': 99.0, 'Bottom': 0.29,'Top': 0.21, 'Middle': 0.031}     },   'red':      {     'Illaoi': {'Top': 83.0, 'Jungle': 8.0, 'Middle': 7.4, 'Bottom': 1.8},      'Ekko': {'Middle': 91.0, 'Top': 6.7, 'Bottom': 2.0, 'Jungle': 0.64},      'Soraka': {'Bottom': 92.0, 'Middle': 7.2, 'Top': 0.71, 'Jungle': 0.34},      'Kaisa': {'Bottom': 92.0, 'Middle': 7.2, 'Top': 0.71, 'Jungle': 0.34},      'Trundle': {'Jungle': 99.0, 'Bottom': 0.29, 'Top': 0.21, 'Middle': 0.031}     } } If you want to know more about the prediction feel free to read the Comments in the prediction.py file. I tried to explain everything. If there a Questions or  any suggestions feel free to Label the issues and make a pull requests The Flask Module Install the requirements run the main.py script the the start and the first request need need some time but the next's request are much faster. To predict something send a get request to localhost:5000/predict/NameOrMatchId  If you want to change the port just open the main.py file and chang there: if __name__ == '__main__':     app.run(port=YourPreferredPort) ", "has_readme": true, "readme_language": "English", "repo_tags": ["python", "league-of-legends", "deep-neural-networks", "cnn-keras", "cassiopeia", "lane-prediction"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbde"}, "repo_url": "https://github.com/austinmadert/monster_generator", "repo_name": "monster_generator", "repo_full_name": "austinmadert/monster_generator", "repo_owner": "austinmadert", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T15:16:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T18:35:16Z", "homepage": null, "size": 69, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185660439, "is_fork": false, "readme_text": "Character Generator for Dungeons and Dragons Characters The project is currently in progress My movtivation from this project comes from my experience playing in roleplaying groups. Dungeons and Dragons (D&D) is a roleplaying game where a small group of players will each assume the role of a fantasy character and act out adventures and combat with terrible monsters via the medium of spoken word and dice rolls. The monsters have text descriptions and statistics that allow the player characters to interact with them. Anyone who has played in a D&D group before will know that there is advantage to be gained by understanding the strategies the monsters are designed to employ. And so novelty becomes essential to the D&D experience (especially for a seasoned group of nerds). Hence, I designed this project to allow me to take some state of the art natural language processing technologies for a spin to help enchance the novelty and gaming experience for my roleplaying groups. I would be remiss in not saying that the inspiration for this project came from both my interest in NLP and deep learning, as well as  an aiweirdness.com post on using neural networks to generate the names for novel D&D spells. What Am I Actually Doing Here? I will use Python's Scrapy library to acquire all the monster descriptions for the latest edition of Dungeons and Dragons. Then I will use the Keras library's LSTM paired with ULMFiT to train a network on a Spark EMR cluster. The end product will be a model that can create novel monsters for Dungeons and Dragons adventures. External Links Coming Soon! ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbdf"}, "repo_url": "https://github.com/gungui98/deeprl-a3c-ai2thor", "repo_name": "deeprl-a3c-ai2thor", "repo_full_name": "gungui98/deeprl-a3c-ai2thor", "repo_owner": "gungui98", "repo_desc": "ai2thor cup picking using a3c", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T17:06:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T16:30:29Z", "homepage": null, "size": 8976, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185643076, "is_fork": false, "readme_text": "A3Thor - object driven navigation in indoor scene using A3C This is final project of final semester in robotic - INT 3409 1 We using A3C - Asynchronous advantage actor critic algorithm to train an agent navigating in side simulated environment ai2thor Overview This project includes implementations of A3C in ./A3C/a3c.py to train the model, using: python main.py --is_ai2thor to visualize result, using: python --is_ai2thor --critic_path */A3C/model/critic-model* --actor_path */A3C/model/actor-model* The default training parameter is 5000 episodes, 5 threads Installation Clone this repository: Install Python dependencies: pip install -r requirements.txt Highly recommend to install tensorflow using conda: conda install tensorflow-gpu Project description  this project using gym-style interface of ai2thor environment objective is simply picking an apple in kitchen environment - FloorPlan28 observation space is first-view RGB 128x128 image from agent's camera maximum step in this project is 500 reward fuction:  -0.01 each time step 1 if agent can pick an apple, the env than terminate 0.01 if agent saw an apple (has been removed in latest code)   a pre-train mobilenet-v2 model on image-net is used an feature extractor for later dense layer both actor and critic model actor optimizer using Advantages + Entropy term to encourage exploration (https://arxiv.org/abs/1602.01783)  Training  this project trained on xenon E5-2667v2 + GTX1070, with 1,444,234 parameters for actor and 1,443,073 params for critic model the objective is simple so that the model converge very fast, detail log and trained model in  ./A3C/  This project greatly thanks to material:  Gym-style ai2thor environment Collection of deep rl algorithms in keras  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1602.01783"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe0"}, "repo_url": "https://github.com/0311snitch/vaibhav", "repo_name": "vaibhav", "repo_full_name": "0311snitch/vaibhav", "repo_owner": "0311snitch", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-26T20:47:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T17:13:30Z", "homepage": null, "size": 25418, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185649042, "is_fork": false, "readme_text": "Essay Rater Web High Performance, Easy-to-use, and Scalable Automated Essay Scoring System with a Robust Web Service Environment  python 3.6 tornado 5.1.1 keras 2.2.4 tensorflow 1.11.0 tensorflow-gpu 1.11.0 torch 0.4.1 torchvision 0.2.1 numpy 1.15.4 scipy 0.16.0 spacy 2.0.16 nltk 3.3 deltas 0.4.7 kenlm 0.0.0 fairseq 0.6.0  Setup Configure environments via a script: cd /path/to/essay-rater-web/utils sh setup.sh Install kenlm python module: pip3 install https://github.com/kpu/kenlm/archive/master.zip Install pytorch fairseq: pip3 install torch torchvision git clone git@github.com:pytorch/fairseq.git cd /path/to/fairseq pip3 install -r requirements.txt python3 setup.py build develop Usage Start the service: cd /path/to/essay-rater-web python3 essay_rater.py Speedtest: cd /path/to/essay-rater-web/tests python3 speedtest.py \\  --type pipeline # choices=[pipeline, inference] \\  --batch_size_list 1 8 16 32 64 128 256 512 \\  --beam_size_list 1 2 3 4 5 \\  --model_paths ../data/model/seq2seq/mlconv_5_3_fp16_128.pt # allow multiple models, seperated by space Unittest: cd /path/to/essay-rater-web/tests python3 test_xxx.py Features  Essay Score Prediction based on LSTM and Hierarchical Regression Spell Checking and Auto Correction Rule-based Grammar Check Grammar Error Correction based on Convolutional Seq2Seq (Fairseq) LanguageTool API Integration LanguageTool Proof\u00adreading Service Integration POS Tagging Principal Component Analysis Real Time Word Count  Preview    ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe1"}, "repo_url": "https://github.com/pengyu965/Caption_Reference_Classification", "repo_name": "Caption_Reference_Classification", "repo_full_name": "pengyu965/Caption_Reference_Classification", "repo_owner": "pengyu965", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-19T02:06:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T16:09:32Z", "homepage": null, "size": 10771, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185640111, "is_fork": false, "readme_text": "Requirement  word2vec  pip3 install word2vec  nltk  pip3 install -U nltk  tensorflow 1.13.0  pip3 install tensorflow-gpu==1.13.0 keras  numpy  pip3 install numpy  others  pip3 install re json html lxml Preprocess Run preprocess_1.py first: python3 ./preprocess/preprocess_1.py   Getting ann files from SCISUMM-DATA, stored in './original_data/annfile/'   Getting xml files from SCISUMM-DATA, stored in './original_data/xmlfile/'   Convert the ann file into JSON, stored in './cache_data/ann_jsonfile/' (ann2json.py is used in this step)   (xml file is used for later negative samle generation) Secondly run negative_sample_gen.py python3 ./preprocess/negative_sample_gen.py   The xml file which doesn't contain any annotated sentence would be stored in './cache_data/negative_xmlfile/' (According to ann_json file to remove the annotated sentence)   The txt files, which are obtained from removing xml tags from xml files, are stored in './cache_data/negative_txtfile/'   Then convert these negative samples into JSON version, which is stored in './cache_data/neg_jsonfile/'   Thirdly run preprocess_2.py python3 ./preprocess/preprocess_2.py   All caption sentences would be stored in \"train_caption_json.json\" in \"./traindata/\" folder   All reference sentences would be stored in \"train_reference_json.json\" in \"./traindata/\" folder   All normal sentences would be stored in \"train_negative_json.json\" in \"./traindata/\" folder   Only maintain the sentence which the token length between [6, 200]   Word2Vec python3 ./embedding/word2vec_train.py   word2vec_train.py is used to add all the word tokens in the training data to the vocabulary, which is './embedding/vocabulary.txt'   word2vec_train.py is used to train the word2vec model and store the model in './embedding/word2vec.bin'   CNN and LSTM model CNN model training python3 main.py --model=CNN --train --data=./traindata/ -lr=0.001 --epoch=30 --bsize=60  LSTM model training python3 main.py --model=LSTM --train --data=./traindata/ -lr=0.001 --epoch=60 --bsize=60  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe2"}, "repo_url": "https://github.com/zameerbharwani/Deep-Learning-for-Metasurface-Optimization", "repo_name": "Deep-Learning-for-Metasurface-Optimization", "repo_full_name": "zameerbharwani/Deep-Learning-for-Metasurface-Optimization", "repo_owner": "zameerbharwani", "repo_desc": "Deep Learning (non-linear regression) for metasurface parameter optimization using tensorflow/keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-28T16:44:02Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T10:30:21Z", "homepage": "", "size": 71015, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185584669, "is_fork": false, "readme_text": "Deep Learning for Metasurface Optimization Optimization of single-element metasurface parameters using deep learning (non-linear regression) with tensorflow/keras and ~5600 Lumerical simulations as training data. Simulations performed under normally incident light. The features that define metasurface are the 1.length (L) 2.width (W) 3. height (H) 4.x-direction periodicity (Ux) 5. y-direction periodicity (Uy). The output is the phase spectrum around and across the visible at increments of 5 nm (450 nm - 800 nm). For the powerpoint, there are animations so I recommend watching it in slide show mode. Everything published in this repo has been done so with permission Background Metasurfaces are used to manipulate light in various manners for a plethora of applications. Current state-of-the-art methods to design these nanostructures are quite medieval and rely on a mere brute force method. That is, given a desired output, what combination of metasurface parameters can give us the values closest to what we seek? To answer this question, researchers rely on simulation software and perform thousands of parameter sweeps in hope that they find the best combination. The cost of the simulations are high, both in terms of time and computing power, especially in a research setting where many people are simultaneously working on the same cluster (see slide 18). In performing these vast parameter sweeps, researchers unknowingly built a powerful dataset. During my time at Harvard, I realized this and decided to aggregate some few thousand of my simulations and use it as a proof of concept that there exists a better and more efficient way to optimize metasurface parameters; deep learning. Attached is part of the presentation for the talk I gave to my colleagues at the Capasso Group in Harvard University's Applied Physics Department along with some results using the scatternet repo from MIT. The Capaso group is primarily comprised of physicists and thus a component of this presentation is on some of the basics of neural networks, which I think might be heplful to some people. Given that the talk was oral and the slides were merely visual aids, I've added some notes at the bottom of some slides for clarity. Due to the success with the scatternet repo (Nanophotonic Particle Simulation and Inverse Design), I have now built a better/simpler version designed for metasurface parameter optimization. Currently, there is only code for the forward design, though the inverse design is the next step. I will not be further developing my inverse design code due to time limitations and thus will not be posting it. I would highly recommend taking a look at the inverse design developed in the scatternet repo. Let it be noted that even though this repo is only for the forward design and thus does not alleviae the problem of brute force, it does offer two things:   Since deep learning is merely vectors/matrices, all computations are analytical and faster by up to 1200x compared to the simulation methods used currently, which are numerical   This code for the forward design sets the foundation for the inverse desgin   Definitions: Forward Design: Given a set of input parameters that define a metasurface, predict the phase spectrum  Inverse Design: Given a desired phase spectrum, what paramaters result in said phase spectrum? Results  Comparing the accuracy from the test set and the validation, we see similar values suggesting that we have likely not overfit our systems, which is positive. Ideally, the accuracy would be higher, but it should be noted that the dataset is excpetionally small, and wasn't originally designed for this purpose. Rather, I developed this dataset for a different project and this project was born as a result of my mere curiousity. Additionally, one of the challenges of designing in the metasurface realm is the issue of coupling and other offects that are very hard to predict. Based on my experience, a larger and better designed dataset could really help with this. Another reason the accuracy might not be as high can be explained by the fact that phase wraps around 2\u03c0, meaning that 0 is the same as 2\u03c0 is the same as 12\u03c0. It is likely (verified by image below) that the network predicts a phase that when wrapped to 2\u03c0 is the same value as the actual phase.   One exciting result shown in the image above is from the test set (top) and validation set (bottom). The reason this is interesting is becaause the network predicted a phase of 0 radians when the FDTD software converged on -2\u03c0. In reality these are the same phase values, but it appears that the network is able to learn that phase wraps, with it being explicitly put in the code. The way to justify it is via the non-linear regression process. An easy example is the case of isotropic structures. For isotropic structures, we know the phase is 0 across the entire spectrum. However, sometimes the FDTD software will provide 2\u03c0 radians as the solution or 0 radians as the solution. This means in the training process, there are isotropic structures with 0 radians and 2\u03c0 radians,a nd through the non-linear regression process, the network detected this pattern because it helped minimize the loss. The same can be said about non-isotropic structures. It is for this reason training/testing/validating/predicting should be performed with the phase wrapped to 2\u03c0 (though this can make interpreting the phase spectrum more challenging, so for analysis purposes unwrapping it might be helpful); this is something I will work to update in my free time. Understanding this Repo I performed some optimization to fine tune the hyperparamters. The default values in core.py represent the optimized values. Files: Input.csv: input features  Output.csv: corresponding phase spectrum  Presentation.pptx.zip: part of my presentation at Harvard SEAS for the Capasso Group (annotated with notes, recommended to view in slideshow mode)  core.py: script  _my_model.h_5: saved model  my_model_weights.h5: saved model weights test_results.zip: Test set result graphs comparing actual vs prediction. Note: when going through the images, keep in mind the y-axis values! Curves might be closer than the seem since the axes are set to automatic!  test_x.txt: results of test(15%)/train(70%)/validation(15%) split   test_y.txt: results of test(15%)/train(70%)/validation(15%) split  train_x.txt: results of test(15%)/train(70%)/validation(15%) split  train_y.txt: results of test(15%)/train(70%)/validation(15%) split  val_x.txt: results of test(15%)/train(70%)/validation(15%) split  val_y.txt: results of test(15%)/train(70%)/validation(15%) split  validation_results.zip: Validation set result graphs comparing actual vs prediction. Note: when going through the images, keep in mind the y-axis values! Curves might be closer than the seem since the axes are set to automatic!   Final Remarks: I will be presenting some of these results at Photonica 2019 on the topic of Machine Learning in Photonics in Belgrade. The conference will be hosted in August: http://www.photonica.ac.rs/index.php ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/zameerbharwani/Deep-Learning-for-Metasurface-Optimization/blob/2e0faa8df2997085486f0f9604ad5f731603e011/my_model.h5", "https://github.com/zameerbharwani/Deep-Learning-for-Metasurface-Optimization/blob/2e0faa8df2997085486f0f9604ad5f731603e011/my_model_weights.h5"], "see_also_links": ["http://www.photonica.ac.rs/index.php"], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe3"}, "repo_url": "https://github.com/ahmedrarga/SimpleImageRecognition", "repo_name": "SimpleImageRecognition", "repo_full_name": "ahmedrarga/SimpleImageRecognition", "repo_owner": "ahmedrarga", "repo_desc": "simple CNN with Keras to classify photos to cats, cars and dogs", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T11:28:56Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T15:05:28Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185629072, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe4"}, "repo_url": "https://github.com/SamitHuang/COMP5213_Project", "repo_name": "COMP5213_Project", "repo_full_name": "SamitHuang/COMP5213_Project", "repo_owner": "SamitHuang", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T16:30:06Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T15:40:19Z", "homepage": null, "size": 429141, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185635258, "is_fork": false, "readme_text": "This is a course project based on the paper \"Hierarchical Imitation and Reinforcement Learning\". Experiment Data The saved model and experiment data are located \"experiments\" folder under \"hierarchical_imitation_learning_Maze_Domain\" and \"hybrid_imitation_reinforcement_Montezuma\" respectively. My Conda Environment Python 2.7 Tensorflow 1.13  Keras 2.2.4   Some required packages:      * The Arcade Learning Environment (https://github.com/mgbellemare/Arcade-Learning-Environment). $ sudo apt-get install libsdl1.2-dev     * cv2  Usage (1) For Maze Navigation, to train the hierachical IL agent and validate the performance (after every episode):      $ python train_hierarchical_dagger.py --model_dir=experiments/retrain     To test the trained agent on unseen mazes:      $ python test_hierarchical_dagger.py --model_dir=experiments/retrain  (2) For the Antari game,     to test the trained model:          $ python test_model.py --model_dir=experiments/retrain    to specify the directory to save the trained model     to train the agent:          $ python run_hybrid_atari_experiment.py --model_dir=experiments/retrain  The code is written based on the reference code in https://github.com/hoangminhle/hierarchical_IL_RL. Example Result of Hierarchical DAgger on Maze Navigation We have multiple random instances of the environment, with 4x4 room structure. The agent (white dot) is supposed to navigate to the destination in the yellow block,  while avoiding all the obstacles (red). Primitive actions are taking one step Up, Down, Left or Right. High level actions are navigating to the Room to the North, South, West, East or Stay (if the target block is in the same room). Here both the meta-controller and low-level controllers are learned with imitation learning.   Example Result of Hybrid Imitation - Reinforcement Learning on Montezuma's Revenge first room Panama Joe the adventurer needs to pick up the key, reverse his own path and go to open one of the two doors. For this instantiation of hybrid Imitation-Reinforcement learning, the meta-controller is trained with DAgger, and low-level controllers are learned with DDQN (Double Q Learning with prioritized experience replay).   Hierarchical Imitation Learning vs. Flat Imitation Learning Comparison    Hybrid Imitation-Reinforcement Learning vs. Hierarchical RL Comparison  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/._metaController.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/._termination_detector0.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/._termination_detector1.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/._termination_detector2.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/._termination_detector3.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/._termination_detector4.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00001_bs32/termination_detector0.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00001_bs32/termination_detector1.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00001_bs32/termination_detector2.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00001_bs32/termination_detector3.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00001_bs32/termination_detector4.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00005_bs16/termination_detector0.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00005_bs16/termination_detector1.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00005_bs16/termination_detector2.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00005_bs16/termination_detector3.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr00005_bs16/termination_detector4.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/termination_detector0.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/termination_detector1.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/termination_detector2.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/termination_detector3.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/trainRecord_lr0001_bs32/termination_detector4.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/deeper_regularization/termination_detector0.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/deeper_regularization/termination_detector1.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/deeper_regularization/termination_detector2.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/deeper_regularization/termination_detector3.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/deeper_regularization/termination_detector4.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/lr1e-3/termination_detector0.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/lr1e-3/termination_detector1.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/lr1e-3/termination_detector2.h5", "https://github.com/SamitHuang/COMP5213_Project/blob/91bad552ce0302cfdf1a92c40107b07b4861c3b1/hierarchical_imitation_learning_Maze_Domain/experiments/lr1e-3/termination_detector3.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1803.00590"]}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe5"}, "repo_url": "https://github.com/chenlabgccri/CancerTypePrediction", "repo_name": "CancerTypePrediction", "repo_full_name": "chenlabgccri/CancerTypePrediction", "repo_owner": "chenlabgccri", "repo_desc": "This is the repository for paper titled as \"Convolutional neural network models for cancer type prediction based on gene expression\".", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T20:47:22Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T16:55:11Z", "homepage": null, "size": 74, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185646504, "is_fork": false, "readme_text": "Predicting all 33-cancer types and their normal tissues with CNN  Folder ending with 33 class contains models which only work for classification of 33 cancer tumors. In order to see the impact of Normal tissues in classification, the codes that are in 34 class folder need to be run. Hyperparameter tuning folder is showing grid search result of some of the hyperparameters for 1D-CNN and Vanilla models. All codes are written in Keras with a simple structure which helps reader understand the modeling stage easier. Background Precise prediction of cancer types is vital for cancer diagnosis and therapy. Important cancer marker genes can be inferred through predictive model. Several studies have attempted to build machine learning models for this task however none has taken into consideration the tissue of origin effects that can potentially bias the identification of cancer markers. Results In this paper, we introduced several Convolutional Neural Network (CNN) models that take unstructured gene expression inputs to classify tumor and non-tumor samples into their designated cancer types or as normal. Based on different designs of gene embeddings and convolution schemes, we implemented three CNN models: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and tested on combined 10,340 samples of 33 cancer types and 731 matched normal tissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent prediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal). Furthermore, we interpreted the 1D-CNN model with a guided saliency technique and identified a total of 2,090 cancer markers (108 per class). The concordance of differential expression of these markers between the cancer type they represent and others is confirmed. In breast cancer, for instance, our model identified well-known markers, such as GATA3 and ESR1. Finally, we extended the 1D-CNN model for prediction of breast cancer subtypes and achieved an average accuracy of 88.42% among 5 subtypes. Conclusions Here we present novel CNN designs for accurate and simultaneous cancer/normal and cancer types prediction based on gene expression profiles, and unique model interpretation scheme to elucidate biologically relevance of cancer marker genes after eliminating the effects of tissue-of-origin. The proposed model had light hyperparameters to be trained and thus can be easily adapt to facilitate cancer diagnosis in the future. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe6"}, "repo_url": "https://github.com/alined908/projectnugget", "repo_name": "projectnugget", "repo_full_name": "alined908/projectnugget", "repo_owner": "alined908", "repo_desc": "Overwatch Computer Vision", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-31T17:11:07Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T23:23:49Z", "homepage": "", "size": 22217, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185692380, "is_fork": false, "readme_text": "Project Nugget A computer vision project created for SF Shock, a professional esports team in the Overwatch League.  Overwatch has no API to interact with and obtain statistics of practice matches, but hopefully with this, one can record scrims from the spectator POV and extract valuable game information/statistics from the VOD. This project uses convolutional neural nets that are trained on datasets created by ourselves. Training set images will be available soon. Getting Started Project Setup $ git clone https://github.com/alined908/projectnugget.git $ cd projectnugget $ pip install opencv-python $ pip install tensorflow $ pip install keras $ pip install numpy $ pip install pandas $ pip install matplotlib $ pip install imutils $ pip install pytesseract  Prerequisites  Dependencies  Python 3.6 +   OBS Settings Overwatch Settings  Colorblind Options: Enemy UI: Blue, Friendly: Yellow Lobby needs to be reset one time after entering a new map (Blizzard's colorblind bug) Resolution: 1920 x 1080   Other  Create these empty folders in Project Nugget/ --> /vods, /vod_data, /csvs/to_csv, /csvs/get_map_stats Format of vod names should be ex. 01.04.2019+SFS+vs+DAL+RIALTO  where date is dd.mm.yyyy    Instructions  Place .mp4 scrim recording(s) in /vods and run python parse_vod.py Clean folder(s) of images in /vod_data  Delete map loading images Example 1 , Example 2 If map is koth, get image Round Reset and replace first image of new round (round 2, 3)   python to_csv.py ----> Runs models on match images to output csv In /csvs/to_csv, Clean csv  Create a column after 'Opponent' called 'Roundtype' and input roundtype manually (Ex. Attack/Defense/Gardens/Shrine). Go over hero's ult charge and correct column (usually just D.Va)   python get_map_stats.py -- > Outputs match statistics in /csvs/get_map_stats  Example Outputs  CSV Output General Statistics | Compositions | Fight Statistics  Acknowledgements  Thanks to Farza for his article on Mood and methodology on CNNs  Todo  Suicides & Eliminate (or don't count) rows that happen x seconds before new round. Implement smart clean csv functionality (ult charge column)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/alined908/projectnugget/blob/c8221447e56e86beb511f9f7b755cb386a5c7eb1/gamestate_model.h5", "https://github.com/alined908/projectnugget/blob/c8221447e56e86beb511f9f7b755cb386a5c7eb1/killfeed_model.h5", "https://github.com/alined908/projectnugget/blob/c8221447e56e86beb511f9f7b755cb386a5c7eb1/left_hero_model.h5", "https://github.com/alined908/projectnugget/blob/c8221447e56e86beb511f9f7b755cb386a5c7eb1/left_ult_model.h5", "https://github.com/alined908/projectnugget/blob/c8221447e56e86beb511f9f7b755cb386a5c7eb1/right_hero_model.h5", "https://github.com/alined908/projectnugget/blob/c8221447e56e86beb511f9f7b755cb386a5c7eb1/right_ult_model.h5", "https://github.com/alined908/projectnugget/blob/b623c8e3ea23ee485afdb4a29c04df0a778d0362/pause_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe7"}, "repo_url": "https://github.com/MMostavi/CNNCancerType", "repo_name": "CNNCancerType", "repo_full_name": "MMostavi/CNNCancerType", "repo_owner": "MMostavi", "repo_desc": "This is the repository for paper titled as \"Convolutional neural network models for cancer type prediction based on gene expression\".", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T15:50:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T06:39:47Z", "homepage": "", "size": 75, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185547081, "is_fork": false, "readme_text": "Predicting all 33-cancer types and their normal tissues with CNN  Folder ending with 33 class contains models which only work for classification of 33 cancer tumors. In order to see the impact of Normal tissues in classification, the codes that are in 34 class folder need to be run. Hyperparameter tuning folder is showing grid search result of some of the hyperparameters for 1D-CNN and Vanilla models. All codes are written in Keras with a simple structure which helps reader understand the modeling stage easier. Background Precise prediction of cancer types is vital for cancer diagnosis and therapy. Important cancer marker genes can be inferred through predictive model. Several studies have attempted to build machine learning models for this task however none has taken into consideration the tissue of origin effects that can potentially bias the identification of cancer markers. Results In this paper, we introduced several Convolutional Neural Network (CNN) models that take unstructured gene expression inputs to classify tumor and non-tumor samples into their designated cancer types or as normal. Based on different designs of gene embeddings and convolution schemes, we implemented three CNN models: 1D-CNN, 2D-Vanilla-CNN, and 2D-Hybrid-CNN. The models were trained and tested on combined 10,340 samples of 33 cancer types and 731 matched normal tissues of The Cancer Genome Atlas (TCGA). Our models achieved excellent prediction accuracies (93.9-95.0%) among 34 classes (33 cancers and normal). Furthermore, we interpreted the 1D-CNN model with a guided saliency technique and identified a total of 2,090 cancer markers (108 per class). The concordance of differential expression of these markers between the cancer type they represent and others is confirmed. In breast cancer, for instance, our model identified well-known markers, such as GATA3 and ESR1. Finally, we extended the 1D-CNN model for prediction of breast cancer subtypes and achieved an average accuracy of 88.42% among 5 subtypes. Conclusions Here we present novel CNN designs for accurate and simultaneous cancer/normal and cancer types prediction based on gene expression profiles, and unique model interpretation scheme to elucidate biologically relevance of cancer marker genes after eliminating the effects of tissue-of-origin. The proposed model had light hyperparameters to be trained and thus can be easily adapt to facilitate cancer diagnosis in the future. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe8"}, "repo_url": "https://github.com/sultansidhu/MNIST-Digit-Recognition", "repo_name": "MNIST-Digit-Recognition", "repo_full_name": "sultansidhu/MNIST-Digit-Recognition", "repo_owner": "sultansidhu", "repo_desc": "A simple deep learning neural network to recognize digits. Uses the Keras library. A deep learning version of \"Hello World!\".", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T03:26:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T03:24:28Z", "homepage": null, "size": 0, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185520733, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbe9"}, "repo_url": "https://github.com/RayXie29/MNIST_DRAW_PREDICT", "repo_name": "MNIST_DRAW_PREDICT", "repo_full_name": "RayXie29/MNIST_DRAW_PREDICT", "repo_owner": "RayXie29", "repo_desc": "Mnist/Drawing/Image Digit recognizer", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T10:01:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T01:09:46Z", "homepage": "", "size": 43263, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185502081, "is_fork": false, "readme_text": "CNN Mnist/Draw/Image Train/Predict  This repo is using CNN model to train a digit recognizer for some interesting applcation. The Whole model is built by Keras sequential module.  Train folder Contain several training python file, including model training by dataset  1.Csv file: This training dataset is the same format as the digit recognizer competition in Kaggle. The first column would be the training label and the rest of the column would be the pixel values of digit image.  2.Image: This format of data will received by using the ImageDigitTrain.py. It will use DigitParser class to parse the digit image in the input image.  3.Draw: In DrawTrain.py, it is implement by openCV module. The input images are drawn by the user and the digit data will in an ascending order.  train.py :  Train the module by csv file. It can show some data visualized information(label counts, confusion matrix, accuracy/loss curve....) by setting the show_flag to true. Only the bacth_size and the epochs are tunable, if further adjustment wants to do, then you can modify the source code for this purpose. After training the model, it will save a model file. Also you can import the trained model for further training by setting the model_path argument   DrawTrain.py :  This python file will train the model by your own drawing picture. You need to draw 10 digits in an ascending order(0~9), and the model will trained by the digits image you draw. But this might has very little effect, since the training data is too less, but it will use cv2.dilate to make the digits thicker and ImageDataGenerator for expanding the dataset.   ImageDigitTrain.py :  This python file will take an image for input and use DigitParser class to parse the digits in the image, and you need to label the digits by yourself. The DigitParser class is written by several openCV functions, but it might only can handle some simple situation(Normal size digits in a simple backgound).    Predict folder Contain severl python files which are for predicting the result.   predict.py :  This python file is for csv file data prediction. Simple input the model path adn the test data, it will output a prediction.csv for result. The test data should have the same format as Kaggle`s(784 columns, eacj column represent pixel value) predictDraw.py :  This python file will predict the digit result you draw. By the way, the canvas is maded by openCV function, every time you press the left mouse button, it will draw an small circle on the black image. When there are many small circle connect togetherm it will look like a line.   ImageDigitPredict.py :  This python file is very similar to ImageDigitTrain.py. It will take an image and model for input, then it will use DigitParser to parse the digit images in the input image. Then use the model to predict the digit images and show the result on the original input image.    extra_modules folder Some modules for image processing ", "has_readme": true, "readme_language": "English", "repo_tags": ["cnn-keras", "cnn-classification", "cnn-model", "mnist", "opencv", "python"], "has_h5": true, "h5_files_links": ["https://github.com/RayXie29/MNIST_DRAW_PREDICT/blob/d853ba90ebb944300b2fcda28e72b0cc1085cd98/model/mnist_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbea"}, "repo_url": "https://github.com/fcsiba/BlindSight", "repo_name": "BlindSight", "repo_full_name": "fcsiba/BlindSight", "repo_owner": "fcsiba", "repo_desc": "FYP Fall 2018:", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T05:31:22Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T05:19:09Z", "homepage": "", "size": 34248, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185533195, "is_fork": false, "readme_text": "BlindSight BlindSight is an Android Application, powered by Python algorithms working on the Server's backend, to help the visually impaired in their daily lives. Primarily, it has two parts,  An Android Application A custom-camera app built using Java on Android Studio that is used to send a picture to a host address whenever the user taps on the screen or clicks the Volume-Up button. Python Scripts On the server, the Python script receives this picture and uses Artificial Intelligence to make sense of this picture and convert it into textual output, which can then be understood by the user as a voice output.  Client - Side : Android Application The Android Application is a custom-built camera app, built using Android's camera 2 api. It is lightweight and robust and designed intuituvely to be helpful for a visually-impaired person. Input : User inputs a touch interaction by touching on the screen or clicking the volume button. Processing : The camera captures whatever on the screen and sends it to the server using a host address. Output : The application receives the output from the Server and converts it into voice output for the user to hear. Server - Side : Python The Server Side is primarily broken up into two parts, Input : The server receives the input at the host address in the form of a picture. Processing : On the basis of the input, the server decides what operation needs to be performed. We have implemented three major operations,  Object Detection Text Recognition Face Detection + Recognition  Once the image is passed through these algorithms, we will have an output in the form of textual data. Output : The Output will be sent back to the user which initiated the connection. Object Detection Object Detection has been implemented using RetinaNet - Focal Loss (Object Detection). RetinaNet is a One-Stage Detector. With Focal Loss and RetinaNet Using ResNet+FPN, it surpasses the Accuracy of two-stage detectors like Faster R-CNN.  It uses Focal Loss. The loss function is reshaped to down-weight easy examples and thus focus training on hard negatives.   (a) and (b) Backbone ResNet is used for deep feature extraction. Feature Pyramid Network (FPN) is used on top of ResNet for constructing a rich multi-scale feature pyramid from one single resolution input image. (Originally, FPN is a two-stage detector which has state-of-the-art results. Please read my review about FPN if interested.) FPN is multiscale, semantically strong at all scales, and fast to compute. There are some modest changes for the FPN here. A pyramid is generated from P3 to P7. Some major changes are: P2 is not used now due to computational reasons. (ii) P6 is computed by strided convolution instead of downsampling. (iii) P7 is included additionally to improve the accuracy of large object detection. Anchors The anchors have the areas of 32\u00b2 to 512\u00b2 on pyramid levels from P3 to P7 respectively. Three aspect ratios {1:2, 1:1, 2:1} are used. For denser scale coverage, anchors of sizes {2\u2070, 2^(1/3), 2^(2/3)} are added at each pyramid level. In total, 9 anchors per level. Across levels, scale is covered from 32 to 813 pixels. Each anchor, there is a length K one-hot vector of classification targets (K: number of classes), and a 4-vector of box regression targets. Anchors are assigned to ground-truth object boxes using IoU threshold of 0.5 and to background if IoU is in [0,0.4). Each anchor is assigned at most one object box, and set the corresponding class entry to one and all other entries to 0 in that K one-hot vector. If anchor is unassigned if IoU is in [0.4,0.5) and ignored during training. Box regression is computed as the offset between anchor and assigned object box, or omitted if there is no assignment. (c) Classification Subnet This classification subnet predicts the probability of object presence at each spatial position for each of the A anchors and K object classes. The subnet is a FCN which applies four 3\u00d73 conv layers, each with C filters and each followed by ReLU activations, followed by a 3\u00d73 conv layer with KA filters. (K classes, A=9 anchors, and C = 256 filters) (d) Box Regression Subnet This subnet is a FCN to each pyramid level for the purpose of regressing the offset from each anchor box to a nearby ground-truth object, if one exists. It is identical to the classification subnet except that it terminates in 4A linear outputs per spatial location. It is a class-agnostic bounding box regressor which uses fewer parameters, which is found to be equally effective.  Text Recognition Optical Character Recognition involves the detection of text content on images and translation of the images to encoded text that the computer can easily understand. We used Python-Tesseract, which is a wrapper for Google's Tesseract-OCR Engine. An open-source algorithm, it uses a step-by-step approach for text recognition,  Line Finding Baseline Fitting Fixed Pitch Detection and Chopping Proportional Word Finding Word Finding Chopping Joined Characters Associating Broken Characters  Face Detection We used two libraries for face detection and recognition.  dlib face_recognition  The dlib library, maintained by Davis King, contains our implementation of \u201cdeep metric learning\u201d which is used to construct our face embeddings used for the actual recognition process. The face_recognition  library, created by Adam Geitgey, wraps around dlib\u2019s facial recognition functionality, making it easier to work with. Installation  Python 3.6.8 |Anaconda custom (64-bit) GCC 4.2.1 Compatible Clang 4.0.1  We used Mac OS X to run our project. You must have conda installed in order to start the installation. To know more about installing Conda, visit this link. Once conda is installed, run the following code, conda create -n retinanet python=3.6 anaconda  It will install Python3.6 and all the required packages for retinanet. Next, activate the virtual environment you just created. source activate retinanet  The following imported libraries were used in this project. You can use pip to manually install each library on your PC, import re    from bs4 import BeautifulSoup import requests from sys import byteorder from array import array from struct import pack import pyaudio import wave from gtts import gTTS  from pygame import mixer # Load the required library import threading from imageai.Detection import ObjectDetection import os import collections from PIL import Image import pytesseract import argparse import cv2 import os from __main__ import * import face_recognition import argparse import pickle import cv2 import sys from gtts import gTTS  import urllib.request import requests import base64 try:     from PIL import Image except ImportError:     import Image import pytesseract import base64 import requests import cv2 import numpy as np from keras.models import load_model import sys import warnings import tensorflow as tf from keras.backend.tensorflow_backend import set_session from pydub import AudioSegment from pydub.playback import play from imutils import paths import face_recognition import argparse import pickle import cv2 import os import shutil  Once installed, you need to download the two models we used,  RetinaNet for ObjectDetection - Place it inside the BlindSight/ folder as 'resnet50_coco_best_v2.0.1.h5' | Download Now Emotion Recognition - Place it inside the face-recognition-opencv/ folder as 'model_5-49-0.62.hdf5'  Once stored, you can run BlindSight.py file to start the code. Face Detection requires a model (needs stored faces) - you will need to use the BlindSight.py's 'save' command to record and save faces. After that, use encode_faces.py to create a model and place it in the face-recognition-opencv/ folder as 'encodings.pickle' using Python's pickle library. Now, your code is capable of recognizing the stored faces. You can store more faces for better recognition. Commands Implemented  Object Recognition Face Recognition + Emotions Detection Text Recognition Recording Notes (Voice to Text and vice versa) Reading Headlines  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/fcsiba/BlindSight/blob/9296b97219944af505ba4282717c95a025f757df/face-recognition-opencv/model_5-49-0.62.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbeb"}, "repo_url": "https://github.com/WeronikaWestwanska/ToBeeOrNotToBee", "repo_name": "ToBeeOrNotToBee", "repo_full_name": "WeronikaWestwanska/ToBeeOrNotToBee", "repo_owner": "WeronikaWestwanska", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T15:35:39Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T14:05:57Z", "homepage": null, "size": 495052, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185618294, "is_fork": false, "readme_text": "Title: To Bee Or Not To Bee Short description This code is a supplement to the article by Weronika W. Westwa\u0144ska and Jerzy S. Respondek titled: \"Counting Instances of Object in Colour Images Using U-Net Network on Example of Honey Bees\" The goal of this work was to examine a novel method for counting instances of Object Of Interest (OOI) in colour digital images, using modern Deep Learning technique called U-Net network. The method works on approximate shaping of OOIs into circles, further modelling data generation, segmentation and counting the OOIs. We also propose a new method for segmentation of OOI, based on a trained U-Net network, which is applied to produce a set of images converted to an OOI heatmap. The heatmap is later binarised for a purpose of OOI counting. The data comes from https://www.kaggle.com/jonathanbyrne/to-bee-or-not-to-bee. We used original set of 550 labelled images, further combined with 1086 images labelled by us, to create conditions for conducting experiments on how our approach in U-Net modelling would influence bees counting error. Details of the working of this method are clearly described in the source code comments. Description of project contents:  data/labelled.train - images used in training U-Net network, data/labelled.validate - images used in calculation of relative bees counting error data/labels.train.db - SQLite3 database with coordinates of bees images from data\\labelled.train directory data/labels.validate.db - SQLite3 database with coordinates of bees images from data\\labelled.validate directory results/set1- 25 different experiments for parameters set 1 (mentioned below) attempting to find how modelling set size influences OOI counting error. For each of 5 subsets for sizes (100, 200, 500, 1000 and 1096) only the best training model was picked up and used for segmentation. results/set2 - 25 different experiments for parameters set 2 (mentioned below) attempting to find how modelling set size influences OOI counting error. For each of 5 subsets for sizes (100, 200, 500, 1000 and 1096) only the best training model was picked up and used for segmentation. .editorconfig - settings file for Visual Studio. BeesDataGenerator.py - routines used in modelling data generation, BeesDataReader.py - routines for reading SQLite3 database and its contents where bees coordinates are stored BeesDataTester.py - routines for calculating OOIs, BeesHeatMap.py - routines for OOI heatmap generation, experiment.sh - simple bash script to automate process of training, FileTools.py - routines for creating directory, Parameters.py - parameters values used in training and segmentation, readme.md - this file, ToBeeOrNotToBee.sln - Visual Studio python project file, ToBeeOrNotToBee.pyproj - Visual Studio python project file, ToBeeOrNotToBee.pyperf - Visual Studio python project file, Tools.py - various routines for training, segmentation, saving data Unet.py - routines for U-Net modelling,  Note: Please bear in mind that what we call in the Python code as train set is an equivalent to modelling set in the article, and validate set is an equivalent to segmentation set in the article. This confusion is caused by naming conventions made at a time of starting the work, which was later clarified when summarising it in a form of the article. Results The most time consuming part of the process is OOI error calculation. The training of U-Net takes about 5 minutes of a PC with NVidia 1080 GTX with 8GB of memory. The segmentation took about 2h 40 minutes to process 540 images in the segmentation set. We decided to train the U-Net network for 2 different sets of parameters describing size of a circle approximating a generic bee's body. Set 1 - circle of bee radius = 20, with Pmin = 0.80, Pmax = 1.00, amount of random training windows per image = 60, minimum percentage of pixels per window to be considered as a bee = 45 Set 2 - circle of bee radius = 16, with Pmin = 0.99, Pmax = 1.00, amount of random training windows per image = 80, minimum percentage of pixels per window to be considered as a bee = 50 For each of the sets parameters (set1, set 2) we run the script (on a Cygwin Windows 10 machine): ./experiments.sh This script generated a set of 25 directories with different suffixes, where a log file and U-Net trained model was stored. The 25 directories would represent training result of U-Net where the modelling sets of size 100, 200, 500, 1000 and 1096 was used to train U-Net network 5 times each. Once we collected the log files, we would pick the best trained models, and use them for segmenting and further calculation of relative bees counting error. The best model was found for Set 1, modelling set equal to 1096 images, where the error = 14.28 %. Below is a table representing training validation accuracy and OOI relative error in percentage points performed for Parameters Set 1.    Set 1 Modelling set size Training Validation Accuracy OOI Error     #1 100 95.13 75.08 %   #2 200 94.65 73.00 %   #3 500 94.25 24.23 %   #4 1000 94.93 16.83 %   #5 1096 94.68 14.27 %     Data for the best result of 14.27% is available in results/set1/SET1_20190428_221502_FILES_COUNT_1096. Segmented images are available in results/set1/SET1_20190428_221502_FILES_COUNT_1096/segmented  Below is a table representing training validation accuracy and OOI relative error in percentage points performed for Parameters Set 2.    Set 2 Modelling set size Training Validation Accuracy OOI Error     #1 100 96.41 75.24 %   #2 200 96.15 74.55 %   #3 500 95.24 26.35 %   #4 1000 96.23 16.87 %   #5 1096 96.12 18.17 %    How to use In order to perform a whole end-to-end process of data generation, training, segmentation and counting of OOIs the user needs to decide on values of Parameters.py (or leave them how they are by default) and then perform the following commands: python Main.py --generate --train python Main.py --segment --count  The first command will generate modelling data and train UNet network. The results will be stored by default in data/train_r16 (depending on value of 'bee_radius'). The second command will use model stored in data/train_r16 and create a new directory data/segmented. This bit can take at least 2 hours (depending on value of sliding_window_step in Parameters.py) as well as graphics card. Note: Please ensure when training the U-Net network that the process is not stuck at the same validation accuracy (sometimes it can happen due to a bug in Keras). In such case, delete the contents of data/train_r16 directory and start over. Typical validation accuracy should reach at least 94%. Last Modified: 2019/05/06 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf5189d7eb8d667d8a1cbec"}, "repo_url": "https://github.com/upkoi/tail", "repo_name": "tail", "repo_full_name": "upkoi/tail", "repo_owner": "upkoi", "repo_desc": "Tampa AI League", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-19T23:27:00Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-08T04:56:45Z", "homepage": null, "size": 2067, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185530902, "is_fork": false, "readme_text": "    Tampa AI League is a monthly in-person competition in the format of a tournament played by eight automated agents (bots) in a real-time grid world environment at midnight. The winner of the tournament takes home an Ethereum prize sponsored by local technology companies. Solutions are heavily encouraged to use recent advances in machine learning / reinforcement learning, but any attempts are welcome as long as they meet the specification outlined below. Python is the default language but we're working on supporting others. Use our self-qualification tool to verify your submission can run on our environment (details below). The core execution engine, SkyPond, and a first game have both been made open source.  Game Details Quick Start  Prerequisites Installation Agent Creation Testing Locally Deployment   Important Limitations Rules & Current Game Parameters Additional Technical Details Questions & Answers Acknowledgments  Game Details    Four keys, representing fragments of an Ethereum private key, are hidden in a maze. Have your agent collect all four keys and you will instantly* be credited the full prize pool. Can't find all four keys? Other agents likely have them and your submission can fight them to win. Players compete in a small custom multi-agent grid world environment, Four Keys, optimized for interaction by machine learning agents. The game is displayed live on a projector. We've balanced the game to help machine learning and non-machine learning agents compete on a relatively level field. See below (and on the event page) for current rules and parameters. Entering a competition is completely free. The game is turn-based, and each agent can see a small portion of the board surrounding them, provided in a frame each turn. They are also given several previous frames of activity, a breadcrumb frame indicating where the agent has been, and a supplement containing additional information about the agent movement recharge, attack recharge, and position in the larger grid. Agents are strongly suggested to be completely stateless and rely only on this information.    Games take a maximum of one hour to complete and are hands-off after submission. The game execution time is a great time to chat with other machine learning practitioners and software developers in the area. *Provide an ethereum address to receive prizes. See below for more information. Depending on the game parameters, there might be several rounds before a winner is declared. Quick Start Initial submissions usually can be created in 15-30 minutes by following these instructions on Linux or a Mac. We haven't had a chance to test this on other environments yet. We've open-sourced / MIT licensed the core execution engine, SkyPond, to allow transparency into the main code that is running the game. 0 - Prerequisites  Docker Python 3.6 Linux or OS X  The execution environment requires python 3 (python 3.6 or above is strongly suggested for installation simplicity) and a docker installation. It works best on Linux or OS X (due to a dependency on OpenAI Gym). Note: Python 2 support is not currently planned. Check your python3 installation with: python3 -V  Verify your docker installation with: docker run hello-world  OS X: Add /var/folders to whitelist [Required] Add /var/folders to the \"File Sharing\" whitelist in docker preferences. This is required so docker can access the temporary directory created by the qualification tool on OS X. If you have a custom temporary directory set, use that instead. Veriy using echo $TMPDIR. Linux: Verify Non-Root Docker Access Make sure you can run docker as a non-root user. See post-installation instructions for more information. If not, the qualification tool will likely fail to run. Resources  OS X Prerequisite Guide Alternative Guide to Installing Python3 on OS X Latest Docker Installation Instructions  1 - Installation Pull down this repository and run the included setup tool to get started. This will install all dependencies,  including SkyPond. Example: git clone https://github.com/upkoi/tail cd tail python3 setup.py install --user   The PyTorch dependency is used for the example training script. If you have trouble installing PyTorch through the automated install try installing it manually with... pip3 install torch==1.0.1  ...and then running the setup script above. Note that PyTorch version 1.0.1 is required to line up with the execution environment. Post-Installation Setup Optionally, run a self-test of the execution environment after installation with: python3 qualify.py --self-test --unrestrict-networking  Note: The first time the qualification tool is run it will need to pull down the latest execution environment docker image. This may take several minutes. 2 - Agent Creation Use the included create.py script to copy an existing example. All examples are MIT licensed and are provided as inspiration and starting points. We offer a starting point for reinforcement learning agents, tolstoy. This is an agent that learns how to find and hold on to keys but doesn't fight often. This agent can be trained in about 20-30 minutes to pass the qualification criteria and is a starting point for modification. Use the create.py script to create a variant of this model... python3 create.py --template neural/tolstoy  Follow the prompts to add a name and an optional Ethereum key for your submission. Once created, cd into the directory and run the included training file. cd yolo python3 train.py  Notes:  If you run into issues on OS X with PyTorch related to libshm or libomp try brew install libomp The training tool might not render the return graph or visualization correctly on some terminal configurations on OS X. We're working on it.     This will train a new agent on a medium-sized board with one small opponent using PPO (Proximal Policy Optimization) - a modern reinforcement learning algorithm. The training script will intermittently test your agent on a larger board with several agents to visualize progress. The top two small frames in the visualization stage show part of what your agent sees, including the visible board features (like walls, other players, and keys) and the breadcrumb layer (stateful tracking of the last 1000 movements). This example model uses a deep convolutional neural network and should reach an (okay) average reward of around 9 in about 10-20 minutes. Feel free to press CTRL+C to stop training at any time after this point. 3 - Testing Locally This sample neural network agent includes everything you need to compete, including the required handler.py file that is used to communicate with your submission. An important part of the competition is self-qualification - validating that your submission is ready for battle and doesn't contain significant errors. To self-qualify your submission at any time, run the qualification tool: mkdir yolo-ready python3 qualify.py --agent ./yolo --output ./yolo-ready     This will:  pull down the validation docker image start it copy your files into it attempt to get your submission information try to break it by playing simulated rounds of the game against it of increasing difficulty.  If successful (and your submission didn't fail any of the tests) the packaging tool will copy your files into a new folder specified by --output. Additional Notes:  Use --visualize to render agent progress during qualification. This will slow down the qualification process by a small amount. This process is not entirely deterministic yet. Feel free to give marginally failing agents another shot.  4 - Deployment Copy the contents of the /output folder onto a floppy disk (or specify your drive path with --output in the first place). Be careful not to copy the folder itself. The root folder on the disk should at least contain the entry.py file and the qualification.dat file and typically also contains an info.json file as well as others. You can validate your packaged submission on your target disk by using --verify. Ex: python3 qualify.py --verify /media/rob/disk  Important Note: After qualification do not edit any part of your agent, including the name! We hash parts of the agent in the qualification process (subject to change) and changing your agent after qualification might cause it to fail. 5 - Yeah! At this point, you know how to create, validate, and package submissions. We encourage you to check out the other samples and use one as a starting point for creating your own agent. The final step is to bring your agent to the tournament. Competitions are currently private. Please e-mail rob@upkoi.com to enter. We're opening up competition to the public soon - check midnightfight.ai for availability. Get Some Hardware A modern floppy disk drive and set of two or more floppy disks are recommended. Here is a link to the same hardware used in the competition itself: RAAYOO USB Floppy Disk Drive This drive was approximately $15 as of May 2019. Important Limitations Disk Space The most significant constraint for the competition is a hard limit of all submission content at approximately 1.44MB as all submissions are required to be made in-person at the event on a single 3.5\" floppy disk. A backup disk is also recommended as magnetic format media is vulnerable to failure due exposure to excessive heat and significant magnetic fields. This constraint adds a model-compression aspect to some machine learning submissions which may need to be considered during your design. It also helps balance the playing field by preventing very large or complex neural network models. We also like floppy disks. Python 3 & Libraries Additionally, the floppy disk must contain a Python 3 file, handler.py, which implements a standardized interface through flask (detailed below and in the examples folder). Your code is run in isolation on a docker image that is preloaded with a standardized set of relevant libraries, including recent versions of:  NumPy Pandas Scipy TensorFlow Keras PyTorch Keras-RL TensorForce AI Gym h5py torch-ac  See the docker_image folder in SkyPond for more information and exact versions. Rules & Current Game Parameters The current game, Four Keys, is a custom grid world environment that was designed to be simple to understand and enter, but challenging to master. The initial iteration of the game contains simplified rules which might be expanded in later versions. The game is a maze-like environment with four keys randomly distributed in a (commonly) 15 by 15 or 30 by 30 tile board. Up to eight players start at points on the outside of the board and take turns moving around to locate keys and attack other players while evading an attack. Once picked up by a player, keys are held in inventory until attacked. The goal is to hold all four keys simultaneously - which concludes a game. Depending on the exact tournament, games might be played just once or multiple times. Players can attack as well as drop individual keys (potentially as a lure). If hit by another player while holding keys, the player loses half of their attack recharge and must also wait eight turns before moving again. The attacking player must wait twenty turns before triggering another attack. Both movement and attack recharge values are private and are not broadcast to other players. Be careful of moving adjacent to other players.    Parameter Value     Game Name Four Keys   Game Version v1.0   Maximum Players 8   Minimum Players Needed 2   Competition Board Size 30 x 30 (900 Squares)   Observable Board Size 7x7 (49 Squares)   Win Condition A single agent obtains all four keys.   Attack Range 1 Square (Adjacent)   Attack Recharge 20 Turns   Attack Movement Penalty (Once Hit - If Holding Keys) Drop all keys and wait for eight turns before movement. Lose half of attack recharge.   Key Drop Logic First randomly selected open tile. This is an important distinction that may result in unexpected behavior in close-quarter combat in narrow corridors or mazes.   Time Limit 60 Minutes   Tie Resolution In the event of a tie, victory will go to the player that was able to obtain a key earliest in the game.    The processing of invalid actions is subject to the discretion of the execution environment. Most invalid actions will result in no action - effectively the loss of a turn. If an agent is disqualified or unreachable during play, the agent will take the most passive action possible, usually standing still on the game board. This is generally not optimal as not moving is an excellent way to get attacked by other players. All permissible behavior, given the constraints, including permanently blocking/neutralizing other players is allowed. We only ask that you try not to deliberately disrupt the game by intentionally taking down the execution environment. Additional Technical Details This is a detailed summary of what the executing environment looks like, what is expected from your submission, and a summary of how the platform works from a technical perspective. See the SkyPond project for the core execution environment. Qualification Token When you pass self-qualification - using the qualification tool above - a qualification.dat file will be generated for your agent. This is a low security token to indicate your current agent passed qualification and needs to be on your submission at time of submission. Do not modify any part of your agent after qualification or it's possible that the execution engine will reject your submission. If you need to make further changes, be sure to re-qualify your submission. You can check if a packaged submission will pass qualification at any time by using python3 qualify.py --verify [agent path]  Entry File The primary required file is handler.py. This must contain a flask app exposing two routes:  /information /react  There are working versions of the routes in the examples folder, and it's strongly recommended that you copy an existing sample to start. As long as you implement these methods to spec in some capacity, we should be able to (at least attempt to) validate your submission. GET /information This is generally only called when your submission is first loaded (this is done automatically when your disk is entered). It retrieves your display name, a description, an e-mail address (used to look up a gravatar to be displayed on the screen with this other information during the competition), and an Ethereum address of where to send the prize money if your agent wins. The transfer happens automatically at the end of the tournament. In the sample files, this information is stored in a JSON configuration file and is forwarded back when the route is called. POST /react This method is called each time the game requires an action from your bot. We strongly encourage a completely stateless implementation. A substantial amount of game state is passed in to help make stateless agents viable. See the example folder for more details. Note that this method is also called several times after your bot is loaded to ensure it works. Execution Resources & Parameters   Your bot, during the competition, is given approximately 500ms to return a result through the react method.   Specific hardware resources might vary but expect approximately one 2ghz CPU core and 1GB of memory should be available.   Your environment will not have access to the internet and will likely appear to be offline.   Your environment may be restarted/reinitialized during the game.   You should not rely on the file system to store information across restarts. Any saved information will be removed.   A failure to use those resources to return a result in the allowed time will result in a passive action taken on your behalf, which will likely involve your bot not moving and being vulnerable to attack. It's strongly recommended that you test your code on a resource-constrained environment by using the qualification tool. It's also encouraged to wrap your main logic or model forward call in a try ... except block and default to a safe action if it fails. In-Game Disqualification The main goal of the execution environment is to keep the game going. If your agent is repeatedly timing / erroring out, or uses excessive resources out of turn, your submission might be automatically disqualified to help keep the game moving forward. For this reason, it's highly recommended to have a simple, stateless implementation that quickly returns a response to the react method. Questions & Answers Can my submission use the internet? A: Nope. Your execution environment is held offline. It's 2019, why am I using a floppy disk drive? A: 1.44MB is a great equalizer. It's relatively simple to take larger off the shelf models and libraries and adapt them to fit the execution model here, but that's not as much of a real challenge as writing your own code or learning how to do model compression to get a robust algorithm to run in a resource-constrained environment. By placing a significant constraint on total size, we place competitors on a more equal footing. Why Four Keys? What are other game formats under consideration? A: We wanted to make a multi-agent game that was easy to learn but difficult to master. This game is a work in progress and the specific rules, and game dynamics might evolve over time. However, the exact rules and a qualifying game engine will be published well advance of the competition. Do I have to use the packaging tool? A: Technically no. Feel free not to use it to verify or test out your submission. However, it will help ensure that your submission won't be rejected at the start of the tournament (a similar validation check happens automatically) and we strongly recommend it. Do I need to use a neural network? A: Not at all. Alternative solutions are allowed and encouraged. We attempted to provide enough stateful information to make it easy for both hardcoded as well as reinforcement learning agents to perform well. We're partial towards reinforcement learning and modern algorithms such as A3C as they can rapidly learn the game. Are there any age requirements? Due to our current venue, we request that you be 21 or older. We might relax that requirement in the future. I'm lazy / tired, can I just e-mail you a zip file?  Does the execution environment have a GPU? A: No, to simplify execution your model must forward on CPU only. This shouldn't be an issue given the file size limitation effectively limiting the number of model parameters. Are teams allowed? How do we submit a team entry? A: Feel free to submit an entry as a team. If you are submitting on behalf of an organization, then it's encouraged that you use or incorporate your organization name in name field (which ultimately is displayed on the screen during the competition). Also, feel free to submit multiple agents that work together given the environment. However, each submission needs to be delivered in person and we ask that the agent names share a common prefix or element to help identify them as teams. To share the prize, have both agents contain the same target Ethereum address. Are there plans to support other languages? How was python selected? A: Python is a great language for building machine learning solutions (it's the most popular language for data science as of 2019). One of the contributing reasons is the corresponding rich ecosystem of libraries that accelerate machine learning development. See above for the available libraries on the execution environment. It's also a popular language in general, scoring high on the 2018 stack overflow developer survey. Additionally, it's a speedy language to pick up, we encourage developers in other languages to give it a whirl. That said, there are plans to eventually add first class support for JavaScript, C#, and Julia. Can you add x package to the base execution environment? A: Changes to the execution environment are carefully considered as they have the potential to change the game dynamics considerably. Part of the challenge is to do well given limited resources. That said, send requests for environment updates to rob@upkoi.com, and if enough requests come in for a given package, we'll consider  integrating it. For now, try including the package - and any dependencies - in your submission. This is challenging for some (more substantial) libraries but possible for others. Can you open source the parent competition-time execution environment and display? A: There's a chance we'll eventually open-source additional parts of the parent competition-time orchestration logic. For now, all of the essentials are included in SkyPond. I live in another city, can I participate? A: Absolutely. There are daily cheap non-stop flights to Tampa from many cities (including New York and San Francisco) and competitors from anywhere in the world are welcome. It's encouraged to secure a spot in the primary competitor list on the Eventbrite page before booking tickets or securing accommodations (use the registration link on the homepage). Come out to Tampa Bay and eat a Cuban sandwich while battling your fellow ML practitioners. What happens if my submission is not accepted by the execution environment on the day of the event? How does the wait list work? A: Competitors are encouraged to arrive at least 30 minutes before the competition. At least 15 minutes before the game, the execution environment is started and submissions are automatically validated. Solutions that don't pass the validation phase have until the minute before the competition start time to make adjustments and pass validation. If there are any competitors in the wait list the minute before the competition then they are swapped in instead (time permitting). The game automatically starts at midnight. Competitors on the wait list are swapped in at the final minute before the competition starts to replace no-shows or agents that failed validation. Submissions on the wait list are processed in the order received. How private is my submission? How is intellectual property handled? A: We will not share your submitted code and will (likely) never look at it. Generally, your submitted code is used for the competition and is permanently removed from the execution environment shortly after the game ends. In the rare event that there is a severe problem with the execution environment that appears to be from malicious intent, submissions might be evaluated privately. If you'd like more information, please feel free to e-mail rob@upkoi.com. Acknowledgments Both TAIL and SkyPond build on top of the work of talented machine learning practitioners and companies from around the world. The competition and core execution were inspired and influenced by presentations, conversations, and related competitions at NeurIPS 2018, MLConf, and Kaggle Days SF 2019. See SkyPond Acknowledgments for more information. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1707.06347.pdf"]}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f2"}, "repo_url": "https://github.com/sage-group/hugin", "repo_name": "hugin", "repo_full_name": "sage-group/hugin", "repo_owner": "sage-group", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-16T09:58:11Z", "repo_watch": 1, "repo_forks": 2, "private": false, "repo_created_at": "2019-05-09T19:08:15Z", "homepage": "http://www.sage.ieat.ro", "size": 71, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185856891, "is_fork": false, "readme_text": "Hugin Hugin helps scientists run Machine Learning experiments on geospatial raster data. Overall Hugin aims to facilitate experimentation with multiple machine learning problems, like:  Classification Segmentation Super-Resolution  Currently Hugin builds on top of the Keras machine learning library but it also aims to support, in the future, additional backends like scikit-learn.  Installation Prerequisits As previously mentioned Hugin builds on top of Keras but, depending on your usecase it might also depend on TensorFlow for providing additional IO functionality (particularly cloud storage), or for supporting models specific for various Keras backends Using pip From PyPi ToDo From source repository You can install Hugin using the following command: pip install git+http://github.com/mneagul/hugin#egg=hugin From source code When installing from source code we recommend installation inside a specially created virtual environment. Installing from source code involves running the setup.py inside you python environment. python setup.py install Using Using Hugin involves two steps:  training prediction  Both steps are driven using dedicated configuration files. Training Training can be started as follows: hugin train --config training_config.yaml An example training configuration can be found in docs/examples/train.yaml. Prediction Prediction can be started as follows: hugin predict \\     --ensemble-config prediction.yaml \\     --input-dir /path/to/input/dir \\     --output-dir /tmp/output An example prediction configuration can be found in docs/examples/predict.yaml The predict command requires at least three arguments:  --ensemble-config: representing the prediction configuration file --input-dir: representing the directory holding data that should server as input for prediction --output-dir: directory for storing the outputs (predictions)  Developing your own models Developing your own is really simple. The only thing needed is to create a python file that creates the corresponding Keras model. The code building the model needs to be resolvable by Hugin: it needs to be available in the PYTHONPATH. Let's consider you are preparing a new segmentation related experiment. The most simple approach would be to create a new directory containing both the source code and model configuration, like in the following example: mysegmentation/ \u251c\u2500\u2500 model.py \u251c\u2500\u2500 predict.yaml \u2514\u2500\u2500 train.yaml  The files involved in this example are:  model.py: it contains the source code for the model. An example can be found in the source distribution, in src/hugin/models/unet/unetv14.py train.yaml: the configuration used for training. An example can be found in docs/examples/train.yaml predict.yaml: the configuration used for prediction. An example can be found in docs/examples/predict.yaml  After creating your model and preparing your experiment configuration you can start training, by running: hugin train --config train.yaml As specified in the model configuration the model will train for 10 epochs and produce the final model. ", "has_readme": true, "readme_language": "English", "repo_tags": ["machine-learning", "earth-observation"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f3"}, "repo_url": "https://github.com/penny4860/retinanet-digit-detector", "repo_name": "retinanet-digit-detector", "repo_full_name": "penny4860/retinanet-digit-detector", "repo_owner": "penny4860", "repo_desc": "Implemented digit detector in natural scene. I used SVHN as the training set, and implemented it using keras-retinanet.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T12:17:47Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T12:12:34Z", "homepage": "", "size": 2679, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185792266, "is_fork": false, "readme_text": "Retinanet Digit Detector I have implemented retinanet digit detector using keras-retinanet package and svhn dataset.  Usage for python code 0. Requirement  python 3.6 tensorflow 1.10.0 keras 2.2.4 keras-retinanet 0.5.0  1. Digit Detection using pretrained weight file In this project, the pretrained weight file is stored in resnet50_full.h5. Test set evaluation (13068-images) score is mAP: 0.8148  Download resnet50_full.h5 to the project-root/snapshots Run infer.py  You can see the following figure:  2. SVHN dataset in Pascal Voc annotation format In this project, I use pascal voc format as annotation information to train object detector. An annotation file of this format can be downloaded from svhn-voc-annotation-format. 3. Training from scratch Please refer to main.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://host.robots.ox.ac.uk/pascal/VOC/"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f4"}, "repo_url": "https://github.com/Thulio-Carvalho/DeepEvents", "repo_name": "DeepEvents", "repo_full_name": "Thulio-Carvalho/DeepEvents", "repo_owner": "Thulio-Carvalho", "repo_desc": "A face recognition implementation using a deep convolutional neural network.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T21:34:42Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T13:23:19Z", "homepage": "", "size": 86837, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185803468, "is_fork": false, "readme_text": "DeepEvents A deep face recognition implementation using a deep convolutional neural network. Information DeepEvents uses Keras with Tensorflow backend to implement the CNN, OpenCV and Dlib's landmark estimation to align faces. Inspired by Google's FaceNet and OpenFace The core idea behind this implementation is to be able to extract multiple people faces from images and generate a 128-dimensional embedding for each one of them. In this vector space, Euclidian distance shows as a good measure of face similarity. Face classification is done by comparing embeddings with already labeled vectors. A SVM (Support Vector Machine) and a KNN classifier are trained to classify new images labels. Install In order to run this code it is needed to have a jupyter notebook + tensorflow + opencv + keras running. It's easier use docker, Dockerfile is provided to build it. Once the container is all set up, run: $ mkdir models && cd models $ wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 $ bzip2 -d shape_predictor_68_face_landmarks.dat.bz2 $ mv shape_predictor_68_face_landmarks.dat landmarks.dat  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Thulio-Carvalho/DeepEvents/blob/ef2ca52f49d69de2c6fb89f0296a462acf207d0c/weights/nn4.small2.v1.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f5"}, "repo_url": "https://github.com/daxiaHuang/keras-contrib", "repo_name": "keras-contrib", "repo_full_name": "daxiaHuang/keras-contrib", "repo_owner": "daxiaHuang", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T03:48:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T03:37:02Z", "homepage": null, "size": 663, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185720875, "is_fork": false, "readme_text": "keras-contrib : Keras community contributions  This library is the official extension repository for the python deep learning library Keras. It contains additional layers, activations, loss functions, optimizers, etc. which are not yet available within Keras itself. All of these additional modules can be used in conjunction with core Keras models and modules. As the community contributions in Keras-Contrib are tested, used, validated, and their utility proven, they may be integrated into the Keras core repository. In the interest of keeping Keras succinct, clean, and powerfully simple, only the most useful contributions make it into Keras. This contribution repository is both the proving ground for new functionality, and the archive for functionality that (while useful) may not fit well into the Keras paradigm.  Installation Install keras_contrib for keras-team/keras For instructions on how to install Keras, see the Keras installation page. git clone https://www.github.com/keras-team/keras-contrib.git cd keras-contrib python setup.py install Alternatively, using pip: sudo pip install git+https://www.github.com/keras-team/keras-contrib.git to uninstall: pip uninstall keras_contrib  Install keras_contrib for tensorflow.keras git clone https://www.github.com/keras-team/keras-contrib.git cd keras-contrib python convert_to_tf_keras.py USE_TF_KERAS=1 python setup.py install to uninstall: pip uninstall tf_keras_contrib For contributor guidelines see CONTRIBUTING.md  Example Usage Modules from the Keras-Contrib library are used in the same way as modules within Keras itself. from keras.models import Sequential from keras.layers import Dense import numpy as np  # I wish Keras had the Parametric Exponential Linear activation.. # Oh, wait..! from keras_contrib.layers.advanced_activations import PELU  # Create the Keras model, including the PELU advanced activation model = Sequential() model.add(Dense(100, input_shape=(10,))) model.add(PELU())  # Compile and fit on random data from keras_contrib.losses.dssim import DSSIMObjective ssim_loss=DSSIMObjective() model.compile(loss=ssim_loss, optimizer='adam') model.fit(x=np.random.random((100, 10)), y=np.random.random((100, 100)), epochs=5, verbose=0)  # Save our model model.save('example.h5') A Common \"Gotcha\" As Keras-Contrib is external to the Keras core, loading a model requires a bit more work. While a pure Keras model is loadable with nothing more than an import of keras.models.load_model, a model which contains a contributed module requires an additional import of keras_contrib: # Required, as usual from keras.models import load_model  # Recommended method; requires knowledge of the underlying architecture of the model from keras_contrib.layers import PELU from keras_contrib.layers import GroupNormalization  # Load our model custom_objects = {'PELU': PELU, 'GroupNormalization': GroupNormalization} model = load_model('example.h5', custom_objects) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.keras.io"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f6"}, "repo_url": "https://github.com/geez0219/keras_application", "repo_name": "keras_application", "repo_full_name": "geez0219/keras_application", "repo_owner": "geez0219", "repo_desc": "Playground to use tf keras ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T22:48:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T22:42:40Z", "homepage": "", "size": 502, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185881477, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f7"}, "repo_url": "https://github.com/bhuvneshsaini/Stock_Price_Prediction", "repo_name": "Stock_Price_Prediction", "repo_full_name": "bhuvneshsaini/Stock_Price_Prediction", "repo_owner": "bhuvneshsaini", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T03:58:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T03:41:01Z", "homepage": null, "size": 30, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185721376, "is_fork": false, "readme_text": "Stock_Price_Prediction_using_Keras_and_Recurrent_Neural_Network Stock Price Prediction case study using Keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f8"}, "repo_url": "https://github.com/shalinikr09/fake-news-detection", "repo_name": "fake-news-detection", "repo_full_name": "shalinikr09/fake-news-detection", "repo_owner": "shalinikr09", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T03:58:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T03:57:14Z", "homepage": null, "size": 36533, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185723225, "is_fork": false, "readme_text": "keras-fake-news-detection Implementation of text summarization using RNN LSTM Word Embedding Used Glove pre-trained vectors, Word2Vec and Doc2Vec to initialize word embedding. Layers Used LSTM cell layer along with a dense layer Requirements  Python 3 Keras (=2.2.4)  Usage Prepare data Dataset is available at FakeNewsNet. Train For training and testing the model using the three different embeddings: $ python word2vecLSTM.py $ python gloveLSTM.py $ python doc2vecLSTM.py ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/shalinikr09/fake-news-detection/blob/c328ed7465e982d17b20a6dd096183b808a6f05a/models/doc2vec-feed-forward-weights.h5", "https://github.com/shalinikr09/fake-news-detection/blob/c328ed7465e982d17b20a6dd096183b808a6f05a/models/glove-feed-forward-weights.h5", "https://github.com/shalinikr09/fake-news-detection/blob/c328ed7465e982d17b20a6dd096183b808a6f05a/models/doc2vec-weights.h5", "https://github.com/shalinikr09/fake-news-detection/blob/c328ed7465e982d17b20a6dd096183b808a6f05a/models/lstm-weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47f9"}, "repo_url": "https://github.com/rishabhshah13/Face-Recogntion-using-Capsule-Networks", "repo_name": "Face-Recogntion-using-Capsule-Networks", "repo_full_name": "rishabhshah13/Face-Recogntion-using-Capsule-Networks", "repo_owner": "rishabhshah13", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-11T04:37:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T05:06:33Z", "homepage": "", "size": 187, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185729771, "is_fork": false, "readme_text": "Face-Recogtion-using-Capsule-Networks A Keras implementation of CapsNet(Capsules Net) was obtained from https://github.com/XifengGuo/CapsNet-Keras This implementation of Capsule Network is based on the paper: Dynamic Routing Between Capsules by Sara Sabour, Nicholas Frosst and Geoffrey E. Hinton. Dataset : Labeled Faces in the Wild(http://vis-www.cs.umass.edu/lfw/index.html) Requirements pip install -r requirements.txt ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://vis-www.cs.umass.edu/lfw/index.html"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47fa"}, "repo_url": "https://github.com/MchZys/Keras-FCN-template", "repo_name": "Keras-FCN-template", "repo_full_name": "MchZys/Keras-FCN-template", "repo_owner": "MchZys", "repo_desc": "A FCN template(container) to quickly train a FCN for a specific semantic segmention task.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-19T05:01:56Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T10:45:36Z", "homepage": "", "size": 9571, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185780233, "is_fork": false, "readme_text": "Keras-FCN-template A FCN template(container) to quickly train/predict a FCN for a specific semantic segmention task. File introdutction Director tree Architecture inspired by https://github.com/matterport/Mask_RCNN. \u251c\u2500\u2500 Apply \u2502\u00a0\u00a0 \u2514\u2500\u2500 Carvana \u2502\u00a0\u00a0     \u251c\u2500\u2500 code \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 CarvanaConfig.py \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 CarvanaDataset.py \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 Deeplabv3Plus.py \u2502       \u2502   \u251c\u2500\u2500 mobilenetv2_unet.py \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u251c\u2500\u2500 prediciton.py \u2502\u00a0\u00a0     \u2502\u00a0\u00a0 \u2514\u2500\u2500 train.py \u2502\u00a0\u00a0     \u2514\u2500\u2500 sample \u2514\u2500\u2500 BaseCode     \u251c\u2500\u2500 config.py     \u251c\u2500\u2500 dataset.py     \u251c\u2500\u2500 losses.py     \u2514\u2500\u2500 model.py  1. BaseCode:basic classes and functions  config.py: creat a base class to set hyperparameter and director. dataset.py: create a class to conduct data preprocessing and provide it to model. losses.py: som basic loss function, such as dice loss. model.py: create a class(FCNModel) getting data, config and lossfunction from three above python file. The FCNmodel dosen't implement a concrete fully convolution network, but obtain it when we rewrite config and feed a FCN to it  2. Apply: use BaseCode to accomplish a specific task In this part, we provide Carvana Image Masking Challenge(https://www.kaggle.com/c/carvana-image-masking-challenge) as a sample.  mobilenetv2_unet.py: Optional FCN(https://github.com/JonathanCMitchell/mobilenet_v2_keras) Deeplabv3Plus.py: Optional FCN(https://github.com/bonlime/keras-deeplab-v3-plus) CarvanaConfig.py: Write a class CarvanaConfig extends class config from Config.py to set hyperparameter fitting our task. And import mobilenetv2_unet(or Deeplabv3plus) as our network. CarvanaDataset.py: Write a class CarvanaDataset extends class dataset from dataset.py to generate data for our model. prediciton.py: Run model.predict and get the result. train.py: Run model.train and get the trained network.  Requirements Python 3.5, TensorFlow 1.4.0, Keras 2.1.6 and other common packages listed in my_py_envn.txt. Start the sample  Install required package in Requirements. Download dataset from kaggle and set the director(as Carvanadataset) Copy your FCN.py to Apply/Carvana and import it as network in Carvanaconfig.py.(Optional) Run Apply/Carvana/train.py. Run Apply/Carvana/predict.py.  One of the results:  ", "has_readme": true, "readme_language": "English", "repo_tags": ["keras", "fcn"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47fb"}, "repo_url": "https://github.com/as1067/Connect4AlphaZero", "repo_name": "Connect4AlphaZero", "repo_full_name": "as1067/Connect4AlphaZero", "repo_owner": "as1067", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T19:34:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T18:09:57Z", "homepage": null, "size": 390101, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185849192, "is_fork": false, "readme_text": "Alpha Zero General (any game, any framework!) A simplified, highly flexible, commented and (hopefully) easy to understand implementation of self-play based reinforcement learning based on the AlphaGo Zero paper (Silver et al). It is designed to be easy to adopt for any two-player turn-based adversarial game and any deep learning framework of your choice. A sample implementation has been provided for the game of Othello in PyTorch, Keras, TensorFlow and Chainer. An accompanying tutorial can be found here. We also have implementations for GoBang and TicTacToe. To use a game of your choice, subclass the classes in Game.py and NeuralNet.py and implement their functions. Example implementations for Othello can be found in othello/OthelloGame.py and othello/{pytorch,keras,tensorflow,chainer}/NNet.py. Coach.py contains the core training loop and MCTS.py performs the Monte Carlo Tree Search. The parameters for the self-play can be specified in main.py. Additional neural network parameters are in othello/{pytorch,keras,tensorflow,chainer}/NNet.py (cuda flag, batch size, epochs, learning rate etc.). To start training a model for Othello: python main.py Choose your framework and game in main.py. Docker Installation For easy environment setup, we can use nvidia-docker. Once you have nvidia-docker set up, we can then simply run: ./setup_env.sh  to set up a (default: pyTorch) Jupyter docker container. We can now open a new terminal and enter: docker exec -ti pytorch_notebook python main.py  Experiments We trained a PyTorch model for 6x6 Othello (~80 iterations, 100 episodes per iteration and 25 MCTS simulations per turn). This took about 3 days on an NVIDIA Tesla K80. The pretrained model (PyTorch) can be found in pretrained_models/othello/pytorch/. You can play a game against it using pit.py. Below is the performance of the model against a random and a greedy baseline with the number of iterations.  A concise description of our algorithm can be found here. Contributing While the current code is fairly functional, we could benefit from the following contributions:  Game logic files for more games that follow the specifications in Game.py, along with their neural networks Neural networks in other frameworks Pre-trained models for different game configurations An asynchronous version of the code- parallel processes for self-play, neural net training and model comparison. Asynchronous MCTS as described in the paper  Contributors and Credits  Shantanu Thakoor and Megha Jhunjhunwala helped with core design and implementation. Shantanu Kumar contributed TensorFlow and Keras models for Othello. Evgeny Tyurin contributed rules and a trained model for TicTacToe. MBoss contributed rules and a model for GoBang. Jernej Habjan contributed RTS game.  Thanks to pytorch-classification and progress. Connect4AlphaZero ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://web.stanford.edu/~surag/posts/alphazero.html"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47fc"}, "repo_url": "https://github.com/linzhongkai/yolov3", "repo_name": "yolov3", "repo_full_name": "linzhongkai/yolov3", "repo_owner": "linzhongkai", "repo_desc": "run without erro", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T15:07:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T14:59:58Z", "homepage": null, "size": 371, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185820931, "is_fork": false, "readme_text": "keras-yolo3  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47fd"}, "repo_url": "https://github.com/meetaime/keras-extensions", "repo_name": "keras-extensions", "repo_full_name": "meetaime/keras-extensions", "repo_owner": "meetaime", "repo_desc": "Extensions for the Keras Deep Learning framework.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T14:45:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T12:54:57Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185798737, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47fe"}, "repo_url": "https://github.com/jppope/dog-cat-image-classification", "repo_name": "dog-cat-image-classification", "repo_full_name": "jppope/dog-cat-image-classification", "repo_owner": "jppope", "repo_desc": "just some keras stuff. cause you know", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T23:58:13Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T23:57:20Z", "homepage": null, "size": 4428, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185887893, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": ["keras", "keras-tensorflow", "python3"], "has_h5": true, "h5_files_links": ["https://github.com/jppope/dog-cat-image-classification/blob/f1ad05179c041f2515afcd29093a143428d2ef0a/first_try.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc47ff"}, "repo_url": "https://github.com/Develop-It-19/Emojifier-V2", "repo_name": "Emojifier-V2", "repo_full_name": "Develop-It-19/Emojifier-V2", "repo_owner": "Develop-It-19", "repo_desc": "Using Word Embeddings to build Emojifier V2 with an LSTM Model in Keras.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T16:42:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T13:41:32Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185806731, "is_fork": false, "readme_text": "Emojifier-V2 Using Word Embeddings to build Emojifier V2 with an LSTM Model in Keras. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4800"}, "repo_url": "https://github.com/Harphies1/RNN", "repo_name": "RNN", "repo_full_name": "Harphies1/RNN", "repo_owner": "Harphies1", "repo_desc": "Recurrent Neural Network with LSTM cell using Keras and tensorflow as Backend", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T12:08:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T12:03:46Z", "homepage": null, "size": 1, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185790984, "is_fork": false, "readme_text": "RNN Recurrent Neural Network with LSTM cell using Keras and tensorflow as Backend ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4801"}, "repo_url": "https://github.com/loadme-learnme/loadme-learnme", "repo_name": "loadme-learnme", "repo_full_name": "loadme-learnme/loadme-learnme", "repo_owner": "loadme-learnme", "repo_desc": "An agnostic data loading pipeline mechanism for machine learning problems, fitting Tensorflow, Keras, Pytorch", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-10T07:40:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T07:39:31Z", "homepage": "", "size": 12, "language": "Python", "has_wiki": true, "license": {"key": "gpl-2.0", "name": "GNU General Public License v2.0", "spdx_id": "GPL-2.0", "url": "https://api.github.com/licenses/gpl-2.0", "node_id": "MDc6TGljZW5zZTg="}, "open_issues_count": 0, "github_id": 185750384, "is_fork": false, "readme_text": "loadme-learnme An agnostic data loading pipeline mechanism for machine learning problem, fitting Tensorflow, Keras, Pytorch ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4802"}, "repo_url": "https://github.com/bestyuan/Time-Series-Forecasting-With-LSTMs", "repo_name": "Time-Series-Forecasting-With-LSTMs", "repo_full_name": "bestyuan/Time-Series-Forecasting-With-LSTMs", "repo_owner": "bestyuan", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T11:01:25Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T10:50:31Z", "homepage": null, "size": 922, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185780863, "is_fork": false, "readme_text": "Time-Series-Forecasting-With-LSTMs Original address\uff1ahttps://machinelearningmastery.com/multivariate-time-series-forecasting-lstms-keras/ ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4803"}, "repo_url": "https://github.com/shoreason/chatme", "repo_name": "chatme", "repo_full_name": "shoreason/chatme", "repo_owner": "shoreason", "repo_desc": "Basic Implementation of a ChatBot using NLTK and Keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T15:15:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T21:07:33Z", "homepage": null, "size": 177, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185871849, "is_fork": false, "readme_text": "chatme Basic Implementation of a Conversational AI using NLTK and Keras to recognize intent by classifying user input ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4804"}, "repo_url": "https://github.com/avyayv/usg_pct_exp", "repo_name": "usg_pct_exp", "repo_full_name": "avyayv/usg_pct_exp", "repo_owner": "avyayv", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-10T04:09:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T04:58:11Z", "homepage": null, "size": 19081, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185728965, "is_fork": false, "readme_text": "usg_pct_exp Here is the code for the analysis on www.analyzeball.com. The neural network (pre-trained, keras and tensorflow), can be found in the model.h5 file. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/avyayv/usg_pct_exp/blob/3e49fd7f1b1341675e4fe62b3a636b73410f8258/model.h5"], "see_also_links": ["http://www.analyzeball.com"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4805"}, "repo_url": "https://github.com/jaynilsonavane/Object-detection-coco", "repo_name": "Object-detection-coco", "repo_full_name": "jaynilsonavane/Object-detection-coco", "repo_owner": "jaynilsonavane", "repo_desc": "Object Detection using common object in context model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T08:22:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T07:55:51Z", "homepage": null, "size": 10, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185752959, "is_fork": false, "readme_text": "Object-detection-coco Object Detection using common object in context model Dependencies To use ImageAI in your application developments, you must have installed the following dependencies before you install ImageAI :   Python 3.5.1 (and later versions) Download (Support for Python 2.7 coming soon)   pip3 Install   Tensorflow 1.4.0 (and later versions) Install or install via pip pip3 install --upgrade tensorflow     Numpy 1.13.1 (and later versions) Install or install via pip pip3 install numpy     SciPy 0.19.1 (and later versions) Install or install via pip pip3 install scipy     OpenCV Install or install via pip pip3 install opencv-python     Pillow Install or install via pip pip3 install pillow     Matplotlib Install or install via pip pip3 install matplotlib     h5py Install or install via pip  pip3 install h5py     Keras 2.x Install or install via pip pip3 install keras    Installation To install ImageAI, run the python installation instruction below in the command line: pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl   or download the Python Wheel imageai-2.0.2-py3-none-any.whl and run the python installation instruction in the command line to the path of the file like the one below: pip3 install C:\\User\\MyUser\\Downloads\\imageai-2.0.2-py3-none-any.whl   Pre-trained coco model -- RetinaNet Pre-trained YOLOV3 Model --YOLOv3 References- https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md#customvideodetection https://pjreddie.com/darknet/yolo/ ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4806"}, "repo_url": "https://github.com/f-kolodziejczak/MNIST_feed_forward", "repo_name": "MNIST_feed_forward", "repo_full_name": "f-kolodziejczak/MNIST_feed_forward", "repo_owner": "f-kolodziejczak", "repo_desc": "Feed forward neural network on keras MNIST dataset to recognise handwritten digits.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T09:52:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T09:47:27Z", "homepage": null, "size": 2, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185771787, "is_fork": false, "readme_text": "MNIST_feed_forward Feed forward neural network on keras MNIST dataset to recognise handwritten digits. If you are having problems with maplotlib and you are using OSX: https://stackoverflow.com/questions/21784641/installation-issue-with-matplotlib-python ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4807"}, "repo_url": "https://github.com/Shah-Jainam/Object-Detection", "repo_name": "Object-Detection", "repo_full_name": "Shah-Jainam/Object-Detection", "repo_owner": "Shah-Jainam", "repo_desc": "Object Detection Using COCO and YOLOv3 Model", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T08:28:41Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T08:26:32Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185758004, "is_fork": false, "readme_text": "Object-Detection Object Detection Using COCO and YOLOv3 Model Dependencies To use ImageAI in your application developments, you must have installed the following dependencies before you install ImageAI :   Python 3.5.1 (and later versions) Download (Support for Python 2.7 coming soon)   pip3 Install   Tensorflow 1.4.0 (and later versions) Install or install via pip pip3 install --upgrade tensorflow     Numpy 1.13.1 (and later versions) Install or install via pip pip3 install numpy     SciPy 0.19.1 (and later versions) Install or install via pip pip3 install scipy     OpenCV Install or install via pip pip3 install opencv-python     Pillow Install or install via pip pip3 install pillow     Matplotlib Install or install via pip pip3 install matplotlib     h5py Install or install via pip  pip3 install h5py     Keras 2.x Install or install via pip pip3 install keras    Installation To install ImageAI, run the python installation instruction below in the command line: pip3 install https://github.com/OlafenwaMoses/ImageAI/releases/download/2.0.2/imageai-2.0.2-py3-none-any.whl   or download the Python Wheel imageai-2.0.2-py3-none-any.whl and run the python installation instruction in the command line to the path of the file like the one below: pip3 install C:\\User\\MyUser\\Downloads\\imageai-2.0.2-py3-none-any.whl   Pre-trained coco model -- RetinaNet Pre-trained YOLOV3 Model --YOLOv3 References- https://github.com/OlafenwaMoses/ImageAI/blob/master/imageai/Detection/VIDEO.md#customvideodetection https://pjreddie.com/darknet/yolo/ ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4808"}, "repo_url": "https://github.com/panxw/MachineLearning", "repo_name": "MachineLearning", "repo_full_name": "panxw/MachineLearning", "repo_owner": "panxw", "repo_desc": "Machine Learning  notes.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-03T07:11:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T02:35:51Z", "homepage": "", "size": 11710, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185712818, "is_fork": false, "readme_text": "Machine Learning  \u6df1\u5ea6\u5b66\u4e60\u5de5\u7a0b\u5e08(\u5fae\u4e13\u4e1a)  - https://mooc.study.163.com/smartSpec/detail/1001319001.htm \u4eba\u5de5\u667a\u80fd\u5b9e\u8df5\uff1aTensorflow\u7b14\u8bb0 - https://www.icourse163.org/learn/PKU-1002536002 Tensorflow Helloworld - https://www.tensorflow.org/alpha/tutorials/keras/basic_classification Tensorflow tutorials - https://www.tensorflow.org/js/tutorials ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4809"}, "repo_url": "https://github.com/cetmann/robustness-interpretability", "repo_name": "robustness-interpretability", "repo_full_name": "cetmann/robustness-interpretability", "repo_owner": "cetmann", "repo_desc": "Code for the Paper 'On the Connection Between Adversarial Robustness and Saliency Map Interpretability' by C. Etmann, S. Lunz, P. Maass, C.-B. Sch\u00f6nlieb", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T16:10:35Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T15:33:19Z", "homepage": null, "size": 38, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185826291, "is_fork": false, "readme_text": "Description Code for the Paper 'On the Connection Between Adversarial Robustness and Saliency Map Interpretability' by C. Etmann, S. Lunz, P. Maass, C.-B. Sch\u00f6nlieb, accepted at ICML 2019. More in-depth documentation to follow. Execution It can be run by executing e.g. python3 adv_reg_experiment.py imagenet_example.ini  This code was tested using the following libraries in Python 3.6: Tensorflow 1.11 Foolbox 1.8.0 Keras 2.2.2 Conda yml file to come. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc480a"}, "repo_url": "https://github.com/afonso12325/videoSlicer", "repo_name": "videoSlicer", "repo_full_name": "afonso12325/videoSlicer", "repo_owner": "afonso12325", "repo_desc": "Simple implementation of video editing in python using some CV and ML techniques to demonstrate what I can do with these tools.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T20:49:13Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T13:54:45Z", "homepage": "", "size": 122818, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185809039, "is_fork": false, "readme_text": "videoSlicer Simple implementation of video editing in python using some CV and ML techniques to demonstrate what I can do with these tools. videoSlicer uses OpenCV, ffmpeg to make what it does. Special thanks to the authors of keras-yolo3 STEPS:  Get video information Get audio from video with ffmpeg Sectorize image  Apply basic transformations to sectors  Apply YOLO (you only look once) object detection  Get YOLO on video  Join sectors  Join audio with ffmpeg  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc480b"}, "repo_url": "https://github.com/alvarorobledo/foolbox", "repo_name": "foolbox", "repo_full_name": "alvarorobledo/foolbox", "repo_owner": "alvarorobledo", "repo_desc": "My custom FYP foolbox implementation", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-30T15:36:57Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T17:27:09Z", "homepage": null, "size": 2069, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185843412, "is_fork": false, "readme_text": "       Foolbox Foolbox is a Python toolbox to create adversarial examples that fool neural networks. It requires Python, NumPy and SciPy.  Installation pip install foolbox We test using Python 2.7, 3.5 and 3.6. Other Python versions might work as well. We recommend using Python 3!  Documentation Documentation is available on readthedocs: http://foolbox.readthedocs.io/ Our paper describing Foolbox is on arXiv: https://arxiv.org/abs/1707.04131  Example import foolbox import keras import numpy as np from keras.applications.resnet50 import ResNet50  # instantiate model keras.backend.set_learning_phase(0) kmodel = ResNet50(weights='imagenet') preprocessing = (np.array([104, 116, 123]), 1) fmodel = foolbox.models.KerasModel(kmodel, bounds=(0, 255), preprocessing=preprocessing)  # get source image and label image, label = foolbox.utils.imagenet_example()  # apply attack on source image # ::-1 reverses the color channels, because Keras ResNet50 expects BGR instead of RGB attack = foolbox.attacks.FGSM(fmodel) adversarial = attack(image[:, :, ::-1], label) # if the attack fails, adversarial will be None and a warning will be printed For more examples, have a look at the documentation. Finally, the result can be plotted like this: # if you use Jupyter notebooks %matplotlib inline  import matplotlib.pyplot as plt  plt.figure()  plt.subplot(1, 3, 1) plt.title('Original') plt.imshow(image / 255)  # division by 255 to convert [0, 255] to [0, 1] plt.axis('off')  plt.subplot(1, 3, 2) plt.title('Adversarial') plt.imshow(adversarial[:, :, ::-1] / 255)  # ::-1 to convert BGR to RGB plt.axis('off')  plt.subplot(1, 3, 3) plt.title('Difference') difference = adversarial[:, :, ::-1] - image plt.imshow(difference / abs(difference).max() * 0.2 + 0.5) plt.axis('off')  plt.show()  Interfaces for a range of other deeplearning packages such as TensorFlow, PyTorch, Theano, Lasagne and MXNet are available, e.g. model = foolbox.models.TensorFlowModel(images, logits, bounds=(0, 255)) model = foolbox.models.PyTorchModel(torchmodel, bounds=(0, 255), num_classes=1000) # etc. Different adversarial criteria such as Top-k, specific target classes or target probability values for the original class or the target class can be passed to the attack, e.g. criterion = foolbox.criteria.TargetClass(22) attack    = foolbox.attacks.LBFGSAttack(fmodel, criterion)  Feature requests and bug reports We welcome feature requests and bug reports. Just create a new issue on GitHub.  Questions & FAQ Depending on the nature of your question feel free to post it as an issue on GitHub, or post it as a question on Stack Overflow using the foolbox tag. We will try to monitor that tag but if you don't get an answer don't hesitate to contact us. Before you post a question, please check our FAQ and our Documentation on ReadTheDocs.  Contributions welcome Foolbox is a work in progress and any input is welcome. In particular, we encourage users of deep learning frameworks for which we do not yet have builtin support, e.g. Caffe, Caffe2 or CNTK, to contribute the necessary wrappers. Don't hestiate to contact us if we can be of any help. Moreoever, attack developers are encouraged to share their reference implementation using Foolbox so that it will be available to everyone.  Citation If you find Foolbox useful for your scientific work, please consider citing it in resulting publications: @article{rauber2017foolbox,   title={Foolbox: A Python toolbox to benchmark the robustness of machine learning models},   author={Rauber, Jonas and Brendel, Wieland and Bethge, Matthias},   journal={arXiv preprint arXiv:1707.04131},   year={2017},   url={http://arxiv.org/abs/1707.04131},   archivePrefix={arXiv},   eprint={1707.04131}, }  You can find the paper on arXiv: https://arxiv.org/abs/1707.04131  Authors  Jonas Rauber Wieland Brendel  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://foolbox.readthedocs.io/"], "reference_list": ["https://arxiv.org/abs/1707.04131", "https://arxiv.org/abs/1707.04131"]}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc480c"}, "repo_url": "https://github.com/GuQiangJS/stock_ai", "repo_name": "stock_ai", "repo_full_name": "GuQiangJS/stock_ai", "repo_owner": "GuQiangJS", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T07:58:53Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T13:29:23Z", "homepage": "https://guqiangjs.github.io/stock_ai/", "size": 6094, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185804483, "is_fork": false, "readme_text": "stock_ai    \u73af\u5883\u76f8\u5173 \u914d\u7f6e conda \u4f7f\u7528 proxy\u3002  conda config --set proxy_servers.http http://192.168.9.180:1080 conda config --set proxy_servers.https https://192.168.9.180:1080   conda create -n stock_ai python=3.7 conda activate stock_ai pip install quantaxis \u6216\u8005 pip --proxy=192.168.9.180:1080 install quantaxis pip install --upgrade tensorflow \u6216\u8005 pip install --upgrade tensorflow-gpu pip install keras pip install async_timeout (quantaxis\u9700\u8981\uff0c\u4f46\u662f\u6ca1\u6709\u5305\u542b\u5728\u5b89\u88c5\u8bf7\u6c42\u4e2d...) pip install pytest pip install coverage  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc480d"}, "repo_url": "https://github.com/hvcl/FusionNet", "repo_name": "FusionNet", "repo_full_name": "hvcl/FusionNet", "repo_owner": "hvcl", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T04:12:16Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-09T04:11:21Z", "homepage": null, "size": 9, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185724675, "is_fork": false, "readme_text": "FusionNet Please install tensorflow, keras, skimage, sklearn, opencv and related libraries using (sudo) pip install if you have the errors related with essential libraries. Run mkdir models to save the checkpoint of model's weights  Data.py read images and membrane labels from data folder Train.py train the model (define in Model.py) with data augmentation (Augment.py) Deploy_full.py predict the result (currently I predicted on the training set, you may set up Utility.py includes all the needed libraries (sorry for my lazy mode).  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc480e"}, "repo_url": "https://github.com/JohnOmena/PDI-UFAL", "repo_name": "PDI-UFAL", "repo_full_name": "JohnOmena/PDI-UFAL", "repo_owner": "JohnOmena", "repo_desc": "Repository of the digital image processing class", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-16T18:05:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T01:15:52Z", "homepage": null, "size": 123, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185702750, "is_fork": false, "readme_text": "PDI-UFAL Repository of the digital image processing class T01 - Tools / Project theme 1. Install tools   Anaconda w. Python 3 (optional virtual environment manager).  Spyder (pycharm).  Jupyter-notebook.  opencv, tensorflow, keras, matplotlib, scikit-learn, scikit-image, pillow, pandas, torch.  2. Run a test script to check if the environment is working. 3. Post a screenshot with the results.   4. Post the theme of the project you have chosen.   Image Captioning  5. Post your information from github.   https://github.com/JohnOmena/PDI-UFAL  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc480f"}, "repo_url": "https://github.com/tmquan/FusionNet", "repo_name": "FusionNet", "repo_full_name": "tmquan/FusionNet", "repo_owner": "tmquan", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T04:10:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T03:43:12Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185721634, "is_fork": false, "readme_text": "FusionNet Please install tensorflow, keras, skimage, sklearn, opencv and related libraries using (sudo) pip install if you have the errors related with essential libraries. Run mkdir models to save the checkpoint of model's weights  Data.py read images and membrane labels from data folder Train.py train the model (define in Model.py) with data augmentation (Augment.py) Deploy_full.py predict the result (currently I predicted on the training set, you may set up Utility.py includes all the needed libraries (sorry for my lazy mode).  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4810"}, "repo_url": "https://github.com/DerekRoy/GestureDetection", "repo_name": "GestureDetection", "repo_full_name": "DerekRoy/GestureDetection", "repo_owner": "DerekRoy", "repo_desc": "Detect 6 different hand gestures shown on a live webcam image. ", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-12T16:06:27Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T14:07:15Z", "homepage": null, "size": 47741, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185811418, "is_fork": false, "readme_text": "GestureDetection  https://youtu.be/RCMpGXbhoY8 Detect 5 different hand gestures shown on a live webcam. \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0      Peace Left Right Forks Stop  Required modules You will need to pip install several python modules to run this code:  pip install numpy pip install tensorflow pip install keras pip install opencv-python pip install imutils  If there are any issues installing these modules please use sudo pip install instead. Notes The hand detection aspect of this code was taken from Victor Dibia, here is the Github Repository if you are interested. While the gesture detector works in some regards it is not adequate enough as is. The model I used was trained on 28,000 images of my hands making these gestures. The pictures were taken in different environments, and with different hand angles and positions. If you would like to work more with this project here is the data. If you can not access the data feel free to contact me. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4811"}, "repo_url": "https://github.com/wdzyh/SegNet_keras", "repo_name": "SegNet_keras", "repo_full_name": "wdzyh/SegNet_keras", "repo_owner": "wdzyh", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-09T00:36:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T00:35:08Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185698596, "is_fork": false, "readme_text": "SegNet_keras ", "has_readme": true, "readme_language": "Malay (macrolanguage)", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4812"}, "repo_url": "https://github.com/isi-vista/Unified-Adversarial-Invariance", "repo_name": "Unified-Adversarial-Invariance", "repo_full_name": "isi-vista/Unified-Adversarial-Invariance", "repo_owner": "isi-vista", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-17T16:50:02Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T21:21:26Z", "homepage": null, "size": 14, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185873409, "is_fork": false, "readme_text": "Unified-Adversarial-Invariance This repository provides access to code for the papers:   \u00a0   \u00a0   A. Jaiswal, Y. Wu, W. AbdAlmageed, and P. Natarajan, \"Unsupervised Adversarial Invariance\" (NeurIPS, 2018)   \u00a0   \u00a0  A. Jaiswal, Y. Wu, W. AbdAlmageed, and P. Natarajan, \"Unified Adversarial Invariance\" (arXiv, 2019).   The first paper presents an approach for invariance to nuisance factors of data through learning a split representation of data and the second extends the approach to additionally induce invariance to biasing factors of data. Dependencies The code is written in Python 2.7 and has the following dependencies.    Package Version Source     joblib 0.11 pip   NumPy 1.14.0 pip   SciPy 1.0.0 pip   TensorFlow 1.8.0 pip   Keras 2.1.2 pip   keras-adversarial --- GitHub    The code has been tested with the versions of these dependencies as specified above. Usage   Install dependencies listed above   Clone this repository   Update the PYTHONPATH environment variable to include this repository. For Linux/MacOS users, the following can be added to the ~/.bashrc file: export PYTHONPATH=/path/to/Unified-Adversarial-Invariance:$PYTHONPATH   Training examples Nuisance: python train.py mnist_rot mnist_rot_model \\                 /path/to/weights/root/ \\                 --bias 0 --streaming-data 1 \\                 --predictor-loss-weight 100 \\                 --decoder-loss-weight 0.1 \\                 --disentangler-loss-weight 1 \\                 --epochs 10000 Bias: python train.py german german_model_1 \\                 /path/to/weights/root/ \\                 --bias 1 --fold-id 1 --streaming-data 0 \\                 --predictor-loss-weight 100 \\                 --decoder-loss-weight 0.1 \\                 --disentangler-loss-weight 1 \\                 --z-discriminator-loss-weight 1 \\                 --epochs 10000 For more details and options: python train.py -h   Prediction examples python predict.py mnist_rot mnist_rot_model /path/to/weights/root/ 9999 \\                   test test_mnist_rot.npy --streaming-data 1 python predict.py german german_model /path/to/weights/root/ 9999 \\                   test test_german_1.npy --streaming-data 0 --fold-id 1 For more details and options: python predict.py -h   Citation Please cite both of our following papers with the BibTeX: @incollection{jaiswal2018uai,     title = {{Unsupervised Adversarial Invariance}},     author = {Jaiswal, Ayush and Wu, Rex Yue and Abd-Almageed, Wael and Natarajan, Prem},     booktitle = {Advances in Neural Information Processing Systems 31},     editor = {S. Bengio and H. Wallach and H. Larochelle and K. Grauman and N. Cesa-Bianchi and R. Garnett},     pages = {5097--5107},     year = {2018},     publisher = {Curran Associates, Inc.} }   @article{jaiswal2019unifai,     title = {{Unified Adversarial Invariance}},     author = {Jaiswal, Ayush and Wu, Yue and AbdAlmageed, Wael and Natarajan, Premkumar},     journal = {arXiv preprint arXiv:1905.03629},     year = {2019} } Disclaimer The code provided in this page is provided \"as is\", without any guarantee made as to its suitability or fitness for any particular use. It may contain bugs, so use of this tool is at your own risk. We take no responsibility for any damage of any sort that may unintentionally be caused through its use. Comments The code has been refactored for release purposes. Please contact us if something does not work or looks problematic. Contact If you have any questions, drop an email to ajaiswal@isi.edu. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4813"}, "repo_url": "https://github.com/dbh08/SVHNDeepLearning", "repo_name": "SVHNDeepLearning", "repo_full_name": "dbh08/SVHNDeepLearning", "repo_owner": "dbh08", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-17T14:20:48Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T13:25:46Z", "homepage": null, "size": 33, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185803894, "is_fork": false, "readme_text": "SVHNDeepLearning to Recognise Multiple Digits in Real-World Images with Convolutional Neural Networks This project explores how Convolutional Neural Networks can be used to identify series of digits in natural images taken from The Street View House Numbers (SVHN) dataset. All the code in this project has been written using Python 2.7 and the Convolutional Neural Networks have been implemented using TensorFlow. External links : SVHN Database : Cropped digits  Training Dataset http://ufldl.stanford.edu/housenumbers/train_32x32.mat Testing Dataset  http://ufldl.stanford.edu/housenumbers/test_32x32.mat  Download the two .mat files and put them in the \"SVHN_Dataset\" directory. But What is SVHN Dataset ? SVHN is a real-world image dataset for developing machine learning and object recognition algorithms with minimal requirement on data preprocessing and formatting. It can be seen as similar in flavor to MNIST (e.g., the images are of small cropped digits), but incorporates an order of magnitude more labeled data (over 600,000 digit images) and comes from a significantly harder, unsolved, real world problem (recognizing digits and numbers in natural scene images). SVHN is obtained from house numbers in Google Street View images. Project Files Environment Python 3.7 I run the code on Macbook Pro 2017, but i dont recommend it, for deep learning use powerfull PCs with good GPU is the best.  Numpy 1.16.3 Keras 2.1.6 Pandas 0.20.3 TensorFlow 1.12.0  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://ufldl.stanford.edu/housenumbers/test_32x32.mat", "http://ufldl.stanford.edu/housenumbers/train_32x32.mat"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4814"}, "repo_url": "https://github.com/NaturezzZ/Inpainting", "repo_name": "Inpainting", "repo_full_name": "NaturezzZ/Inpainting", "repo_owner": "NaturezzZ", "repo_desc": "Implement of 2018 NVIDIA Inpainting work", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T02:40:43Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T11:08:46Z", "homepage": "", "size": 121351, "language": "Python", "has_wiki": true, "license": {"key": "gpl-3.0", "name": "GNU General Public License v3.0", "spdx_id": "GPL-3.0", "url": "https://api.github.com/licenses/gpl-3.0", "node_id": "MDc6TGljZW5zZTk="}, "open_issues_count": 0, "github_id": 185783336, "is_fork": false, "readme_text": "Overview  This is a open source project based on Guilin Liu et al., M.: Image Inpainting for Irregular Holes Using Partial Convolutions. arXiv preprint arXiv: 1804.07723v2(2018). As the project of introduction to AI, a PKU course, it is a simplified version of the NVIDIA work because of our lack of computing resource but it still makes good performance in most situations. This project is trained on a part of Places365 dataset and you can use codes in prepare folder to prepare for your training dataset. You need to prepare your training dataset at first, and then you should change the dirs in the picmaker.py(in the folder train) and start training. It requires keras based on tensorflow and tables. You can install them by pypi. There is also a user interface(Simplified Chinese) based on C++ in the folder show and you should put the .pkl file in the corresponding folder.  Authors  Naiqian Zheng zhengnaiqian@pku.edu.cn Junwei Yang Dianze Li You can contact us for any questions.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4815"}, "repo_url": "https://github.com/arinjayakhare1/Real-Time-Tweet-Classifier-using-RLAN", "repo_name": "Real-Time-Tweet-Classifier-using-RLAN", "repo_full_name": "arinjayakhare1/Real-Time-Tweet-Classifier-using-RLAN", "repo_owner": "arinjayakhare1", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-30T17:41:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T05:13:19Z", "homepage": null, "size": 3419, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185730466, "is_fork": false, "readme_text": "CDACProjectUsingRLAN Prerequisite to run the program -   Install mongo db on your system.   Install pymongo, nltk, tensorflow, keras, tweepy, pandas, numpy for your python using pip or conda. Open a python terminal and type the following commands- import nltk nltk.download('all')   Create a db called CDAC    Create 3 collections.   The 3 collections are -    i)tweetIdScore - keep classification scores of classified ids   ii)tweetIdText - store tweet id and text   iii)tweetIdStatus - keep Status of tweets    Columns  in collections are -    i)tweetIdText - tweet_id, tweet_text, created_at, geo, entities, extended_entities, retweeted_status   ii)tweetIdStatus - tweet_id, classified, trained, clusterAnalysis, viralTrained   iii)tweetIdScore - tweet_id, violentExtremism, nonViolentExtremism, radicalViolence, nonRadicalViolence, notRelevant    How to run the program -   Open Terminal.   Type sudo service mongodb start.   Type mongo.    Type the following commands -   i)use CDAC;  ii)db.createCollection(\"tweetIdText\");  iii)db.createCollection(\"tweetIdScore\");  iv)db.createCollection(\"tweetIdStatus\");  v)db.createCollection(\"twitterKeys\");  vi)db.tweetIdText.createIndex({\"TTL\":1},{expireAfterSeconds:0});  vii)db.tweetIdScore.createIndex({\"TTL\":1},{expireAfterSeconds:0});  viii)db.tweetIdStatus.createIndex({\"TTL\":1},{expireAfterSeconds:0});  ix)db.twitterKeys.createIndex({\"TTL\":1},{expireAfterSeconds:0});    Exit the mongo shell by typing \"ctrl+d\" .   Go to initTrainer Directory and run initTrainer.py to initiallty train the model. Make sure the models are created in the models folder.   Run initKeys file to initially store the keys   Run main.py   The functions are quite straightforward. In case of any queries, I can be contacted at 9971273053 or arinjayakhare1@gmail.com ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4816"}, "repo_url": "https://github.com/Nick-AI/DeepCWAVE", "repo_name": "DeepCWAVE", "repo_full_name": "Nick-AI/DeepCWAVE", "repo_owner": "Nick-AI", "repo_desc": "Estimating wave heights from 20 orthogonal parameters derived from SAR images", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-18T01:15:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T11:20:33Z", "homepage": "", "size": 5536, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185784909, "is_fork": false, "readme_text": "DeepCWAVE To produce predictions  Call python RunModel.py specifying the input, which can be an absolute or relative path to a netcdf file (passing entire directories here is currently unstable, working on fixing that) Optionally can specify the directory where the output file will be saved with the output flag, otherwise it will just save in the current working directory Don't use the weights argument yet, there's currently only one model anyway Call format: python RunModel.py [-h] [--outdir OUTDIR] [--weights WEIGHTS] input Calling python RunModel.py -h will give you more information about the arguments Sample call: python RunModel.py /path/to/data/S1A_ALT_coloc201701S.nc --outdir /path/to/destination/  This will read /path/to/data/S1A_ALT_coloc201701S.nc and produce an output file at /path/to/destination/S1A_ALT_coloc201701S_preds.csv    Dependencies  Python 3.6.8 'numpy': '1.16.2', 'sklearn': '0.20.3', 'pandas': '0.23.4', 'keras': '2.2.4', 'tensorflow': '1.11.0', 'netCDF4': '1.4.2'  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Nick-AI/DeepCWAVE/blob/22190535e144e98bed6e6186a2642e9945f2de75/models/fullModels/hsALT_regressor.h5", "https://github.com/Nick-AI/DeepCWAVE/blob/22190535e144e98bed6e6186a2642e9945f2de75/models/fullModels/hsWW3v2_regressor.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4817"}, "repo_url": "https://github.com/NlGHT/NewVoiceRecognitionModel", "repo_name": "NewVoiceRecognitionModel", "repo_full_name": "NlGHT/NewVoiceRecognitionModel", "repo_owner": "NlGHT", "repo_desc": "Building a voice recognition model using Keras with MFCC of 12", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-17T10:31:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T09:13:47Z", "homepage": null, "size": 1741430, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185766246, "is_fork": false, "readme_text": null, "has_readme": false, "readme_language": null, "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/NlGHT/NewVoiceRecognitionModel/blob/0e81f17e1ce180ea39414461242e4d67247128bf/model.h5", "https://github.com/NlGHT/NewVoiceRecognitionModel/blob/0e81f17e1ce180ea39414461242e4d67247128bf/model_2.h5", "https://github.com/NlGHT/NewVoiceRecognitionModel/blob/0e81f17e1ce180ea39414461242e4d67247128bf/model_3.h5", "https://github.com/NlGHT/NewVoiceRecognitionModel/blob/5263b8f295a4b2c282f99e4f49477f680c44a4c5/model_4.h5", "https://github.com/NlGHT/NewVoiceRecognitionModel/blob/c64b52da621be3f87100890daa89696c6d5e4596/model_xd.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4818"}, "repo_url": "https://github.com/AloshkaD/TrackGym-baseline", "repo_name": "TrackGym-baseline", "repo_full_name": "AloshkaD/TrackGym-baseline", "repo_owner": "AloshkaD", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-12T15:29:46Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T21:32:50Z", "homepage": null, "size": 12039, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185874566, "is_fork": false, "readme_text": "Repository for the TrackGym stable_baseline version. For more details refer to the paper entitled \"Learning to Intercept Drones via Deep Reinforcement Learning.\" The paper does not use stable_baseline, but the method is the same. Tested on Windows and Linux Dependencies: Airsim:https://github.com/Microsoft/AirSim stable_baseline:https://github.com/hill-a/stable-baselines openai_gym:http://gym.openai.com/docs/ Unreal engine (only 4.18 was tested): git clone -b 4.18 https://github.com/EpicGames/UnrealEngine.git For a list of free unreal envs: https://github.com/Microsoft/AirSim/releases Register gym After the dependencies were installed, and this repo is downloaded, the custom gym environment must be regestered like this: head to where gym is installed and find __init__.py. e.g., /home/a/anaconda3/envs/rl/lib/python3.5/site-packages/gym/envs/__init__.py add these lines just below # Algorithmic on top of the page. # Algorithmic # ---------------------------------------- register(     id='TrackSimEnv-v1',     entry_point='gym_trackairsim.envs:TrackSimEnv', )    Add the custom class to stable_baseline Stable_baseline must also be updated with the custom network that's proposed in the paper. Head to where stable_baseline is installed and find policies.py. e.g., /home/a/anaconda3/envs/rl/lib/python3.5/site-packages/stable_baselines/common/policies.py add the libraries below and call the trackgym ''' from keras.models import Sequential,Model from keras.layers import Dense, Activation,Reshape, Flatten, Conv2D, Permute, concatenate,Input, Conv1D from keras.optimizers import Adam import keras.backend as K from TrackSimClient import * from gym_trackairsim.envs.myTrackGymClient import * global trackgym trackgym = myTrackGymClient() ''' Then add the custom network. ''' velocity_length=3 WINDOW_LENGTH = 1 nb_actions = 6 batch_size= 1 array_length=3 def my_cnn(scaled_images, **kwargs): #print('raw input  ',scaled_images) rgbad_array=scaled_images[0,:,:,:] #print('processed input shape',rgbad_array.shape)  #rgb_img,d_img,vel_array=trackgym.state_splitter(rgbad_array,velocity_length, display = False) #d_img=np.expand_dims(d_img, axis=2) #rgb_img=np.concatenate((scaled_images[:,0,0,0], rgb_img),axis=0) rgb_img=scaled_images[:,:,:,:4] d_img=scaled_images[:,:,:,4] d_img=tf.reshape(d_img, (-1,80, 160, 1)) vel_array=scaled_images[:,:,:,5] vel=trackgym.retrive_array(vel_array,array_length,verbos=True)  #vel=tf.reshape(vel, (-1,3)) rgb_img_input = Input(tensor=rgb_img) d_img_input = Input(tensor=d_img) #vel_input = Input(tensor=vel) conv_rgb_1=Conv2D(32,(4, 4), strides=(4, 4), activation='relu',input_shape=rgb_img_input.shape , data_format = \"channels_last\")(rgb_img_input) conv_rgb_2=Conv2D(64,(3, 3), strides=(2, 2), activation='relu')(conv_rgb_1)  conv_d_1=Conv2D(32,(4, 4), strides=(4, 4), activation='relu',input_shape=d_img_input.shape , data_format = \"channels_last\")(d_img_input) conv_d_2=Conv2D(64,(3, 3), strides=(2, 2), activation='relu')(conv_d_1)   flat_rgb=Flatten()(conv_rgb_2) flat_d=Flatten()(conv_d_2) #flat_vel=tf.reshape((vel_input),(-1,3))   merge = concatenate([flat_d, flat_rgb])  hidden1 = Dense(256, activation='relu')(merge) hidden2 = Dense(256, activation='relu')(hidden1)  predictions = Dense(nb_actions, kernel_initializer='zeros', activation='linear')(hidden2)  return predictions   ''' Then add the custom class class MyCnnPolicy(FeedForwardPolicy):     \"\"\"     Policy object that implements actor critic, using a CNN (the nature CNN)      :param sess: (TensorFlow session) The current TensorFlow session     :param ob_space: (Gym Space) The observation space of the environment     :param ac_space: (Gym Space) The action space of the environment     :param n_env: (int) The number of environments to run     :param n_steps: (int) The number of steps to run for each environment     :param n_batch: (int) The number of batch to run (n_envs * n_steps)     :param reuse: (bool) If the policy is reusable or not     :param _kwargs: (dict) Extra keyword arguments for the nature CNN feature extraction     \"\"\"      def __init__(self, sess, ob_space, ac_space, n_env, n_steps, n_batch, reuse=False, **_kwargs):         super(MyCnnPolicy, self).__init__( sess,ob_space, ac_space, n_env, n_steps, n_batch, reuse,                                         cnn_extractor=my_cnn, feature_extraction=\"cnn\", **_kwargs)  Finally, register the class. The _policy_registry should look like this. _policy_registry = {     ActorCriticPolicy: {         \"CnnPolicy\": CnnPolicy,         \"CnnLstmPolicy\": CnnLstmPolicy,         \"CnnLnLstmPolicy\": CnnLnLstmPolicy,         \"MlpPolicy\": MlpPolicy,         \"MlpLstmPolicy\": MlpLstmPolicy,         \"MlpLnLstmPolicy\": MlpLnLstmPolicy,         \"MyCnnPolicy\": MyCnnPolicy,     } }  Train the network: ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://gym.openai.com/docs/"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc4819"}, "repo_url": "https://github.com/nathanReitinger/fingerprinters", "repo_name": "fingerprinters", "repo_full_name": "nathanReitinger/fingerprinters", "repo_owner": "nathanReitinger", "repo_desc": "ML-based canvas fingerprinting blocker", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T20:47:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T19:24:00Z", "homepage": "", "size": 36122, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185858868, "is_fork": false, "readme_text": "ML-CB This is a repository for the code accompanying the paper Semantic Privacy! abstract With the aim of increasing online privacy, we present a novel, machine-learning (ML) based approach to blocking one of the three main ways website visitors are surreptitiously tracked: canvas fingerprinting. Because the act of canvas fingerprinting uses, at its core, a javascript program, and many of these javascript programs are reused across the web, we are able to fit an ML model around a semantic understanding of a program's source code (via jsNice) and acheive a highly accurate classifier. Several models are presented (e.g., support vector machine, bag of words, gloVe embedding) and analyzed, most achieving accuracies around ~98%. A tool like this may be easily integrated into a browser, giving way to more accurate \u201cad-blocking\u201d and a vast improvement over current approaches which suffer from high false positive or false negative rates.  The following models are not optimized. This work was conducted for COMS E6998 (Spring 2019) final project. See http://www.cs.columbia.edu/~suman/ml_pa_fm.html  ml-cb The ml-cb.py file sets up several keras models (SVM, BOW, gloVe embedding) to train on pre-labeled text-based programs, either plaintext or jsNiceified. example python3 ml-cb.py cnn The cnn.py file sets up a typical cnn to train on a set of pre-labeled data, in either plaintext or jsNiceified. example python3 cnn.py data Download and unzip the data folder from OSF: https://osf.io/q9r2t/download . |-- README.md |-- cnn.py |-- data                <== put in root |   |-- CNN |   |   `-- original |   `-- TEXT |       |-- glove.6B.100d.txt |       |-- jsnice_examples.csv |       `-- plain_examples.csv |-- ml-cb.py `-- nr2645_semanticPrivacy.pdf  The data folder should contain two datasets:  CNN/original/data  This contains images in /original as true and flase (positive and negative) examples (.png) /split as true and false split into test and train sets (.png)  TEXT/  This contains a csv of either jsnice_examples.csv as programs filtered through jsNice plain_examples.csv as plaintext programs  The text and images from the datasets were taken from a non-public dataset. Here is the description for how the dataset was obtained: Selenium  was  used  to  crawl  roughly  half-a-million  websites (484463)  ranked  by  Alexa  Top  Sites.  A  Chromebrowser  was  driven,  in  parallel,  using  24  browsers,  to  each URL.  The  browsers  used  a  custom  extension  which  preloaded  the  webpage\u2019s  javascript and  added  a  hook  to  the  toDataURL()  function---a lynchin function when using canvas-based fingerprinting.  This  allowed  each  canvas  drawing  from  the  initiating  domain  to be downloaded in base64 encoded string  form.  The  strings  were  then  inserted  into  a  SQL database.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.cs.columbia.edu/~suman/ml_pa_fm.html"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc481a"}, "repo_url": "https://github.com/sfb833-a3/commix", "repo_name": "commix", "repo_full_name": "sfb833-a3/commix", "repo_owner": "sfb833-a3", "repo_desc": "Composition models for words and phrases (in Python/Tensorflow)", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-17T12:23:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T14:59:09Z", "homepage": null, "size": 145, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185820753, "is_fork": false, "readme_text": "commix Code for the paper: Corina Dima, Dani\u00ebl de Kok, Neele Witte, Erhard Hinrichs. 2019. No Word is an Island \u2014 a Transformation Weighting Model for Semantic Composition. Transactions of the Association for Computational Linguistics 2019. Creating phrasal representations, and in particular representing low-frequency phrases like purple car is a task for composition models of distributional semantics. A composition model is a function f that combines the vectors of individual words, e.g. u for black and v for car into a phrase representation p (black car): p = f(u, v) The project contains implementations for a range of existing composition models and the new model proposed in the paper. The models can be used to construct the vectorial representations for different syntactic constructions, for example compounds (apple tree), adjective-noun phrases (black car) or adverb-adjective phrases (very large). The paper gives further details about the different composition models and experiments on different phrase types and languages. Prerequisites The code is written in Python and uses the  TensorFlow library. To use the program the following prerequisites need to be installed.  Python 3.7 TensorFlow 1.13.1 gensim 3.7.3 keras 2.2.4  The code can be ran on a CPU or GPU. Training A composition model can be trained with python training.py embeddings dataset_directory --composition_model --save_path    embeddings the path to the file that contains word embeddings of the format: .bin/.txt /.w2v   data_dir the path to the directory that contains the phrase dataset (train/test/dev splits)   --composition_model composition model to be used. Available models are addition, bilinear, scalar_addition, vector_addition, matrix, fulllex, wmask, trans_weight, trans_weight_transformations, trans_weight_features, trans_weight_matrix   --save_path defines the directory the trained models and predictions are saved to. If not specified, these are saved to ./trained_models   For other available options, see the help: python training.py -h  To evaluate a trained composition model run python evaluation.py embeddings predictions ranks    embeddings the path to the file that contains word embeddings of the format: .bin/.txt /.w2v   predictions the path to the file were the predictions for a dataset are stored   ranks the path to the file were the rank for each instance is saved to   Composition Datasets Several composition datasets were used to evaluate the composition models on English, German and Dutch. The majority of the datasets are available here. Project Name The project name is a portmanteau from the words composition and mix. It is also a (archaic) verb, meaning to mix, to mingle. The project name is pronounced just like the word comics.  License Apache License v2.0 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://hdl.handle.net/11022/0000-0007-D3BF-4"], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc481b"}, "repo_url": "https://github.com/hackstock/faceoff", "repo_name": "faceoff", "repo_full_name": "hackstock/faceoff", "repo_owner": "hackstock", "repo_desc": "A face recognition application powered by VGG-Face", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T21:41:54Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T18:51:14Z", "homepage": null, "size": 140, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185854762, "is_fork": false, "readme_text": "FaceOff is a toy face recognition application built with VGG-Face, OpenCV, and Tensorflow/Keras. Requirements FaceOff requires Python 3.x but not greater than 3.6. This is important because Tensorflow doesn't support Python 3.7 yet. Setting Up The Python Environment Having clonned this repo, it is recommended that you bootstrap a separate Python virtual environment to setup the codebase. To create a virtual environment, you can either use Anaconda or the default venv module that ships with Python. Follow the instructions below to setup using either Anaconda, or venv. Setting Up With Anaconda  Download and install Anaconda from https://www.anaconda.com/ Run \"conda env create -f environment.yml\" Run \"conda activate faceoff\"  Setting Up With Python's Venv Module  CD into the base directory of this repository on your machine Run \"python3 -v venv env\" Run \"source env/bin/activate\" Run \"pip install -r requirements.txt\"  Downloading Pre-Trained VGG-Face Weights File Because this application depends on the VGG-Face model to encode important facial features, you'll need to download the weights file. Click here to download. It's a big file (about 500MB and named vgg_face_weights.h5) so be patient. After downloading, copy the weights file into the root of the codebase on your machine. Running The Application In the root of the codebase, fun the following command:* python3 main.py --weights vgg_face_weights.h5 --distance 0.2 The flag --weights specifies the path to the pre-trained VGG-Face weights file and --distance specifies the similarity threshold used to determine matched faces. The value for --distance MUST ALWAYS be a number between 0 and 1 since the application uses the Cosine Similarity Measure. You can see a video of it in action on my AI Weekend Hacks playlist How Does This Work  OpenCV's HaarsCascade classifier is used to detect faces from the camera feed. Rectangles are drawn where the classifier sees faces Pressing the 'r' key will let your register a new user by entering his/her fullname in the terninal Whiles registering a new user, make sure he/she is the only face seen by the camera VGG-Face is used to encode facial features into 2622-dimensional vectors and store in /faces as [username].npy When multiple faces are identified by the classifier, matching is done by comparing cosine distance of all known faces. When matches are found, their names are shown. Otherwise, \"unknown user\" is shown.  Do You Want To Improve This? You're definitely welcome to submit pull requests for improvements. Below are some of the immediate improvements that can be made:  Use OpenCV's DNN classifer instead of HaarsCascade classifier. Use DLib to detect faces from varied angles instead of HaarsCascade which doesn't really work well. Improve performance Improve documentation.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc481c"}, "repo_url": "https://github.com/MasaSip/mc-diippi", "repo_name": "mc-diippi", "repo_full_name": "MasaSip/mc-diippi", "repo_owner": "MasaSip", "repo_desc": "Project work at a deep learning course", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T08:46:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T10:44:16Z", "homepage": null, "size": 29963, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185780059, "is_fork": false, "readme_text": "Rap lyrics generation using LSTM and custom embeddings 1. Problem and data description The problem is to create a freestyle rap generator in Finnish. When completed, the rap bot should be able to continue the rap based on a given first line. The generated sentence should ideally both rhyme and be meaningful. For that, a measure of rhyming words needs to be created as well as how well two words rhyme, and embed that information into words. A language model for text generation can be created both in character and word level. In this project, we are concentrating on utilizing word level n-grams. An n-gram of words is a sequence of n words in order. Some examples of 6-grams are for example \u201cA naughty dog chases a cat\u201d, and \u201ca cat with a long hair\u201d. Fixed length n-grams are commonly used to create language models. In language models, recurrent neural networks (RNN) are commonly used, including its variants GRU and LSTM. Both of these are already studied for plain text generation. However, enhancing them with a heuristic for creating rhymes is not yet implemented. Research questions of our study are: How does the length of an input sequence effect on the ability of RNN to generate rap lyrics? Which RNN structure is most suitable for generating rap lyrics? How does the use of a rhyme heuristic effect on the performance of an RNN model in generating rap lyrics? The data consisted of 18 480 lines (95 048 words) of Finnish rap lyrics. The lyrics were retrieved from genius.com. The artists were chosen based that their lyrics are known to favour rhyming lyrics i.e. their lyrics are also predictable^1 The chosen artists were Heikki Kuula, Ruger Hauer, Stig, Br\u00e4di, Are, Petri Nyg\u00e5rd, Paperi T, Stepa, Tuuttim\u00f6rk\u00f6, Iso H, and Raptori. Section 2. Method The data was preprocessed into sequences of 20 input words corresponding to a single matching output word. The task of the model is to predict the next word according to the previous words given. Our model consists of 20 long short-term memory (LSTM) units. Their structure was also covered in the lectures. Hidden size of 256 was used with a 0.1 dropout between the layers. The model was trained with logarithmic loss and Adam optimizer with batches of 128.  We chose to implement LSTM structure using Keras, because it provides higher abstraction than Pytorch for creating a language model. The structure of the model was inspired by a blog post of Jason Brownlee^2 The whole model consisted of 20 sequental LSTM and mid processing sells. The architecture of the model is presented in depiction below.  A weighted version of the Levenshtein distance, which also known as edit distance, was used as a heuristic of rhyming. Levenshtein distance is a measure of similarity of two words. The distance tells the number of insertions, deletion or substitutions needed to transform the word into another. Thus, the more similar the words are, the smaller the distance is. The distance is 0 only if the words are the same and it can be maximum the amount of letters in a longer word. Levenshtein distance between words v and w can be formal expressed as:  Weighted Levenstein distance denotes that custom weights are used for fine-tuning the edit distance. To mimic rhyming, we used exponentially decaying weigh defined as:  Where i is the index of the letter. Two distance matrixes were computed, normal for measuring the similarity of the meaning of the word and reverse for measuring the rhyming of the word. The reverse indexes mean that the last letter of the word was at index position i=0. The computational implementation of Levenshtein distance was inspired by the Wagner-Fischer algorithm. Next, we processed the whole dataset of n=23990 unique words to create a nxn matrix with elements M(i,j) denoting the Levenshtein distance between words i and j. For faster computation, the part of computing distance matrix was implemented in Scala. The size of the matrix was 4.6GB and was not included in version control. Next, PCA was computed to both distance matrixes in order compress the information. These were saved as embedding-beginning.npy, (23990, 64) and embedding-end.npy (23990, 63) After concatenating the embedding matrixes, we gained an custom embedding layer to be used in our model in between each LSTM cell. Results Since an important goal of the project was to produced rhyming patterns, some kind of a metric for rhyming was needed. We decided to measure rhyming with the Levenshtein distance variant we also used for the embedding. In the training data, consecutive last words of a row had an average distance of 0.9876, while random word pairs in the had an average distance of 1.4579. This was expected, as we had observed that rhyming words tended to be close to one another. For automatically generated text, a lower average distance would suggest a better model. Neither of our models reached as low Levenshtein distances between as the training data. The values were 1.4811 for the custom embedding model and  1.3595 for the random embedding. The results at the lowest validation loss (for both models around 7) were somewhat unexpected, as they are virtually equal to random sampling, the random embedding being even lower. As for the meaningfulness of the lyrics, both of the models produced relatively nonsensical sentences. No overfitted meaningful sentences appeared either. However, the verses were mostly properly formatted in both models. Experiments We tried using fully connected classification (no word embedding, just class labels as inputs and outputs). For a subset of data, that produced overfitted results. For the full dataset, the model was too large to be run in reasonable time. Due to time constraints, we were not able to compare the performance of LSTM and GRU models. Conclusion The modified Levenshtein distance was a decent measure of rhyming. This could be seen both intuitively in the visualizations, as well as in the consecutive last words having a smaller average distance. These distances were retained in the word embedding as well (see data/closest1000.txt for reference). However, this did not help the model build rhyming patterns. This might have been due to eg. a small number of occurences per word, too few examples of any single rhyming pattern, or a number of other reasons. Code and Demos The full source of the project can be found at https://github.com/MasaSip/mc-diippi. Under the demos folder there is a visual demo that shows how the rhyme embedding works. Rhyming words can be seen clustering in the scatter plot. The demo can be run under the name embedding_demo.py.    Example output of the text the models created can be found under the example_output folder in the demos folder. There is one 2000 word sample of each of the models. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518ad7eb8d65f78fc481d"}, "repo_url": "https://github.com/karthik137/tensorflow", "repo_name": "tensorflow", "repo_full_name": "karthik137/tensorflow", "repo_owner": "karthik137", "repo_desc": "Tensor flow programs", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-12T16:49:59Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-09T20:06:00Z", "homepage": null, "size": 16, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185864060, "is_fork": false, "readme_text": "Prerequisites Install jupyter notebook $ python3 -m pip install --upgrade pip  $ python3 -m pip install jupyter   Install tensor flow sudo pip install tensorflow   Execute sample two_number_relation.py? $ python two_number_relation.py  WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/keras/utils/losses_utils.py:170: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Epoch 1/500 2019-05-10 01:32:52.219494: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA 2019-05-10 01:32:52.241978: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2304000000 Hz 2019-05-10 01:32:52.242674: I tensorflow/compiler/xla/service/service.cc:150] XLA service 0x559b46c18dd0 executing computations on platform Host. Devices: 2019-05-10 01:32:52.242705: I tensorflow/compiler/xla/service/service.cc:158]   StreamExecutor device (0): <undefined>, <undefined> 6/6 [==============================] - 0s 15ms/sample - loss: 17.4228 Epoch 2/500 6/6 [==============================] - 0s 87us/sample - loss: 13.9606 Epoch 3/500 6/6 [==============================] - 0s 78us/sample - loss: 11.2315 Epoch 4/500 6/6 [==============================] - 0s 83us/sample - loss: 9.0793 Epoch 5/500 6/6 [==============================] - 0s 84us/sample - loss: 7.3810 Epoch 6/500 6/6 [==============================] - 0s 75us/sample - loss: 6.0400 Epoch 7/500 6/6 [==============================] - 0s 75us/sample - loss: 4.9802 Epoch 8/500 6/6 [==============================] - 0s 76us/sample - loss: 4.1417 Epoch 9/500 6/6 [==============================] - 0s 74us/sample - loss: 3.4774 Epoch 10/500 6/6 [==============================] - 0s 75us/sample - loss: 2.9503 Epoch 11/500 6/6 [==============================] - 0s 75us/sample - loss: 2.5312 Epoch 12/500 6/6 [==============================] - 0s 75us/sample - loss: 2.1971 Epoch 13/500 6/6 [==============================] - 0s 75us/sample - loss: 1.9301 Epoch 14/500 6/6 [==============================] - 0s 77us/sample - loss: 1.7159 Epoch 15/500 6/6 [==============================] - 0s 77us/sample - loss: 1.5433 Epoch 16/500 6/6 [==============================] - 0s 78us/sample - loss: 1.4035 Epoch 17/500 6/6 [==============================] - 0s 77us/sample - loss: 1.2896 Epoch 18/500 6/6 [==============================] - 0s 73us/sample - loss: 1.1963 Epoch 19/500 6/6 [==============================] - 0s 73us/sample - loss: 1.1191 Epoch 20/500 6/6 [==============================] - 0s 74us/sample - loss: 1.0547 Epoch 21/500 6/6 [==============================] - 0s 77us/sample - loss: 1.0004 Epoch 22/500 6/6 [==============================] - 0s 74us/sample - loss: 0.9542 Epoch 23/500 6/6 [==============================] - 0s 163us/sample - loss: 0.9145 Epoch 24/500 6/6 [==============================] - 0s 87us/sample - loss: 0.8798 Epoch 25/500 6/6 [==============================] - 0s 91us/sample - loss: 0.8493 Epoch 26/500 6/6 [==============================] - 0s 101us/sample - loss: 0.8220 Epoch 27/500 6/6 [==============================] - 0s 91us/sample - loss: 0.7974 Epoch 28/500 6/6 [==============================] - 0s 85us/sample - loss: 0.7750 Epoch 29/500 6/6 [==============================] - 0s 79us/sample - loss: 0.7543 Epoch 30/500 6/6 [==============================] - 0s 88us/sample - loss: 0.7350 Epoch 31/500 6/6 [==============================] - 0s 101us/sample - loss: 0.7169 Epoch 32/500 6/6 [==============================] - 0s 90us/sample - loss: 0.6999 Epoch 33/500 6/6 [==============================] - 0s 84us/sample - loss: 0.6837 Epoch 34/500 6/6 [==============================] - 0s 77us/sample - loss: 0.6682 Epoch 35/500 6/6 [==============================] - 0s 77us/sample - loss: 0.6533 Epoch 36/500 6/6 [==============================] - 0s 76us/sample - loss: 0.6390 Epoch 37/500 6/6 [==============================] - 0s 80us/sample - loss: 0.6252 Epoch 38/500 6/6 [==============================] - 0s 85us/sample - loss: 0.6118 Epoch 39/500 6/6 [==============================] - 0s 84us/sample - loss: 0.5988 Epoch 40/500 6/6 [==============================] - 0s 92us/sample - loss: 0.5862 Epoch 41/500 6/6 [==============================] - 0s 105us/sample - loss: 0.5738 Epoch 42/500 6/6 [==============================] - 0s 169us/sample - loss: 0.5618 Epoch 43/500 6/6 [==============================] - 0s 129us/sample - loss: 0.5501 Epoch 44/500 6/6 [==============================] - 0s 88us/sample - loss: 0.5387 Epoch 45/500 6/6 [==============================] - 0s 100us/sample - loss: 0.5275 Epoch 46/500 6/6 [==============================] - 0s 164us/sample - loss: 0.5166 Epoch 47/500 6/6 [==============================] - 0s 90us/sample - loss: 0.5059 Epoch 48/500 6/6 [==============================] - 0s 95us/sample - loss: 0.4955 Epoch 49/500 6/6 [==============================] - 0s 99us/sample - loss: 0.4853 Epoch 50/500 6/6 [==============================] - 0s 93us/sample - loss: 0.4753 Epoch 51/500 6/6 [==============================] - 0s 85us/sample - loss: 0.4655 Epoch 52/500 6/6 [==============================] - 0s 87us/sample - loss: 0.4559 Epoch 53/500 6/6 [==============================] - 0s 112us/sample - loss: 0.4465 Epoch 54/500 6/6 [==============================] - 0s 85us/sample - loss: 0.4374 Epoch 55/500 6/6 [==============================] - 0s 84us/sample - loss: 0.4284 Epoch 56/500 6/6 [==============================] - 0s 82us/sample - loss: 0.4196 Epoch 57/500 6/6 [==============================] - 0s 78us/sample - loss: 0.4109 Epoch 58/500 6/6 [==============================] - 0s 83us/sample - loss: 0.4025 Epoch 59/500 6/6 [==============================] - 0s 106us/sample - loss: 0.3942 Epoch 60/500 6/6 [==============================] - 0s 82us/sample - loss: 0.3861 Epoch 61/500 6/6 [==============================] - 0s 99us/sample - loss: 0.3782 Epoch 62/500 6/6 [==============================] - 0s 173us/sample - loss: 0.3704 Epoch 63/500 6/6 [==============================] - 0s 129us/sample - loss: 0.3628 Epoch 64/500 6/6 [==============================] - 0s 88us/sample - loss: 0.3554 Epoch 65/500 6/6 [==============================] - 0s 81us/sample - loss: 0.3481 Epoch 66/500 6/6 [==============================] - 0s 83us/sample - loss: 0.3409 Epoch 67/500 6/6 [==============================] - 0s 78us/sample - loss: 0.3339 Epoch 68/500 6/6 [==============================] - 0s 84us/sample - loss: 0.3270 Epoch 69/500 6/6 [==============================] - 0s 79us/sample - loss: 0.3203 Epoch 70/500 6/6 [==============================] - 0s 78us/sample - loss: 0.3137 Epoch 71/500 6/6 [==============================] - 0s 76us/sample - loss: 0.3073 Epoch 72/500 6/6 [==============================] - 0s 81us/sample - loss: 0.3010 Epoch 73/500 6/6 [==============================] - 0s 82us/sample - loss: 0.2948 Epoch 74/500 6/6 [==============================] - 0s 80us/sample - loss: 0.2887 Epoch 75/500 6/6 [==============================] - 0s 79us/sample - loss: 0.2828 Epoch 76/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2770 Epoch 77/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2713 Epoch 78/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2657 Epoch 79/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2603 Epoch 80/500 6/6 [==============================] - 0s 79us/sample - loss: 0.2549 Epoch 81/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2497 Epoch 82/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2446 Epoch 83/500 6/6 [==============================] - 0s 80us/sample - loss: 0.2395 Epoch 84/500 6/6 [==============================] - 0s 79us/sample - loss: 0.2346 Epoch 85/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2298 Epoch 86/500 6/6 [==============================] - 0s 79us/sample - loss: 0.2251 Epoch 87/500 6/6 [==============================] - 0s 79us/sample - loss: 0.2205 Epoch 88/500 6/6 [==============================] - 0s 77us/sample - loss: 0.2159 Epoch 89/500 6/6 [==============================] - 0s 78us/sample - loss: 0.2115 Epoch 90/500 6/6 [==============================] - 0s 81us/sample - loss: 0.2072 Epoch 91/500 6/6 [==============================] - 0s 79us/sample - loss: 0.2029 Epoch 92/500 6/6 [==============================] - 0s 79us/sample - loss: 0.1987 Epoch 93/500 6/6 [==============================] - 0s 76us/sample - loss: 0.1947 Epoch 94/500 6/6 [==============================] - 0s 80us/sample - loss: 0.1907 Epoch 95/500 6/6 [==============================] - 0s 78us/sample - loss: 0.1867 Epoch 96/500 6/6 [==============================] - 0s 156us/sample - loss: 0.1829 Epoch 97/500 6/6 [==============================] - 0s 89us/sample - loss: 0.1791 Epoch 98/500 6/6 [==============================] - 0s 80us/sample - loss: 0.1755 Epoch 99/500 6/6 [==============================] - 0s 95us/sample - loss: 0.1719 Epoch 100/500 6/6 [==============================] - 0s 96us/sample - loss: 0.1683 Epoch 101/500 6/6 [==============================] - 0s 81us/sample - loss: 0.1649 Epoch 102/500 6/6 [==============================] - 0s 87us/sample - loss: 0.1615 Epoch 103/500 6/6 [==============================] - 0s 83us/sample - loss: 0.1582 Epoch 104/500 6/6 [==============================] - 0s 123us/sample - loss: 0.1549 Epoch 105/500 6/6 [==============================] - 0s 89us/sample - loss: 0.1517 Epoch 106/500 6/6 [==============================] - 0s 84us/sample - loss: 0.1486 Epoch 107/500 6/6 [==============================] - 0s 97us/sample - loss: 0.1456 Epoch 108/500 6/6 [==============================] - 0s 153us/sample - loss: 0.1426 Epoch 109/500 6/6 [==============================] - 0s 157us/sample - loss: 0.1397 Epoch 110/500 6/6 [==============================] - 0s 92us/sample - loss: 0.1368 Epoch 111/500 6/6 [==============================] - 0s 83us/sample - loss: 0.1340 Epoch 112/500 6/6 [==============================] - 0s 84us/sample - loss: 0.1312 Epoch 113/500 6/6 [==============================] - 0s 89us/sample - loss: 0.1285 Epoch 114/500 6/6 [==============================] - 0s 84us/sample - loss: 0.1259 Epoch 115/500 6/6 [==============================] - 0s 81us/sample - loss: 0.1233 Epoch 116/500 6/6 [==============================] - 0s 83us/sample - loss: 0.1208 Epoch 117/500 6/6 [==============================] - 0s 84us/sample - loss: 0.1183 Epoch 118/500 6/6 [==============================] - 0s 80us/sample - loss: 0.1159 Epoch 119/500 6/6 [==============================] - 0s 82us/sample - loss: 0.1135 Epoch 120/500 6/6 [==============================] - 0s 82us/sample - loss: 0.1111 Epoch 121/500 6/6 [==============================] - 0s 85us/sample - loss: 0.1089 Epoch 122/500 6/6 [==============================] - 0s 84us/sample - loss: 0.1066 Epoch 123/500 6/6 [==============================] - 0s 82us/sample - loss: 0.1044 Epoch 124/500 6/6 [==============================] - 0s 79us/sample - loss: 0.1023 Epoch 125/500 6/6 [==============================] - 0s 83us/sample - loss: 0.1002 Epoch 126/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0981 Epoch 127/500 6/6 [==============================] - 0s 80us/sample - loss: 0.0961 Epoch 128/500 6/6 [==============================] - 0s 86us/sample - loss: 0.0941 Epoch 129/500 6/6 [==============================] - 0s 83us/sample - loss: 0.0922 Epoch 130/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0903 Epoch 131/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0885 Epoch 132/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0866 Epoch 133/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0849 Epoch 134/500 6/6 [==============================] - 0s 84us/sample - loss: 0.0831 Epoch 135/500 6/6 [==============================] - 0s 85us/sample - loss: 0.0814 Epoch 136/500 6/6 [==============================] - 0s 96us/sample - loss: 0.0797 Epoch 137/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0781 Epoch 138/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0765 Epoch 139/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0749 Epoch 140/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0734 Epoch 141/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0719 Epoch 142/500 6/6 [==============================] - 0s 83us/sample - loss: 0.0704 Epoch 143/500 6/6 [==============================] - 0s 80us/sample - loss: 0.0690 Epoch 144/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0675 Epoch 145/500 6/6 [==============================] - 0s 85us/sample - loss: 0.0662 Epoch 146/500 6/6 [==============================] - 0s 147us/sample - loss: 0.0648 Epoch 147/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0635 Epoch 148/500 6/6 [==============================] - 0s 94us/sample - loss: 0.0622 Epoch 149/500 6/6 [==============================] - 0s 105us/sample - loss: 0.0609 Epoch 150/500 6/6 [==============================] - 0s 108us/sample - loss: 0.0596 Epoch 151/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0584 Epoch 152/500 6/6 [==============================] - 0s 100us/sample - loss: 0.0572 Epoch 153/500 6/6 [==============================] - 0s 136us/sample - loss: 0.0560 Epoch 154/500 6/6 [==============================] - 0s 104us/sample - loss: 0.0549 Epoch 155/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0538 Epoch 156/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0527 Epoch 157/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0516 Epoch 158/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0505 Epoch 159/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0495 Epoch 160/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0485 Epoch 161/500 6/6 [==============================] - 0s 115us/sample - loss: 0.0475 Epoch 162/500 6/6 [==============================] - 0s 217us/sample - loss: 0.0465 Epoch 163/500 6/6 [==============================] - 0s 105us/sample - loss: 0.0455 Epoch 164/500 6/6 [==============================] - 0s 96us/sample - loss: 0.0446 Epoch 165/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0437 Epoch 166/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0428 Epoch 167/500 6/6 [==============================] - 0s 96us/sample - loss: 0.0419 Epoch 168/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0410 Epoch 169/500 6/6 [==============================] - 0s 94us/sample - loss: 0.0402 Epoch 170/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0394 Epoch 171/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0386 Epoch 172/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0378 Epoch 173/500 6/6 [==============================] - 0s 87us/sample - loss: 0.0370 Epoch 174/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0362 Epoch 175/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0355 Epoch 176/500 6/6 [==============================] - 0s 96us/sample - loss: 0.0348 Epoch 177/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0341 Epoch 178/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0334 Epoch 179/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0327 Epoch 180/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0320 Epoch 181/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0313 Epoch 182/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0307 Epoch 183/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0301 Epoch 184/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0294 Epoch 185/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0288 Epoch 186/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0282 Epoch 187/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0277 Epoch 188/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0271 Epoch 189/500 6/6 [==============================] - 0s 94us/sample - loss: 0.0265 Epoch 190/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0260 Epoch 191/500 6/6 [==============================] - 0s 157us/sample - loss: 0.0255 Epoch 192/500 6/6 [==============================] - 0s 99us/sample - loss: 0.0249 Epoch 193/500 6/6 [==============================] - 0s 101us/sample - loss: 0.0244 Epoch 194/500 6/6 [==============================] - 0s 110us/sample - loss: 0.0239 Epoch 195/500 6/6 [==============================] - 0s 104us/sample - loss: 0.0234 Epoch 196/500 6/6 [==============================] - 0s 108us/sample - loss: 0.0230 Epoch 197/500 6/6 [==============================] - 0s 107us/sample - loss: 0.0225 Epoch 198/500 6/6 [==============================] - 0s 254us/sample - loss: 0.0220 Epoch 199/500 6/6 [==============================] - 0s 193us/sample - loss: 0.0216 Epoch 200/500 6/6 [==============================] - 0s 98us/sample - loss: 0.0211 Epoch 201/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0207 Epoch 202/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0203 Epoch 203/500 6/6 [==============================] - 0s 100us/sample - loss: 0.0199 Epoch 204/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0194 Epoch 205/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0190 Epoch 206/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0187 Epoch 207/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0183 Epoch 208/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0179 Epoch 209/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0175 Epoch 210/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0172 Epoch 211/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0168 Epoch 212/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0165 Epoch 213/500 6/6 [==============================] - 0s 209us/sample - loss: 0.0161 Epoch 214/500 6/6 [==============================] - 0s 99us/sample - loss: 0.0158 Epoch 215/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0155 Epoch 216/500 6/6 [==============================] - 0s 94us/sample - loss: 0.0152 Epoch 217/500 6/6 [==============================] - 0s 87us/sample - loss: 0.0148 Epoch 218/500 6/6 [==============================] - 0s 86us/sample - loss: 0.0145 Epoch 219/500 6/6 [==============================] - 0s 94us/sample - loss: 0.0142 Epoch 220/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0139 Epoch 221/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0137 Epoch 222/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0134 Epoch 223/500 6/6 [==============================] - 0s 87us/sample - loss: 0.0131 Epoch 224/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0128 Epoch 225/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0126 Epoch 226/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0123 Epoch 227/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0121 Epoch 228/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0118 Epoch 229/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0116 Epoch 230/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0113 Epoch 231/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0111 Epoch 232/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0109 Epoch 233/500 6/6 [==============================] - 0s 110us/sample - loss: 0.0107 Epoch 234/500 6/6 [==============================] - 0s 158us/sample - loss: 0.0104 Epoch 235/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0102 Epoch 236/500 6/6 [==============================] - 0s 109us/sample - loss: 0.0100 Epoch 237/500 6/6 [==============================] - 0s 105us/sample - loss: 0.0098 Epoch 238/500 6/6 [==============================] - 0s 102us/sample - loss: 0.0096 Epoch 239/500 6/6 [==============================] - 0s 98us/sample - loss: 0.0094 Epoch 240/500 6/6 [==============================] - 0s 99us/sample - loss: 0.0092 Epoch 241/500 6/6 [==============================] - 0s 139us/sample - loss: 0.0090 Epoch 242/500 6/6 [==============================] - 0s 120us/sample - loss: 0.0088 Epoch 243/500 6/6 [==============================] - 0s 209us/sample - loss: 0.0087 Epoch 244/500 6/6 [==============================] - 0s 130us/sample - loss: 0.0085 Epoch 245/500 6/6 [==============================] - 0s 99us/sample - loss: 0.0083 Epoch 246/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0081 Epoch 247/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0080 Epoch 248/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0078 Epoch 249/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0076 Epoch 250/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0075 Epoch 251/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0073 Epoch 252/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0072 Epoch 253/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0070 Epoch 254/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0069 Epoch 255/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0067 Epoch 256/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0066 Epoch 257/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0065 Epoch 258/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0063 Epoch 259/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0062 Epoch 260/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0061 Epoch 261/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0060 Epoch 262/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0058 Epoch 263/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0057 Epoch 264/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0056 Epoch 265/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0055 Epoch 266/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0054 Epoch 267/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0053 Epoch 268/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0052 Epoch 269/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0050 Epoch 270/500 6/6 [==============================] - 0s 87us/sample - loss: 0.0049 Epoch 271/500 6/6 [==============================] - 0s 88us/sample - loss: 0.0048 Epoch 272/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0047 Epoch 273/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0046 Epoch 274/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0045 Epoch 275/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0045 Epoch 276/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0044 Epoch 277/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0043 Epoch 278/500 6/6 [==============================] - 0s 163us/sample - loss: 0.0042 Epoch 279/500 6/6 [==============================] - 0s 105us/sample - loss: 0.0041 Epoch 280/500 6/6 [==============================] - 0s 106us/sample - loss: 0.0040 Epoch 281/500 6/6 [==============================] - 0s 105us/sample - loss: 0.0039 Epoch 282/500 6/6 [==============================] - 0s 98us/sample - loss: 0.0039 Epoch 283/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0038 Epoch 284/500 6/6 [==============================] - 0s 102us/sample - loss: 0.0037 Epoch 285/500 6/6 [==============================] - 0s 117us/sample - loss: 0.0036 Epoch 286/500 6/6 [==============================] - 0s 93us/sample - loss: 0.0035 Epoch 287/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0035 Epoch 288/500 6/6 [==============================] - 0s 91us/sample - loss: 0.0034 Epoch 289/500 6/6 [==============================] - 0s 94us/sample - loss: 0.0033 Epoch 290/500 6/6 [==============================] - 0s 5ms/sample - loss: 0.0033 Epoch 291/500 6/6 [==============================] - 0s 89us/sample - loss: 0.0032 Epoch 292/500 6/6 [==============================] - 0s 77us/sample - loss: 0.0031 Epoch 293/500 6/6 [==============================] - 0s 83us/sample - loss: 0.0031 Epoch 294/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0030 Epoch 295/500 6/6 [==============================] - 0s 77us/sample - loss: 0.0029 Epoch 296/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0029 Epoch 297/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0028 Epoch 298/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0028 Epoch 299/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0027 Epoch 300/500 6/6 [==============================] - 0s 98us/sample - loss: 0.0027 Epoch 301/500 6/6 [==============================] - 0s 172us/sample - loss: 0.0026 Epoch 302/500 6/6 [==============================] - 0s 136us/sample - loss: 0.0025 Epoch 303/500 6/6 [==============================] - 0s 84us/sample - loss: 0.0025 Epoch 304/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0024 Epoch 305/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0024 Epoch 306/500 6/6 [==============================] - 0s 76us/sample - loss: 0.0023 Epoch 307/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0023 Epoch 308/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0022 Epoch 309/500 6/6 [==============================] - 0s 77us/sample - loss: 0.0022 Epoch 310/500 6/6 [==============================] - 0s 75us/sample - loss: 0.0022 Epoch 311/500 6/6 [==============================] - 0s 75us/sample - loss: 0.0021 Epoch 312/500 6/6 [==============================] - 0s 75us/sample - loss: 0.0021 Epoch 313/500 6/6 [==============================] - 0s 78us/sample - loss: 0.0020 Epoch 314/500 6/6 [==============================] - 0s 82us/sample - loss: 0.0020 Epoch 315/500 6/6 [==============================] - 0s 77us/sample - loss: 0.0019 Epoch 316/500 6/6 [==============================] - 0s 78us/sample - loss: 0.0019 Epoch 317/500 6/6 [==============================] - 0s 80us/sample - loss: 0.0019 Epoch 318/500 6/6 [==============================] - 0s 79us/sample - loss: 0.0018 Epoch 319/500 6/6 [==============================] - 0s 78us/sample - loss: 0.0018 Epoch 320/500 6/6 [==============================] - 0s 80us/sample - loss: 0.0018 Epoch 321/500 6/6 [==============================] - 0s 76us/sample - loss: 0.0017 Epoch 322/500 6/6 [==============================] - 0s 85us/sample - loss: 0.0017 Epoch 323/500 6/6 [==============================] - 0s 86us/sample - loss: 0.0016 Epoch 324/500 6/6 [==============================] - 0s 78us/sample - loss: 0.0016 Epoch 325/500 6/6 [==============================] - 0s 77us/sample - loss: 0.0016 Epoch 326/500 6/6 [==============================] - 0s 75us/sample - loss: 0.0015 Epoch 327/500 6/6 [==============================] - 0s 76us/sample - loss: 0.0015 Epoch 328/500 6/6 [==============================] - 0s 74us/sample - loss: 0.0015 Epoch 329/500 6/6 [==============================] - 0s 75us/sample - loss: 0.0015 Epoch 330/500 6/6 [==============================] - 0s 75us/sample - loss: 0.0014 Epoch 331/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0014 Epoch 332/500 6/6 [==============================] - 0s 106us/sample - loss: 0.0014 Epoch 333/500 6/6 [==============================] - 0s 146us/sample - loss: 0.0013 Epoch 334/500 6/6 [==============================] - 0s 81us/sample - loss: 0.0013 Epoch 335/500 6/6 [==============================] - 0s 106us/sample - loss: 0.0013 Epoch 336/500 6/6 [==============================] - 0s 105us/sample - loss: 0.0013 Epoch 337/500 6/6 [==============================] - 0s 92us/sample - loss: 0.0012 Epoch 338/500 6/6 [==============================] - 0s 86us/sample - loss: 0.0012 Epoch 339/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0012 Epoch 340/500 6/6 [==============================] - 0s 208us/sample - loss: 0.0012 Epoch 341/500 6/6 [==============================] - 0s 288us/sample - loss: 0.0011 Epoch 342/500 6/6 [==============================] - 0s 97us/sample - loss: 0.0011 Epoch 343/500 6/6 [==============================] - 0s 96us/sample - loss: 0.0011 Epoch 344/500 6/6 [==============================] - 0s 90us/sample - loss: 0.0011 Epoch 345/500 6/6 [==============================] - 0s 86us/sample - loss: 0.0010 Epoch 346/500 6/6 [==============================] - 0s 95us/sample - loss: 0.0010 Epoch 347/500 6/6 [==============================] - 0s 88us/sample - loss: 9.9958e-04 Epoch 348/500 6/6 [==============================] - 0s 88us/sample - loss: 9.7904e-04 Epoch 349/500 6/6 [==============================] - 0s 85us/sample - loss: 9.5893e-04 Epoch 350/500 6/6 [==============================] - 0s 87us/sample - loss: 9.3923e-04 Epoch 351/500 6/6 [==============================] - 0s 84us/sample - loss: 9.1994e-04 Epoch 352/500 6/6 [==============================] - 0s 90us/sample - loss: 9.0105e-04 Epoch 353/500 6/6 [==============================] - 0s 88us/sample - loss: 8.8254e-04 Epoch 354/500 6/6 [==============================] - 0s 89us/sample - loss: 8.6441e-04 Epoch 355/500 6/6 [==============================] - 0s 91us/sample - loss: 8.4665e-04 Epoch 356/500 6/6 [==============================] - 0s 93us/sample - loss: 8.2926e-04 Epoch 357/500 6/6 [==============================] - 0s 89us/sample - loss: 8.1223e-04 Epoch 358/500 6/6 [==============================] - 0s 86us/sample - loss: 7.9555e-04 Epoch 359/500 6/6 [==============================] - 0s 94us/sample - loss: 7.7920e-04 Epoch 360/500 6/6 [==============================] - 0s 85us/sample - loss: 7.6319e-04 Epoch 361/500 6/6 [==============================] - 0s 84us/sample - loss: 7.4752e-04 Epoch 362/500 6/6 [==============================] - 0s 84us/sample - loss: 7.3216e-04 Epoch 363/500 6/6 [==============================] - 0s 84us/sample - loss: 7.1713e-04 Epoch 364/500 6/6 [==============================] - 0s 86us/sample - loss: 7.0239e-04 Epoch 365/500 6/6 [==============================] - 0s 104us/sample - loss: 6.8797e-04 Epoch 366/500 6/6 [==============================] - 0s 84us/sample - loss: 6.7384e-04 Epoch 367/500 6/6 [==============================] - 0s 85us/sample - loss: 6.5999e-04 Epoch 368/500 6/6 [==============================] - 0s 93us/sample - loss: 6.4644e-04 Epoch 369/500 6/6 [==============================] - 0s 88us/sample - loss: 6.3316e-04 Epoch 370/500 6/6 [==============================] - 0s 90us/sample - loss: 6.2015e-04 Epoch 371/500 6/6 [==============================] - 0s 89us/sample - loss: 6.0742e-04 Epoch 372/500 6/6 [==============================] - 0s 90us/sample - loss: 5.9494e-04 Epoch 373/500 6/6 [==============================] - 0s 85us/sample - loss: 5.8272e-04 Epoch 374/500 6/6 [==============================] - 0s 85us/sample - loss: 5.7075e-04 Epoch 375/500 6/6 [==============================] - 0s 87us/sample - loss: 5.5903e-04 Epoch 376/500 6/6 [==============================] - 0s 111us/sample - loss: 5.4754e-04 Epoch 377/500 6/6 [==============================] - 0s 103us/sample - loss: 5.3630e-04 Epoch 378/500 6/6 [==============================] - 0s 151us/sample - loss: 5.2528e-04 Epoch 379/500 6/6 [==============================] - 0s 91us/sample - loss: 5.1449e-04 Epoch 380/500 6/6 [==============================] - 0s 88us/sample - loss: 5.0392e-04 Epoch 381/500 6/6 [==============================] - 0s 108us/sample - loss: 4.9357e-04 Epoch 382/500 6/6 [==============================] - 0s 93us/sample - loss: 4.8344e-04 Epoch 383/500 6/6 [==============================] - 0s 95us/sample - loss: 4.7350e-04 Epoch 384/500 6/6 [==============================] - 0s 87us/sample - loss: 4.6378e-04 Epoch 385/500 6/6 [==============================] - 0s 103us/sample - loss: 4.5425e-04 Epoch 386/500 6/6 [==============================] - 0s 96us/sample - loss: 4.4492e-04 Epoch 387/500 6/6 [==============================] - 0s 94us/sample - loss: 4.3578e-04 Epoch 388/500 6/6 [==============================] - 0s 96us/sample - loss: 4.2683e-04 Epoch 389/500 6/6 [==============================] - 0s 120us/sample - loss: 4.1806e-04 Epoch 390/500 6/6 [==============================] - 0s 240us/sample - loss: 4.0948e-04 Epoch 391/500 6/6 [==============================] - 0s 104us/sample - loss: 4.0106e-04 Epoch 392/500 6/6 [==============================] - 0s 92us/sample - loss: 3.9282e-04 Epoch 393/500 6/6 [==============================] - 0s 89us/sample - loss: 3.8476e-04 Epoch 394/500 6/6 [==============================] - 0s 95us/sample - loss: 3.7685e-04 Epoch 395/500 6/6 [==============================] - 0s 88us/sample - loss: 3.6911e-04 Epoch 396/500 6/6 [==============================] - 0s 88us/sample - loss: 3.6153e-04 Epoch 397/500 6/6 [==============================] - 0s 88us/sample - loss: 3.5411e-04 Epoch 398/500 6/6 [==============================] - 0s 85us/sample - loss: 3.4683e-04 Epoch 399/500 6/6 [==============================] - 0s 84us/sample - loss: 3.3971e-04 Epoch 400/500 6/6 [==============================] - 0s 87us/sample - loss: 3.3273e-04 Epoch 401/500 6/6 [==============================] - 0s 85us/sample - loss: 3.2590e-04 Epoch 402/500 6/6 [==============================] - 0s 83us/sample - loss: 3.1920e-04 Epoch 403/500 6/6 [==============================] - 0s 84us/sample - loss: 3.1265e-04 Epoch 404/500 6/6 [==============================] - 0s 89us/sample - loss: 3.0623e-04 Epoch 405/500 6/6 [==============================] - 0s 90us/sample - loss: 2.9993e-04 Epoch 406/500 6/6 [==============================] - 0s 87us/sample - loss: 2.9377e-04 Epoch 407/500 6/6 [==============================] - 0s 85us/sample - loss: 2.8774e-04 Epoch 408/500 6/6 [==============================] - 0s 83us/sample - loss: 2.8183e-04 Epoch 409/500 6/6 [==============================] - 0s 84us/sample - loss: 2.7604e-04 Epoch 410/500 6/6 [==============================] - 0s 88us/sample - loss: 2.7037e-04 Epoch 411/500 6/6 [==============================] - 0s 90us/sample - loss: 2.6482e-04 Epoch 412/500 6/6 [==============================] - 0s 88us/sample - loss: 2.5937e-04 Epoch 413/500 6/6 [==============================] - 0s 88us/sample - loss: 2.5405e-04 Epoch 414/500 6/6 [==============================] - 0s 87us/sample - loss: 2.4883e-04 Epoch 415/500 6/6 [==============================] - 0s 86us/sample - loss: 2.4372e-04 Epoch 416/500 6/6 [==============================] - 0s 86us/sample - loss: 2.3871e-04 Epoch 417/500 6/6 [==============================] - 0s 85us/sample - loss: 2.3381e-04 Epoch 418/500 6/6 [==============================] - 0s 87us/sample - loss: 2.2901e-04 Epoch 419/500 6/6 [==============================] - 0s 85us/sample - loss: 2.2430e-04 Epoch 420/500 6/6 [==============================] - 0s 91us/sample - loss: 2.1970e-04 Epoch 421/500 6/6 [==============================] - 0s 89us/sample - loss: 2.1518e-04 Epoch 422/500 6/6 [==============================] - 0s 86us/sample - loss: 2.1076e-04 Epoch 423/500 6/6 [==============================] - 0s 84us/sample - loss: 2.0643e-04 Epoch 424/500 6/6 [==============================] - 0s 86us/sample - loss: 2.0219e-04 Epoch 425/500 6/6 [==============================] - 0s 153us/sample - loss: 1.9804e-04 Epoch 426/500 6/6 [==============================] - 0s 90us/sample - loss: 1.9397e-04 Epoch 427/500 6/6 [==============================] - 0s 96us/sample - loss: 1.8999e-04 Epoch 428/500 6/6 [==============================] - 0s 101us/sample - loss: 1.8608e-04 Epoch 429/500 6/6 [==============================] - 0s 90us/sample - loss: 1.8226e-04 Epoch 430/500 6/6 [==============================] - 0s 85us/sample - loss: 1.7852e-04 Epoch 431/500 6/6 [==============================] - 0s 91us/sample - loss: 1.7485e-04 Epoch 432/500 6/6 [==============================] - 0s 121us/sample - loss: 1.7126e-04 Epoch 433/500 6/6 [==============================] - 0s 98us/sample - loss: 1.6774e-04 Epoch 434/500 6/6 [==============================] - 0s 90us/sample - loss: 1.6430e-04 Epoch 435/500 6/6 [==============================] - 0s 86us/sample - loss: 1.6092e-04 Epoch 436/500 6/6 [==============================] - 0s 89us/sample - loss: 1.5762e-04 Epoch 437/500 6/6 [==============================] - 0s 89us/sample - loss: 1.5438e-04 Epoch 438/500 6/6 [==============================] - 0s 86us/sample - loss: 1.5121e-04 Epoch 439/500 6/6 [==============================] - 0s 89us/sample - loss: 1.4810e-04 Epoch 440/500 6/6 [==============================] - 0s 85us/sample - loss: 1.4506e-04 Epoch 441/500 6/6 [==============================] - 0s 101us/sample - loss: 1.4208e-04 Epoch 442/500 6/6 [==============================] - 0s 273us/sample - loss: 1.3916e-04 Epoch 443/500 6/6 [==============================] - 0s 101us/sample - loss: 1.3630e-04 Epoch 444/500 6/6 [==============================] - 0s 85us/sample - loss: 1.3350e-04 Epoch 445/500 6/6 [==============================] - 0s 90us/sample - loss: 1.3076e-04 Epoch 446/500 6/6 [==============================] - 0s 88us/sample - loss: 1.2807e-04 Epoch 447/500 6/6 [==============================] - 0s 85us/sample - loss: 1.2544e-04 Epoch 448/500 6/6 [==============================] - 0s 86us/sample - loss: 1.2287e-04 Epoch 449/500 6/6 [==============================] - 0s 87us/sample - loss: 1.2034e-04 Epoch 450/500 6/6 [==============================] - 0s 86us/sample - loss: 1.1787e-04 Epoch 451/500 6/6 [==============================] - 0s 87us/sample - loss: 1.1545e-04 Epoch 452/500 6/6 [==============================] - 0s 89us/sample - loss: 1.1308e-04 Epoch 453/500 6/6 [==============================] - 0s 87us/sample - loss: 1.1075e-04 Epoch 454/500 6/6 [==============================] - 0s 86us/sample - loss: 1.0848e-04 Epoch 455/500 6/6 [==============================] - 0s 87us/sample - loss: 1.0625e-04 Epoch 456/500 6/6 [==============================] - 0s 87us/sample - loss: 1.0407e-04 Epoch 457/500 6/6 [==============================] - 0s 85us/sample - loss: 1.0193e-04 Epoch 458/500 6/6 [==============================] - 0s 88us/sample - loss: 9.9838e-05 Epoch 459/500 6/6 [==============================] - 0s 86us/sample - loss: 9.7788e-05 Epoch 460/500 6/6 [==============================] - 0s 85us/sample - loss: 9.5779e-05 Epoch 461/500 6/6 [==============================] - 0s 86us/sample - loss: 9.3813e-05 Epoch 462/500 6/6 [==============================] - 0s 88us/sample - loss: 9.1885e-05 Epoch 463/500 6/6 [==============================] - 0s 94us/sample - loss: 8.9998e-05 Epoch 464/500 6/6 [==============================] - 0s 89us/sample - loss: 8.8149e-05 Epoch 465/500 6/6 [==============================] - 0s 87us/sample - loss: 8.6338e-05 Epoch 466/500 6/6 [==============================] - 0s 86us/sample - loss: 8.4564e-05 Epoch 467/500 6/6 [==============================] - 0s 86us/sample - loss: 8.2829e-05 Epoch 468/500 6/6 [==============================] - 0s 86us/sample - loss: 8.1126e-05 Epoch 469/500 6/6 [==============================] - 0s 106us/sample - loss: 7.9460e-05 Epoch 470/500 6/6 [==============================] - 0s 92us/sample - loss: 7.7829e-05 Epoch 471/500 6/6 [==============================] - 0s 157us/sample - loss: 7.6230e-05 Epoch 472/500 6/6 [==============================] - 0s 86us/sample - loss: 7.4663e-05 Epoch 473/500 6/6 [==============================] - 0s 86us/sample - loss: 7.3130e-05 Epoch 474/500 6/6 [==============================] - 0s 99us/sample - loss: 7.1628e-05 Epoch 475/500 6/6 [==============================] - 0s 105us/sample - loss: 7.0157e-05 Epoch 476/500 6/6 [==============================] - 0s 106us/sample - loss: 6.8716e-05 Epoch 477/500 6/6 [==============================] - 0s 95us/sample - loss: 6.7304e-05 Epoch 478/500 6/6 [==============================] - 0s 148us/sample - loss: 6.5923e-05 Epoch 479/500 6/6 [==============================] - 0s 305us/sample - loss: 6.4568e-05 Epoch 480/500 6/6 [==============================] - 0s 106us/sample - loss: 6.3241e-05 Epoch 481/500 6/6 [==============================] - 0s 94us/sample - loss: 6.1943e-05 Epoch 482/500 6/6 [==============================] - 0s 87us/sample - loss: 6.0670e-05 Epoch 483/500 6/6 [==============================] - 0s 85us/sample - loss: 5.9425e-05 Epoch 484/500 6/6 [==============================] - 0s 85us/sample - loss: 5.8204e-05 Epoch 485/500 6/6 [==============================] - 0s 85us/sample - loss: 5.7008e-05 Epoch 486/500 6/6 [==============================] - 0s 84us/sample - loss: 5.5837e-05 Epoch 487/500 6/6 [==============================] - 0s 84us/sample - loss: 5.4691e-05 Epoch 488/500 6/6 [==============================] - 0s 83us/sample - loss: 5.3567e-05 Epoch 489/500 6/6 [==============================] - 0s 84us/sample - loss: 5.2467e-05 Epoch 490/500 6/6 [==============================] - 0s 84us/sample - loss: 5.1388e-05 Epoch 491/500 6/6 [==============================] - 0s 85us/sample - loss: 5.0333e-05 Epoch 492/500 6/6 [==============================] - 0s 84us/sample - loss: 4.9299e-05 Epoch 493/500 6/6 [==============================] - 0s 87us/sample - loss: 4.8286e-05 Epoch 494/500 6/6 [==============================] - 0s 89us/sample - loss: 4.7294e-05 Epoch 495/500 6/6 [==============================] - 0s 88us/sample - loss: 4.6324e-05 Epoch 496/500 6/6 [==============================] - 0s 82us/sample - loss: 4.5371e-05 Epoch 497/500 6/6 [==============================] - 0s 86us/sample - loss: 4.4440e-05 Epoch 498/500 6/6 [==============================] - 0s 85us/sample - loss: 4.3528e-05 Epoch 499/500 6/6 [==============================] - 0s 82us/sample - loss: 4.2633e-05 Epoch 500/500 6/6 [==============================] - 0s 83us/sample - loss: 4.1757e-05 [[18.981148]]   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a1"}, "repo_url": "https://github.com/DWCTOD/flask-keras-cnn-image-retrieval-master", "repo_name": "flask-keras-cnn-image-retrieval-master", "repo_full_name": "DWCTOD/flask-keras-cnn-image-retrieval-master", "repo_owner": "DWCTOD", "repo_desc": "\u722c\u53d6\u767e\u5ea6\u56fe\u7247\uff0c\u642d\u5efa\u81ea\u5df1\u7684\u56fe\u7247\u7d22\u5f15\u5e93\u5b9e\u73b0\u7b80\u5355\u7684\u4ee5\u56fe\u641c\u56fe\u529f\u80fd\uff0c\u8fd8\u6709\u53ef\u89c6\u5316\u6548\u679c", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-22T04:55:10Z", "repo_watch": 5, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T15:19:55Z", "homepage": null, "size": 1671, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185428270, "is_fork": false, "readme_text": "Image Retrieval Engine Based on Keras \u6f14\u793a \u8be6\u7ec6\u8bb2\u89e3\u3001\u64cd\u4f5c\u622a\u56fe\u548c\u89c6\u9891\u6f14\u793a \u4ee5\u56fe\u641c\u56fe\u5c55\u793a \u9996\u53d1\u4e8e\u516c\u4f17\u53f7\uff1aAI\u7b97\u6cd5\u4e0e\u56fe\u50cf\u5904\u7406  \u73af\u5883 import keras Using Theano backend \u672c\u4eba\u91c7\u7528\u7684\u662fkeras 2.24\u7248\u672c\uff0c\u4f7f\u7528python3.6 \u539f\u6587\u4f5c\u8005\u5907\u6ce8\uff1ahttps://github.com/willard-yuan/flask-keras-cnn-image-retrieval keras 2.0.1 \u53ca 2.0.5 \u7248\u672c\u5747\u7ecf\u8fc7\u6d4b\u8bd5\u53ef\u7528\u3002\u63a8\u8350Python 2.7\uff0c\u652f\u6301Python 3.6. \u6b64\u5916\u9700\u8981numpy, matplotlib, os, h5py, argparse. \u63a8\u8350\u4f7f\u7528anaconda\u5b89\u88c5 \u6587\u4ef6\u8bf4\u660e img \u6587\u4ef6\u5939\uff1a\u56fe\u7247\u5e93\u2014\u2014\u7528\u4e8e\u642d\u5efa\u7d22\u5f15\u5e93\u548c\u540e\u7eed\u67e5\u8be2\u7ed3\u679c\u7684\u663e\u793a 1.jpg 2.jpg \uff1a\u7528\u4e8e\u6d4b\u8bd5 extract_cnn_vgg16_keras.py:\u63d0\u53d6\u56fe\u7247\u7279\u5f81 index.py\uff1a\u5efa\u7acb\u7d22\u5f15\u5e93\u5e76\u4fdd\u5b58\u6a21\u578b lol.h5\uff1a\u672c\u4eba\u4e8b\u5148\u5efa\u597d\u7684\u6a21\u578b\uff0c\u53ef\u4ee5\u7528\u4e8e\u76f4\u63a5\u6d4b\u8bd5 pachong.py\uff1a\u722c\u53d6\u767e\u5ea6\u7684\u56fe\u7247 query_online.py\uff1a\u8fd0\u884c\u4ee3\u7801\u5b9e\u73b0\u4ee5\u56fe\u641c\u56fe\u5728\u7ebf\u6d4b\u8bd5 \u4f7f\u7528 #\u64cd\u4f5c\u6c47\u603b # \uff081\uff09\u6253\u5f00\u7ec8\u7aef\u6267\u884cindex.py\u4ee3\u7801 python3 index.py -database img -index lol.h5 # \u6b64\u65f6\u5df2\u7ecf\u5c06\u56fe\u7247\u5e93\u8f6c\u6210\u7d22\u5f15\u5e93\u5e76\u4fdd\u5b58\u548c\u8f93\u51falol.h5\u6a21\u578b # (2)\u7ee7\u7eed\u8fd0\u884c\u4ee3\u7801 query_online.py python3 query_online.py # \u6309\u7167\u63d0\u793a\uff0c\u5982\u679c\u8981\u9000\u51fa\u8f93\u5165 exit \u67e5\u8be2\u76f4\u63a5enter # \u8f93\u5165\u6d4b\u8bd5\u56fe\u7247 \u540d\u5b57\u5373\u53ef\uff08\u5982\u679c\u6d4b\u8bd5\u56fe\u7247\u989d\u4ee3\u7801\u4e0d\u5728\u540c\u4e00\u8def\u5f84\u4e0b\u9700\u8981\u589e\u52a0\u8def\u5f84\u2014\u2014\u8fd9\u91cc \u8bbe\u7f6e\u76f8\u5bf9\u8def\u5f84\uff09 \u5907\u6ce8\uff1a\u672c\u4eba\u5bf9\u6e90\u4ee3\u7801\u8fdb\u884c\u4e86\u4e00\u4e9b\u4fee\u6539 1\uff09\u589e\u52a0\u4e86\u4e00\u4e2a\u5f02\u5e38\u5904\u7406\u64cd\u4f5c\uff0c\u4e3b\u8981\u662f\u4e3a\u4e86\u65b9\u4fbf\uff0c\u5373\u4f7f\u624b\u8bef\u8f93\u9519\u4e5f\u80fd\u7ee7\u7eed\u8fd0\u884c\uff0c\u8fd9\u4e2a\u5728\u4e4b\u524d\u7684\u6587\u7ae0\u4e2d\u6709\u8bb2\u89e3\u8fc7\uff1b \u5b66\u4f1a\u8fd9\u62db\u518d\u4e5f\u4e0d\u6015\u624b\u8bef\u8ba9\u4ee3\u7801\u5d29\u6389 \uff082\uff09\u4f5c\u8005\u6700\u7ec8\u663e\u793a\u7684\u7ed3\u679c\u53ea\u80fd\u4e00\u5f20\u4e00\u5f20\u7684\u5c55\u793a\uff0c\u6ca1\u6709\u5bf9\u6bd4\u56fe\uff0c\u56e0\u6b64\u6211\u7a0d\u5fae\u4fee\u6539\u4e86\u4e00\u4e0b\uff0c\u8ba9\u53ef\u89c6\u5316\u7684\u6548\u679c\u66f4\u52a0\u7684\u7f8e\u89c2\u4e00\u4e9b\uff0c\u6709\u5174\u8da3\u7684\u53ef\u4ee5\u53c2\u8003\u6211\u7684\u4ee3\u7801\uff1b \uff083\uff09\u6211\u5bf9\u53c2\u6570\u8f93\u5165\u4e5f\u8fdb\u884c\u4e86\u4fee\u6539\uff0c\u5c06\u6a21\u578b\u540d\u5b57\u548c\u56fe\u7247\u5e93\u7684\u8def\u5f84\u90fd\u56fa\u5b9a\u4e86\uff0c\u8fd9\u6837\u5b50\u6d4b\u8bd5\u7684\u65f6\u5019\u6bd4\u8f83\u65b9\u4fbf\uff0c\u5927\u5bb6\u5728\u4f7f\u7528\u7684\u65f6\u5019\u8bf7\u6ce8\u610f\u4e0b\uff0c\u5982\u679c\u4fee\u6539\u4e86\u540d\u5b57\u8981\u5bf9\u5e94\u8d77\u6765\u3002  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/DWCTOD/flask-keras-cnn-image-retrieval-master/blob/c863f59754e209f38d7466c9de6971223a472d5d/lol.h5"], "see_also_links": ["http://mp.weixin.qq.com/s?__biz=MzU4NTY4Mzg1Mw==&mid=2247484695&idx=1&sn=530d383d799e1aaa4554747098c53e01&chksm=fd8783f5caf00ae38c93613aab97df7feb9d6b13c7018e1fa2f378ec86bb97d164e93e7a1a2b&scene=21#wechat_redirect"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a2"}, "repo_url": "https://github.com/wakafengfan/Capsule-pytorch", "repo_name": "Capsule-pytorch", "repo_full_name": "wakafengfan/Capsule-pytorch", "repo_owner": "wakafengfan", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T05:26:38Z", "repo_watch": 3, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T08:10:44Z", "homepage": null, "size": 8, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185353665, "is_fork": false, "readme_text": "Capsule-pytorch \u53c2\u8003\u82cf\u795e\u7684keras\u7248capsule\uff0c\u4ee5\u53cabinzhouchn\u7684pytorch\u7248\u672ccapsule\uff0c\u5f00\u53d1\u4e86pytorch\u7248\u7684\"\u5355\u6570\u5b57\u8bad\u7ec3\u3001\u53cc\u6570\u5b57\u6d4b\u8bd5\"\uff0c\u51c6\u786e\u7387\u53ef\u8fbe\u523096%\u4ee5\u4e0a\u3002 Reference:  https://github.com/bojone/Capsule https://github.com/binzhouchn/capsule-pytorch  Requirement:  Python 3.6 PyTorch 1.0  \u7279\u522b\u611f\u8c22xueshi\u5927\u795e\u5e2e\u5fd9review\u5230\u4ee3\u7801\u7684\u95ee\u9898[\u62b1\u62f3] ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a3"}, "repo_url": "https://github.com/2bright/adapow2", "repo_name": "adapow2", "repo_full_name": "2bright/adapow2", "repo_owner": "2bright", "repo_desc": "Adapow2 is a serial of adaptive gradient descent optimizers by adjusting the power of 2 of a tiny step size.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-27T05:35:06Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T05:43:21Z", "homepage": "", "size": 47, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185328560, "is_fork": false, "readme_text": "Adapow2 Adapow2\u662f\u4e00\u4e2a\u57fa\u4e8etensorflow.keras\u5f00\u53d1\u7684\u795e\u7ecf\u7f51\u7edc\u81ea\u9002\u5e94\u4f18\u5316\u7b97\u6cd5\uff0c\u76ee\u6807\u662f\u5b8c\u5168\u66ff\u4ee3\u4f20\u7edf\u7684sgd\uff0c\u5e76\u903c\u8fd1adam\u7684\u4f18\u5316\u901f\u5ea6\u3002\u8be5\u7b97\u6cd5\u5728\u6267\u884c\u6bcf\u4e00\u6b65\u4f18\u5316\u65f6\uff0c\u65b9\u5411\u4e0esgd\u76f8\u540c\uff0c\u4f46\u6b65\u957f\u901a\u8fc7\u81ea\u9002\u5e94\u673a\u5236\u52a8\u6001\u8c03\u6574\uff0c\u4f7f\u5f97\u8be5\u7b97\u6cd5\u6709\u63a5\u8fd1adam\u7684\u4f18\u5316\u901f\u5ea6\u3002 Adapow2 is a neural network adaptive optimization algorithm based on tensorflow.keras. The goal is to completely replace the traditional sgd and approximate the optimization speed of adam. When the algorithm performs each step of optimization, the direction is the same as sgd, but the step size is dynamically adjusted by the adaptive mechanism, which makes the algorithm have an optimization speed close to adam. usage install git clone git@github.com:2bright/adapow2.git cd adapow2 pip install -e .  !!! MUST !!! In order to run adapow2, you must replace 'training_arrays.py' of your tensorflow installation with 'adapow2/tf-keras-hack/training_arrays.py'. For me, the path is '/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py'. The following operations may illustrate what you should do. mv /path/to/tensorflow/python/keras/engine/training_arrays.py /path/to/adapow2/tf-keras-hack/training_arrays.py.backup ln -s /path/to/adapow2/tf-keras-hack/training_arrays.py /path/to/tensorflow/python/keras/engine/training_arrays.py  run examples git clone git@github.com:2bright/kv_prod_union.git cd kv_prod_union pip install -e .  cd /path/to/adapow2/examples/adapow2_vs_adam python3.6 adam_mnist.py python3.6 adapow2_mnist.py  cd /path/to/adapow2/examples/tests python3.6 test_MultiStepProbing.py  kv_prod_union is for hyperparameter sampling management. For 'adapow2/examples/tests', the loss-acc figure is stored in data directory. for research You can set config['store_history_state'] hyperparameter of optimizer to be True, and inspect how step size change during training. Or you can modify test_MultiStepProbing.py file, use OHS to inspect hyperspace slice of optimization path. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a4"}, "repo_url": "https://github.com/mengzhou-sun/mengzhou", "repo_name": "mengzhou", "repo_full_name": "mengzhou-sun/mengzhou", "repo_owner": "mengzhou-sun", "repo_desc": "Keras with Tensorflow Gc-net", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T15:48:30Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T14:59:03Z", "homepage": "", "size": 22, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185424445, "is_fork": false, "readme_text": "mengzhou This program is mainly a combination of keras and tensorflow structures, based on the prediction of disparity maps from left and right original images. Data Sources\uff1a: Scene Flow Datasets(https://lmb.informatik.uni-freiburg.de/resources/datasets/SceneFlowDatasets.en.html) ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a5"}, "repo_url": "https://github.com/AliGangeh/keras_mini_project", "repo_name": "keras_mini_project", "repo_full_name": "AliGangeh/keras_mini_project", "repo_owner": "AliGangeh", "repo_desc": "creating a preceptron with keras", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T02:28:26Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T05:28:06Z", "homepage": null, "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185326747, "is_fork": false, "readme_text": "keras_mini_project creating a preceptron with keras ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a6"}, "repo_url": "https://github.com/daroodar/k_torch", "repo_name": "k_torch", "repo_full_name": "daroodar/k_torch", "repo_owner": "daroodar", "repo_desc": "A Keras-like wrapper for PyTorch", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T05:48:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T09:40:46Z", "homepage": null, "size": 40, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185371682, "is_fork": false, "readme_text": "k_torch k_torch enables you to code in Keras-style while using Pytorch at backend. It's a keras-like wrapper for PyTorch! ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/daroodar/k_torch/blob/85a249a59253ffbea5bbc5ea950c2a5beba526db/k_torch/some_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a7"}, "repo_url": "https://github.com/LilAnthony123/text-classification-keras", "repo_name": "text-classification-keras", "repo_full_name": "LilAnthony123/text-classification-keras", "repo_owner": "LilAnthony123", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-24T03:30:33Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T09:32:39Z", "homepage": null, "size": 416, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185370066, "is_fork": false, "readme_text": "text-classification-keras \u57fa\u4e8egithub\u4e0a\u4e00\u4e2a\u6587\u672c\u5206\u7c7b\u7684\u4ee3\u7801\u5bf9\u5176tcnn\u548ctextrnn\u6a21\u578b\u7528keras\u5199\u4e86\u4e00\u904d\uff0c\u5e76\u52a0\u5165\u4e86attention\uff0c\u5b66\u4e60\u7ec3\u624b  \u6570\u636e\u96c6\u653e\u5728/data/cnews\u4e0b\uff0c\u5728\u6b64\u4e0b\u8f7d\uff1a\u94fe\u63a5: https://pan.baidu.com/s/1hugrfRu \u5bc6\u7801: qfud ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a8"}, "repo_url": "https://github.com/jefmud/bottle-resnet-microservice", "repo_name": "bottle-resnet-microservice", "repo_full_name": "jefmud/bottle-resnet-microservice", "repo_owner": "jefmud", "repo_desc": "A simple ResNet50 microservice built on Bottle Framework", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T11:56:24Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T11:16:19Z", "homepage": null, "size": 69, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185385776, "is_fork": false, "readme_text": "bottle-resnet-microservice A simple ResNet50 microservice built on Bottle Framework Uses Keras and ResNet50 API to identify images run as a microservice on a local or remote server.  I am using ResNet50 which is a pretty strong identification library with no modifications. There is a GET and POST API as well as a micro form to try/test the service.  The microservice responds with JSON which has a \"success\" indication, and a \"top-5\" predictions array which are returned.  I will update later with a small tweak to allow more or less predictions, but for now, top-5 will work to see if this will test. This project was inspired by Adrian Rosebrock's Keras+Flask project (https://blog.keras.io/author/adrian-rosebrock.html) a well-written article, but the code didn't work for me \"out of the box\" (due to the absence of a TensorFlow default_graph) but the concepts are solid so I borrowed some of his logic and expanded it.  I made the design choices to build on top of Bottle (a Python microframework) and GEvent a tight high-performance WSGI server. With the '--deploy' command line switch, you will have a viable microservice which runs as a \"Greenlet.\"  No guarantees on can be made on scalability since it does use the very demanding TensorFlow on the backend!  Since there is nothing about the microservice that is \"secret\" it is intended to run only as HTTP.  It could easily incorporate certificates Depending on your preference, this could easily be ported to incorporate Tornado, GUnicorn, CherryPy as the WSGI server. Your requirements will differ as security concerns are addressed and versions are rapidly improving. Requirements:  Python 3.5+ Keras TensorFlow GEvent Pillow Bottle NumPy requests  Visit Keras site for more information on applications of this type. https://keras.io/applications/ Please reference documentation for GEvent to learn more about deployment http://www.gevent.org/ Please reference the documentation on Bottle microframework to learn more about the routing/request methodology https://bottlepy.org ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.gevent.org/"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830a9"}, "repo_url": "https://github.com/djl70/cv-bird-id", "repo_name": "cv-bird-id", "repo_full_name": "djl70/cv-bird-id", "repo_owner": "djl70", "repo_desc": "Code used for my final project in Computer Vision at Texas State University, Spring 2019", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T06:14:28Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T01:35:54Z", "homepage": null, "size": 10, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185298592, "is_fork": false, "readme_text": "Automatic Identification of Bird Species using Computer Vision Usage The following steps should mostly work, but there may be additional steps you need to take to make things work perfectly. This certainly is not the cleanest code you'll see. I learned Python and Keras as I went. 1. Install Keras on Windows 10  Install CUDA 10.0 Sign up for the NVIDIA Developer Program Download cuDNN (v7.5.1 for CUDA 10.0) Follow the instructions here to install cuDNN Finally, install TensorFlow and Keras:  pip3 install tensorflow-gpu pip3 install keras 2. Download the datasets  CUB-200-2011 NABirds  3. Split the data  Run split_data_cub.py and/or split_data_nabirds.py to split the CUB-200-2011 and NABirds datasets, respectively (some modifications may need to be made to the paths) An extra directory named images is made in this process. I moved the child folders of that directory to the same level and then deleted it  4. Train the model  Make sure mode = \"train\" is set inside main.py Uncomment the lines for the dataset you wish to use Uncomment the lines for the base model you wish to use Rename the file the model will be saved to Run main.py  5. Test the model  Make sure mode = \"test\" is set inside main.py Uncomment the lines for the dataset you wish to use Make sure the name of the model being loaded matches the one you saved after training Run main.py. The output is the loss and accuracy  Unused/outdated files I kept these files here as a remnant of other things I tried. They are not used.  crop_to_bounding_box.py nabirds.py split_data.py try_model.py  ", "has_readme": true, "readme_language": "English", "repo_tags": ["bird-species-classification"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://www.vision.caltech.edu/visipedia/CUB-200-2011.html", "http://dl.allaboutbirds.org/nabirds"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830aa"}, "repo_url": "https://github.com/jgrizou/audio_features", "repo_name": "audio_features", "repo_full_name": "jgrizou/audio_features", "repo_owner": "jgrizou", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-28T17:47:44Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T08:46:31Z", "homepage": null, "size": 11, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185360189, "is_fork": false, "readme_text": "Embedding:  https://github.com/IBM/MAX-Audio-Embedding-Generator   docker run -it -p 5000:5000 codait/max-audio-embedding-generator   https://github.com/librosa/librosa/issues/223   conda install -c roebel pysndfile   https://forge-2.ircam.fr/roebel/pysndfile  Deep Learning:  https://projects.csail.mit.edu/soundnet/ https://github.com/pseeth/soundnet_keras/blob/master/soundnet.py https://github.com/camila-ud/SoundNet-keras https://github.com/camila-ud/SoundNet-keras/blob/master/TP's/SoundNet_arias_ibarra.ipynb  Blog:  https://aqibsaeed.github.io/2016-09-03-urban-sound-classification-part-1/  Python libraries:  https://github.com/librosa/librosa https://github.com/Yaafe/Yaafe https://github.com/MTG/essentia https://github.com/tyiannak/pyAudioAnalysis  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ab"}, "repo_url": "https://github.com/zxqcreations/YOLOv3-withWebServer", "repo_name": "YOLOv3-withWebServer", "repo_full_name": "zxqcreations/YOLOv3-withWebServer", "repo_owner": "zxqcreations", "repo_desc": "A Flask server, use openCV for reading data from camera, detect objections by YOLOv3-tensorflow backend,", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-08T06:38:23Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T03:16:06Z", "homepage": null, "size": 124, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185312541, "is_fork": false, "readme_text": "keras-yolo3  Updated Forked from keras-yolo3 by qqwweee  Change PIL library to OpenCV to handle the real-time video stream from camera. Use flask to setup a web based stream server, detects objects and resopnses to web browser. Setup yolov3 pre-trained weights and then start server with python3 server.py  Introduction A Keras implementation of YOLOv3 (Tensorflow backend) inspired by allanzelener/YAD2K.  Quick Start  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ac"}, "repo_url": "https://github.com/Isaisaisa/ActivityMonitoring", "repo_name": "ActivityMonitoring", "repo_full_name": "Isaisaisa/ActivityMonitoring", "repo_owner": "Isaisaisa", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-27T07:40:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T07:12:23Z", "homepage": null, "size": 52, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185343159, "is_fork": false, "readme_text": "ActivityMonitoring Installing libraries pip install -r requirements.txt  Configure config file We have a config file in this project calling config.py. This file should be copied and the copied file should be renamed to config_dev.py. Then you can configure the paths to the origin data for training and testing. The constant SAVEPATH defines where the processed data will be saved e.g. preprocessed data or the extracted features. The respective folders are created when starting the process/program. Run project python PATH\\TO\\PROJECT\\ActivityMonitoring\\TestingField.py  Project structure For each Process step we have a separated class/file. The file TestingField.py starts all the Process at once and includes some boolean variables as switches. # Load the data and Preprocessing  LOAD_DATA_AND_PREPROCESS = True # Save preprocessed data SAVE_PREPROCESSED_DATA = True # Load the saved preprocessed data LOAD_PREPROCESSED_DATA = True # Feature Extraction FEATURE_EXTRACTION = True # Save the extracted Features SAVE_FEATURE_VECTORS = True # Load saved feature vectors LOAD_FEATURE_VECTORS = True # Feature Selection SELECT_FEATURES = False # Train the MLP and use it to classify test data TRAIN_AND_CLASSIFY = True # Swap between two version of MLP USE_KERAS_MLP = True  Set all switches to True when you run the program for the first time. Only the last switch USE_KERAS_MLP can be set to True or False. It depends on which MLP you want to train. If you use True then the Keras MLP will be used, if you use False then the Sklearn MLP will be trained. The settings for the sklearn MLP can be changed in the ''parameter'' dictionary in TestingField.py. The settings for the keras MLP can be changed in the MlpClassifier.py. When you start it for the first time, some new folders will be created under the path you specified under SAVEPATH in the configuration file (see the heading Configure config file). If you have already run the program and all files with the features etc. are created, then you can set most of the switches to False, except LOAD_FEATURE_VECTORS, TRAIN_AND_CLASSIFY and maybe USE_KERAS_MLP. Further classes/files: DataLoader.py: Loads the origin data for processing PreProcessing.py: Prepossesses the data so it is normalized and the sequences have the same length FeatureExtractor.py: Extracts handcrafted features chosen from the library tsfresh FeatureSelector.py: Choose features using PCA MlpClassifier.py: Classifies the features using keras Multi Layer Perceptron (MLP) All process-specific settings can be read in the report (MLP settings or handcrafted features). ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ad"}, "repo_url": "https://github.com/kapitsa2811/wsf", "repo_name": "wsf", "repo_full_name": "kapitsa2811/wsf", "repo_owner": "kapitsa2811", "repo_desc": "figDetection", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T06:42:04Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T02:17:17Z", "homepage": null, "size": 35, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185304330, "is_fork": false, "readme_text": "Image Retrieval Engine Based on Keras  \u6f14\u793a \u6f14\u793a\u5730\u5740(\u4e0d\u80fd\u8bbf\u95ee\uff0c\u6ca1\u94b1\u7eedVPS\u4e86)\uff0c\u8dd1\u5728CPU\u4e0a\uff0cweb\u754c\u9762\u91c7\u7528\u7684SoTu \u73af\u5883 In [1]: import keras Using Theano backend. keras 2.0.1 \u53ca 2.0.5 \u7248\u672c\u5747\u7ecf\u8fc7\u6d4b\u8bd5\u53ef\u7528\u3002\u63a8\u8350Python 2.7\uff0c\u652f\u6301Python 3.6. \u6b64\u5916\u9700\u8981numpy, matplotlib, os, h5py, argparse. \u63a8\u8350\u4f7f\u7528anaconda\u5b89\u88c5 \u4f7f\u7528  \u6b65\u9aa4\u4e00  python index.py -database <path-to-dataset> -index <name-for-output-index>  \u6b65\u9aa4\u4e8c  python query_online.py -query <path-to-query-image> -index <path-to-index-flie> -result <path-to-images-for-retrieval> \u251c\u2500\u2500 database \u56fe\u50cf\u6570\u636e\u96c6 \u251c\u2500\u2500 extract_cnn_vgg16_keras.py \u4f7f\u7528\u9884\u8bad\u7ec3vgg16\u6a21\u578b\u63d0\u53d6\u56fe\u50cf\u7279\u5f81 |\u2500\u2500 index.py \u5bf9\u56fe\u50cf\u96c6\u63d0\u53d6\u7279\u5f81\uff0c\u5efa\u7acb\u7d22\u5f15 \u251c\u2500\u2500 query_online.py \u5e93\u5185\u641c\u7d22 \u2514\u2500\u2500 README.md \u793a\u4f8b # \u5bf9database\u6587\u4ef6\u5939\u5185\u56fe\u7247\u8fdb\u884c\u7279\u5f81\u63d0\u53d6\uff0c\u5efa\u7acb\u7d22\u5f15\u6587\u4ef6featureCNN.h5 python index.py -database database -index featureCNN.h5  # \u4f7f\u7528database\u6587\u4ef6\u5939\u5185001_accordion_image_0001.jpg\u4f5c\u4e3a\u6d4b\u8bd5\u56fe\u7247\uff0c\u5728database\u5185\u4ee5featureCNN.h5\u8fdb\u884c\u8fd1\u4f3c\u56fe\u7247\u67e5\u627e\uff0c\u5e76\u663e\u793a\u6700\u8fd1\u4f3c\u76843\u5f20\u56fe\u7247 python query_online.py -query database/001_accordion_image_0001.jpg -index featureCNN.h5 -result database \u66f4\u65b0  \u9488\u5bf9\u8fd1\u671f\u6709\u5c0f\u4f19\u4f34\u53cd\u6620\u7684keras\u7248\u672c\u7684\u95ee\u9898\uff0c\u5df2\u5c06\u5176\u8fdb\u884c\u5230\u6700\u65b0\u7248\u672c\uff0c\u5e76\u4e14\u7279\u5f81\u63d0\u53d6\u4ee3\u7801\u5927\u5e45\u7cbe\u7b80\u3002 \u663e\u793a\u68c0\u7d22\u5f97\u5230\u7684\u56fe\u7247\uff0c \u53ef\u81ea\u7531\u5b9a\u4e49\u67e5\u8be2\u56fe\u7247\u53ca\u68c0\u7d22\u56fe\u7247\u96c6  Goal   \u91cd\u65b0\u7528flask\u5199web\u754c\u9762\uff0c\u5df2\u5b8c\u6210\u3002  \u8bba\u6587\u63a8\u8350 awesome-cbir-papers \u95ee\u9898\u6c47\u603b  query_online.py line 28\u62a5\u9519\uff0c\u5c06index.py line 62\u6ce8\u91ca\uff0c\u4f7f\u7528line 61.  ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://202.120.39.161:55555/"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ae"}, "repo_url": "https://github.com/ahammaa/DeepLearningApp", "repo_name": "DeepLearningApp", "repo_full_name": "ahammaa/DeepLearningApp", "repo_owner": "ahammaa", "repo_desc": "This mini project is used for Image Classification using the deep learning technique Convolutional Neural Networks (CNN).", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-13T08:53:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T06:39:06Z", "homepage": null, "size": 11227, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185337648, "is_fork": false, "readme_text": "This mini project is used for Image Classification using the deep learning technique Convolutional Neural Networks (CNN). The used dataset is MNIST dataset, which stands for Modified National Institute of Standards and Technology database. It is a large database of handwritten digits that is commonly used for training various image processing systems. Keras API is used. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano. It was developed with a focus on enabling fast experimentation. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830af"}, "repo_url": "https://github.com/Sotaneum/CocoToSTPhoto", "repo_name": "CocoToSTPhoto", "repo_full_name": "Sotaneum/CocoToSTPhoto", "repo_owner": "Sotaneum", "repo_desc": "COCO DataSet instances to STPhoto", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T14:19:42Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T04:57:52Z", "homepage": null, "size": 29, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185323710, "is_fork": false, "readme_text": "CocoToSTPhoto  COCO DataSet instances to STPhoto Copyright (c) 2019 InfoLab (Donggun LEE) https://pypi.org/project/cocotostphoto/  HOW TO USE python CocoToSTPhoto.py \"d:/instances\" \"c:/annotation/stphoto.json\" \"d:/images\" python CocoToSTPhoto.py \"d:/instances/instances_val2018.json\" \"c:/annotation/stphoto.json\", \"d:/images\"                                #input file or folder                   output file          image folder HOW TO INSTALL  requirement # python 3.6 -- tensorflow pip install tensorflow==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo  # python 3.6 -- tensorflow-gpu pip install tensorflow-gpu==1.9.0 exifread>=2.1.2 piexif>=1.1.2 pillow>=6.0.0 matplotlib>=3.1.0 scikit-image>=0.15.0 IPython>=7.5.0 keras>=2.2.4 cython>=0.29.7 deepgeo  install pip install cocotostphoto   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://infolab.kunsan.ac.kr", "http://duration.digimoon.net"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b0"}, "repo_url": "https://github.com/ironcarocto/controller_ironcar_octonomous", "repo_name": "controller_ironcar_octonomous", "repo_full_name": "ironcarocto/controller_ironcar_octonomous", "repo_owner": "ironcarocto", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-18T13:57:32Z", "repo_watch": 1, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-07T15:20:39Z", "homepage": null, "size": 2399, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 4, "github_id": 185428415, "is_fork": false, "readme_text": "IRONCAR Run it ! Launch the python script which will run the car: $ python3 simple_command.py This script allows for a few optionnal parameters. The full syntax is : $ python3 simple_command.py [-h] [--resolution RESOLUTION RESOLUTION]                          [--model-path PATH] [--speed SPEED] [--preview]                          [--regression] [--log-level LOGLEVEL] where :  -h: prints the help message --resolution / -r: the resolution (width height) of the camera. You must pass 2 integers after the --resolution option - for example, --resolution 100 50. Default = 240 176 --model-path / -m: absolute path to the Keras model. keras.load_model will be used to load the model, which must comply with this function requirements (if you used keras to save your model, you should be fine). Default = '/home/pi/ironcar/autopilots/octo240x123_nvidia.hdf5' --speed / -s: the ratio (from 0 to 1) of the max speed to be used. 1 means that the IronCar will be allowed to use the maximum hardware speed (which is very fast, start with a low value such as 0.2) Default = 0.2 --preview / -p: prints a preview of the picamera directly onto the terminal. Neat, but you probably don't want that in production. --regression / -R: assume a regression model (default is classification) --log-level / -l: the log level used (from CRITICAL to DEBUG). Default = INFO  Hardware setup You will find a tutorial on google docs here  . Raspberry-pi Setup Easy setup with install.sh You can easily setup everything on the raspi using the install.sh bash. To do so, go on your raspi and do: $ ./install.sh It will install keras, tensorflow, nodejs and some other dependencies in the requirements. This should take 2-3 hours... (scipy is very long to install). At the end of the install, you will need to choose if you want to enable the pi camera, i2c connections and augment the swap size (which is very small by default). And that's it, you should be ready to go to the launching part!! Manual setup You can install the requirements from requirements_raspi.txt yourself, but you will need to install tensorflow as well as nodejs and npm. You will also need to install the node packages from package.json. Last you will need to configure your camera and any other device to be enabled on the raspi. Laptop Setup You need to install the requirements_laptop.txt on your laptop only if you want to train your car with a gamepad and with the controller.py script. You can do it like this: $ pip3 install requirements_laptop.txt Otherwize, there is nothing needed for this part on the laptop, you will only use your browser to connect to the raspi via a node client. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/ironcarocto/controller_ironcar_octonomous/blob/18c7119c75ac7075232929707e71534a699fbc42/autopilots/autopilot_500k.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b1"}, "repo_url": "https://github.com/zxh1993/tf_keras_example", "repo_name": "tf_keras_example", "repo_full_name": "zxh1993/tf_keras_example", "repo_owner": "zxh1993", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T09:06:47Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T06:06:21Z", "homepage": null, "size": 96, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185331665, "is_fork": false, "readme_text": "tensorflow keras ", "has_readme": true, "readme_language": null, "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b2"}, "repo_url": "https://github.com/Debayon/Image-Classifier", "repo_name": "Image-Classifier", "repo_full_name": "Debayon/Image-Classifier", "repo_owner": "Debayon", "repo_desc": "Deep Learning, Convolution Neural Net, Image Classifier", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T09:09:16Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T09:05:58Z", "homepage": "", "size": 3, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185364350, "is_fork": false, "readme_text": "Image-Classifier  Convolutoin Neural Network used. Dependencies: Keras, Pandas, Numpy  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b3"}, "repo_url": "https://github.com/riroan/myKerasModel", "repo_name": "myKerasModel", "repo_full_name": "riroan/myKerasModel", "repo_owner": "riroan", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T18:40:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T16:25:01Z", "homepage": null, "size": 718, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185439396, "is_fork": false, "readme_text": "myKerasModel my first Keras Model dataset from kaggle ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/riroan/myKerasModel/blob/a5c16312174059e66506acd0c63c373a157280ac/2.%20alphabet_recognization/alphabet_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b4"}, "repo_url": "https://github.com/gafergus/CV_scientist", "repo_name": "CV_scientist", "repo_full_name": "gafergus/CV_scientist", "repo_owner": "gafergus", "repo_desc": "A framework for rapidly testing CV neural network models in Keras + TF", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-21T17:12:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T17:51:41Z", "homepage": "", "size": 76, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185451980, "is_fork": false, "readme_text": "CV_scientist A framework for rapidly testing CV neural network models in Keras + TF ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b5"}, "repo_url": "https://github.com/evancam/MLInstrumentRecognition", "repo_name": "MLInstrumentRecognition", "repo_full_name": "evancam/MLInstrumentRecognition", "repo_owner": "evancam", "repo_desc": "Using deep learning to perform automatic recognition of musical instruments in music", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-15T04:42:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T20:43:52Z", "homepage": null, "size": 807, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185475178, "is_fork": false, "readme_text": "MLInstrumentRecognition Using deep learning to perform automatic recognition of musical instruments in music Requirements keras tensorflow (GPU version) pillow numpy matplotlib glob scipy ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b6"}, "repo_url": "https://github.com/nric/ProximalPolicyOptimizationContinuousKeras", "repo_name": "ProximalPolicyOptimizationContinuousKeras", "repo_full_name": "nric/ProximalPolicyOptimizationContinuousKeras", "repo_owner": "nric", "repo_desc": "This is an Tensorflow 2.0 (Keras) implementation of a Open Ai's proximal policy optimization PPO algorithem for continuous action spaces.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T11:16:29Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T10:29:40Z", "homepage": null, "size": 25, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185379670, "is_fork": false, "readme_text": "ProximalPolicyOptimizationContinuousKeras This is an Tensorflow 2.0 (Keras) implementation of a Open Ai's proximal policy optimization PPO algorithem for continuous action spaces. Goal was to make it understanable yet not deviate from the original PPO idea: https://arxiv.org/abs/1707.06347 Part of the code base is from https://github.com/liziniu/RL-PPO-Keras . However, the code there had errors but mainly it did not use a GAE type reward and no entropy bonus system. I gave my best to comment the code but I did not include a fundamental lecutre on the logic behind PPO. I highly recommend to watch these two videos to undestand what happens: https://youtu.be/WxQfQW48A4A https://youtu.be/5P7I-xPq8u8 The most complete explenation and also part of the code (i.e. Memory Class) is from the open ai spinning up project: https://spinningup.openai.com/en/latest/algorithms/ppo.html I did NOT test this in depth. There might be errors. In a first attempt, the best score was somewhere around -70 for bipedap-walker which seems to show some leraning but not great learning. TODO / Next steps:  Try some parameters to find a reasonably quick leraning agent. Currently does not converge or only very slowly. try use tf.distribution to replace maual Probability Density and entropy calculations. Currently, the two outputs of actor (mu and sigma) are concatenated and then disassembled for the loss. Because the loss depends on both outputs at the same time (mu and sigma). I found this to be the only alternative to writing a custom train fuction with keras.function which seems not to work with TF 2.0 alpha. I should at least try to find a more elegant method. read and implement tf.probability layers independant_normal - does this even make sense here?  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1707.06347"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b7"}, "repo_url": "https://github.com/EuphoriaYan/ChatRobot-For-Keras2", "repo_name": "ChatRobot-For-Keras2", "repo_full_name": "EuphoriaYan/ChatRobot-For-Keras2", "repo_owner": "EuphoriaYan", "repo_desc": "Refer to https://github.com/shen1994/ChatRobot . Change some code so it can run in keras 2.", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-29T08:53:15Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T13:34:34Z", "homepage": null, "size": 13875, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185408832, "is_fork": false, "readme_text": "ChatRobot Raw repo: https://github.com/shen1994/ChatRobot Change some code so it can run in keras 2. 0. \u7279\u522b\u63d0\u9192 \u00a0   \u5173\u4e8ekeras\u73af\u5883\u4e0bseq2seq\u9519\u8bef\u4fee\u6539 ('_OptionalInputPlaceHolder' object has no attribute 'inbound_nodes') 0.0 \u4f7f\u7528keras2.1.0\u7248\u672c\u7684\u7b2c\u4e09\u65b9\u5e93(\u4e0d\u63a8\u8350) 0.1 \u5c06recurrentshop\\engine.py\u6587\u4ef6\u4e2d837\u548c842\u884c\u4e2dinbound_nodes\u66f4\u6539\u4e3a_inbound_nodes   1. \u6548\u679c\u5c55\u793a 1.0 python train.py\u6267\u884c\u6548\u679c\u56fe  1.1 python test.py\u6267\u884c\u6548\u679c\u56fe  1.2 python chat_robot.py\u6267\u884c\u6548\u679c\u56fe  2. \u8f6f\u4ef6\u5b89\u88c5   \u6a21\u578b\u642d\u5efa\u7b2c\u4e09\u65b9\u5e93Keras-2.1.6.tar.gz \u79c1\u4eba\u5730\u5740: \u94fe\u63a5: https://pan.baidu.com/s/1ypoEgf6ITjcNalzTRtnvmw \u5bc6\u7801: uot8 recurrentshop\u4e0b\u8f7d\u5730\u5740: https://github.com/farizrahman4u/recurrentshop seq2seq \u4e0b\u8f7d\u5730\u5740: https://github.com/farizrahman4u/seq2seq \u5fae\u535a\u6570\u636e(\u5173\u4e8e\u9910\u996e\u4e1a,\u6570\u636e\u672a\u6e05\u6d17)\u4e0b\u8f7d\u5730\u5740 \u79c1\u4eba\u5730\u5740: \u94fe\u63a5: https://pan.baidu.com/s/1g6l4_IDkLdLAjvrWf5sheQ \u5bc6\u7801: fxy3   3. \u53c2\u8003\u94fe\u63a5  seq2seq\u8bb2\u89e3: http://jacoxu.com/encoder_decoder seq2seq\u6570\u636e\u8bfb\u53d6: http://suriyadeepan.github.io/2016-06-28-easy-seq2seq seq2seq\u8bba\u6587\u5730\u5740: https://arxiv.org/abs/1409.3215 \u00a0 seq2seq+attention\u8bba\u6587\u5730\u5740: https://arxiv.org/abs/1409.0473 \u00a0 ChatRobot\u542f\u53d1\u8bba\u6587: https://arxiv.org/abs/1503.02364 seq2seq\u6e90\u7801: https://github.com/farizrahman4u/seq2seq \u00a0 seq2seq\u6e90\u7801\u9700\u6c42: https://github.com/farizrahman4u/recurrentshop \u00a0 beamsearch\u6e90\u7801\u53c2\u8003: https://github.com/yanwii/seq2seq bucket\u6e90\u7801\u53c2\u8003: https://github.com/1228337123/tensorflow-seq2seq-chatbot-zh  4. \u6267\u884c\u547d\u4ee4   \u751f\u6210\u5e8f\u5217\u6587\u4ef6,\u5c06\u6587\u5b57\u7f16\u7801\u4e3a\u6570\u5b57,\u4e0d\u8db3\u8865\u96f6 python data_process.py \u751f\u6210word2vec\u5411\u91cf,\u5305\u62ec\u7f16\u7801\u5411\u91cf\u548c\u89e3\u7801\u5411\u91cf python word2vec.py \u8bad\u7ec3\u7f51\u7edc python train.py \u6d4b\u8bd5 python test.py \u6a21\u578b\u8bc4\u5206 python score.py \u667a\u80fd\u95ee\u7b54 python chat_robot.py \u7ed8\u5236word2vec\u5411\u91cf\u5206\u5e03\u56fe python word2vec_plot.py   5. \u66f4\u65b0   Word2cut\u6a21\u578b\u5bf9\u964c\u751f\u8bcd\u6c47\u7684\u5206\u8bcd\u672a\u89e3\u51b3,\u6709\u65f6\u95f4\u641e\u5b9a\u4e00\u4e0b   ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/EuphoriaYan/ChatRobot-For-Keras2/blob/354c41d20693fd9213b7de643ba7626e0d00e736/model/seq2seq_model_weights.h5", "https://github.com/EuphoriaYan/ChatRobot-For-Keras2/blob/354c41d20693fd9213b7de643ba7626e0d00e736/word2cut/model/train_model.hdf5"], "see_also_links": ["http://jacoxu.com/encoder_decoder", "http://suriyadeepan.github.io/2016-06-28-easy-seq2seq"], "reference_list": ["https://arxiv.org/abs/1409.3215", "https://arxiv.org/abs/1409.0473", "https://arxiv.org/abs/1503.02364"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b8"}, "repo_url": "https://github.com/cgoliver/Async-DQN-MountainCar", "repo_name": "Async-DQN-MountainCar", "repo_full_name": "cgoliver/Async-DQN-MountainCar", "repo_owner": "cgoliver", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T10:47:31Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T01:36:32Z", "homepage": null, "size": 1108, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185298663, "is_fork": false, "readme_text": "Asyncronous RL in Tensorflow + Keras + OpenAI's Gym  This is a Tensorflow + Keras implementation of asyncronous 1-step Q learning as described in \"Asynchronous Methods for Deep Reinforcement Learning\". Since we're using multiple actor-learner threads to stabilize learning in place of experience replay (which is super memory intensive), this runs comfortably on a macbook w/ 4g of ram. It uses Keras to define the deep q network (see model.py), OpenAI's gym library to interact with the Atari Learning Environment (see atari_environment.py), and Tensorflow for optimization/execution (see async_dqn.py). Requirements  tensorflow gym [gym's atari environment] (https://github.com/openai/gym#atari) skimage Keras  Usage Training To kick off training, run: python async_dqn.py --experiment breakout --game \"Breakout-v0\" --num_concurrent 8  Here we're organizing the outputs for the current experiment under a folder called 'breakout', choosing \"Breakout-v0\" as our gym environment, and running 8 actor-learner threads concurrently. See this for a full list of possible game names you can hand to --game. Visualizing training with tensorboard We collect episode reward stats and max q values that can be vizualized with tensorboard by running the following: tensorboard --logdir /tmp/summaries/breakout  This is what my per-episode reward and average max q value curves looked like over the training period:   Evaluation To run a gym evaluation, turn the testing flag to True and hand in a current checkpoint file: python async_dqn.py --experiment breakout --testing True --checkpoint_path /tmp/breakout.ckpt-2690000 --num_eval_episodes 100  After completing the eval, we can upload our eval file to OpenAI's site as follows: import gym gym.upload('/tmp/breakout/eval', api_key='YOUR_API_KEY') Now we can find the eval at https://gym.openai.com/evaluations/eval_uwwAN0U3SKSkocC0PJEwQ Next Steps See a3c.py for a WIP async advantage actor critic implementation. Resources I found these super helpful as general background materials for deep RL:  David Silver's \"Deep Reinforcement Learning\" lecture Nervana's Demystifying Deep Reinforcement Learning blog post  Important notes  In the paper the authors mention \"for asynchronous methods we average over the best 5 models from 50 experiments\". I overlooked this point when I was writing this, but I think it's important. These async methods seem to vary in performance a lot from run to run (at least in my implementation of them!). I think it's a good idea to run multiple seeded versions at the same time and average over their performance to get a good picture of whether or not some architectural change is good or not. Equivalently don't get discouraged if you don't see performance on your task right away; try rerunning the same code a few more times with different seeds. This repo has no affiliation with Deepmind or the authors; it was just a simple project I was using to learn TensorFlow. Feedback is highly appreciated.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://videolectures.net/rldm2015_silver_reinforcement_learning/", "http://www.nervanasys.com/demystifying-deep-reinforcement-learning/"], "reference_list": ["http://arxiv.org/pdf/1602.01783v1.pdf"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830b9"}, "repo_url": "https://github.com/shivakrishna2497/Predicting-the-on-site-of-Diabetes-using-Keras-using-", "repo_name": "Predicting-the-on-site-of-Diabetes-using-Keras-using-", "repo_full_name": "shivakrishna2497/Predicting-the-on-site-of-Diabetes-using-Keras-using-", "repo_owner": "shivakrishna2497", "repo_desc": "I collected the Pima Indians onset of diabetes dataset from UCI Machine Learning repository,It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years, As such, it is a binary classification problem (onset of diabetes as 1 or not as 0).   I built my first neural network using keras which takes numerical input and numerical output  Number of Instances: 768  Number of Attributes: 8 plus class   For Each Attribute: (all numeric-valued)    1. Number of times pregnant    2. Plasma glucose concentration a 2 hours in an oral glucose tolerance test    3. Diastolic blood pressure (mm Hg)    4. Triceps skin fold thickness (mm)    5. 2-Hour serum insulin (mu U/ml)    6. Body mass index (weight in kg/(height in m)^2)    7. Diabetes pedigree function    8. Age (years)    9. Class variable (0 or 1)   Steps I followed in building a neural network using Keras:  1)Load Data  I have loaded the file directly using the NumPy function loadtxt(). There are eight input variables and one output variable ,Once loaded I split the dataset into input variables (X) and the output class variable (Y)  2)Define Network  I created a Sequential() model and added layers one at a time , first layer has 12 neurons and expects 8 input variables, second hidden layer has 8 neurons and finally, the output layer has 1 neuron to predict the class (onset of diabetes or not)  3) Compile Network  I Compiled the model using tensorflow as back-end,I used \u201cbinary_crossentropy\u201c(loss function) and  default gradient descent algorithm \u201cadam\u201d  4)Fit Network  I fit the network with my training set(80%) by calling the fit() function on the model,For this problem, I gave a small number of epochs=150 and a small batch size of 10  5)Evaluate Network  I evaluated the performance of the network on the same training dataset and got a training accuracy of 76.55% and after making predictions in next step I evaluated the performance of the network using testdataset where I got a testing accuracy of 76.62%  6)Make Predictions  predictions will be in the range between 0 and 1 as there's a sigmoid activation function on the output layer and I converted them into a binary prediction for this classification task by rounding them", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-20T15:33:07Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T06:53:26Z", "homepage": null, "size": 29, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185339955, "is_fork": false, "readme_text": "Predicting-the-on-site-of-Diabetes-using-Keras-using- I collected the Pima Indians onset of diabetes dataset from UCI Machine Learning repository,It describes patient medical record data for Pima Indians and whether they had an onset of diabetes within five years, As such, it is a binary classification problem (onset of diabetes as 1 or not as 0). I built my first neural network using keras which takes numerical input and numerical output Number of Instances: 768 Number of Attributes: 8 plus class For Each Attribute: (all numeric-valued)   Number of times pregnant   Plasma glucose concentration a 2 hours in an oral glucose tolerance test   Diastolic blood pressure (mm Hg)   Triceps skin fold thickness (mm)   2-Hour serum insulin (mu U/ml)   Body mass index (weight in kg/(height in m)^2)   Diabetes pedigree function   Age (years)   Class variable (0 or 1)   Steps I followed in building a neural network using Keras: 1)Load Data I have loaded the file directly using the NumPy function loadtxt(). There are eight input variables and one output variable ,Once loaded I split the dataset into input variables (X) and the output class variable (Y) 2)Define Network I created a Sequential() model and added layers one at a time , first layer has 12 neurons and expects 8 input variables, second hidden layer has 8 neurons and finally, the output layer has 1 neuron to predict the class (onset of diabetes or not)  Compile Network  I Compiled the model using tensorflow as back-end,I used \u201cbinary_crossentropy\u201c(loss function) and  default gradient descent algorithm \u201cadam\u201d 4)Fit Network I fit the network with my training set(80%) by calling the fit() function on the model,For this problem, I gave a small number of epochs=150 and a small batch size of 10 5)Evaluate Network I evaluated the performance of the network on the same training dataset and got a training accuracy of 76.55% and after making predictions in next step I evaluated the performance of the network using testdataset where I got a testing accuracy of 76.62% 6)Make Predictions Predictions will be in the range between 0 and 1 as there's a sigmoid activation function on the output layer and I converted them into a binary prediction for this classification task by rounding them ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ba"}, "repo_url": "https://github.com/JeremyBouhi/titanicWithFlaskAndDatmo", "repo_name": "titanicWithFlaskAndDatmo", "repo_full_name": "JeremyBouhi/titanicWithFlaskAndDatmo", "repo_owner": "JeremyBouhi", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T08:30:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T08:02:29Z", "homepage": null, "size": 39, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185352218, "is_fork": false, "readme_text": "titanicWithFlaskAndDatmo doc :  https://github.com/datmo/datmo pip install datmo datmo init  on doit cr\u00e9er un environnement de base : cpu/gpu? Quelle version de python? Pytorch/caffe/keras\u2026?? ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830bb"}, "repo_url": "https://github.com/prithviraju/howzthejosh", "repo_name": "howzthejosh", "repo_full_name": "prithviraju/howzthejosh", "repo_owner": "prithviraju", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T07:03:17Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-07T06:35:49Z", "homepage": null, "size": 793, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185337009, "is_fork": false, "readme_text": "How'z the Josh! This software recognizes human emotions from a video or webcam feed. Powered by OpenCV and Deep Learning. Installation Clone the repository: git clone https://github.com/prithviraju/howzthejosh.git cd howzthejosh/ pip3 install -r requirements.txt python3 emotions.py  To train new models for emotion classification  Download the fer2013.tar.gz file from here Move the downloaded file to the datasets directory inside this repository. Untar the file: tar -xzf fer2013.tar Download train_emotion_classifier.py from orriaga's repo here Run the train_emotion_classification.py file: python3 train_emotion_classifier.py  Deep Learning Model The model used is from this research paper written by Octavio Arriaga, Paul G. Pl\u00f6ger, and Matias Valdenegro.  Credit  Computer vision powered by OpenCV. Design powered by petercunha Neural network scaffolding powered by Keras with Tensorflow. Convolutional Neural Network (CNN) deep learning architecture is from this research paper. Pretrained Keras model and much of the OpenCV code provided by GitHub user oarriaga.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/prithviraju/howzthejosh/blob/e9b8faa33757c7711c766e9e6149f45420417459/models/emotion_model.hdf5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830bc"}, "repo_url": "https://github.com/Peppags/C_RNNCrispr", "repo_name": "C_RNNCrispr", "repo_full_name": "Peppags/C_RNNCrispr", "repo_owner": "Peppags", "repo_desc": "C-RNNCrispr", "description_language": "Quechua", "repo_ext_links": null, "repo_last_mod": "2019-05-07T12:13:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T12:02:26Z", "homepage": null, "size": 3975, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185393038, "is_fork": false, "readme_text": "C-RNNCrispr Overview C-RNNCrispr is a deep learning-based model for CRISPR/Cas9 sgRNA on-target activity prediction. Dependencies  Python 3.6.4 Keras 2.1.0 Tensorflow 1.4.0  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Peppags/C_RNNCrispr/blob/6561ec307275daef5564165ad5ea5784c665bc1f/weights/C_RNNCrispr_weights.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830bd"}, "repo_url": "https://github.com/bitsbuffer/Hand-Written-Text-Detection", "repo_name": "Hand-Written-Text-Detection", "repo_full_name": "bitsbuffer/Hand-Written-Text-Detection", "repo_owner": "bitsbuffer", "repo_desc": "Hand Written Text Detection on PDF documents using Faster R- CNN", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T18:27:40Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T19:12:42Z", "homepage": null, "size": 62961, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185463442, "is_fork": false, "readme_text": "Hand Written Text Detection on PDF documents using Faster R- CNN The current solution involves the use of Faster R-CNN with regional proposal network to detection hand written text on scanned pdf documents, the hand written text also involves finding of signature. We are trying to detect to groups of text  Signature Any other hand written text  The Faster R-CNN uses the pretrained Resnet50 model trained on COCO hand written dataset. The code is written in Keras with Tensorflow backend and is compatible with Python3. Data  We used a public repository of government contracts available here. https://www.gsa.gov/real-estate/real-estate-services/leasing-policy-procedures/lease-documents We used the VOTT data labeling tool to speed our data labeling task, available at this link. https://github.com/Microsoft/VoTT  Usage   train_frcnn.py can be used to train a model. To train on Pascal VOC data, simply do: python train_frcnn.py -p /path/to/pascalvoc/.   simple_parser.py provides an alternative way to input data, using a text file. Simply provide a text file, with each line containing: filepath,x1,y1,x2,y2,class_name For example: /data/imgs/img_001.jpg,837,346,981,456,cow The classes will be inferred from the file. To use the simple parser instead of the default pascal voc style parser, use the command line option -o simple. For example python train_frcnn.py -o simple -p my_data.txt.   Running train_frcnn.py will write weights to disk to an hdf5 file, as well as all the setting of the training run to a pickle file. These settings can then be loaded by test_frcnn.py for any testing.   test_frcnn.pycan be used to perform inference, given pretrained weights and a config file. Specify a path to the folder containing images: python test_frcnn.py -p /path/to/test_data/   NOTES:  config.py contains all settings for the train or test run. The default settings match those in the original Faster-RCNN paper. The anchor box sizes are [128, 256, 512] and the ratios are [1:1, 1:2, 2:1].  Example output    ISSUES:  The model is trained for a very limited number of epochs and data, hence the model will not be able to capture all handwritten text. If you run out of memory, try reducing the number of ROIs that are processed simultaneously. Try passing a lower -n to train_frcnn.py. Alternatively, try reducing the image size from the default value of 600 (this setting is found in config.py.  ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "keras", "faster-rcnn", "object-detection"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830be"}, "repo_url": "https://github.com/jolinxql/SpaceFusion", "repo_name": "SpaceFusion", "repo_full_name": "jolinxql/SpaceFusion", "repo_owner": "jolinxql", "repo_desc": "clone from https://github.com//golsun/SpaceFusion", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T20:12:03Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T20:08:06Z", "homepage": "https://github.com//golsun/SpaceFusion", "size": 111, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185470771, "is_fork": false, "readme_text": "SpaceFusion SpaceFusion is a learning paradigm proposed to align and structure the unstructured latent spaces learned by different models trained over different datasets. Of particular interest is its application to neural conversation modelling, where space fusion can be used to jointly optimize the relevance and diversity of generated responses. [New] A Keras implementation is provided here  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1902.11205"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830bf"}, "repo_url": "https://github.com/amoghmisra27/String-Play-Ground", "repo_name": "String-Play-Ground", "repo_full_name": "amoghmisra27/String-Play-Ground", "repo_owner": "amoghmisra27", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T12:10:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T11:36:10Z", "homepage": null, "size": 5, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185388839, "is_fork": false, "readme_text": "String-Play-Ground The following program recognises the images from the EMNIST dataset and catagorises them,Then from the testing dataset it recognises the characters and forms a string of length 50 then applies the following functions.  Morris Prat (KMP) String Matching Algorithm Longest Common Subsequence Longest Common Substring  Requirements: 1)Python Compilers 2)Tensorflow 3)Keras 4)Seaborn 5)Numpy 6)Pandas 7)Matplotlib 8)EMNIST ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c0"}, "repo_url": "https://github.com/13070151771/cat-eye-detection", "repo_name": "cat-eye-detection", "repo_full_name": "13070151771/cat-eye-detection", "repo_owner": "13070151771", "repo_desc": "\u732b\u773c\u68c0\u6d4b", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-07T07:08:09Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T06:17:42Z", "homepage": null, "size": 858240, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185333314, "is_fork": false, "readme_text": "\u57fa\u4e8eyolov3\u7684\u732b\u773c\u76ee\u6807\u68c0\u6d4b \u6dfb\u52a0map\u90e8\u5206\u3002windows\u53ef\u8fd0\u884c \u5bf9\u89c6\u9891\u76ee\u6807\u53ef\u7528kcf.py\u8ddf\u8e2a\u9ad8\u7f6e\u4fe1\u5ea6bbox  Download YOLOv3 weights from YOLO website. Convert the Darknet YOLO model to a Keras model. Run YOLO detection.  wget https://pjreddie.com/media/files/yolov3.weights python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5 python yolo_video.py [OPTIONS...] --image, for image detection mode, OR python yolo_video.py [video_path] [output_path (optional)]  For Tiny YOLOv3, just do in a similar way, just specify model path and anchor path with --model model_file and --anchors anchor_file. Usage Use --help to see usage of yolo_video.py: usage: yolo_video.py [-h] [--model MODEL] [--anchors ANCHORS]                      [--classes CLASSES] [--gpu_num GPU_NUM] [--image]                      [--input] [--output]  positional arguments:   --input        Video input path   --output       Video output path  optional arguments:   -h, --help         show this help message and exit   --model MODEL      path to model weight file, default model_data/yolo.h5   --anchors ANCHORS  path to anchor definitions, default                      model_data/yolo_anchors.txt   --classes CLASSES  path to class definitions, default                      model_data/coco_classes.txt   --gpu_num GPU_NUM  Number of GPU to use, default 1   --image            Image detection mode, will ignore all positional arguments    MultiGPU usage: use --gpu_num N to use N GPUs. It is passed to the Keras multi_gpu_model().  Training   Generate your own annotation file and class names file. One row for one image; Row format: image_file_path box1 box2 ... boxN; Box format: x_min,y_min,x_max,y_max,class_id (no space). For VOC dataset, try python voc_annotation.py Here is an example: path/to/img1.jpg 50,100,150,200,0 30,50,200,120,3 path/to/img2.jpg 120,300,250,600,2 ...    Make sure you have run python convert.py -w yolov3.cfg yolov3.weights model_data/yolo_weights.h5 The file model_data/yolo_weights.h5 is used to load pretrained weights.   Modify train.py and start training. python train.py Use your trained weights or checkpoint weights with command line option --model model_file when using yolo_video.py Remember to modify class path or anchor path, with --classes class_file and --anchors anchor_file.   If you want to use original pretrained weights for YOLOv3: 1. wget https://pjreddie.com/media/files/darknet53.conv.74 2. rename it as darknet53.weights 3. python convert.py -w darknet53.cfg darknet53.weights model_data/darknet53_weights.h5 4. use model_data/darknet53_weights.h5 in train.py  Some issues to know   The test environment is  Python 3.5.2 Keras 2.1.5 tensorflow 1.6.0    Default anchors are used. If you use your own anchors, probably some changes are needed.   The inference result is not totally the same as Darknet but the difference is small.   The speed is slower than Darknet. Replacing PIL with opencv may help a little.   Always load pretrained weights and freeze layers in the first stage of training. Or try Darknet training. It's OK if there is a mismatch warning.   The training strategy is for reference only. Adjust it according to your dataset and your goal. And add further strategy if needed.   For speeding up the training process with frozen layers train_bottleneck.py can be used. It will compute the bottleneck features of the frozen model first and then only trains the last layers. This makes training on CPU possible in a reasonable time. See this for more information on bottleneck features.   ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://pjreddie.com/darknet/yolo/"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c1"}, "repo_url": "https://github.com/Jeffery-Zhou/behavioral-cloning-fixed", "repo_name": "behavioral-cloning-fixed", "repo_full_name": "Jeffery-Zhou/behavioral-cloning-fixed", "repo_owner": "Jeffery-Zhou", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T05:04:36Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T03:15:40Z", "homepage": null, "size": 14040, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185312473, "is_fork": false, "readme_text": "End to End Learning for Self-Driving Cars The goal of this project was to train a end-to-end deep learning model that would let a car drive itself around the track in a driving simulator. Project structure    File Description     data.py Methods related to data augmentation, preprocessing and batching.   model.py Implements model architecture and runs the training pipeline.   model.json JSON file containing model architecture in a format Keras understands.   model.h5 Model weights.   weights_logger_callback.py Implements a Keras callback that keeps track of model weights throughout training.   drive.py Implements driving simulator callbacks, essentially communicates with the driving simulator app providing model predictions based on real-time data simulator app is sending.    Data collection and balancing The provided driving simulator had two different tracks. One of them was used for collecting training data, and the other one \u2014 never seen by the model \u2014 as a substitute for test set. The driving simulator would save frames from three front-facing \"cameras\", recording data from the car's point of view; as well as various driving statistics like throttle, speed and steering angle. We are going to use camera data as model input and expect it to predict the steering angle in the [-1, 1] range. I have collected a dataset containing approximately 1 hour worth of driving data around track 1. This would contain both driving in \"smooth\" mode (staying right in the middle of the road for the whole lap), and \"recovery\" mode (letting the car drive off center and then interfering to steer it back in the middle). Just as one would expect, resulting dataset was extremely unbalanced and had a lot of examples with steering angles close to 0. So I applied a designated random sampling which ensured that the data is as balanced across steering angles as possible. This process included splitting steering angles into n bins and using at most 200 frames for each bin: df = read_csv('data/driving_log.csv')  balanced = pd.DataFrame()  # Balanced dataset bins = 1000     # N of bins bin_n = 200     # N of examples to include in each bin (at most)  start = 0 for end in np.linspace(0, 1, num=bins):       df_range = df[(np.absolute(df.steering) >= start) & (np.absolute(df.steering) < end)]     range_n = min(bin_n, df_range.shape[0])     balanced = pd.concat([balanced, df_range.sample(range_n)])     start = end balanced.to_csv('data/driving_log_balanced.csv', index=False) Histogram of the resulting dataset looks fairly balanced across most \"popular\" angles.    Please, mind that we are balancing dataset across absolute values, as by applying horizontal flip during augmentation we end up using both positive and negative steering angles for each frame. Data augmentation and preprocessing After balancing ~1 hour worth of driving data we ended up with 7698 samples, which most likely wouldn't be enough for the model to generalise well. However, as many pointed out, there a couple of augmentation tricks that should let you extend the dataset significantly:  Left and right cameras. Along with each sample we receive frames from 3 camera positions: left, center and right. Although we are only going to use central camera while driving, we can still use left and right cameras data during training after applying steering angle correction, increasing number of examples by a factor of 3.  cameras = ['left', 'center', 'right'] steering_correction = [.25, 0., -.25] camera = np.random.randint(len(cameras)) image = mpimg.imread(data[cameras[camera]].values[i]) angle = data.steering.values[i] + steering_correction[camera]  Horizontal flip. For every batch we flip half of the frames horizontally and change the sign of the steering angle, thus yet increasing number of examples by a factor of 2.  flip_indices = random.sample(range(x.shape[0]), int(x.shape[0] / 2)) x[flip_indices] = x[flip_indices, :, ::-1, :] y[flip_indices] = -y[flip_indices]  Vertical shift. We cut out insignificant top and bottom portions of the image during preprocessing, and choosing the amount of frame to crop at random should increase the ability of the model to generalise.  top = int(random.uniform(.325, .425) * image.shape[0]) bottom = int(random.uniform(.075, .175) * image.shape[0]) image = image[top:-bottom, :]  Random shadow. We add a random vertical \"shadow\" by decreasing brightness of a frame slice, hoping to make the model invariant to actual shadows on the road.  h, w = image.shape[0], image.shape[1] [x1, x2] = np.random.choice(w, 2, replace=False) k = h / (x2 - x1) b = - k * x1 for i in range(h):     c = int((i - b) / k)     image[i, :c, :] = (image[i, :c, :] * .5).astype(np.int32) We then preprocess each frame by cropping top and bottom of the image and resizing to a shape our model expects (32\u00d7128\u00d73, RGB pixel intensities of a 32\u00d7128 image). The resizing operation also takes care of scaling pixel values to [0, 1]. image = skimage.transform.resize(image, (32, 128, 3)) To make a better sense of it, let's consider an example of a single recorded sample that we turn into 16 training samples by using frames from all three cameras and applying aforementioned augmentation pipeline.       Augmentation pipeline is applied using a Keras generator, which lets us do it in real-time on CPU while GPU is busy backpropagating! Model I started with the model described in Nvidia paper and kept simplifying and optimising it while making sure it performs well on both tracks. It was clear we wouldn't need that complicated model, as the data we are working with is way simpler and much more constrained than the one Nvidia team had to deal with when running their model. Eventually I settled on a fairly simple architecture with 3 convolutional layers and 3 fully connected layers.    This model can be very briefly encoded with Keras. from keras import models from keras.layers import core, convolutional, pooling  model = models.Sequential() model.add(convolutional.Convolution2D(16, 3, 3, input_shape=(32, 128, 3), activation='relu')) model.add(pooling.MaxPooling2D(pool_size=(2, 2))) model.add(convolutional.Convolution2D(32, 3, 3, activation='relu')) model.add(pooling.MaxPooling2D(pool_size=(2, 2))) model.add(convolutional.Convolution2D(64, 3, 3, activation='relu')) model.add(pooling.MaxPooling2D(pool_size=(2, 2))) model.add(core.Flatten()) model.add(core.Dense(500, activation='relu')) model.add(core.Dense(100, activation='relu')) model.add(core.Dense(20, activation='relu')) model.add(core.Dense(1)) I added dropout on 2 out of 3 dense layers to prevent overfitting, and the model proved to generalise quite well. The model was trained using Adam optimiser with a learning rate = 1e-04 and mean squared error as a loss function. I used 20% of the training data for validation (which means that we only used 6158 out of 7698 examples for training), and the model seems to perform quite well after training for ~20 epochs. Results The car manages to drive just fine on both tracks pretty much endlessly. It rarely goes off the middle of the road, this is what driving looks like on track 2 (previously unseen).    You can check out a longer video compilation of the car driving itself on both tracks. Clearly this is a very basic example of end-to-end learning for self-driving cars, nevertheless it should give a rough idea of what these models are capable of, even considering all limitations of training and validating solely on a virtual driving simulator. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/Jeffery-Zhou/behavioral-cloning-fixed/blob/54b8df742c25a2f7104c9557dd2ead236b862b00/model.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1604.07316"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c2"}, "repo_url": "https://github.com/AmbitionC/Tianchi_DiabetesPredict", "repo_name": "Tianchi_DiabetesPredict", "repo_full_name": "AmbitionC/Tianchi_DiabetesPredict", "repo_owner": "AmbitionC", "repo_desc": "\u5929\u6c60\u5927\u6570\u636e\u7ade\u8d5b\u6570\u636e\u96c6&\u4ee3\u7801", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-07T04:50:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T04:33:36Z", "homepage": null, "size": 20814, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185321341, "is_fork": false, "readme_text": "Tianchi_DiabetesPredict \u5929\u6c60\u5927\u6570\u636e\u7ade\u8d5b\u6570\u636e\u96c6&\u4ee3\u7801 Code 1.\u795e\u7ecf\u7f51\u7edc\u535a\u6587\u94fe\u63a5.txt \u2003\u2003\u795e\u7ecf\u7f51\u7edc\u76f8\u5173\u7684\u4e00\u7bc7\u535a\u6587 2.Reference \u2003\u2003\u6bd4\u8d5b\u8fc7\u7a0b\u4e2d\u7684\u53c2\u8003\u6587\u732e \u2003\u2003\u5176\u4e2d\u5305\u62ec\u6570\u636e\u9884\u5904\u7406\u3001BoxCox\u53d8\u6362\u7b49\u5185\u5bb9 3.\u6570\u636e\u53ef\u89c6\u5316 \u2003\u2003\u6bd4\u8d5b\u8fc7\u7a0b\u4e2d\uff0c\u5bf9\u7279\u5f81\u548c\u7ed3\u679c\u7684\u5206\u5e03\u4e00\u4e9b\u56fe \u2003\u2003\u7279\u5f81\u6743\u91cd\u7684\u7ed3\u679c\u56fe\u7b49 4.process&predict_data \u2003\u2003\u6bd4\u8d5b\u8fc7\u7a0b\u4e2d\u6570\u636e\u5904\u7406\u548c\u9884\u6d4b\u7684\u4ee3\u7801 \u2003\u2003\u9700\u8981\u6ce8\u610f\u7684\u662f\uff0c\u4ee3\u7801\u4e2d\u8bfb\u53d6\u6570\u636e\u7684\u8def\u5f84\u9700\u8981\u4fee\u6539 \u2003\u2003\u6570\u636e\u5904\u7406\u8fd0\u884c\u5f97\u5230\u7684\u5904\u7406\u540e\u7684\u6570\u636e\u96c6\u6587\u4ef6\u6ca1\u6709\u4fdd\u7559\uff0c\u9700\u8981\u81ea\u5df1\u8fd0\u884c\u540e\u83b7\u5f97\uff5e 5.keras_code \u2003\u2003\u91c7\u7528\u795e\u7ecf\u7f51\u7edckeras\u8fdb\u884c\u9884\u6d4b\u90e8\u5206\u7684\u4ee3\u7801 DataSet \u2003\u2003\u4e3b\u529e\u65b9\u63d0\u4f9b\u7684\u539f\u59cb\u6570\u636e\u96c6 Reference \u2003\u2003\u4eba\u5de5\u667a\u80fd\u8f85\u52a9\u7cd6\u5c3f\u75c5\u9057\u4f20\u98ce\u9669\u9884\u6d4b\u53c2\u8003\u6587\u732e \u2003\u2003\u6bd4\u8d5b\u4e3b\u529e\u65b9\u63d0\u4f9b\u53c2\u8003\u6587\u732e 2.\u77e5\u7f51\u4e0b\u8f7d\u6587\u732e \u2003\u2003\u77e5\u7f51\u4e0a\u4e0b\u8f7d\u7684\u76f8\u5173\u7684\u6587\u7ae0 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c3"}, "repo_url": "https://github.com/fyumoto/IT_detection_DL_LSTM", "repo_name": "IT_detection_DL_LSTM", "repo_full_name": "fyumoto/IT_detection_DL_LSTM", "repo_owner": "fyumoto", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-07T12:54:35Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T12:53:26Z", "homepage": null, "size": 75005, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185401639, "is_fork": false, "readme_text": "#prediction folder contains the implementation of stock market volatility prediction using LSTM Neural Network. Keras is used as a wrapper with Tensorflow backend. #run cd prediction python run.python #detection folder contains the implementation of anomalous time series detection using discrete signal processing. Matlab scripting language is used for the implementation. #run #open the script (deect_anomaly.m) with matlab and click the button run #from command line matlab -nodesktop -nosplash -r \"detect_anomaly\" #litigation-classifier-and-visualizations folder contains code for huge amount of unstructered data (e.g., litigations) precessing, #classification and visualizations. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c4"}, "repo_url": "https://github.com/cjspoerer/rcnn-sat", "repo_name": "rcnn-sat", "repo_full_name": "cjspoerer/rcnn-sat", "repo_owner": "cjspoerer", "repo_desc": "Recurrent convolutional neural networks for object recognition", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-01T09:24:38Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T13:17:45Z", "homepage": null, "size": 26, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185405885, "is_fork": false, "readme_text": "rcnn-sat Code for feedforward and recurrent neural network models used in this preprint (this link is a placeholder, preprint coming soon!). The code has been tested with TensorFlow 1.13 and Python 3.6. Using the code The following code snippet shows how to build the Keras model and generate a prediction for a random image. A full example of extracting activations from a pre-trained model is given here. import numpy as np import tensorflow as tf tf.enable_v2_behavior() from rcnn_sat import preprocess_image, bl_net  # create the model with randomly initialised weights input_layer = tf.keras.layers.Input((128, 128, 3)) model = bl_net(input_layer, classes=565, cumulative_readout=True)  # predict a random image img = np.random.randint(0, 256, [1, 128, 128, 3], dtype=np.uint8) model.predict(preprocess_image(img)) # softmax for each time step Pre-trained model weights The checkpoint files for pre-trained eco-set and imagenet models are hosted here. Notes on pre-trained models:  Pre-trained models expect 128 x 128 images as input. ImageNet models have classes=1000 and ecoset models have classes=565 BL models were trained with cumulative_readout=False but can be tested using either option Model predictions correspond to the order of the categories within the files in pretrained_output_categories.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c5"}, "repo_url": "https://github.com/chaitu67/Malaria-detection", "repo_name": "Malaria-detection", "repo_full_name": "chaitu67/Malaria-detection", "repo_owner": "chaitu67", "repo_desc": "image classification model that detects the presence of malaria", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-09T17:52:32Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T01:17:13Z", "homepage": "https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria", "size": 4, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185296104, "is_fork": false, "readme_text": "Malaria-detection image classification model that detects the presence of malaria The following steps need to be followed to start the project. -download the images from source(https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria) -save it on you loacal machine -open the python file which is present. -change the path in the program to the orginal path where you have saved the images References source-https://www.kaggle.com/iarunava/cell-images-for-detecting-malaria Concepts-https://medium.com/@RaghavPrabhu/understanding-of-convolutional-neural-network-cnn-deep-learning-99760835f148 Implimentation-https://towardsdatascience.com/building-a-convolutional-neural-network-cnn-in-keras-329fbbadc5f5 ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c6"}, "repo_url": "https://github.com/suemarsouza/GCP_CloudMLSamples", "repo_name": "GCP_CloudMLSamples", "repo_full_name": "suemarsouza/GCP_CloudMLSamples", "repo_owner": "suemarsouza", "repo_desc": "Samples got from gcp quicklabs", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T00:19:20Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T00:05:38Z", "homepage": null, "size": 84040, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185289069, "is_fork": false, "readme_text": "AI Platform Training and Prediction  Welcome to the AI Platform Training and Prediction sample code repository. This repository contains samples for how to use AI Platform for model training and serving. Note: If you\u2019re looking for our guides on how to do Machine Learning on Google Cloud Platform (GCP) using other services, please checkout our other repository: ML on GCP, which has guides on how to bring your code from various ML frameworks to Google Cloud Platform using things like Google Compute Engine or Kubernetes. Overview The repository is organized by tasks:  Training Prediction Training and Prediction  Each task can be broken down to general usage (CPU/GPU) to specific features:  Hyperparameter Tuning TPUs  Scroll down to see what we have available, each task may provide a notebook or code solution. Where the code solution will have a README guide and the notebook solution is a full walkthrough. Our code guides are designed to provide you with the code and instructions on how to run the code, but leave you to do the digging, where our notebook tutorials try to walk you through the whole process by having the code available in the notebook throughout the guide. If you don\u2019t see something for the task you\u2019re trying to complete, please head down to our section What do you want to see? Setup For installation instructions and overview, please see the documentation. Please refer to README.md in each sample directory for more specific instructions. Getting Started If this is your first time using AI Platform, we suggest you take a look at the Introduction to AI Platform docs to get started. AI Platform Training Notebook Tutorial:  scikit-learn: Random Forest Classifier - How to train a Random Forest Classifier in scikit-learn using a text based dataset, Census, to predict a person\u2019s income level. XGBoost - How to train an XGBoost model using a text based dataset, Census, to predict a person\u2019s income level.  Code Guide:  Tensorflow: Linear Classifier with Stochastic Dual Coordinate Ascent (SDCA) Optimizer / Deep Neural Network Classifier - How to train a Linear Classifier with SDCA and a DNN using a text (discrete feature) based dataset, Criteo, to predict how likely a person is to click on an advertisement. Tensorflow: Linear Regression with Stochastic Dual Coordinate Ascent (SDCA) / Deep Neural Network Regressor - How to train a Linear Regressor with SDCA and a DNN using the a text based dataset of Reddit Comments to predict the score of a Reddit thread using a wide and deep model. Tensorflow: ResNet - How to train a model for image recognition using the CIFAR10 dataset to classify image content (training on one CPU, a single host with multiple GPUs, and multiple hosts with CPU or multiple GPUs).  Cloud TPUs Tensor Processing Units (TPUs) are Google\u2019s custom-developed ASICs used to accelerate machine-learning workloads. You can run your training jobs on AI Platform, using Cloud TPU.  Tensorflow: ResNet - Using the ImageNet dataset with Cloud TPUs on AI Platform. Tensorflow: HP Tuning - ResNet - How to run hyperparameter tuning jobs on AI Platform with Cloud TPUs using TensorFlow's tf.metrics. Tensorflow: Hypertune - ResNet - How to run hyperparameter tuning jobs on AI Platform with Cloud TPUs using the cloudml-hypertune package. Tensorflow: Cloud TPU Templates - A collection of minimal templates that can be run on Cloud TPUs on Compute Engine, Cloud Machine Learning, and Colab.  Hyperparameter Tuning (HP Tuning) Notebook Tutorial:  scikit-learn: Lasso Regressor - How to train a Lasso Regressor in scikit-learn using a text based dataset, auto mpg, to predict a car's miles per gallon. XGBoost: XGBRegressor - How to train a Regressor in XGBoost using a text based dataset, auto mpg, to predict a car's miles per gallon.  Containers  Keras: Sequential / Dense - How to train a Keras model using the Nightly Build of TensorFlow on AI Platform using a structured dataset, sonar signals, to predict whether the given sonar signals are bouncing off a metal cylinder or off a cylindrical rock. PyTorch: Deep Neural Network - How to train a PyTorch model on AI Platform using a custom container with a image dataset, mnist, to classify handwritten digits. PyTorch: Sequential - How to train a PyTorch model on AI Platform using a custom container with a structured dataset, sonar signals, to predict whether the given sonar signals are bouncing off a metal cylinder or off a cylindrical rock. PyTorch: Sequential / HP Tuning - How to train a PyTorch model on AI Platform using a custom container and Hyperparameter Tuning with a structured dataset, sonar signals, to predict whether the given sonar signals are bouncing off a metal cylinder or off a cylindrical rock.  AI Platform Prediction (Online Predictions) Notebook Tutorial:  scikit-learn: Model Serving - How to train a Random Forest Classifier in scikit-learn on your local machine using a text based dataset, Census, to predict a person\u2019s income level and deploy it on AI Platform to create predictions. XGBoost: Model Serving -  How to train an XGBoost model on your local machine using a text based dataset, Census, to predict a person\u2019s income level and deploy it on AI Platform to create predictions.  Complete Guide: Model Training and Prediction on AI Platform Code Guide:  Tensorflow: Deep Neural Network Regressor - How to train a DNN on a text based molecular dataset from Kaggle to predict the molecular energy. Tensorflow: Softmax / Fully-connected layer - How to train a fully connected model with softmax using an image dataset of flowers to recognize the type of a flower from its image.  Hyperparameter Tuning (HP Tuning) Code Guide:  Keras: Sequential / Dense - Keras - How to train a Keras sequential and Dense model using a text based dataset, Census, to predict a person\u2019s income level using a single node model. Tensorflow Pre-made Estimator: Deep Neural Network Linear Combined Classifier -How to train a DNN using Tensorflow\u2019s Pre-made Estimators using a text based dataset, Census, to predict a person\u2019s income level. TensorFlow Pre-made Estimator, an estimator is \u201ca high-level TensorFlow API that greatly simplifies machine learning programming.\u201d Tensorflow Custom Estimator: Deep Neural Network - How to train a DNN using Tensorflow\u2019s Custom Estimators using a text based dataset, Census, to predict a person\u2019s income level. TensorFlow Custom Estimator, which is when you write your own model function. Tensorflow: Deep Neural Network - How to train a DNN using TensorFlow\u2019s low level APIs to create your DNN model on a single node using a text based dataset, Census, to predict a person\u2019s income level. Tensorflow: Matrix Factorization / Deep Neural Network with Softmax - How to train a Matrix Factorization and DNN with Softmax using a text based dataset, MovieLens 20M, to make movie recommendations.  Templates   TensorFlow Estimator Trainer Package Template - When training a Tensorflow model, you have to create a trainer package, here we have a template that simplifies creating a trainer package for AI Platform. Take a look at this list with some introductory examples.   Tensorflow: Cloud TPU Templates - A collection of minimal templates that can be run on Cloud TPUs on Compute Engine, Cloud Machine Learning, and Colab.   Scikit-learn Pipelines Trainer Package Template - You can use this as starter code to develop a scikit-learn model for training and prediction on AI Platform. Examples to be added.   Additional Resources   Cloud TPU   Please see the Cloud TPU guide for how to use Cloud TPU.   Google Samples  Genomics Ancestry Inference - Genomics ancestry inference using 1000 Genomes dataset    What do you want to see? If you came looking for a sample we don\u2019t have, please file an issue using the Sample / Feature Request template on this repository. Please provide as much detail as possible about the AI Platform sample you were looking for, what framework (Tensorflow, Keras, scikit-learn, XGBoost, PyTorch...), the type of model, and what kind of dataset you were hoping to use! Jump below if you want to contribute and add that missing sample. How to contribute? We welcome external sample contributions! To learn more about contributing new samples, checkout our CONTRIBUTING.md guide. Please feel free to add new samples that are built in notebook form or code form with a README guide. Want to contribute but don't have an idea? Check out our Sample Request Page and assign the issue to yourself so we know you're working on it! Documentation We host AI Platform documentation here ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c7"}, "repo_url": "https://github.com/wencoast/Deep_learning_1st_project", "repo_name": "Deep_learning_1st_project", "repo_full_name": "wencoast/Deep_learning_1st_project", "repo_owner": "wencoast", "repo_desc": "AlexNet and  ResNet  performance comparison", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-26T05:19:04Z", "repo_watch": 1, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T11:44:11Z", "homepage": "https://github.com/wencoast/Deep_learning_1st_project", "size": 31363, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185390058, "is_fork": false, "readme_text": "Deep_learning_1st_project Simple neural network and ResNet performance comparison Running Environment  python 3.6.7 Tensorflow 1.12.0 keras 2.1.6-tf  Running the Code for SNN    Path Function     SNN/snn_fashion_mnist.py The original structure and hyperparameters   SNN/snn_fashion_mnist_lr_optimizer.py Compare Momentum and Adam with different learning rates   SNN/snn_fashion_mnist_dropout.py Compare different dropout rates   SNN/snn_fashion_mnist_optimal.py The final model with optimal settings    TODO List  Comparison of Deep network architectures Simple neural network and ResNet.[Done!] Using Momentum optimizer and Adam optimizer with different learning rate.[Done!] Using dropout.[Done!] Using batch normalization.[Done!] Using different activation functions including relu, tanh, leaky_relu, Sigmoid, etc. [Done!]  Using data augmentation.[Done!] Using different optimizers such as ADAGRAD, ADADELTA, ADAM, RMSPROP, MOM. [Done!] Using local response normalization.[Done!]  ", "has_readme": true, "readme_language": "English", "repo_tags": ["deep-learning", "python", "keras", "inception-resnet", "tensorflow", "resnet"], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c8"}, "repo_url": "https://github.com/YousefAlKafif/CNNmodel", "repo_name": "CNNmodel", "repo_full_name": "YousefAlKafif/CNNmodel", "repo_owner": "YousefAlKafif", "repo_desc": "A convolutional neural network model I created to classify cats and dogs. #Built using Keras that ran on TensorFlow framework", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-17T02:53:18Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T03:26:48Z", "homepage": "", "size": 8276, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185313993, "is_fork": false, "readme_text": "CNNmodel This is a convolutional neural network model I created to classify cats and dogs. I used the Keras API and ran on the TensorFlow backend. I managed to achieve a final validation accuracy of 87.3 % and training accuracy of 95.25% , indicating that the model has slightly overfitted. Some conventional methods I used that increased my accuracy :-  Building a fully connected ANN ontop of the CNN layers. Which used 'Adam' as the gradient descent algorithm. Data augmentation to ensure a more robust model is trained, also because my dataset only consisted of 5,000 images of cats and 5,000 images of dogs. Multiple layers of Convolution + Max Pooling in order to reduce size whilst retaining valuable data - thus increasing computational speed and accounting for spatial variance. The 'relu' rectifier activation function to eliminate negative values, in order to decrease linearity. Doubling the # of feature detectors in consecutive convolutional layers. The 'dropout' method with a rate of 0.5 in the fully connected ANN layer to prevent overfitting.  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/YousefAlKafif/CNNmodel/blob/5637f6afe5748431055052ab332c1f712b33e176/cat_or_dog_model.h5"], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830c9"}, "repo_url": "https://github.com/ilhanturkmen/Crime_Prediction_using_MachineLearningAlgorithms", "repo_name": "Crime_Prediction_using_MachineLearningAlgorithms", "repo_full_name": "ilhanturkmen/Crime_Prediction_using_MachineLearningAlgorithms", "repo_owner": "ilhanturkmen", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-11T07:19:45Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T15:55:55Z", "homepage": null, "size": 10540, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185434767, "is_fork": false, "readme_text": "Crime Prediction using Machine Learning Algorithms Installation process is for Mac IOS Install brew ruby -e \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install) Install Python3 brew install python3 enter \"pip3\" on the terminal if an error shows up install it like this: brew postinstall python3 Now you need to install the packages using pip3, you can do it like this: pip3 install numpy,tqdm,pandas,scikit-learn,tensorflow,keras,seaborn,matplotlib,csv,os,pickle If any import error occurs, kindly install that library using this syntax \"pip3 install library-name\" After installing everything you just need to go to the folder, run get_data file loke this:- python3 get_data.py Then you can run LR or laso or rnd_frst with the same syntax. Each algo will train and test on the data and will tell mean absolute error. tsne_vis and lle_vis will create data's visualization tsne and lle should be run before tsne_vis and lle_vis plot_gmap will make a map.html file which when run and zoomed will show the heatmap. density_plot wil make density_plot of the data. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ca"}, "repo_url": "https://github.com/laurialho/visual-reward", "repo_name": "visual-reward", "repo_full_name": "laurialho/visual-reward", "repo_owner": "laurialho", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-25T18:37:43Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T20:04:58Z", "homepage": null, "size": 76, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185470362, "is_fork": false, "readme_text": "Visual Reward for Autonomous Driving All in one package for demonstrating visual reward in reinforcement learning with Carla driving simulator. This code is used in bachelor's thesis Visual Reward for Autonomous Driving (http://bsc.laurialho.fi). How to install Download and install all used progams and libraries, which are listed beneath. After that, copy the files from this repository into Carla folder. How to run  Start Carla server with arguments listed in sub section 'Start arguments for server'. Create demonstration videos for visual reward. Create them by driving the route with make_demonstration.py. Run main.py script with arguments. You can take a look of start arguments with --help command. Default arguments also exist.  Used programs and libraries Precompiled Carla 0.9.4 for Windows (http://carla.org/2019/03/01/release-0.9.4/) python==3.7.0 keras==2.2.4 numpy==1.16.2 tensorflow-gpu==1.13.1 sklearn==0.0 opencv==3.4.2 numpy==1.16.2 matplotlib==3.0.3 sklearn==0.0 pygame==1.9.6 Start arguments for server CarlaUE4.exe /Game/Carla/Maps/Town03 -windowed -ResX=960 -ResY=960 -benchmark -fps=60 -carla-server -carla-settings=\"settings.ini\" Tested environment Tested to work with following setup: Windows Server 2019, Intel Core i7-7820X, 64 GB RAM, 2x GTX 1080 Ti, 512 GB NVME SSD. License Take a look of license file. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://bsc.laurialho.fi", "http://carla.org/2019/03/01/release-0.9.4/"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830cb"}, "repo_url": "https://github.com/ThomasWangWeiHong/Large-Scale-Palm-Tree-Detection-in-High-Resolution-Satellite-Images-Using-U-Net", "repo_name": "Large-Scale-Palm-Tree-Detection-in-High-Resolution-Satellite-Images-Using-U-Net", "repo_full_name": "ThomasWangWeiHong/Large-Scale-Palm-Tree-Detection-in-High-Resolution-Satellite-Images-Using-U-Net", "repo_owner": "ThomasWangWeiHong", "repo_desc": "Python implementation of Convolutional Neural Network (CNN) proposed in academic paper", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T10:35:56Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-07T10:28:47Z", "homepage": null, "size": 6, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185379542, "is_fork": false, "readme_text": "Large-Scale-Palm-Tree-Detection-in-High-Resolution-Satellite-Images-Using-U-Net Python implementation of Convolutional Neural Network (CNN) proposed in academic paper This repository includes functions to preprocess the input images and their respective polygons so as to create the input image patches and mask patches to be used for model training. The CNN used here is the modified u - net model implemented in the paper 'Large Scale Palm Tree Detection in High Resolution Satellite Images Using U - Net' by Freudenberg M., Nolke N., Agostini A., Urban K., Worgotter F., Kleinn C. (2019). The main differences between the implementations in the paper and the implementation in this repository is as follows:  Sigmoid layer is used as the last layer instead of the softmax layer, in consideration of the fact that this is a binary classification problem The dice coefficient function is used as the loss function in place of the binary cross - entropy loss function, in consideration of the fact that this is a semantic segmentation problem, whereby emphasis should be placed on accuracy of target delineation No cropping is done for both training and inference processes, in order to speed up the prediction process without significant accuracy loss  Requirements:  cv2 glob json numpy rasterio keras (tensorflow backend)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830cc"}, "repo_url": "https://github.com/subhashverma/CNN-Based-automated-weed-removal-Bot-using-Raspberry-Pi-3", "repo_name": "CNN-Based-automated-weed-removal-Bot-using-Raspberry-Pi-3", "repo_full_name": "subhashverma/CNN-Based-automated-weed-removal-Bot-using-Raspberry-Pi-3", "repo_owner": "subhashverma", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-11T11:03:42Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-07T06:02:37Z", "homepage": null, "size": 18288, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185331172, "is_fork": false, "readme_text": "CNN-Based-automated-weed-removal-Bot-using-Raspberry-Pi-3 The paper presents a fully convolutional neural network model for semantic pixel-wise segmentation. In this paper, we propose a crop-weed classification system for potato which is row vegetation the model incorporates spatial information by considering image sequences. Exploiting the crop arrangement information that is observed from the image sequences enables our system to robustly estimate into crop and weed, i.e., a semantic segmentation. After classifying the contours in the captured image by the robot, the robotic cutter removes the weed. We provide a thorough experimental evaluation, which shows that our system generalizes well to previously unseen fields under varying environmental conditions such as variation in light and alignment a key capability to actually use such systems in precision farming. We provide a robust system that classifies best compared to the already existing systems which focus on image processing and supervised learning algorithms. The location of the weed is found through image processing and then weed cutter reaches the desired coordinate and defoliate the unwanted plant. Software Dependencies  opencv python3.5 Tensorflow Keras Numpy Pandas Plantcv (https://github.com/danforthcenter/plantcv) Raspbian OS  Collaborators  Subhash Chander Verma (Student, ECE, AIT) Tejaswini T M (Student, ECE, AIT) Mr. Siddesh M.B. (Asst. Professor, ECE, AIT) Mrs. Kruthika KR (Asst. Professor, ECE, AIT)  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830cd"}, "repo_url": "https://github.com/JosseArturo/CarND-Behavioral-Cloning-P3", "repo_name": "CarND-Behavioral-Cloning-P3", "repo_full_name": "JosseArturo/CarND-Behavioral-Cloning-P3", "repo_owner": "JosseArturo", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-29T04:10:50Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T03:02:35Z", "homepage": null, "size": 30061, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185310722, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/JosseArturo/CarND-Behavioral-Cloning-P3/blob/d81f8486389b7fdbb2c03dc36981f1196bfae82a/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830ce"}, "repo_url": "https://github.com/hoongjai/Selfdriving_behaviour_cloning", "repo_name": "Selfdriving_behaviour_cloning", "repo_full_name": "hoongjai/Selfdriving_behaviour_cloning", "repo_owner": "hoongjai", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-12T02:36:00Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T12:16:13Z", "homepage": null, "size": 58769, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 0, "github_id": 185395345, "is_fork": false, "readme_text": "Behavioral Cloning Project  Overview This repository contains starting files for the Behavioral Cloning Project. In this project, you will use what you've learned about deep neural networks and convolutional neural networks to clone driving behavior. You will train, validate and test a model using Keras. The model will output a steering angle to an autonomous vehicle. We have provided a simulator where you can steer a car around a track for data collection. You'll use image data and steering angles to train a neural network and then use this model to drive the car autonomously around the track. We also want you to create a detailed writeup of the project. Check out the writeup template for this project and use it as a starting point for creating your own writeup. The writeup can be either a markdown file or a pdf document. To meet specifications, the project will require submitting five files:  model.py (script used to create and train the model) drive.py (script to drive the car - feel free to modify this file) model.h5 (a trained Keras model) a report writeup file (either markdown or pdf) video.mp4 (a video recording of your vehicle driving autonomously around the track for at least one full lap)  This README file describes how to output the video in the \"Details About Files In This Directory\" section. Creating a Great Writeup A great writeup should include the rubric points as well as your description of how you addressed each point.  You should include a detailed description of the code used (with line-number references and code snippets where necessary), and links to other supporting documents or external references.  You should include images in your writeup to demonstrate how your code works with examples. All that said, please be concise!  We're not looking for you to write a book here, just a brief description of how you passed each rubric point, and references to the relevant code :). You're not required to use markdown for your writeup.  If you use another method please just submit a pdf of your writeup. The Project The goals / steps of this project are the following:  Use the simulator to collect data of good driving behavior Design, train and validate a model that predicts a steering angle from image data Use the model to drive the vehicle autonomously around the first track in the simulator. The vehicle should remain on the road for an entire loop around the track. Summarize the results with a written report  Dependencies This lab requires:  CarND Term1 Starter Kit  The lab enviroment can be created with CarND Term1 Starter Kit. Click here for the details. The following resources can be found in this github repository:  drive.py video.py writeup_template.md  The simulator can be downloaded from the classroom. In the classroom, we have also provided sample data that you can optionally use to help train your model. Details About Files In This Directory drive.py Usage of drive.py requires you have saved the trained model as an h5 file, i.e. model.h5. See the Keras documentation for how to create this file using the following command: model.save(filepath) Once the model has been saved, it can be used with drive.py using this command: python drive.py model.h5 The above command will load the trained model and use the model to make predictions on individual images in real-time and send the predicted angle back to the server via a websocket connection. Note: There is known local system's setting issue with replacing \",\" with \".\" when using drive.py. When this happens it can make predicted steering values clipped to max/min values. If this occurs, a known fix for this is to add \"export LANG=en_US.utf8\" to the bashrc file. Saving a video of the autonomous agent python drive.py model.h5 run1 The fourth argument, run1, is the directory in which to save the images seen by the agent. If the directory already exists, it'll be overwritten. ls run1  [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_424.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_451.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_477.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_528.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_573.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_618.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_697.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_723.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_749.jpg [2017-01-09 16:10:23 EST]  12KiB 2017_01_09_21_10_23_817.jpg ... The image file name is a timestamp of when the image was seen. This information is used by video.py to create a chronological video of the agent driving. video.py python video.py run1 Creates a video based on images found in the run1 directory. The name of the video will be the name of the directory followed by '.mp4', so, in this case the video will be run1.mp4. Optionally, one can specify the FPS (frames per second) of the video: python video.py run1 --fps 48 Will run the video at 48 FPS. The default FPS is 60. Why create a video  It's been noted the simulator might perform differently based on the hardware. So if your model drives succesfully on your machine it might not on another machine (your reviewer). Saving a video is a solid backup in case this happens. You could slightly alter the code in drive.py and/or video.py to create a video of what your model sees after the image is processed (may be helpful for debugging).  Tips  Please keep in mind that training images are loaded in BGR colorspace using cv2 while drive.py load images in RGB to predict the steering angles.  How to write a README A well written README file can enhance your project and portfolio.  Develop your abilities to create professional README files by completing this free course. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/hoongjai/Selfdriving_behaviour_cloning/blob/3419179a3c626b2ee3a46c83e6c685daecb52718/model.h5"], "see_also_links": ["http://www.udacity.com/drive"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830cf"}, "repo_url": "https://github.com/monash128/Face-Recognition-using-FaceNet", "repo_name": "Face-Recognition-using-FaceNet", "repo_full_name": "monash128/Face-Recognition-using-FaceNet", "repo_owner": "monash128", "repo_desc": "Detect and Recognize human faces from videos using FaceNet", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-07T20:15:19Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T19:30:43Z", "homepage": null, "size": 13671, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185465809, "is_fork": false, "readme_text": "Face-Recognition-using-FaceNet Facial Recognition This code helps in facial recognition using facenets (https://arxiv.org/pdf/1503.03832.pdf). The main concepts talked about triplet loss function to compare images of different person. I have used haar cascade for detecting face from videos. It is faster but MTCNN has better accuracy. MTCNN requires resources to run efficiently. Description A facial recognition system is a technology capable of identifying or verifying a person from a digital image or a video frame from a video source. There are multiples methods in which facial recognition systems work, but in general, they work by comparing selected facial features from given image with faces within a database. Python Implementation Network Used - Inception Network Original Paper - Facenet by Google Procedure  I have used a trained model face-rec_Google.h5 file which gets loaded at runtime.You need to have images in your database. The code check /images folder for that. You can either paste your pictures there or you can click it using web cam. Run preparedb.py for preparing database with tags and respective image vectors. Run face_recg_video.py to run the application.  References:  Florian Schroff, Dmitry Kalenichenko, James Philbin (2015). FaceNet: A Unified Embedding for Face Recognition and Clustering Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, Lior Wolf (2014). DeepFace: Closing the gap to human-level performance in face verification The pretrained model used is inspired by Victor Sy Wang's implementation and was loaded using his code: https://github.com/iwantooxxoox/Keras-OpenFace. implementation also took a lot of inspiration from the official FaceNet github repository: https://github.com/davidsandberg/facenet  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/monash128/Face-Recognition-using-FaceNet/blob/5c72fadc14147b044d5014868738d8cccc608956/face-rec_Google.h5"], "see_also_links": [], "reference_list": ["https://arxiv.org/pdf/1503.03832.pdf"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d0"}, "repo_url": "https://github.com/fanfuhan/keras_binary_classification", "repo_name": "keras_binary_classification", "repo_full_name": "fanfuhan/keras_binary_classification", "repo_owner": "fanfuhan", "repo_desc": "keras\u4f7f\u7528\u8fc1\u79fb\u5b66\u4e60\u5b9e\u73b0\u4e8c\u5206\u7c7b\u95ee\u9898", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-07T10:33:01Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T09:25:04Z", "homepage": "", "size": 996, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185368506, "is_fork": false, "readme_text": "\u6ce8\u610f \u81ea\u884c\u5efa\u7acbdata \u548c model_data\u6587\u4ef6\u5939 \u8be6\u7ec6\u4ecb\u7ecd \u5177\u4f53\u4ecb\u7ecd\u89c1\u535a\u5ba2 ", "has_readme": true, "readme_language": "Chinese", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d1"}, "repo_url": "https://github.com/smartadpole/CycleGAN_Pytorch", "repo_name": "CycleGAN_Pytorch", "repo_full_name": "smartadpole/CycleGAN_Pytorch", "repo_owner": "smartadpole", "repo_desc": "\u7528\u4e8e\u56fe\u7247\u98ce\u683c\u8fc1\u79fb\u7684\u751f\u6210\u5f0f\u5bf9\u6297\u7f51\u7edc", "description_language": "Chinese", "repo_ext_links": null, "repo_last_mod": "2019-05-07T10:12:37Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T08:40:39Z", "homepage": null, "size": 7525, "language": "Python", "has_wiki": true, "license": {"key": "other", "name": "Other", "spdx_id": "NOASSERTION", "url": null, "node_id": "MDc6TGljZW5zZTA="}, "open_issues_count": 0, "github_id": 185359076, "is_fork": false, "readme_text": "  CycleGAN and pix2pix in PyTorch \u514b\u9686\u81ea https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix \uff1b We provide PyTorch implementations for both unpaired and paired image-to-image translation. The code was written by Jun-Yan Zhu and Taesung Park, and supported by Tongzhou Wang. This PyTorch implementation produces results comparable to or better than our original Torch software. If you would like to reproduce the same results as in the papers, check out the original CycleGAN Torch and pix2pix Torch code Note: The current software works well with PyTorch 0.41+. Check out the older branch that supports PyTorch 0.1-0.3. You may find useful information in training/test tips and frequently asked questions. To implement custom models and datasets, check out our templates. To help users better understand and adapt our codebase, we provide an overview of the code structure of this repository. CycleGAN: Project |  Paper |  Torch  Pix2pix:  Project |  Paper |  Torch  EdgesCats Demo | pix2pix-tensorflow | by Christopher Hesse  If you use this code for your research, please cite: Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks. Jun-Yan Zhu*,  Taesung Park*, Phillip Isola, Alexei A. Efros. In ICCV 2017. (* equal contributions) [Bibtex] Image-to-Image Translation with Conditional Adversarial Networks. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, Alexei A. Efros. In CVPR 2017. [Bibtex] Talks and Course pix2pix slides: keynote | pdf, CycleGAN slides: pptx | pdf CycleGAN course assignment code and handout designed by Prof. Roger Grosse for CSC321 \"Intro to Neural Networks and Machine Learning\" at University of Toronto. Please contact the instructor if you would like to adopt it in your course. Other implementations CycleGAN  [Tensorflow] (by Harry Yang), [Tensorflow] (by Archit Rathore), [Tensorflow] (by Van Huy), [Tensorflow] (by Xiaowei Hu),  [Tensorflow-simple] (by Zhenliang He),  [TensorLayer] (by luoxier), [Chainer] (by Yanghua Jin), [Minimal PyTorch] (by yunjey), [Mxnet] (by Ldpe2G), [lasagne/keras] (by tjwei) pix2pix  [Tensorflow] (by Christopher Hesse), [Tensorflow] (by Eyy\u00fcb Sariu),  [Tensorflow (face2face)] (by Dat Tran),  [Tensorflow (film)] (by Arthur Juliani), [Tensorflow (zi2zi)] (by Yuchen Tian), [Chainer] (by mattya), [tf/torch/keras/lasagne] (by tjwei), [Pytorch] (by taey16)  Prerequisites  Linux or macOS Python 3 CPU or NVIDIA GPU + CUDA CuDNN  Getting Started Installation  Clone this repo:  git clone https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix cd pytorch-CycleGAN-and-pix2pix  Install [PyTorch](http://pytorch.org and) 0.4+ and other dependencies (e.g., torchvision, visdom and dominate).  For pip users, please type the command pip install -r requirements.txt. For Conda users, we provide a installation script ./scripts/conda_deps.sh. Alternatively, you can create a new Conda environment using conda env create -f environment.yml. For Docker users, we provide the pre-built Docker image and Dockerfile. Please refer to our Docker page.    CycleGAN train/test  Download a CycleGAN dataset (e.g. maps):  bash ./datasets/download_cyclegan_dataset.sh maps  To view training results and loss plots, run python -m visdom.server and click the URL http://localhost:8097. Train a model:  #!./scripts/train_cyclegan.sh python train.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan To see more intermediate results, check out ./checkpoints/maps_cyclegan/web/index.html.  Test the model:  #!./scripts/test_cyclegan.sh python test.py --dataroot ./datasets/maps --name maps_cyclegan --model cycle_gan  The test results will be saved to a html file here: ./results/maps_cyclegan/latest_test/index.html.  pix2pix train/test  Download a pix2pix dataset (e.g.facades):  bash ./datasets/download_pix2pix_dataset.sh facades  Train a model:  #!./scripts/train_pix2pix.sh python train.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA   To view training results and loss plots, run python -m visdom.server and click the URL http://localhost:8097. To see more intermediate results, check out  ./checkpoints/facades_pix2pix/web/index.html.   Test the model (bash ./scripts/test_pix2pix.sh):   #!./scripts/test_pix2pix.sh python test.py --dataroot ./datasets/facades --name facades_pix2pix --model pix2pix --direction BtoA  The test results will be saved to a html file here: ./results/facades_pix2pix/test_latest/index.html. You can find more scripts at scripts directory. To train and test pix2pix-based colorization models, please add --model colorization and --dataset_mode colorization. See our training tips for more details.  Apply a pre-trained model (CycleGAN)  You can download a pretrained model (e.g. horse2zebra) with the following script:  bash ./scripts/download_cyclegan_model.sh horse2zebra  The pretrained model is saved at ./checkpoints/{name}_pretrained/latest_net_G.pth. Check here for all the available CycleGAN models. To test the model, you also need to download the  horse2zebra dataset:  bash ./datasets/download_cyclegan_dataset.sh horse2zebra  Then generate the results using  python test.py --dataroot datasets/horse2zebra/testA --name horse2zebra_pretrained --model test --no_dropout   The option --model test is used for generating results of CycleGAN only for one side. This option will automatically set --dataset_mode single, which only loads the images from one set. On the contrary, using --model cycle_gan requires loading and generating results in both directions, which is sometimes unnecessary. The results will be saved at ./results/. Use --results_dir {directory_path_to_save_result} to specify the results directory.   For your own experiments, you might want to specify --netG, --norm, --no_dropout to match the generator architecture of the trained model.   Apply a pre-trained model (pix2pix) Download a pre-trained model with ./scripts/download_pix2pix_model.sh.  Check here for all the available pix2pix models. For example, if you would like to download label2photo model on the Facades dataset,  bash ./scripts/download_pix2pix_model.sh facades_label2photo  Download the pix2pix facades datasets:  bash ./datasets/download_pix2pix_dataset.sh facades  Then generate the results using  python test.py --dataroot ./datasets/facades/ --direction BtoA --model pix2pix --name facades_label2photo_pretrained   Note that we specified --direction BtoA as Facades dataset's A to B direction is photos to labels.   If you would like to apply a pre-trained model to a collection of input images (rather than image pairs), please use --model test option. See ./scripts/test_single.sh for how to apply a model to Facade label maps (stored in the directory facades/testB).   See a list of currently available models at ./scripts/download_pix2pix_model.sh   Docker We provide the pre-built Docker image and Dockerfile that can run this code repo. See docker. Datasets Download pix2pix/CycleGAN datasets and create your own datasets. Training/Test Tips Best practice for training and testing your models. Frequently Asked Questions Before you post a new question, please first look at the above Q & A and existing GitHub issues. Custom Model and Dataset If you plan to implement custom models and dataset for your new applications, we provide a dataset template and a model template as a starting point. Code structure To help users better understand and use our code, we briefly overview the functionality and implementation of each package and each module. Pull Request You are always welcome to contribute to this repository by sending a pull request. Please run flake8 --ignore E501 . and python ./scripts/test_before_push.py before you commit the code. Please also update the code structure overview accordingly if you add or remove files. Citation If you use this code for your research, please cite our papers. @inproceedings{CycleGAN2017,   title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networkss},   author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A},   booktitle={Computer Vision (ICCV), 2017 IEEE International Conference on},   year={2017} }   @inproceedings{isola2017image,   title={Image-to-Image Translation with Conditional Adversarial Networks},   author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},   booktitle={Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on},   year={2017} }  Related Projects CycleGAN-Torch | pix2pix-Torch | pix2pixHD | iGAN | BicycleGAN | vid2vid Cat Paper Collection If you love cats, and love reading cool graphics, vision, and learning papers, please check out the Cat Paper Collection. Acknowledgments Our code is inspired by pytorch-DCGAN. ", "has_readme": true, "readme_language": "English", "repo_tags": ["deeplearning", "gan", "pytorch", "python3", "imagetransition", "style-transfer"], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://localhost:8097", "http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-handout.pdf", "http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/", "http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pdf", "http://people.csail.mit.edu/junyanz/projects/pix2pix/pix2pix.bib", "http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.pdf", "http://efrosgans.eecs.berkeley.edu/CVPR18_slides/pix2pix.key", "http://pytorch.org", "http://efrosgans.eecs.berkeley.edu/CVPR18_slides/CycleGAN.pptx", "http://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip", "http://www.cs.toronto.edu/~rgrosse/"], "reference_list": ["https://arxiv.org/pdf/1703.10593.pdf", "https://arxiv.org/pdf/1611.07004.pdf"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d2"}, "repo_url": "https://github.com/IRONMANMARK/MS-machine-learning", "repo_name": "MS-machine-learning", "repo_full_name": "IRONMANMARK/MS-machine-learning", "repo_owner": "IRONMANMARK", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-23T06:39:55Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T20:59:22Z", "homepage": null, "size": 17428, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185477259, "is_fork": false, "readme_text": "Using machine learning approach to identify peptide modification sequence from mass spectrum overview This project is using machine learning approach to identify the ppeptide modification sequence from mass spectrum.  First the library.py will read in the raw data and put it in a database, the database is using sqlite package witch is a build in database for Python. Second, the fetch_data.py will purge the raw data in the database and create two tables that one contains unique peptide sequence and one contains unique peptide modification sequence. Finally the cnn.py will use the unique moodification sequence as an index to pick out the mass spectrum data that belong to a specific peptide and put it in a matrix and use this matrix to train the model once at a time. I am using 90% of the data to train the model and 10% the data to test the data. how to use  These code run under Python 3.6 first run the library.py to generate the raw data database from mgf file. second run the fetch data to select part of the peptide data in the databse and generate the unique peptide sequence table and modification table. third run the cnn.py which using the unique peptide sequence index to extract the specific data out. And using the data to train the model.  Important notice  The mgf file file is just a small mgf file used to test the code. The original mgf raw data file will generate a 245GB database file and contains 250 million peptide mass spectrum data and around 2.2 x 10^8 entrys. So the file here is just a test of concept. Do NOT use the database you generated to run the cnn.py. Because the sample is too small, it doesn't mean anything. Also, using the databse generated will cause seriously issue. Because the labels for the trainning is smller than it should have. Use the database provide here to run the cnn.py. When you run the cnn.py code after some time there will be a resource exhausted error. This is because the code is NOT meant for laptop or ordinary desktop. This cnn.py has already shrink 8 times to just be able run on a 64GB memory server. So you can try this code but it will have a resourse exhausted error after several seconds. And the code can run perfectly fine if there is proper server. It will take at least a week even for that test database I provided to finish the training in a server, so the result will not be available. The training time is super long so that is why I fetch fraction of the whole data. When the cnn.py runing, it should look like this. The warning I got is just because I do not got enough memory on my laptop.    database visualization  I am using SQLite Expert to visualize the databaze    Packages need to be installed  Tensorflow keras tqdm  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d3"}, "repo_url": "https://github.com/mrkswlsn/semi-supervised-gan", "repo_name": "semi-supervised-gan", "repo_full_name": "mrkswlsn/semi-supervised-gan", "repo_owner": "mrkswlsn", "repo_desc": "semi-supervised classification with generative adversarial network", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-06-02T18:37:11Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T02:33:46Z", "homepage": "", "size": 22, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185306620, "is_fork": false, "readme_text": "Semi-supervised classification of objects in aerial imagery   Problem 1: Aerial imagery usually have very high resolution with sparsely located objects; a lot of effort goes into detection models running in regions that don't contain objects.   Problem 2: It's difficult to label all operating conditions that affect detection of objects.   Solution to problems 1 and 2  Feed chips of lower resolution imagery to semi-supervised GAN to determine if chip contains object or not. Use unlabelled data and reserve detector time for regions that contain objects we are looking for in higher resolution. Discover high resolution version of chips that contain objects we're looking to train detector.    Ideas  YOLO and other detectors will not work because object may be at single pixel or subpixels during resize for active detection. Use semi-supervised learning with GAN. Use one hot vectors as outputs. Put grids over image and decide what grids to turn on or off based on if they contain objects. Get inspiration from FastBox / YOLO on combining detections.  Links  Object in satellite images are small and sparsely located small objects in cluttered environments over broad swaths is a time consuming task for analysts; adam van etten @ medium Relational networks (keras implementation) for dealing with objects that occur together; alan lee @ github  References  Simple neural network for relational reasoning; Santoro et al, 2017 Improved techniques for training GANs; Salimans et al, 2016 One-hot GAN from Jeffrey Donahue dissertation Active frame, location, and video selection by Karasev et al, 2014  Task1: Semi-supervised object detection in aerial imagery  Starting with semi-supervised GAN model from OpenAI Contributions  Extend model for detection instead of classification Application to aerial imagery Incorporate labels from Donahue instead of just noise    Task2: Closed-loop proposal generation of sparsely located objects in high resolution aerial imagery  Extend task 1 to active detection Resample large image into 512x512 for training discriminator Goal is to use GAN to generate combinations of OCs reflecting objects of interest present  Extend semi-supervised GAN Instead of classification, use unet for detection (detection is object present in grid or not) Use 3 labels Real images have labels 0 for non object and 1 for object Generated images have labels all 2   Start with noise first as in openai approach Instead of noise maybe use discrete classes like donahue to limit possibility to n classes depending on number of detection possible and maybe classes fall out with an extra class if it contains multiple objects?  Task3: Conduct experiments combining task1 and task2 models  Active frame, location, and detector selection Focus on finding single specific objects like vehicles Main idea to explore is objects are located near other objects e.g (cars on roads and near buildings) For training data put capture boxes that slightly overlap on object of interest Use feedback loop i.e if grid one contains object will grid2 contain object? given low res image / positions of already detected grids and undetected ones recommendation system  This helps if one particular object is difficult but have similar oc/context as another easier one   Challenge is that objects are very small and sparsely located Conduct hyperparameter seearch on binary label input image size e.g 4x4 in terms of speed and accuracy Prediction network needs to be very fast Detection task is to select regions likely to contain the object at very low resolution using context (e.g vehicles are located on roads and near buildings)  Training data  Satellite data nadir Cerberus oblique  generation  Resize large image A = NxM into smaller image B = 512x512 Generate a 64x64 binary image mask C that is 1 where every B contains an object of interest This means each 8x8 block in B gets a label in C Also try 4x4 or even 2x2  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": ["https://arxiv.org/abs/1706.01427", "https://arxiv.org/abs/1606.03498", "https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6909670"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d4"}, "repo_url": "https://github.com/xandriaw/iqa", "repo_name": "iqa", "repo_full_name": "xandriaw/iqa", "repo_owner": "xandriaw", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T18:38:09Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T16:18:58Z", "homepage": null, "size": 81170, "language": "Python", "has_wiki": true, "license": {"key": "apache-2.0", "name": "Apache License 2.0", "spdx_id": "Apache-2.0", "url": "https://api.github.com/licenses/apache-2.0", "node_id": "MDc6TGljZW5zZTI="}, "open_issues_count": 0, "github_id": 185438496, "is_fork": false, "readme_text": "Image quality assessment   This repository provides an implementation of an aesthetic and technical image quality model based on Google's research paper \"NIMA: Neural Image Assessment\". You can find a quick introduction on their Research Blog. NIMA consists of two models that aim to predict the aesthetic and technical quality of images, respectively. The models are trained via transfer learning, where ImageNet pre-trained CNNs are used and fine-tuned for the classification task. The provided code allows to use any of the pre-trained models in Keras. We further provide Docker images for local CPU training and remote GPU training on AWS EC2, as well as pre-trained models on the AVA and TID2013 datasets. We welcome all kinds of contributions, especially new model architectures and/or hyperparameter combinations that improve the performance of the currently published models (see Contribute). Trained models    Predictions from aesthetic model            Predictions from technical model         We provide trained models, for both aesthetic and technical classifications, that use MobileNet as the base CNN. The models and their respective config files are stored under models/MobileNet. They achieve the following performance    Model Dataset EMD LCC SRCC     MobileNet aesthetic AVA 0.071 0.626 0.609   MobileNet technical TID2013 0.107 0.652 0.675    Getting started   Install Docker   Build docker image docker build -t nima-cpu . -f Dockerfile.cpu   In order to train remotely on AWS EC2   Install Docker Machine   Install AWS Command Line Interface   Predict In order to run predictions on an image or batch of images you can run the prediction script  Single image file  ./predict  \\ --docker-image nima-cpu \\ --base-model-name MobileNet \\ --weights-file $(pwd)/models/MobileNet/weights_mobilenet_technical_0.11.hdf5 \\ --image-source $(pwd)/src/tests/test_images/42039.jpg   All image files in a directory  ./predict  \\ --docker-image nima-cpu \\ --base-model-name MobileNet \\ --weights-file $(pwd)/models/MobileNet/weights_mobilenet_technical_0.11.hdf5 \\ --image-source $(pwd)/src/tests/test_images  Train locally on CPU   Download dataset (see instructions under Datasets)   Run the local training script (e.g. for TID2013 dataset)   ./train-local \\ --config-file $(pwd)/models/MobileNet/config_mobilenet_technical.json \\ --samples-file $(pwd)/data/TID2013/tid_labels_train.json \\ --image-dir /path/to/image/dir/local  This will start a training container from the Docker image nima-cpu and create a timestamp train job folder under train_jobs, where the trained model weights and logs will be stored. The --image-dir argument requires the path of the image directory on your local machine. In order to stop the last launched container run CONTAINER_ID=$(docker ps -l -q) docker container stop $CONTAINER_ID  In order to stream logs from last launched container run CONTAINER_ID=$(docker ps -l -q) docker logs $CONTAINER_ID --follow  Train remotely on AWS EC2  Configure your AWS CLI. Ensure that your account has limits for GPU instances and read/write access to the S3 bucket specified in config file [link]  aws configure   Launch EC2 instance with Docker Machine. Choose an Ubuntu AMI based on your region (https://cloud-images.ubuntu.com/locator/ec2/). For example, to launch a p2.xlarge EC2 instance named ec2-p2 run (NB: change region, VPC ID and AMI ID as per your setup)  docker-machine create --driver amazonec2 \\                       --amazonec2-region eu-west-1 \\                       --amazonec2-ami ami-58d7e821 \\                       --amazonec2-instance-type p2.xlarge \\                       --amazonec2-vpc-id vpc-abc \\                       ec2-p2   ssh into EC2 instance  docker-machine ssh ec2-p2   Update NVIDIA drivers and install nvidia-docker (see this blog post for more details)  # update NVIDIA drivers sudo add-apt-repository ppa:graphics-drivers/ppa -y sudo apt-get update sudo apt-get install -y nvidia-375 nvidia-settings nvidia-modprobe  # install nvidia-docker wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker_1.0.1-1_amd64.deb sudo dpkg -i /tmp/nvidia-docker_1.0.1-1_amd64.deb && rm /tmp/nvidia-docker_1.0.1-1_amd64.deb    Download dataset to EC2 instance (see instructions under Datasets). We recommend to save the AMI with the downloaded data for future use.   Run the remote EC2 training script (e.g. for AVA dataset)   ./train-ec2 \\ --docker-machine ec2-p2 \\ --config-file $(pwd)/models/MobileNet/config_mobilenet_aesthetic.json \\ --samples-file $(pwd)/data/AVA/ava_labels_train.json \\ --image-dir /path/to/image/dir/remote  The training progress will be streamed to your terminal. After the training has finished, the train outputs (logs and best model weights) will be stored on S3 in a timestamped folder. The S3 output bucket can be specified in the config file. The --image-dir argument requires the path of the image directory on your remote instance. Contribute We welcome all kinds of contributions and will publish the performances from new models in the performance table under Trained models. For example, to train a new aesthetic NIMA model based on InceptionV3 ImageNet weights, you just have to change the base_model_name parameter in the config file models/MobileNet/config_mobilenet_aesthetic.json to \"InceptionV3\". You can also control all major hyperparameters in the config file, like learning rate, batch size, or dropout rate. Datasets This project uses two datasets to train the NIMA model:  AVA used for aesthetic ratings (data) TID2013 used for technical ratings  For training on AWS EC2 we recommend to build a custom AMI with the AVA images stored on it. This has proven much more viable than copying the entire dataset from S3 to the instance for each training job. Label files The train script requires JSON label files in the format [   {     \"image_id\": \"231893\",     \"label\": [2,8,19,36,76,52,16,9,3,2]   },   {     \"image_id\": \"746672\",     \"label\": [1,2,7,20,38,52,20,11,1,3]   },   ... ]  The label for each image is the normalized or un-normalized frequency distribution of ratings 1-10. For the AVA dataset these frequency distributions are given in the raw data files. For the TID2013 dataset we inferred the normalized frequency distribution, i.e. probability distribution, by finding the maximum entropy distribution that satisfies the mean score. The code to generate the TID2013 labels can be found under data/TID2013/get_labels.py. For both datasets we provide train and test set label files stored under data/AVA/ava_labels_train.json data/AVA/ava_labels_test.json  and data/TID2013/tid2013_labels_train.json data/TID2013/tid2013_labels_test.json  For the AVA dataset we randomly assigned 90% of samples to the train set, and 10% to the test set, and throughout training a 5% validation set will be split from the training set to evaluate the training performance after each epoch. For the TID2013 dataset we split the train/test sets by reference images, to ensure that no reference image, and any of its distortions, enters both the train and test set. Serving NIMA with TensorFlow Serving TensorFlow versions of both the technical and aesthetic MobileNet models are provided, along with the script to generate them from the original Keras files, under the contrib/tf_serving directory. There is also an already configured TFS Dockerfile that you can use. To get predictions from the aesthetic or technical model:  Build the NIMA TFS Docker image docker build -t tfs_nima contrib/tf_serving Run a NIMA TFS container with docker run -d --name tfs_nima -p 8500:8500 tfs_nima Install python dependencies to run TF serving sample client  virtualenv -p python3 contrib/tf_serving/venv_tfs_nima source contrib/tf_serving/venv_tfs_nima/bin/activate pip install -r contrib/tf_serving/requirements.txt   Get predictions from aesthetic or technical model by running the sample client  python -m contrib.tf_serving.tfs_sample_client --image-path src/tests/test_images/42039.jpg --model-name mobilenet_aesthetic python -m contrib.tf_serving.tfs_sample_client --image-path src/tests/test_images/42039.jpg --model-name mobilenet_technical  Cite this work Please cite Image Quality Assessment in your publications if this is useful for your research. Here is an example BibTeX entry: @misc{idealods2018imagequalityassessment,   title={Image Quality Assessment},   author={Christopher Lennan and Hao Nguyen and Dat Tran},   year={2018},   howpublished={\\url{https://github.com/idealo/image-quality-assessment}}, }  Maintainers  Christopher Lennan, github: clennan Hao Nguyen, github: MrBanhBao  Copyright See LICENSE for details. ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": true, "h5_files_links": ["https://github.com/xandriaw/iqa/blob/6fade024c271eadf0d0a0e2d05989e1de0ef1a63/models/MobileNet/weights_mobilenet_aesthetic_0.07.hdf5", "https://github.com/xandriaw/iqa/blob/6fade024c271eadf0d0a0e2d05989e1de0ef1a63/models/MobileNet/weights_mobilenet_technical_0.11.hdf5"], "see_also_links": ["http://academictorrents.com/details/71631f83b11d3d79d8f84efe0a7e12f0ac001460", "http://www.ponomarenko.info/tid2013.htm"], "reference_list": ["https://arxiv.org/pdf/1709.05424.pdf"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d5"}, "repo_url": "https://github.com/Hari-04/Crosslingual-Transfer-Inflection-Generation", "repo_name": "Crosslingual-Transfer-Inflection-Generation", "repo_full_name": "Hari-04/Crosslingual-Transfer-Inflection-Generation", "repo_owner": "Hari-04", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-08T04:18:10Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T19:42:16Z", "homepage": null, "size": 293, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185467390, "is_fork": false, "readme_text": "Crosslingual-Transfer-Inflection-Generation Introduction to the problem statement Inflection is the change in the form of a word to express different grammatical categories like tense, gender, voice, etc. Natural language processing systems must be able to generate these inflected forms to perform other tasks like translation, speech recognition, etc. effectively. Not all world\u2019s languages are distributed equally. Some languages simply have more resources compared to other languages. So, in this project we explored how we can transfer knowledge from high-resource languages to genetically related low-resource languages. So, given a lemma and a bundle of morphological features, we have to generate a target inflected form. The dataset used contains many high-resource language examples and fewer low-resource language examples. In this project, the task is to perform morphological inflection in the low-resource language by exploiting some similarity to the high-resource language. Architecture of our model: We created a Encoder-Decoder style model with Luong's attention for solving this sequence to sequence learning problem. Both Encoder and Decoder have three components: Input layer, embedding layer and Stacked LSTM layer. We created and trained a language model whose weights are used to initialize the weights of the LSTM layers. We have used global attention to provide richer source encoding by taking all the time step in the source into consideration. We initially created a alignment vector which determines the similarity score between the encoder and the decoder by comparing current target hidden state with the every hidden state in source. We computed the context vector by performing the weighted average of alignment vector and the encoder states. The context vector and decoder states are concatenated and transferred using the tanh function to get the final decoding vector. This is passed through a softmax to predict the normalized value i.e., probability of the next word in the target sequence. One other change that improved the performance was training on multiple high-resource languages datasets for a given low-resource language. All the models were implemented using Keras framework.  Running in sd mode #Tested on Ubuntu 19.04. Should work on all linux machines # Create env conda env create -f environment.yml source activate cltmi # Train using only adyghe-kabardian dataset (If you are using any data other than the sample data provided. Please download the data and place it in 'data' directory. Download site: https://github.com/sigmorphon/2019/tree/master/task1) ./run_model.sh sd kabardian adyghe Running in all mode In this mode we train on multiple high-resource languages for a given low-resource target language #Tested on Ubuntu 19.04. Should work on all linux machines # Create env conda env create -f environment.yml source activate cltmi # Train using all kabardian language dataset (If you are using any data other than the sample data provided. Please download the data and place it in 'total_data' directory. Download site: https://github.com/sigmorphon/2019/tree/master/task1) ./run_model.sh all kabardian Performance of the model Changing the initial weights of the LSTM layers from random initialization to the weights produced by the language model improved the model performance and the validation accuracy increased. Also, for a given target language  using the multiple source language datasets in the training process and using the global attention further improved the accuracy. From the results below, we can see our model was performing better than or on-par with the baseline models for the two target languages Kazakh and Occitan. Similar results were achieved with other target languages also.   The time taken for running ~30 epochs was also considerably lower than the baseline models. References  https://sigmorphon.github.io/sharedtasks/2019/ Minh-Thang Luong Hieu Pham Christopher D. Manning: Effective Approaches to Attention-based Neural Machine Translation Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate Ryan Cotterell, Georg HeigoldK. 2017. Cross-lingual, Character-Level Neural Morphological Tagging  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": [], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d6"}, "repo_url": "https://github.com/Immowelt/iwlearn", "repo_name": "iwlearn", "repo_full_name": "Immowelt/iwlearn", "repo_owner": "Immowelt", "repo_desc": "\"Production First\" Machine Learning Framework", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-23T13:52:13Z", "repo_watch": 0, "repo_forks": 1, "private": false, "repo_created_at": "2019-05-07T08:53:40Z", "homepage": "", "size": 122, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185361606, "is_fork": false, "readme_text": "Machine Learning for the real world  Begin a Big Data project with defining the business goals and measuring metrics to be improved, not with downloading some CSV data and launching a Jupiter notebook. Avoid disconnect between data scientists and software developers / devops. Deploy a solid Machine Learning infrastructure into production even before you start training your first model. Complement the algorithms from Tensorflow, Keras and Scikit-Learn with the solid software-engineering practices required to deploy, operate, monitor and update ML-based products. Detect frequent data problems (missing data, skewed datasets, noisy features, etc)  We call this Production First Machine Learning iwlearn captures all the best practices and lessons learned we had so far while creating, operating, monitoring and updating real-world Machine Learning models on immowelt.de, a leading German real estate marketplace. By sharing this project, we not only hope you'd avoid the mistakes we've made, but also we suggest some process patterns allowing to increase the ROI of Big Data projects. Contents  Process Patterns  Start with Metrics Production first Rules before Models Avoid Data Contamination Model Aging Soft Model Rollout   Feature Engineering Patterns  Simple Value Clustered Mean Removal All Combination Frequency Features Representing Geography One-Hot Encoding Image augmentation Eye Check   Developing with iwlearn  Installation and system requirements Tutorial Advanced Tutorial Architectural concepts Reference    Process Patterns Start with Metrics Intent  Find out whether the problem at hand needs to be solved at all and if yes, what metrics exactly capture the desired outcome. Create a way to measure problem solving progress. Create a way to monitor whether the AI model running in production still adequately solves the problem.  Problem Most machine learning tutorials use one simple metric to measure the performance of the models (usually, it is accuracy). In the business setting, accuracy might not represent the correct metric to be optimized or tracked. What metrics are the best representation of the business goal? Discussion Consider the following example: the model is a binary classifier deciding whether to give 10% rebate to our customer, based on information we know about him and his real estate. The purpose of the model is to increase the conversion rate. The false positive would be then reducing the price in the situations when the user would also pay the normal price anyway. The false negative is not giving the price reduction for persons who have really needed it and would therefore bounce. If our goal and the top priority is increasing the conversion rate, then the false positives are nice to be avoided, but they are not a critical problem (let's give them the weight of 0.1), while it is absolutely important to avoid the false negatives (their weight should be 0.9). On the other hand, if our top priority is to increase the revenue instead, then vice versa, the false positives must get the weight 0.9 and the false negatives the weight 0.1. The accuracy metric is giving to the both cases the weight 0.5, which is almost never right from the business perspective. In this example, either the recall metric should be used to maximize conversion rate (precision metric to maximize revenue), or we should define a custom loss function directly measuring the bounce rate / the revenue, and evaluate our model in terms of this loss function as compared with the bounce rate / revenue given without usage of the model. Solution  Define a numeric metric adequately representing the property of the business system you want to improve. Measure this metric for some period of time (ideally using pre-saved historical data) to get the idea about how good is this metric before developing the model. Decide whether this metric can be substantially improved with a model. For example, a bounce rate of 0.01 would be not the best case of using AI models to be improved, because creating of AI models is a very costy process, and the outcome will not pay off the expenses, because we can only improve the metric by 1 percent point. On the other hand, a bounce rate of 0.9 is too not very suitable for AI models, because it can be most probably improved by using some simple \"quick win\" solution. An AI model can help you get from 0.6 to 0.2 though. Use the defined metric when training and evaluating the model. Deploy the model for part of users in an A/B test and compare the metric changes the model creates in the production. After rolling out the model to all users, monitor the metrics and its changes over time to be alerted in case of any changes in data or user behavior.  Disadvantages If you're a novice in machine learning and using tutorials to get started, most probably you will get some kind of unbiased metric by default (accuracy, RMSE, categorical cross-entropy). Usually it is not so easy to change the metrics, because sometimes you would need to implement them yourself. Production first Intent Avoid type errors when rolling out models into production. Problem ML models do not have any typization and would happily accept any garbage as input, silently producing random numbers as output. Accidental changes in the data conversion or order of columns can force the model to make wrong predictions. This problem is especially critical for the models influencing the humans, for example for the dynamic pricing models. Besides, if the model uses a lot of features, a mistake in one feature implementation will just reduce the model performance somewhat, so the bug can live in production for weeks and months before being caught. Discussion Most ML tutorials start with loading a data set from CSV files or some online data source. Then the raw data is processed by some inline throw-away code and sent to the model for training. Sometimes, the whole is even implemented not in the same programming language that is used in production. As a result, when the model is ready, it has to be re-implemented in production, possibly using a different implementation of the same ML algorithm or even another programming language. This situation is a source of all possible errors related to inconsistent implementation. The most common and simple mistake is that the features used in production are not the same as those used for training. A real story: once we've removed one obsolete feature and added a new one. As a result, the new trained model has expected the same input array shape as the previous model. We've forgot to implement the new feature in the production, so that the new model was fed with the data of an obsolete feature. Some more subltle mistake could be made because of changes in the feature definitions. In another real story, our old feature was returning the price of the real estate as is, while the new feature code returned -1 for all outlier prices. Again, we've forgot to implement this tiny change in production. This can be avoided if the exact the same feature code is used both for training and production. Solution  Use Python both for training/development and prediction/production phases. No R, no models reimplemented in C# or Java. Inherit data sources from BaseDataSource and implement them to fetch data from the databases or other external services and to put all the relevant data into a Python dictionary. Inherit features from BaseFeature and write code converting the raw data into values suitable for the model. Create model by instantiating one of the ready models from iwlearn, or inheriting from some base model. Load the training and test sets, train the model as usual Deploy the model along with the unchanged features and data sources into production.  Disadvantages  You cannot use other languages or tools for training. You need to write a substantial amount of code before you get to the business and can start training the model. Could be not suitable if the prediction phase is to be performed on the customer device (Android, iOS, Javascript).  Rules before Models Intent  Reduce development time and costs of the \"smart data\" solution. Have the decisions of the \"smart data\" system to be easily explained to humans.  Problem Developing a AI model both takes time to create useful features and/or neuronal network architectures, and requires usage of expensive hardware for training. Also, decisions of an AI model usually cannot be explained to humans, which is especially bad in case of the wrong decisions. Discussion Rules can be something as simple as an IF statement following some condition. For example, if our goal is to determine whether some user is interested in professional relocation services, the condition can be the number of rooms in the apartments he is looking at, and the threshold for a positive decision can be \"greater than 3\". The condition and threshold can be first created by asking the domain experts. But better approach would be to support rules with data, especially when determining the optimal threshold. This would have the advantage that the performance of the rule can be evaluated before the rollout, and also monitored afterwards in production - both in terms of the goal metric we've defined in the previous step. Solution  Prepare a data set containing all historical data needed to evaluate the conditions of the rule. Divide this data set into training, test and validation sets. For each condition in the rule, develop an extraction logic (the \"features\") to convert the data from the dataset into values suitable for an IF statement. The preferred way is to inherit these features from the BaseFeature class so that they can be reused \"as is\" for AI models in the future. Display the distributions and statistics of the features so you can get an idea, what features are usable for your goal and which thresholds could be reasonable. For the binary classification tasks we display the feature distributions separately for the class 0 and class 1 and select the features having most unsimilar distributions. Write a rule with one or several IF statements using the values from the previous step and hard-coded conditions. Evaluate the rule using the test set and get the metric you've chosen to optimize. Change the rule or thresholds and repeat the steps until you've created a rule having the metrics according to the goal you have set. Evaluate the rule against the validation set to check that you haven't overfitted to the test set. Take the rule into production.  Disadvantages  Rules are generally weaker than an AI model, because the latter can support unlimited number of features and capture an unlimited number of interactions between them, while rules created by humans are limited by 7 +- 2 features. Rules require a classic feature engineering beforeahed, which is infeasible for such inputs like images, texts or sounds.  Avoid Data Contamination Intent Ensure that the information contained in the data is the same between training and production. Problem When using historical data for training, the data can be invisibly contaminated with some signal related to the label and bearing the information from the future. If such data is used for training, the model will not perform well in production, because it will miss the information it has used during the training. The problem can be very tricky to spot (see example in the discussion). Discussion To demonstrate the problem we will assume a fictional requirement: we would require to remove company logos from the images used to depict real estates and would allow this only to customers that have booked this feature. For this fictional task we would need to create a model delivering 1 for images containing any company logos and 0 for all other images. The model is intended to be used in production just after image upload, and to send an automatic email to the customers with a warning. For training of this model, we would fetch the images stored currently (at the time point of model training) on NAS, label them manually, and go through some kind of CNN. Now, let's assume, there is some human being who is checking all images manually and removing the ones with company logo - except of the images belonging to the customers that have booked this feature. And let's assume, currently only the companies named \"Big Agency\" and \"International Flats\" have booked it. If images are not only disabled, but also removed from the NAS, only images from \"Big Agency\" and \"International Flats\" would contain logos in our dataset. Our model would perform very well during evaluation and suck in production, because it would only reliably detect logos on images belonging to \"Big Agency\" and \"International Flats\". In the worst case, the model will only learn the typical image size used by the \"Big Agency\" and therefore mark a lot of other innocent images accidentally having the same image size. In case nobody have informed us about this manual logo cleanup process, it could be very hard to detect and debug this situation. We can see this kind of dataset skew as a signal leaking from the future back to the past. We mostly use historical data for our datasets, and they can contain some information or changes that are in the past for us, but in the future comparing with the time point in a life cycle of an offer, when our model usually needs to deliver a prediction. So that our dataset can get contaminated by the data from \"the future\". Solution The sure way to avoid any possible data contamination without analyzing each and every feature is to make a snapshot of the data during the prediction time, exact as it is being seen by the model at that very moment of prediction. Therefore, models running in production should create and save sample data to be used for the future training. On the other hand, avoid bloat of your samples storage system and do not include immutable data into the sample, only a reference to it. Disadvantage  When starting a new project, there is no model running in production and saving samples for the training. Either we would need to create an empty model that is only saving the samples, not predicting anything, and wait for some time until enough data are stored. Or we need to prepare the first dataset from existing historical data to bootstrap the training process. In this case, care has to be taken to not include any contaminated data into the dataset. Usage of immutable historized data (like data in Clickhouse, Hadoop, some SQL Server tables known to be immutable) is strongly recommended.  Model Aging Intent Keep or improve the model performance over long periods of time. Problem Every model released to production ages with the time. Distribution of feature values change with the time: prices rise, web site collecting the information removes some fields or introduces new ones, image quality improves and the image content changes (eg. there will be more fully furnished apartments than before). Therefore, the thresholds used by nodes of the random forest or the weights used by nodes of a DNN will be more and more outdated with time. Discussion Each model released to production should be updated regularly to prevent the model aging. The update cycle can vary from model to model. For example, for dynamic pricing we need to update it every month, while for some image recognition, it might be enough to update once a year. Solution  Every model running in production should save the samples its getting for prediction, to build up a training set used for the next model re-training Monitor performance of models in production to recognize the need of a re-training.  Disadvantages  Making a model always re-trainable makes the training procedure and scripts more complicated A quick database is required to store samples during prediction time, so that the prediction speed is not slowed down.  Soft Model Rollout Intent Allow comparizon of several models in a multivariate live test. Problem Model evaluation with cross-validation or a set-aside validation set, often leads to overly-optimistic results. There are various reasons for that, for example:  generally speaking the training sets tend to be small, so that it is easy to overfit on a small test or validation set, data contamination problems are possible when bootstrapping training sets for a novel model, the training set could have skewed class distribution, because one class might be easier to label than another, any ETL steps might be re-implemented differently in production compared with the training scripts.  Therefore, the real performance of a model can often be only assessed in the first days of it running in production. In the use case where the model is influencing humans (like dynamic pricing), it might be a critical issue, if a model gets deployed, having a bad real performance. Discussion Soft rollout can alleviate the problem. In this case, the old rule or model and the new rule or model are running in parallel and both are making predictions. Only predictions of the old rule or model are used to perform actions. After enough predicted samples, the new model can be evaluated and promoted to be the master model whose predictions are used for performing some actions. We have also conceived more complicated soft rollout scenarious, where the predictions of both (or several) models are combined together with a high weight for the old model and low weight of the new model, and the weight of the new model is getting higher and higher the better its performance is, while the weight of the old model gets lower correspondingly. Solution  Always store the sample data and the corresponding prediction, of all models running in production, even though not every model is allowed to take any action. Always ask all models and rules for their prediction - avoid the algorithm that would call the rule A first, and then depending on its prediction either call rule B or model C. Instead, let all rules and models to make a prediction, store it for further evaluation and monitoring, and then use the prediction of B or C depending on the value of prediction made by A.  Disadvantages  Having a big zoo of models in production, all making the inference and storing the results in the database can eat a lot of hardware resources. It is up to you to decide whether running some old model is still worth it. Old models require old features and old datasources to be working. If the underlying data table or database is going to be eliminated due to some architectural change of the portal, the datasorce would just stop working and the model will be forcefully removed from the run.  Feature Engineering Patterns Performance of many classic ML algorithms depends very strongly on how exactly the input features represent their values. It is naive to believe that it would be possible to just read the fields from the database, send them to some ML algorithm, give it a huge hardware box, and let it crunch. The resulting model performance (accuracy, recall, precision etc) will be mostly very poor. Proper feature engineering helps the ML algorithms to extract most signal from the noise. Learning, what approaches are working and what not when trying to convert the features to be more useful for some ML algorithm, is an important part of being an experienced ML engineer. To make things worse, these approaches are also depend on the algorithm being using (RandomForest, Bayes, Deep Learning). Below you can find some typical feature representations, which we had tried. You might want to try them out and check whether they are helpful in your situation. Simple Value The simplest feature is to return the value of some field itself, for example the living area in square meters. Some classifiers are sensitive to outliers, so be sure to remove them. The best way is either to remove samples with outliers, or to replace outliers with some value indicating \"missing\". We've found that using -1 for missing values works good enough in many cases. Do not use 0 for missing value. Note that the Simple Value pattern works best if it represents points in some continuous euclidean space. For example, numeric zip codes or numeric country codes do not represent points in space (difference between two zip codes is not a distance) and are unsuitable. Another consideration is to remove unwanted variability from the value. For example, real estate prices depend on many factors, but one of the strongest factors is the living area. If you suppose that the notion of \"luxurious\" or \"simple\" apartments is a good signal for your classifier, then using the square meter price is better than using the price itself, because the high absolute price does not indicate a luxury estate (it could be just big one). You should also consider temporal changes in the value. Speaking of real estate prices, we have up to 40% rise per year. If your model is running in production for months and years, the thresholds and factors it had learned on the old prices could be outdated quickly and the model's performance will sink. To avoid this issue, you could try to use the Clustered Mean Removal pattern instead of the Simple Value. Clustered Mean Removal Most of ML algorithms work better if the features are normalized (i.e. their values are converted into the range of 0 to 1 or -1 to 1). Normalization will be performed for all samples of the dataset. But if the samples are different enough from each other, bringing them to the common range could remove some useful signal. For example, if we have real estates from different geographical locations in the data set, and the prices vary up to 10x depending on location, then most of the normalized prices would be situated around 0.1 and around 0.9, with very little samples in the middle. if the purpose of the model should not depend on location, such a feature would send a wrong signal. Instead, we can cluster the samples first, trying to get a possibly homogene clusters, and then calculate cluster means and remove the mean of the corresponding cluster from each sample value. Clustering can be both performed with ML algorithms, as well as with some simple rules (in the example above, it is enough to cluster real estate by zip code, estate type and living area). Using this pattern can also help to solve the temporal changes in the data (see example in the Simple Value pattern). Besides, we can introduce the knowledge about average (expected) values into the model, even if this information cannot be inferred from the dataset itself (for example, if the dataset has too litte samples, but we have pre-calculated average values from some other statistical data source). We have both tried to substract the mean value from the feature, or divide by it. If you cannot decide, whether substracting or diving is better, you can use the All-Combination pattern. All Combination Sometimes it is hard to say, what version of the same feature would work better for your model. Should you use the absolute price, or price relative to mean, or divide the price by living area? Should your feature count frequencies of some events in the last day, or last 7 days, or last month? We've found out that it could be helpful to include ALL versions of the feature into the model. After training the model, feature importances can be evaluated and the features that don't contribute much can be removed. But often it is the case that all of the versions have similar importances and they are all used equally well by the model. In this case, just leave them all in the model. Usually, the computational overhead is not too high to generate all of them. Frequency Features Sometimes it makes sense to count some events, given the data stored in the sample. For example, we can count, how often a real estate with given living area has been listed in the same zip code in the past - and return this count as the feature value for that sample. We can use historized offers stored in the Clickhouse database to calculate that. Be sure to limit the time span when calculating the count, otherwise it would always grow and your model will age quicker than expected. Another thing to care about when calculating counts is considering the traffic changes with the time: if the company spends in one year more on advertisements than in another year, or some law changes shift the market and lead to increase or decrease of customer streams, the counts will change too. If the models task is not related to reacting on market shifts, it is better to eliminate this noise, for example by dividing the counts by the overall counts - in the above example, you would count apartments for rent in some zip code with the living area between 60 and 80, listed during the last year, and then divide it by the overall count of all apartments for rent listed in that zip code last year. If you also need to eliminate the geographic signal, you can alternatively use the count of all apartments for rent from the last year as the divisor. Frequencies are our preferred way to make features that are very hard to represent in some other way - see Representing Geography for some examples. Another typical usage of this pattern is training a sub-model making a prediction for some partial aspect of the problem, and return its predicted scores (which are also, in a sense, probabilities or frequences). For example, when predicting real estate price using Random Forest, it is tempting to use information from the real estate images. Unfortunately, Random Forest cannot predict on images, so we've created a Tensorflow sub-model taking images and returning scores, and use these scores as one of the features for the final Random Forest model. Representing Geography Inventing geography or space-related features is a notoriously hard task. It is not helpful to return zip codes as int numbers, because the model would try to build some distances between the zip codes or multiply them with some factors, which obviosly doesn't bring much signal. Also, using lattitude and longitude as features didn't bring the expected success, even though they are points in an euclidean space. We think, it didn't work, because our prediction target required splitting the space into very big number of small \"islands\" or \"enclaves\" and most ML algorithms are very limited in this relation (in the simplest case they only have one line to divide the samples into classes). We have also tried one-hot encoding for countries, which also wasn't successful. If we would made 195 columns to represent all worlds countries, most of these columns was empty in the training set, bearing no signal. Because Random Forest implementation in sklearn does not treat the columns of the one-hot encoded features as a single feature, but rather as unrelated columns, it might endup choosing a lot of no-signal columns for some trees, increasing the amount of bad trees in the forest and therefore reducing the prediction performance. On the other hand, if you only take the countries that are present in the training set, how do you encode the feature, if some new country appears during the prediction? Another aspect of the problem to take care of are the temporal changes. Geography changes continuously: new cities emerge, some zip codes will be combined or splitted, some new land will be cleared for development, so that new lat/long coordinates appear that have never been used before, etc. Encoding geography directly could therefore increase the aging speed of your model. So far, the best way we've found to handle the geographic information , is to incorporate it into other features, i.e. using it when building the clusters for the Clustered Mean Removal and for the Frequency Features. One-Hot Encoding We often use one-hot encoding for categorical features (for example, whether some EXIF tag is present in the image). Care should be taken to handle the following cases:  Some new category appears, or existing categories are renamed, split or combined, after model rollout into production. Some categories appear too rarely in the training set, so that the model fails to capture them as a tiny signal. If there are many categories that appear too rarely, the performance of Random Forest can diminish, or the training of a DNN can be slowed down (as it would get batches with many empty columns).  Usually, we group all the rare categories together into the column \"Miscellaneous\", and use error logging to indicate the appearance of unexpected categories during prediction. Image augmentation While some DNN tutorials mention image augmentation as something you could do, we strongly recommend to always augment images. This is similar to the feature engineering, because it is our way to help the model to recognize, which aspects of data should be invariant (i.e. should not influence the resulting prediction). For example, you should randomly mirror the images for the model recognizing kitchens, because kitchens exist in any different shape. If the model has to recognize floor plans, you should not mirror the images, because floor plans contain text and mirrored text is invalid. Do not just apply all possible augmentations to images. Instead, always consider what the model would learn from them. In one our training case, adding only two augmentations had helped to increase the F-score by 20%. Eye Check Before using a new feature for training, unless you are under strong time pressure, you should always print its values for several thousand samples, or else print some basic statistics about it (min, max, avg, median, number of unique values, histogram, etc). This is also true for images: write a debugging function that would convert the already augmented and normalized tensor back to an image and show it. You will be glad doing this, not only because it gives a lot of intuition about the feature performance, but also helps to find out implementation errors, that would otherwise never be found, especially if your model has a lot of features and one new feature more does not add lot of new signal. Developing with iwlearn Installation and system requirements iwlearn is only tested for Python 2 (we use Python 2.7). It uses MongoDB as a persistence layer, and Scikit-Learn, Tensorflow and Keras as ML algorithms. pip install iwlearn To try the tutoral [https://github.com/Immowelt/iwlearn/tree/master/tutorial], you should use your local MongoDB instance, which can be installed like this: mkdir /tmp/mongo sudo docker run -d -p 27017:27017 -v /tmp/mongo:/data/db mongo Tutorial Throughout this tutorial we will use the following example. We need to predict, whether a user is interested in professional relocation services, based on the real estates he has watched. According to our process pattern, we will:  Define the metric we want to improve Implement a rule first  Define the sample class Understanding sample structure Understanding datasources Implement sample retrieving Implement Features Implement the Rule Boilerplate Generate a dataset Data-based rule definition Implement the Rule Evaluate Rule Performance Bring the Rule Online   Monitor Rule Performance Replace rules with a model  Train a model Bring the model online   File structure  Define the metric we want to improve We are building a binary classifier, with the \"positive\" class (class 1) meaning the user who really needs a relocation service. Our prediction will be used to send an email to the user, suggesting him to use our professional relocation service. Let's say, we have already sent such emails before, and measured 6% unsubscribes after the campaign: users are known to react allergically to unexpected emails. We compare the damage from the unsubscribed user with the possible revenue from using our relocation services, and decide that false positives are critical (sending email to a user who don't need relocation service), while false negatives essentially represent the current state (no email is sent to the users who need our help) and can be easier tolerated. Therefore, we choose the precision of our binary classifier as our main metric to watch: we need at least the precision of 98% and should achieve as high recall as possible. Note that the precision is 100% minus the unsubscribe rate, so that our controlling metric is directly related to the business goal. Implement a rule first We begin working on the problem by implementing some rule of thumb first. Our first idea is that people who rent expensive apartments or houses might outsource the relocation instead of doing it themselves with the help of friends, especially for larger apartments with more than 3 rooms. But before we can start testing this idea, we must consider, how this model or rule will be used in production. There will be a batch job running each night, retrieving all users who haven't received our email yet, and predicting for each of them whether they need our help. Our task is to implement a web service getting user key as input and returning the prediction as output. Define the sample class The first step of our service would be to load all data needed for prediction given the user key. In iwlearn, it is accomplished by defining and creating a sample. A sample class must inherit from BaseSample and represents all data needed for prediction, at the same time being a DTO for storing this sample for the training set. Each sample must define entityid - the ID of the business domain entity being processed (in our case the user key). Note that because each entity can be predicted several times (for example, each time user visits our website a new prediction has to be made), entityid is not an unique key for samples. We should name the sample carefuly, because it is also the name of the collection in MongoDB, where our training set will be stored. class RelocationUserSample(BaseSample):     def __init__(self, userkey):         BaseSample.__init__(self, userkey) Understanding sample structure We can now add some dummy data to our sample to play with it around userkey = 12345 sample = RelocationUserSample(userkey) sample['SomeData'] = 'data' sample.data['OtherData'] = 5    # note that using sample or sample.data is the same. Sample.data exists because                                 # of historical reasons and its usage is not recommended if 'SomeData' in sample:     print(sample['SomeData'])  del sample['OtherData'] Sample behaves like a simple dict. Historically, we have really just used a dict instance for it. The difference of the sample compared to a dict is not only the entityid, but also that you can compose new samples from existing samples and datasource instances (see below) in a structured and cached way. When developing our sample, we need to plan its dictionary structure. Usually, it is recommended to have only a limited number of keys in the root level. Some of them are created automatically, like \"entityid\", or \"created\" (the timestamp of sample construction). Every time we add some other sample to ours, its name will be added as a root-level key to our sample, with all its values below. Having this clear structure and keeping it in mind helps finding out the data we need in our samples. Understanding datasources Having the sample, we next could connect to our databases, read the data, and store it into the sample directly. If this is the quickest and robust way with your databases, don't hesitate doing just that. We rather prefer using our predined datasource instances for data retrieval. iwlearn comes with data sources for MongoDB, MS SQL Server, Couchbase and Clickhouse databases. A data source is a class getting connection string as constructor parameter and providing methods like get_row_as_dict or get_rows_as_dict that you can call with the query as parameter. The class provides thread-safe connection handling, opening and closing connection as needed, as well as all usual data format conversion that the python database driver requires (for example, Decimals are converted to floats and naive datetime Python instances are converted to UTC format for the Mongo driver). For the case that your sample is so simple that all its data can be retrieved from a single data source, iwlearn also comes with base sample classes, one per each data source, so you can directly inherit your sample from a corresponding base class and literally only define, what query to use to load it. Implement sample retrieving Now we can proceed with implementing our RelocationUserSample.  For our sample, we need two pieces of data:  The list of IDs of the apartments each user has watched on our website. Let's say we can get it from Couchbase. Attributes of each apartment given its ID (price, area, location etc). Let's say they are stored in an SQL Server.  We implement the first piece of information as a DataSource and add it to our sample: class WatchedRealEstatesDataSource(CouchBaseDataSource):     def __init__(self):         CouchBaseDataSource.__init__(self, COUCHBASE_CONNECTION_STRING)      # makeimpl is the method we're always overriding when inheriting from data source or sample     # we can define any number of parameters. In our case, we need to know, data of which user we have to retrieve     def makeimpl(self, userkey):         user_profile = self.get_document(userkey)         return user_profile['VisitedRealEstates']  # list of estate ids  #...          class RelocationUserSample(BaseSample):     def __init__(self, userkey):         BaseSample.__init__(self, userkey)              def makeimpl(self):             # The following line will make WatchedRealEstatesDataSource and add its result to the self under the key         # with the same name as the name of the data source, without the \"DataSource\" suffix.                  # Because the data source needs to know, data of which user has to be retrieved, we can pass it here, because         # all **kwargs parameter passed to self.add will be passed to the makeimpl method of the data source.         self.add(WatchedRealEstatesDataSource(), userkey=self.entityid)                  # Always return the retrieved dict in makeimpl         return self.data  #...  sample = RelocationUserSample(12345) sample.make() print sample['WatchedRealEstates'] # will print a list of estate ids retrieved from Couchbase          The second piece of data type of data could also be implemented as a data source. But we think, it is generic enough to be reused for other models, so it implement it as a sub-Sample: class RealEstateSample(SQLSample):     def __init__(self, estateid):         SQLSample.__init__(self, estateid, SQL_CONNECTION_STRING)      # note that we don't need any parameters here (and they are not allowed), because a sample should be able     # to completely retrieve itself only knowning its self.entityid     def makeimpl(self):         query = \"\"\"         select price, livingarea, rooms, zipcode, estatetype, distributiontype         from RealEstates         where estateid = ?         \"\"\"          params = (self.entityid, )         return self.get_row_as_dict(query, params) # dict of fields defined in the select statement          #...  class RelocationUserSample(BaseSample):     def __init__(self, userkey):         BaseSample.__init__(self, userkey)              def makeimpl(self):             # The following line will make WatchedRealEstatesDataSource and add its result to the self under the key         # with the same name as the name of the data source, without the \"DataSource\" suffix.                  # Because the data source needs to know, data of which user has to be retrieved, we can pass it here, because         # all **kwargs parameter passed to self.add will be passed to the makeimpl method of the data source.                  # note that self.add also returns the retrieved dict(), so we can store it into estate_ids variable and use         # later in our makeimpl code         estate_ids = self.add(WatchedRealEstatesDataSource(), userkey=self.entityid)                  # now we just iterate the estateids and load the RealEstateSamples          self['WatchedRealEstateAttributes'] = []         for estateid in estate_ids:             sub_sample = RealEstateSample(estateid)             try:                 sub_sample.make()                 self.data['WatchedRealEstateAttributes'].append(sub_sample.data)             except:                 logging.exception('Cannot make sample %s' % estateid)                  # Always return the retrieved dict in makeimpl         return self.data                  #...           sample = RelocationUserSample(12345) sample.make() print sample['WatchedRealEstates'] # will print [list of estateids] print sample['WatchedRealEstateAttributes'] # will print [ {'price': .., 'livingarea': .., ..}, ...]           Because the sample will be persisted and can be reused for further models having more features, we recommend to retrieve more information than actually needed for the current model. In this way, it will be possible to base new features on existing data sets without reloading the data from the original data source. It is especially important for the mutable data sources, because this will avoid the data contamination problem. For example, in the RealEstateSample shown above, we would really prefer select * rather than selecting a predefined set of fields. Implement Features Next step is to define features we need for our rule. The features use the data stored in the sample and calculate some value that can be used as input for prediction. The output_shape of the features is not limited so that it is possible to define features returning several values, or multidimensional values (images, sound, video). To test our idea, we will calculate medians of living area and rooms and the percentage of houses for rent, across all real estates the user has watched. Detecting expensive real estates is a little trickier, because we must compare with the average price in the same zip code. When creating a feature, you need to implement the get method. You can add any top-level keys of your sample as parameters of the method (lowercasing the first letter), and you will get the corresponding value passed to the method. For example, if you have this kind of sample: sample = MySample() sample['SomeData'] = 123 sample['OtherData'] = {'some property': 'some value'} sample['SomeDataForOtherFeatures'] = ... then you can develop a feature like this class MyFeature(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)         self.output_shape = ()      def get(self, someData, otherData):         print someData  # 123         print otherData # dict {'some property': 'some value'}                  if otherData['some property']:             return someData * 100         else:             return BaseFeature.MISSING_VALUE Now we are ready to implement our features: import numpy as np  from iwlearn import BaseFeature  from pricingengine import get_price  class LivingAreaMedian(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)      def get(self, watchedRealEstateAttributes):         if len(watchedRealEstateAttributes) == 0:             return BaseFeature.MissingValue         return np.median([estate['livingarea'] for estate in watchedRealEstateAttributes])  class RoomMedian(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)      def get(self, watchedRealEstateAttributes):         if len(watchedRealEstateAttributes) == 0:             return BaseFeature.MissingValue         return np.median([estate['rooms'] for estate in watchedRealEstateAttributes])  class PercentageHousesForRent(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)      def get(self, watchedRealEstateAttributes):         if len(watchedRealEstateAttributes) == 0:             return BaseFeature.MissingValue         return np.sum([1 for estate in watchedRealEstateAttributes                        if estate['estatetype'] == 'HOUSE' and estate['distributiontype'] == 'RENT']) / \\                len(watchedRealEstateAttributes)  class ExpensivePrice(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)      def get(self, watchedRealEstateAttributes):         prices = []         for estate in watchedRealEstateAttributes:             if estate['distributiontype'] != 'RENT':                 continue             hisprice = estate['price']             meanprice = get_price(estate['zipcode'], estate['livingarea'], estate['estatetype'],                                   estate['distributiontype'])             prices.append((hisprice - meanprice) / meanprice)         if len(prices) == 0:             return BaseFeature.MissingValue          return np.median(prices) Implement the Rule Boilerplate Before we can start defining a rule in a data-based way, we need to prepare some boilerplate code. The rule must be implemented as a class inheriting from BaseRule. It must define a list of features it is using in its constructor, and implement the method _implement_rule, which recieves the values of the features as its parameters (similarly like a feature is receiving the data from sample in its parameters). This method must return None, if the rule cannot make any prediction, or a some prediction, which is an instance of RulePrediction class, or our own class inheriting from it. from iwlearn import BaseRule from iwlearn.rules import RulePrediction  import features as ff  class RelocationRule(BaseRule):     def __init__(self):         BaseRule.__init__(self,                           [                               ff.LivingAreaMedian(),                               ff.RoomMedian(),                               ff.PercentageHousesForRent(),                               ff.ExpensivePrice()])         self.sampletype = RelocationUserSample                                                                 def _implement_rule(self, livingAreaMedian, roomMedian, percentageHousesForRent, expensivePrice):         return None Generate a dataset We are now all set and done to see how the values of our features are distributed in the real historical data. To do this, we will generate a data set. Usually, our data sets are generated automatically, because each prediction would add a sample in our samples database. But in the beginning, we don't have anything making predictions, so either we would create a dummy code that would just make and store a sample, put it in production and wait for some time until the dataset is created in a natural way, or we can retrieve some data from other historical data sources and fake the pre-existing data set. We call this faking process bootstrapping. To bootstrap our dataset, we need to create samples and store them in our samples database (MongoDb). The clean way to do it looks like this: import iwlearn.mongo as mongo  from tutorial.common.samples import RelocationUserSample  mongo.setmongouri('mongodb://localhost:27017/')   # for this tutorial, we use a DUMMY local Mongo instance  # For this tutorial, we fake our data. In a real project, # load here your real keys and labels directly from the databases DUMMY_userkeys = ['user' + str(x) for x in xrange(0, 10000)] DUMMY_userlabels = [1 if x < MAGIC else 0 for x in xrange(0, 10000)]  for userkey, label in zip(DUMMY_userkeys, DUMMY_userlabels):     sample = RelocationUserSample(userkey=userkey)     sample['RelocationLabel'] = label     sample.make()     mongo.insert_sample(sample) Note that in the code above, each sample will be made independently, so in total 10000 separate calls to Couchbase and 10000 separate calls to SQL Server will be made. Usually, this would be prohibitevily slow. We can hack it by using multi_get requests for Couchbase and a better select statement for the SQL Server, for example like this: import iwlearn.mongo as mongo  from tutorial.common.samples import RelocationUserSample  mongo.setmongouri('mongodb://localhost:27017/')   # for this tutorial, we use a DUMMY local Mongo instance  recentestates = sqlconnection.get_rows_as_dict(\"\"\"         select estateid, price, livingarea, rooms, zipcode, estatetype, distributiontype         from RealEstates         where created > getdate() - 30         \"\"\") estatedata = dict() for row in recentestates:     estatedata[row.estateid] = row      userkey = load_user_keys() # for example similarly as above userlabels = load_user_labels() # for example similarly as above  couchbasedocs = couchbaseconnection.multi_get(userkey) couchbasedata = dict() for doc in couchbasedocs:     couchbasedata[doc.userkey] = doc  for userkey, label in zip(DUMMY_userkeys, DUMMY_userlabels):     sample = RelocationUserSample(userkey=userkey)     sample['RelocationLabel'] = label     sample['WatchedRealEstates'] = couchbasedata[userkey]     sample['WatchedRealEstatesAttributes'] = [estatedata[estateid] for estateid in couchbasedata[userkey]]     mongo.insert_sample(sample) This would run quicker, but actually it is a hack, because our sample data is generated not by the exactly the same code as it will be in production. If our sql or couchbase commands in the script are different with those we have implemented in the data source, the samples made in production will have a different data structure than samples made during the bootstrapping - in the worst case we wouldn't even detect that. So use this approach very carefully and always prefer making samples individually, better even in production, if you really can afford it. Having the samples now, we can't finally generate our dataset:     mongo.setmongouri('mongodb://localhost:27017/')     DataSet.generate('train', RelocationRule(), filter={'entityid': {'$regex': r'^user[0-9]*?[0-7]$'}})     DataSet.generate('test', RelocationRule(), filter={'entityid': {'$regex': r'^user[0-9]*?[8-9]$'}}) This will produce two subdirectories input/train and input/test, containing local files with the data from our samples. We have splited our sampels already into the train and test sets, so that we can avoid overfitting when defining our rule. In the filter parameter of generate we can pass any statement that pymongo.find accepts. The samples loaded from MongoDb will be then passed to the features defined in the RelocationRule, and the feature values will be persisted into local files. This is to support model hyperparameter experimenting. If you change anything in your rule or model algorithm or its hyperparameters, you can re-train the models or re-check the rule, without needing to retrieve and generate the dataset again. Only when you need to change or extent features, the dataset will need to be regenerated. Data-based rule definition We can now load the dataset and plot the histograms:     train = DataSet('train')     print 'Number of samples %d' % len(train)      train.plot_data()    This will display 4 histograms, one for each rule. Every histogram will show two distributions of the corresponding value - one for the class 0 and another for the class 1. After looking at the distrubutions, it will be possible to determine the thresholds for our rule visually: just draw a vertical line separating the distributions of the two classes in an optimal way. So now we can finish implementing the rule:     def _implement_rule(self, livingAreaMedian, roomMedian, percentageHousesForRent, expensivePrice):         if livingAreaMedian >= 75 and roomMedian >= 3 and \\             percentageHousesForRent > 0.3 and expensivePrice > 1.0:             return RulePrediction(1)    # class 1 means \"positive\"         return RulePrediction(0) Evaluate Rule Performance We can evaluate rule performance before going live by using our test set.     test = DataSet('test')     rule = RelocationRule()     rule.evaluate_and_print(test) This will print something like Evaluation of RelocationRule(RelocationRule) -------------------------------------------------------------------------------- Number of test samples 2000 confusion_matrix [[1800    0]  [  86  114]] f1_score 0.7261146496815287 precision_score 1.0 recall_score 0.57  Bring the Rule Online Recall of 57% with Precision 100% satisfies our needs, so we can now take this rule into production: import cherrypy  import iwlearn.mongo as mongo  from tutorial.common.rules import RelocationRule from tutorial.common.samples import RelocationUserSample  class RelocationService():     def __init__(self):         self.rule = RelocationRule()         self.collection = mongo.mongoclient()['Tutorial']['Predictions']      @cherrypy.expose     def predict(self, userkey):         sample = RelocationUserSample(userkey=userkey)          if sample.make() is not None:             mongo.insert_sample(sample)              prediction = self.rule.predict(sample)             mongo.insert_check(prediction, sample)     # save prediction for rule monitoring and evaluation             pre_dict = prediction.create_dictionary()  # this will serialize the prediction to a way suitable                                                        # for storing in MongoDB or transmitting over the wire             return pre_dict         return 'cannot make sample' Monitor Rule Performance The predictions of the rule will be saved into the Predictions collection. Each prediction has a reference to the sample document stored in the collection RelocationUserSamples. If we now implement an additional code that would track which users have reacted on our email campaign, find out the corresponding samples and store the label there, we would have the following advantages:  Our labeled data set will grow in size allowing for better training and evaluation. We can re-tune the rule thresoholds after some time to prevent its aging. Last but not least, we can evaluate the real performance of our rule in production, and eventually decide to replace the rule with a model.  The future versions of iwlearn will introduce some functionality helping to label samples, and to monitor and to visualize real performance. Replace rules with a model After having run one or several rules in production, sometimes there is an intuition that a ML model would achieve a better performance, for example due to much more features or combination of unrelated features. Another situation when you want to train the model is when a rule was infeasible from the beginning on (for example image classification). Train a model If you jumped to this chapter directly, please read all the chapters from Define the sample class to Generate data set, because you will also need them for the model. The chapters will left you with the Sample and Features defined and the test and train datasets generated. The next step is to define and train the model. iwlearn comes with two base models, using Scikit-Learn and Keras respectively as their engines. The best practice is to inherit your model from one of them, or directly from the BaseModel: import tutorial.common.features as ff from tutorial.common.samples import RelocationUserSample  class RelocationModel(ScikitLearnModel):     def __init__(self):         ScikitLearnModel.__init__(self,                                   name = 'RelocationModel',                                   features = [                                     ff.LivingAreaMedian(),                                     ff.RoomMedian(),                                     ff.PercentageHousesForRent(),                                     ff.ExpensivePrice()],                                   sampletype=RelocationUserSample,                                   labelkey='RelocationLabel') Even though such inheritance seems to be redundant in this particular, it makes possible to override methods for training, creating labels, dealing with scores, defining neuronal network architecture for Keras-based models, etc so that in our experience it is always almost needed for one reason or another. If you don't need it in your project, you can instantiate ScikitLearnModel directly. Having the model, we can train it with train = DataSet('train') test = DataSet('test') print 'Samples in train %d, in test %d' % (len(train), len(test))  model = RelocationModel() model.train(train) model.evaluate_and_print(test) This will evaluate to something like Evaluation of RelocationModel(RelocationModel) -------------------------------------------------------------------------------- 01. RoomMedian Importance 0.5087875046910646 02. LivingAreaMedian Importance 0.22190641952636728 03. ExpensivePrice Importance 0.2133446725970341 04. PercentageHousesForRent Importance 0.05596140318553421 Number of test samples 2000 f1_score 0.9558441558441559 precision_score 0.9945945945945946 recall_score 0.92  Just like we have expected, an ML model performs better: with the similar precision it has a much higher recall. So we take it online. Bring the model online Currently we would pickle the model, save it to disk and then load and use it in production. We're considering options to integrate a model version control system. To use the model instead of the rule in our service, we just replace the rule with the model: import cherrypy  import iwlearn.mongo as mongo  from tutorial.common.rules import RelocationRule from tutorial.common.samples import RelocationUserSample from tutorial.common.models import RelocationModel  class RelocationService():     def __init__(self):         self.model = RelocationModel.load('/path_to_model')         self.collection = mongo.mongoclient()['Tutorial']['Predictions']      @cherrypy.expose     def predict(self, userkey):         sample = RelocationUserSample(userkey=userkey)          if sample.make() is not None:             mongo.insert_sample(sample)              prediction = self.model.predict(sample)             mongo.insert_check(prediction, sample)     # save prediction for rule monitoring and evaluation             pre_dict = prediction.create_dictionary()  # this will serialize the prediction to a way suitable                                                        # for storing in MongoDB or transmitting over the wire             return pre_dict         return 'cannot make sample' File structure We recommend to use similar project files structure like in the tutorial: src \\     common \\                # Content of the files corresponds to the name          datasources.py           samples.py         features.py         models.py         rules.py     serving \\         # All components related to serving predictions in production     training \\         template \\             10_xxx.py - 19_xxx.py # Scripts related to bootstrapping samples, storing them in Mongo, updating them with                                    # additional information, generating datasets, etc             20_xxx.py - 29_xxx.py # Scripts related to visualizing data, understanding its distribution, finding outlier, etc             30_xxx.py - 30_xxx.py # Scripts related to evaluating and trying out different rules             40_xxx.py - 40_xxx.py # Scripts related to trying out and evaluating ML models.         experiment_one \\             # You copy the template folder every time you want to add features to the model, create new model etc, and             # change the scripts directly here in place.         experiment_two \\         experiment_three \\  Advanced Tutorial The advanced tutorial builds up on the Tutorial and demonstrates the following topics:  Using Keras-based Models Implementing Regressor Models Advanced ScikitLearnModels  Feature selection Learning curve Validation curves Hyperparameter selection    You can find the source code here [https://github.com/Immowelt/iwlearn/tree/master/tutorial] Using Keras-based Models Keras-Tutorial is being developed. Inherit your models from iwlearn.models.BaseKerasClassifierModel or BaseKerasRegressorModel and define the method _createkerasmodel(), where you need to define the keras model architecture, compile the model and set it to self.kerasmodel. Implementing Regressor Models Tutorial is being developed. Advanced ScikitLearnModels While model training shown in the basic tutorial could be enough for some not very challenging tasks, usually it is not so easy to train models. In a more realistic, professional setting, we might want to start with the feature selection, plotting learning and validation curves to determine required dataset size and model architecture, as well as to fine-tune hyper-parameter to achieve a little better model performance. Feature selection To show the feature selection in the tutorial, we define two more features that are intentionally less useful than the rest of the features: class RoomMedianNoisy(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)         self.output_shape = ()      def get(self, watchedRealEstateAttributes):         return np.median([estate['rooms'] for estate in watchedRealEstateAttributes]) + 100 * random.random()  class PureNoise(BaseFeature):     def __init__(self):         BaseFeature.__init__(self)         self.output_shape = ()      def get(self, watchedRealEstateAttributes):         return random.random() we now define a new model using these features and generate new train and test datasets. Then, we can perform the automated feature selection procedure: train = DataSet('train') test = DataSet('test') print 'Samples in train %d, in test %d' % (len(train), len(test))  model = RelocationModel() model.train(train) model.evaluate_and_print(test)  scored_features = model.feature_selection(test, step=1, n_splits=4)  selected_features = [] for i, (feature, score) in enumerate(zip(model.features, scored_features)):     logging.info('%s %s %s ' % (i, feature.name, score))     if score <= 1:         selected_features.append(feature) This will output something like Evaluation of RelocationModel(RelocationModelPro) -------------------------------------------------------------------------------- 01. LivingAreaMedian Importance 0.3737243851030765 02. ExpensivePrice Importance 0.3511122606354373 03. PercentageHousesForRent Importance 0.14942548830713584 04. RoomMedianNoisy Importance 0.06586973954153408 05. PureNoise Importance 0.059868126412816185 Number of test samples 2000 f1_score 0.8518518518518519 precision_score 0.9044943820224719 recall_score 0.805 INFO:root:Recursive Feature Elimination CV INFO:root:Optimale Anzahl an Features: 3 INFO:root:Optimale Feature: [1 2 3 1 1] INFO:root:0 LivingAreaMedian 1  INFO:root:1 RoomMedianNoisy 2 INFO:root:2 PureNoise 3 INFO:root:3 PercentageHousesForRent 1  INFO:root:4 ExpensivePrice 1   Note that our noisy features have got the worst importances, and the feature selection has ranked all features. The rank of 1 is the best, the worse the feature is, the higher its rank is. Learning curve Learning curve helps detecting under- and overfitting by calculating model performance for differeren dataset sizes. (http://mlwiki.org/index.php/Learning_Curves). We can plot it for our model:     train = DataSet('train')     test = DataSet('test')     print 'Samples in train %d, in test %d' % (len(train), len(test))      model = RelocationModel()     model.train(train)      plt = model.plot_learning_curve(train)     plt.show() The learning curve uses the accuracy by default to score the performance of the model. You can pass another score using the scoring parameter. We use the learning curve to understand whether we could benefit from adding more samples into the dataset, as well as to determine the minimal data set size used for crossvalidation. Validation curves Validation curves use the same idea as the learning curve, but they variate other hyperparameters instead of the dataset size, for example, if we use RandomForestClassifier, we can change number of estimators. train = DataSet('train') test = DataSet('test') print 'Samples in train %d, in test %d' % (len(train), len(test))  model = RelocationModel() model.train(train)  logging.info('Validation Curve n_estimators') n_estimators_range = np.linspace(start=30, stop=110, num=int((110 - 30) / 3), dtype=int) logging.info(len(n_estimators_range)) plt = model.plot_validation_curve(dataset=train, param_name=\"n_estimators\", param_range=n_estimators_range) plt.show()  logging.info('Validation Curve max_features') max_features_range = np.linspace(start=1, stop=4, dtype=int) logging.info(len(max_features_range)) plt = model.plot_validation_curve(dataset=train, param_name=\"max_features\", param_range=max_features_range) plt.show()  logging.info('Validation Curve min_samples_leaf') min_samples_leaf_range = np.linspace(start=1, stop=6, num=6, dtype=int) logging.info(len(min_samples_leaf_range)) plt = model.plot_validation_curve(dataset=train, param_name=\"min_samples_leaf\", param_range=min_samples_leaf_range) plt.show() Hyperparameter selection ScikitLearnModels provide the method train_with_hyperparameters_optimization performing grid search of hyperparameters and ensuring the most optimal model training compared with the simple train method using default hyperparameters. To demonstrate the hyperparameter optimization, we define an additional feature pumping a lot of noise, so that more than default number of estimators is required to get good performance. train = DataSet('train-hyper') test = DataSet('test-hyper') print 'Samples in train %d, in test %d' % (len(train), len(test))  model = RelocationModelHyper()  # Train the model in a simple way to provide a baseline of the model performance model.train(train) model.evaluate_and_print(test)  # Now perform training with the hyperparameter optimization n_estimators_range = np.linspace(start=100, stop=600, num=5, dtype=int) max_features_range = np.linspace(start=1, stop=7, num=3, dtype=int) min_samples_leaf_range = np.linspace(start=1, stop=4, num=3, dtype=int)  folds = 5 # use TrainingsSizeCrossValidation.xlsx and learningCurve to number of folds right param_distributions = {\"n_estimators\": n_estimators_range, \"max_features\": max_features_range, \"min_samples_leaf\": min_samples_leaf_range} configuration = {'n_splits': folds, 'param_distributions': param_distributions, 'params_distributions_test_proportion': 0.2, 'test_size': 0.25} model = RelocationModelHyper() model.train_with_hyperparameters_optimization(dataset=train, **configuration) model.evaluate_and_print(test) This will output something like f1_score 0.7988505747126436 max_features auto min_samples_leaf 1 n_estimators 100 precision_score 0.9391891891891891 recall_score 0.695 ... f1_score 0.8081395348837209 max_features 10 min_samples_leaf 1 n_estimators 600 precision_score 0.9652777777777778 recall_score 0.695  Architectural concepts TBD Reference Read the source code: [https://github.com/Immowelt/iwlearn] ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://mlwiki.org/index.php/Learning_Curves"], "reference_list": []}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d7"}, "repo_url": "https://github.com/Net-Mist/style-transfer-tf2", "repo_name": "style-transfer-tf2", "repo_full_name": "Net-Mist/style-transfer-tf2", "repo_owner": "Net-Mist", "repo_desc": "A Tensorflow2 adaptive style transfer implementation", "description_language": "English", "repo_ext_links": null, "repo_last_mod": "2019-05-24T05:26:24Z", "repo_watch": 2, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T00:25:42Z", "homepage": null, "size": 46285, "language": "Python", "has_wiki": true, "license": null, "open_issues_count": 0, "github_id": 185290791, "is_fork": false, "readme_text": "Style Transfer This is an implementation of adaptive style transfer in tensorflow 2. It demonstrates how to:  Manage a dataset using tf.dataset and tfrecord files. Build models using tf.keras high level API. Write the training loop of a GAN using tf.function and tf.GradientTape . Export and infer using tf.SavedModel, TensorRT and TfSlim. Manage flags and logging using google abseil python package.  In the near future this code will also:  Allow training style transfer networks following Cartoon-GAN paper The code is writen but I'm still looking for good parameters :) Demonstrate how to use style transfer on Android. I wrote models with the same kind of optimization presented in mobilenet v2 paper. The models train well, can be use for inference on a computer and can be exported in tfLite format but I still have some bug in the android app.  Please note that the tensorflow 2 TensorRT API is still work in progress and that you need a more recent version of tensorflow than the 2.0.0a0 if you want to use tensorRT. To compile and build a docker image with a more recent version of tensorflow 2 please see the readme inside trt_docker subdir You will find some results in folder imgs. For instance: Original image:  After style transfer with Picasso:  Installation The easiest way to install this style transfer is by using docker. You need to install docker, docker-compose and nvidia-docker then run: docker-compose build style-transfer Training adaptive style transfer  Content images used for training: Places365-Standard high-res train images (105GB). The research team that releases the adaptive style transfer paper also releases style image on their owncloud  Start docker If you want to use docker for the training, a solution is to run the docker-compose style-transfer app. For training you need to specified 3 environments variables :  PICTURE_DATASET: the path of the pictures ART_DATASET : the path of the art images EXPERIMENT : the folder where the program writes logs and checkpoints  for instance run : export PICTURE_DATASET=.... export ART_DATASET=.... export EXPERIMENT=.... docker-compose run style-transfer And you will get bash prompt in a valid tensorflow 2 environment Start training As soon as you are in an environment running tensorflow 2, try : python3 app.py --action training To train the model with default parameters. You can see the most important parameters with python3 app.py --help and all the parameters with python3 app.py --helpfull  If you choose to not use docker, then you probably need to change the default paths where are stored the dataset and the tfrecords. The parameters to do this are : --picture_dataset_path, --picture_tfrecord_path, --style_dataset_path, --style_tfrecord_path and --training_dir --style_tfrecord_prefix is also an option you should specified if you want to create several tfrecord files for several artists in the same folder. If you want to train a model with the same optimizations than the mobilenet-v2 then add the flag : --mobilenet  For instance, if you want to train a picasso style transfer with the same training parameters than the original paper, you can first run 200000 iterations with a learning rate of 0.0001 then 100000 iterations with a learning rate of 0.00002. Inside docker it will be: python3 app.py --action training \\                --training_dir /opt/experiment/picasso \\                --style_tfrecord_prefix picasso python3 app.py --action training \\                --training_dir /opt/experiment/picasso_2 \\                --style_tfrecord_prefix picasso \\                --n_iterations 100000 \\                --lr 0.00002 \\                --pretrained_ckpt /opt/experiment/picasso/checkpoints/ Training Cartoon GAN It is basically the same thing than training a adaptive style transfer, The only difference is that you start by an initialization where you pretrain the generative network. Inside docker it will be : python3 app.py --action training \\                --training_method initialization_cartoon \\                --n_iterations 50000 \\                --training_dir /opt/experiment/cartoon \\                --learning_rate 0.001  python3 app.py --action training \\                --training_method cartoon_gan \\                --n_iterations 200000 \\                --training_dir /opt/experiment/cartoon2 \\                --pretrained_ckpt /opt/experiment/cartoon/checkpoints Please note the flag --training_method to choose if we want to train an adaptive style transfer (default), a cartoon GAN or training the generative network for the initialization part of the cartoon GAN. Export a trained model to saved_model format To export a model you need to specified:  The path of checkpoint to load (can be a file or a folder. In this case it will load the more recent one) The path to write the exported model the export format you want to use (SavedModel, TensorRT or TfLite) and some precision (FP16 or 32 for tensorRT...) The model architecture corresponding to the checkpoint (std or mobilenet)  For instance to export a model in TFLite format: python3 app.py  --action export --export_format tflite --mobilenet --export_path /opt/export/model.tflite Do inference To perform inference you need to provide :  The path to a exported model The path of the folder containing the images, videos... The path where to write the transformed images and videos  And you need to specified if your using tflite. For instance : python3 app.py --action infer \\                --inference_model_path ... \\                --inference_input_dir ... \\                --inference_output_dir ...  \\                --inference_tflite Ref Adaptative Style Transfer  https://github.com/CompVis/adaptive-style-transfer https://compvis.github.io/adaptive-style-transfer/ https://arxiv.org/pdf/1807.10201.pdf  Instance Normalization  https://arxiv.org/pdf/1607.08022.pdf  ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://openaccess.thecvf.com/content_cvpr_2018/papers/Chen_CartoonGAN_Generative_Adversarial_CVPR_2018_paper.pdf", "http://data.csail.mit.edu/places/places365/train_large_places365standard.tar"], "reference_list": ["https://arxiv.org/pdf/1807.10201.pdf", "https://arxiv.org/pdf/1807.10201.pdf", "https://arxiv.org/pdf/1607.08022.pdf"]}, {"_id": {"$oid": "5cf518d07eb8d667ecf830d8"}, "repo_url": "https://github.com/liza183/ascends-toolkit", "repo_name": "ascends-toolkit", "repo_full_name": "liza183/ascends-toolkit", "repo_owner": "liza183", "repo_desc": null, "description_language": null, "repo_ext_links": null, "repo_last_mod": "2019-05-31T19:09:14Z", "repo_watch": 0, "repo_forks": 0, "private": false, "repo_created_at": "2019-05-07T15:46:09Z", "homepage": null, "size": 2263, "language": "Python", "has_wiki": true, "license": {"key": "mit", "name": "MIT License", "spdx_id": "MIT", "url": "https://api.github.com/licenses/mit", "node_id": "MDc6TGljZW5zZTEz"}, "open_issues_count": 2, "github_id": 185433058, "is_fork": false, "readme_text": " ASCENDS: Advanced data SCiENce toolkit for Non-Data Scientists What is it for? ASCENDS is a toolkit that is developed to assist scientists or any persons who want to use their data for machine leearning. We know that there are so many tools available for data scientists, but most of them require programming skills and often overwhelming. We aim to provide a set of simple but powerful tools for non-data scientists to be able to intuitively perform various advanced data analysis and machine learning techniques with simple interfaces (a command-line interface and a web-based GUI). The current version of ASCENDS mainly focuses on two different machine learning tasks - classification and regression (value prediction).   What is classification? Users can train a predictive model (mapping function) that predicts a category (Y) from input variables (X) using ASCENDS. For instance, ASCENDS can train a model for predicting whether an email is spam or not-spam.   What is regression? Users can train a predictive model (mapping function) that approximates a continuous output variable (y) from input variables (X) using ASCENDS. For instance, ASCENDS can train a model for predicting a value of house based on square footage, number of bedrooms, number of cars that can be parked in its garages, number of storages.   ASCENDS principles  Supporting various classification/regression techniques (Linear regression, logistic regression, random forest, support vector machine, neural network, ...) Supporting Feature selection based on various criteria Provides automatic hyperparameter tuning No programming skills required; but ASCENDS library can be used in your code if needed Using standard CSV (comma separated values) format data set Built on top of open source projects (keras, tensorflow, scikit-learn, etc.)  Although ASCENDS has been originally developed for material scientists' research on high temperature alloy design, the tool can be also used for many other applications. We encourage you to cite using the following BibTex citation, if you have used our tool: @misc{snapnets,   author       = {Sangkeun Lee and Dongwon Shin and Jian Peng},   title        = {{ASCENDS}: A data SCiENce toolkit for Non-Data Scientists},   howpublished = {\\url{https://code.ornl.gov/slz/ascends-toolkit}},   month        = jan,   year         = 2019 }  List of ORNL contributors  Sangkeun Lee, Core Developer (lees4@ornl.gov, leesangkeun@gmail.com) Dongwon Shin, (shind@ornl.gov) Jian Peng (pengj@ornl.gov)  Installation (With Anaconda) ASCENDS requires Python version 3.X, and using Anaconda is recommended. Please download Anaconda from https://www.anaconda.com/download/ and install first. Once you installed Anaconda (with Python 3.X), you can create a new Python environment for ascends by doing: conda create --name ascends  To activate an environment:  On Windows, in your Anaconda Prompt, run  activate ascends   On macOS and Linux, in your Terminal Window, run  conda activate ascends  You will see the active environment in parentheses at the beginning of your command prompt: (ascends) $  Please install pip in your local conda environment by doing: (ascends) $ conda install --yes pip  Then, install ascends-toolkit by doing: (ascends) $ pip install ascends-toolkit  Now you're ready to use ascends. Please see the next section for a quick start guide. To check if you properly installed ascends-toolkit, run (ascends) $ train_regression.py -h  If you encounter tensorflow error, please install tensorflow manually by doing conda install tensorflow  If you see the usage help of regression trainer, you're ready to go. Now have fun with ascends-toolkit. To deactivate the current Anaconda environment, after using ascends.  On Windows, in your Anaconda Prompt, run  deactivate    On macOS and Linux, in your Terminal Window, run  conda deactivate  To make sure ascends-toolkit has been properly installed and lear how the tool can be used, please go over the following tutorials. Getting Started: Classification To train your first model, we are going to use the Iris data set, which is one of the classic datasets introduced by the British statistician and biologist Ronald Fisher in his 1936 paper. Download the iris.csv into your data directory. (Save as the following link) Link: https://raw.githubusercontent.com/pandas-dev/pandas/master/pandas/tests/data/iris.csv The Iris data set consists of 50 samples from each of 3 species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters. Using ASCENDS, we will train a model that can predict a species of Iris, when new input data is given. ASCENDS uses standard CSV (Comma Separated Values) file format, and the file requires to have a header in the first line. The following shows the first 5 lines of the Iris data set. SepalLength,SepalWidth,PetalLength,PetalWidth,Name 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa  Let's try to train a classification model by executing the following command to train a classification model using ASENDS: In this tutorial, we assume tat you already created a output directory to save output files and stored data files in data directory. train_classifier.py data/iris.csv output/iris Name --mapping \"{'Name': {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}}\" --num_of_class 3 data/iris.csv is the input file that we just downloaded in the data folder. output/iris is the path and the tag that will be used for output files. Name is the target column name. So, ASCENDS will train a model that predicts Name when four other column (SepalLength,SepalWidth,PetalLength,PetalWidth) values are given. As we can see in the first 5 lines of data file, values for the column Name is not numerical. For training, we need to map the categorical values into numerical values. This is done by using --mapping option. The example command will map Iris-setosa to 0, Iris-versicolor to 1, and Iris-virginica to 2 for all values of column Name. Then you will see, the following result. $ train_classifier.py data/iris.csv output/iris Name --mapping \"{'Name': {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}}\" Using TensorFlow backend.   * ASCENDS: Advanced data SCiENce toolkit for Non-Data Scientists   * Classifier ML model trainer    programmed by Matt Sangkeun Lee (lees4@ornl.gov)    [ Data Loading ]  Loading data from :data/iris.csv  Columns to ignore :None  Input columns :['SepalLength' 'SepalWidth' 'PetalLength' 'PetalWidth']  Target column :Name  Using default scikit-learn hyperparameters   Overriding parameters from command-line arguments ..   The following parameters will be used:    [ Model Evaluation ] * (RF)  accuracy =    0.947 5-fold cross validation   Saving tuned hyperparameters to file:  data/iris.csv,Model=RF,Scaler=StandardScaler.tuned.prop   [ Model Save ] * Training initiated .. * Training done. * Trained model saved to file: ./output/iris,Model=RF,accuracy=0.9466666666666667,Scaler=StandardScaler.pkl  When training is done. ASCENDS shows an expected accuracy that is calculated via 5-fold cross validation. The result says that expected accuracy is 94.7% (not bad!). The trained model is saved as a file (~.pkl) So, now let's see how the trained model file can be used for classification with unknown (data that has not been used for training) input data. (Note: the accuracy can slightly differ because the tool randomly shuffle the data set when creating folds and we used a 'Random' Forest model.) Copy the following text into a new file and save it to data\\iris_test_input.csv. SepalLength,SepalWidth,PetalLength,PetalWidth 7.2,2.5,4.1,1.3 5.2,5.5,4.1,1.3  As we can see above, we don't know what class each line belongs. Let's run the following: classify_with_model.py output/iris\\,Model\\=RF\\,accuracy\\=0.9466666666666667\\,Scaler\\=StandardScaler.pkl data/iris_test_input.csv output/iris_test_prediction.csv --mapping \"{'Name': {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica':2}}\"  Executing the above command will predict category for the input data data\\iris_test_input.csv and result will be saved in output/iris_test_prediction.csv. Note that we specified the trained model file we achieved ealier via train_classifier.py command. When you open up the generated output file output/iris_test_prediction.csv, ,SepalLength,SepalWidth,PetalLength,PetalWidth,Name 0,7.2,2.5,4.1,1.3,Iris-versicolor 1,5.2,5.5,4.1,1.3,Iris-versicolor  We can see that the model thanks that both are Iris-versicolor. Getting Started: Regression Let's have some fun with regression. We are going to use Boston Housing Data. Download the file from following link and save in data/BostonHousing.csv. Link: https://raw.githubusercontent.com/selva86/datasets/master/BostonHousing.csv The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository  crim: per capita crime rate by town zn: proportion of residential land zoned for lots over 25,000 sq.ft. indus: proportion of non-retail business acres per town chas: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) nox: nitric oxides concentration (parts per 10 million) rm: average number of rooms per dwelling age: proportion of owner-occupied units built prior to 1940 dis: weighted distances to \ufb01ve Boston employment centers rad: index of accessibility to radial highways tax: full-value property-tax rate per $10,000 ptratio: pupil-teacher ratio by town b: 1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town lstat: % lower status of the population medv: Median value of owner-occupied homes in $1000s  The following shows how to train a regression model using ASCENDS to predict medv. $ train_regression.py data/BostonHousing.csv output/BostonHousing medv Using TensorFlow backend.   * ASCENDS: Advanced data SCiENce toolkit for Non-Data Scientists   * Regression (value prediction) ML model trainer    programmed by Matt Sangkeun Lee (lees4@ornl.gov)    [ Data Loading ]  Loading data from :data/BostonHousing.csv  Columns to ignore :None  Input columns :['crim' 'zn' 'indus' 'chas' 'nox' 'rm' 'age' 'dis' 'rad' 'tax' 'ptratio'  'b' 'lstat']  Target column :medv  Using default scikit-learn hyperparameters   Overriding parameters from command-line arguments ..   The following parameters will be used:  {'scaler_option': 'StandardScaler', 'rf_n_estimators': '100', 'rf_max_features': 'auto', 'rf_max_depth': 'None', 'rf_min_samples_split': '2', 'rf_min_samples_leaf': '1', 'rf_bootstrap': 'True', 'rf_criterion': 'mse', 'rf_min_weight_fraction_leaf': '0.', 'rf_max_leaf_nodes': 'None', 'rf_min_impurity_decrease': '0.', 'nn_n_neighbors': '5', 'nn_weights': 'uniform', 'nn_algorithm': 'auto', 'nn_leaf_size': '30', 'nn_p': '2', 'nn_metric': 'minkowski', 'nn_metric_params': 'None', 'kr_alpha': '1', 'kr_kernel': 'linear', 'kr_gamma': 'None', 'kr_degree': '3', 'kr_coef0': '1', 'br_n_iter': '300', 'br_alpha_1': '1.2e-6', 'br_alpha_2': '1.e-6', 'br_tol': '1.e-3', 'br_lambda_1': '1.e-6', 'br_lambda_2': '1.e-6', 'br_compute_score': 'False', 'br_fit_intercept': 'True', 'br_normalize': 'False', 'svm_kernel': 'rbf', 'svm_degree': '3', 'svm_coef0': '0.0', 'svm_tol': '1e-3', 'svm_c': '1.0', 'svm_epsilon': '0.1', 'svm_shrinking': 'True', 'svm_gamma': 'auto', 'net_structure': '16 16 16', 'net_layer_n': '3', 'net_dropout': '0.0', 'net_l_2': '0.01', 'net_learning_rate': '0.01', 'net_epochs': '100', 'net_batch_size': '2'}   [ Model Evaluation ]  Saving test charts to :  output/BostonHousing,Model=RF,MAE=2.271302387431676,R2=0.8571003609225923,Scaler=StandardScaler.png * (RF)  MAE =    2.271, R2 =    0.857 via 5-fold cross validation   Saving tuned hyperparameters to file:  data/BostonHousing.csv,Model=RF,Scaler=StandardScaler.tuned.prop   [ Model Save ] * Training initiated .. * Training done. * Trained model saved to file: ./output/BostonHousing,Model=RF,MAE=2.271302387431676,R2=0.8571003609225923,Scaler=StandardScaler.pkl  Expected MAE (Mean Absolute Error) is 2.271 and R2 (https://bit.ly/2pP83Eb) is 0.857. Copy the following text into a new file and save it to data/BostonHousing_test_input.csv. crim,zn,indus,chas,nox,rm,age,dis,rad,tax,ptratio,b,lstat 0.00532,15,1.31,\"0\",0.538,5.575,62.1,4.09,1,296,15.3,396.9,4.98 0.02231,0,7.07,\"0\",0.469,5.421,78.9,3.9671,2,242,14.8,396.9,8.14  Similar to classification example, let's run the following: predict_with_model.py ./output/BostonHousing,Model=RF,MAE=2.271302387431676,R2=0.8571003609225923,Scaler=StandardScaler.pkl data/BostonHousing_test_input.csv output/BostonHousing_test_prediction.csv  (Note: the accuracy can slightly differ because the tool randomly shuffle the data set when creating folds and we used a 'Random' Forest model.) Executing the above command will predict category for the input data data/BostonHousing_test_input.csv and result will be saved in output/BostonHousing_test_prediction.csv. Note that we specified the trained model file we achieved ealier via train_regression.py command. Using Web-based GUI: Regression The current version of ascends-toolkit provides a web-based GUI (graphic user interface) for regression tasks. To start the ascends-server, you need to download the source code from the repository. With ascends Anaconda environment activated, go to the ascends-toolkit repository. Then, run the command as follows. (ascends-test) slzmbpro:ascends-toolkit slz$ python ascends_server.py Using TensorFlow backend. (ascends) $ python ascends_server.py   * ASCENDS: Advanced data SCiENce toolkit for Non-Data Scientists   * Web Server ver 0.1    programmed by Matt Sangkeun Lee (lees4@ornl.gov)   please go to : http://localhost:7777/  Open a web-browser window and go to url: http://localhost:7777 then you will see the following:  You can open a standard csv file by clicking File from the top-down menu and selecting a file.  After a file was loaded, the 'All Columns' module allows you to choose what input and output variables for your model. You can perform correlation analysis between the selected input and outut collumns using the 'Correlation Analysis' module. Once you are ready to proceed, click 'Go to ML' button to move on the machine learning test module.  Within the ML tab, you can test various ML algorithms such as random forest, neural network, nearest neighbors, etc, and compare their accuracy. You can use either use a default hyper parameter setting or load a custom hyperparameter file for further optimization. For later prediction, you can save your trained model and it can be used later either with command-line interface or web-based GUI.  Prediction tab allows you to load a file (which must have the input columns that you used for training the model you selected) and predict target column values. You can export the prediction results in various formats including csv. API Reference Ascends-toolkit is intended to be used via command-line interface or web-based interface; however, if needed, users may still be able to use ascends-toolkit's APIs. The following shows an example of performing a regression task using the core ascends-toolkit APIs. Please see API reference for more details. License MIT License Please contact us at lees4@ornl.gov, shind@ornl.gov for more details ", "has_readme": true, "readme_language": "English", "repo_tags": [], "has_h5": false, "h5_files_links": [], "see_also_links": ["http://localhost:7777"], "reference_list": []}]